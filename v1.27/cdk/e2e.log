  I0819 12:00:27.936354      18 e2e.go:117] Starting e2e run "6912050d-e89f-476f-82af-26dee370257f" on Ginkgo node 1
  Aug 19 12:00:27.972: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692446427 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug 19 12:00:28.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:00:28.186: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug 19 12:00:28.218: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug 19 12:00:28.223: INFO: e2e test version: v1.27.4
  Aug 19 12:00:28.224: INFO: kube-apiserver version: v1.27.4
  Aug 19 12:00:28.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:00:28.229: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.045 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/19/23 12:00:28.568
  Aug 19 12:00:28.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-runtime @ 08/19/23 12:00:28.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:00:28.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:00:28.592
  STEP: create the container @ 08/19/23 12:00:28.595
  W0819 12:00:28.604580      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/19/23 12:00:28.604
  STEP: get the container status @ 08/19/23 12:00:34.641
  STEP: the container should be terminated @ 08/19/23 12:00:34.645
  STEP: the termination message should be set @ 08/19/23 12:00:34.645
  Aug 19 12:00:34.645: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/19/23 12:00:34.645
  Aug 19 12:00:34.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6337" for this suite. @ 08/19/23 12:00:34.668
• [6.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/19/23 12:00:34.679
  Aug 19 12:00:34.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:00:34.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:00:34.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:00:34.702
  STEP: Creating pod liveness-f7796ae3-615a-451b-b571-c0110857caa2 in namespace container-probe-2539 @ 08/19/23 12:00:34.705
  Aug 19 12:00:38.728: INFO: Started pod liveness-f7796ae3-615a-451b-b571-c0110857caa2 in namespace container-probe-2539
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:00:38.728
  Aug 19 12:00:38.733: INFO: Initial restart count of pod liveness-f7796ae3-615a-451b-b571-c0110857caa2 is 0
  Aug 19 12:00:56.785: INFO: Restart count of pod container-probe-2539/liveness-f7796ae3-615a-451b-b571-c0110857caa2 is now 1 (18.051436982s elapsed)
  Aug 19 12:01:16.833: INFO: Restart count of pod container-probe-2539/liveness-f7796ae3-615a-451b-b571-c0110857caa2 is now 2 (38.099816193s elapsed)
  Aug 19 12:01:36.888: INFO: Restart count of pod container-probe-2539/liveness-f7796ae3-615a-451b-b571-c0110857caa2 is now 3 (58.155197429s elapsed)
  Aug 19 12:01:56.940: INFO: Restart count of pod container-probe-2539/liveness-f7796ae3-615a-451b-b571-c0110857caa2 is now 4 (1m18.207029489s elapsed)
  Aug 19 12:03:09.124: INFO: Restart count of pod container-probe-2539/liveness-f7796ae3-615a-451b-b571-c0110857caa2 is now 5 (2m30.390878514s elapsed)
  Aug 19 12:03:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:03:09.129
  STEP: Destroying namespace "container-probe-2539" for this suite. @ 08/19/23 12:03:09.151
• [154.486 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/19/23 12:03:09.167
  Aug 19 12:03:09.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 12:03:09.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:03:09.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:03:09.204
  STEP: creating the pod @ 08/19/23 12:03:09.211
  STEP: waiting for pod running @ 08/19/23 12:03:09.225
  STEP: creating a file in subpath @ 08/19/23 12:03:11.237
  Aug 19 12:03:11.242: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7773 PodName:var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:03:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:03:11.242: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:03:11.242: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-7773/pods/var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/19/23 12:03:11.33
  Aug 19 12:03:11.335: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7773 PodName:var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:03:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:03:11.335: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:03:11.335: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-7773/pods/var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 08/19/23 12:03:11.425
  Aug 19 12:03:11.945: INFO: Successfully updated pod "var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f"
  STEP: waiting for annotated pod running @ 08/19/23 12:03:11.945
  STEP: deleting the pod gracefully @ 08/19/23 12:03:11.949
  Aug 19 12:03:11.949: INFO: Deleting pod "var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f" in namespace "var-expansion-7773"
  Aug 19 12:03:11.960: INFO: Wait up to 5m0s for pod "var-expansion-4c33dc24-38e2-476f-affc-fc78c03f4e6f" to be fully deleted
  Aug 19 12:03:44.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7773" for this suite. @ 08/19/23 12:03:44.047
• [34.888 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/19/23 12:03:44.055
  Aug 19 12:03:44.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 12:03:44.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:03:44.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:03:44.077
  Aug 19 12:03:44.094: INFO: created pod
  STEP: Saw pod success @ 08/19/23 12:03:48.107
  Aug 19 12:04:18.108: INFO: polling logs
  Aug 19 12:04:18.130: INFO: Pod logs: 
  I0819 12:03:44.854044       1 log.go:198] OK: Got token
  I0819 12:03:44.854181       1 log.go:198] validating with in-cluster discovery
  I0819 12:03:44.854478       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0819 12:03:44.854507       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3701:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692447224, NotBefore:1692446624, IssuedAt:1692446624, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3701", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ea7ffbd4-cfe3-44e2-a567-598f780c3d3d"}}}
  I0819 12:03:44.865031       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0819 12:03:44.871555       1 log.go:198] OK: Validated signature on JWT
  I0819 12:03:44.871683       1 log.go:198] OK: Got valid claims from token!
  I0819 12:03:44.871716       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3701:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692447224, NotBefore:1692446624, IssuedAt:1692446624, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3701", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ea7ffbd4-cfe3-44e2-a567-598f780c3d3d"}}}

  Aug 19 12:04:18.131: INFO: completed pod
  Aug 19 12:04:18.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3701" for this suite. @ 08/19/23 12:04:18.142
• [34.094 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/19/23 12:04:18.15
  Aug 19 12:04:18.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:04:18.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:18.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:18.17
  STEP: Setting up server cert @ 08/19/23 12:04:18.197
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:04:18.537
  STEP: Deploying the webhook pod @ 08/19/23 12:04:18.546
  STEP: Wait for the deployment to be ready @ 08/19/23 12:04:18.557
  Aug 19 12:04:18.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:04:20.579
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:04:20.591
  Aug 19 12:04:21.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/19/23 12:04:21.666
  STEP: Creating a configMap that should be mutated @ 08/19/23 12:04:21.682
  STEP: Deleting the collection of validation webhooks @ 08/19/23 12:04:21.714
  STEP: Creating a configMap that should not be mutated @ 08/19/23 12:04:21.77
  Aug 19 12:04:21.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9755" for this suite. @ 08/19/23 12:04:21.83
  STEP: Destroying namespace "webhook-markers-2585" for this suite. @ 08/19/23 12:04:21.839
• [3.697 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/19/23 12:04:21.847
  Aug 19 12:04:21.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-pred @ 08/19/23 12:04:21.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:21.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:21.866
  Aug 19 12:04:21.870: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 19 12:04:21.879: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 12:04:21.884: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-214 before test
  Aug 19 12:04:21.890: INFO: nginx-ingress-controller-kubernetes-worker-5crrj from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.890: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: kube-state-metrics-5b95b4459c-kbc75 from kube-system started at 2023-08-19 11:54:21 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.890: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: metrics-server-v0.5.2-6cf8c8b69c-wcvjk from kube-system started at 2023-08-19 11:54:21 +0000 UTC (2 container statuses recorded)
  Aug 19 12:04:21.890: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:04:21.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:04:21.890: INFO: oidc-discovery-validator from svcaccounts-3701 started at 2023-08-19 12:03:44 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.890: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
  Aug 19 12:04:21.890: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-145 before test
  Aug 19 12:04:21.895: INFO: nginx-ingress-controller-kubernetes-worker-xnrcp from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.895: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:04:21.895: INFO: coredns-5c7f76ccb8-trw9q from kube-system started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.895: INFO: 	Container coredns ready: true, restart count 0
  Aug 19 12:04:21.895: INFO: dashboard-metrics-scraper-6b8586b5c9-lgkqw from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.895: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 19 12:04:21.896: INFO: kubernetes-dashboard-6869f4cd5f-gfcl6 from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.896: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 19 12:04:21.896: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-52lf7 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:04:21.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:04:21.896: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:04:21.896: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-69-13 before test
  Aug 19 12:04:21.902: INFO: default-http-backend-kubernetes-worker-65fc475d49-vjk9h from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: nginx-ingress-controller-kubernetes-worker-wb2rk from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: calico-kube-controllers-85f9fb94df-xpnfw from kube-system started at 2023-08-19 11:54:32 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: sonobuoy from sonobuoy started at 2023-08-19 12:00:13 +0000 UTC (1 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: sonobuoy-e2e-job-1e976644cf094697 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container e2e ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-vmdc9 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:04:21.902: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:04:21.902: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/19/23 12:04:21.903
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/19/23 12:04:23.922
  STEP: Trying to apply a random label on the found node. @ 08/19/23 12:04:23.937
  STEP: verifying the node has the label kubernetes.io/e2e-37a82a04-9633-4335-bea3-728969df8c4f 42 @ 08/19/23 12:04:23.946
  STEP: Trying to relaunch the pod, now with labels. @ 08/19/23 12:04:23.951
  STEP: removing the label kubernetes.io/e2e-37a82a04-9633-4335-bea3-728969df8c4f off the node ip-172-31-15-214 @ 08/19/23 12:04:25.969
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-37a82a04-9633-4335-bea3-728969df8c4f @ 08/19/23 12:04:25.98
  Aug 19 12:04:25.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8328" for this suite. @ 08/19/23 12:04:25.991
• [4.151 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/19/23 12:04:25.998
  Aug 19 12:04:25.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:04:25.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:26.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:26.018
  Aug 19 12:04:26.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/19/23 12:04:27.496
  Aug 19 12:04:27.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2887 --namespace=crd-publish-openapi-2887 create -f -'
  Aug 19 12:04:29.974: INFO: stderr: ""
  Aug 19 12:04:29.974: INFO: stdout: "e2e-test-crd-publish-openapi-1386-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 19 12:04:29.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2887 --namespace=crd-publish-openapi-2887 delete e2e-test-crd-publish-openapi-1386-crds test-cr'
  Aug 19 12:04:30.044: INFO: stderr: ""
  Aug 19 12:04:30.044: INFO: stdout: "e2e-test-crd-publish-openapi-1386-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug 19 12:04:30.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2887 --namespace=crd-publish-openapi-2887 apply -f -'
  Aug 19 12:04:30.510: INFO: stderr: ""
  Aug 19 12:04:30.510: INFO: stdout: "e2e-test-crd-publish-openapi-1386-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 19 12:04:30.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2887 --namespace=crd-publish-openapi-2887 delete e2e-test-crd-publish-openapi-1386-crds test-cr'
  Aug 19 12:04:30.576: INFO: stderr: ""
  Aug 19 12:04:30.576: INFO: stdout: "e2e-test-crd-publish-openapi-1386-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/19/23 12:04:30.576
  Aug 19 12:04:30.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2887 explain e2e-test-crd-publish-openapi-1386-crds'
  Aug 19 12:04:30.762: INFO: stderr: ""
  Aug 19 12:04:30.762: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1386-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Aug 19 12:04:32.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2887" for this suite. @ 08/19/23 12:04:32.078
• [6.087 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/19/23 12:04:32.085
  Aug 19 12:04:32.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:04:32.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:32.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:32.109
  STEP: Starting the proxy @ 08/19/23 12:04:32.113
  Aug 19 12:04:32.113: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1847 proxy --unix-socket=/tmp/kubectl-proxy-unix2970373106/test'
  STEP: retrieving proxy /api/ output @ 08/19/23 12:04:32.164
  Aug 19 12:04:32.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1847" for this suite. @ 08/19/23 12:04:32.169
• [0.091 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/19/23 12:04:32.177
  Aug 19 12:04:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename events @ 08/19/23 12:04:32.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:32.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:32.197
  STEP: Create set of events @ 08/19/23 12:04:32.201
  STEP: get a list of Events with a label in the current namespace @ 08/19/23 12:04:32.22
  STEP: delete a list of events @ 08/19/23 12:04:32.224
  Aug 19 12:04:32.224: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/19/23 12:04:32.248
  Aug 19 12:04:32.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6739" for this suite. @ 08/19/23 12:04:32.258
• [0.090 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/19/23 12:04:32.267
  Aug 19 12:04:32.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:04:32.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:32.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:32.289
  STEP: creating a ConfigMap @ 08/19/23 12:04:32.293
  STEP: fetching the ConfigMap @ 08/19/23 12:04:32.298
  STEP: patching the ConfigMap @ 08/19/23 12:04:32.304
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/19/23 12:04:32.311
  STEP: deleting the ConfigMap by collection with a label selector @ 08/19/23 12:04:32.316
  STEP: listing all ConfigMaps in test namespace @ 08/19/23 12:04:32.325
  Aug 19 12:04:32.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4226" for this suite. @ 08/19/23 12:04:32.332
• [0.074 seconds]
------------------------------
S
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/19/23 12:04:32.342
  Aug 19 12:04:32.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:04:32.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:32.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:32.364
  STEP: Creating configMap configmap-8738/configmap-test-da122a55-7fb4-4268-820e-a56318985ccb @ 08/19/23 12:04:32.371
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:04:32.377
  STEP: Saw pod success @ 08/19/23 12:04:36.399
  Aug 19 12:04:36.403: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-72ecb6ba-ff28-4c2b-9d55-3ebed2d9d473 container env-test: <nil>
  STEP: delete the pod @ 08/19/23 12:04:36.412
  Aug 19 12:04:36.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8738" for this suite. @ 08/19/23 12:04:36.432
• [4.098 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/19/23 12:04:36.441
  Aug 19 12:04:36.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 12:04:36.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:36.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:36.461
  STEP: Creating a pod to test substitution in volume subpath @ 08/19/23 12:04:36.465
  STEP: Saw pod success @ 08/19/23 12:04:40.488
  Aug 19 12:04:40.492: INFO: Trying to get logs from node ip-172-31-15-214 pod var-expansion-1db68dae-7852-42b2-8f96-65eabcf2fb8e container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 12:04:40.5
  Aug 19 12:04:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8226" for this suite. @ 08/19/23 12:04:40.523
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/19/23 12:04:40.532
  Aug 19 12:04:40.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subpath @ 08/19/23 12:04:40.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:04:40.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:04:40.553
  STEP: Setting up data @ 08/19/23 12:04:40.556
  STEP: Creating pod pod-subpath-test-projected-xmdq @ 08/19/23 12:04:40.567
  STEP: Creating a pod to test atomic-volume-subpath @ 08/19/23 12:04:40.567
  STEP: Saw pod success @ 08/19/23 12:05:04.642
  Aug 19 12:05:04.646: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-subpath-test-projected-xmdq container test-container-subpath-projected-xmdq: <nil>
  STEP: delete the pod @ 08/19/23 12:05:04.655
  STEP: Deleting pod pod-subpath-test-projected-xmdq @ 08/19/23 12:05:04.67
  Aug 19 12:05:04.670: INFO: Deleting pod "pod-subpath-test-projected-xmdq" in namespace "subpath-4402"
  Aug 19 12:05:04.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4402" for this suite. @ 08/19/23 12:05:04.678
• [24.153 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/19/23 12:05:04.686
  Aug 19 12:05:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:05:04.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:05:04.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:05:04.708
  STEP: Creating pod busybox-c9c5e714-0c8a-4153-b5e6-ed4156d4c937 in namespace container-probe-2256 @ 08/19/23 12:05:04.712
  Aug 19 12:05:06.732: INFO: Started pod busybox-c9c5e714-0c8a-4153-b5e6-ed4156d4c937 in namespace container-probe-2256
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:05:06.732
  Aug 19 12:05:06.736: INFO: Initial restart count of pod busybox-c9c5e714-0c8a-4153-b5e6-ed4156d4c937 is 0
  Aug 19 12:09:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:09:07.346
  STEP: Destroying namespace "container-probe-2256" for this suite. @ 08/19/23 12:09:07.363
• [242.687 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/19/23 12:09:07.375
  Aug 19 12:09:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:09:07.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:07.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:09:07.401
  STEP: Creating a test namespace @ 08/19/23 12:09:07.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:07.426
  STEP: Creating a pod in the namespace @ 08/19/23 12:09:07.429
  STEP: Waiting for the pod to have running status @ 08/19/23 12:09:07.439
  STEP: Deleting the namespace @ 08/19/23 12:09:09.451
  STEP: Waiting for the namespace to be removed. @ 08/19/23 12:09:09.459
  STEP: Recreating the namespace @ 08/19/23 12:09:20.464
  STEP: Verifying there are no pods in the namespace @ 08/19/23 12:09:20.481
  Aug 19 12:09:20.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7479" for this suite. @ 08/19/23 12:09:20.49
  STEP: Destroying namespace "nsdeletetest-4862" for this suite. @ 08/19/23 12:09:20.499
  Aug 19 12:09:20.503: INFO: Namespace nsdeletetest-4862 was already deleted
  STEP: Destroying namespace "nsdeletetest-8551" for this suite. @ 08/19/23 12:09:20.503
• [13.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/19/23 12:09:20.514
  Aug 19 12:09:20.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 12:09:20.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:20.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:09:20.537
  STEP: apply creating a deployment @ 08/19/23 12:09:20.541
  Aug 19 12:09:20.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7868" for this suite. @ 08/19/23 12:09:20.561
• [0.055 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/19/23 12:09:20.57
  Aug 19 12:09:20.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:09:20.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:20.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:09:20.59
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2230 @ 08/19/23 12:09:20.595
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/19/23 12:09:20.607
  STEP: creating service externalsvc in namespace services-2230 @ 08/19/23 12:09:20.607
  STEP: creating replication controller externalsvc in namespace services-2230 @ 08/19/23 12:09:20.625
  I0819 12:09:20.634768      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2230, replica count: 2
  I0819 12:09:23.686315      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0819 12:09:26.686461      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/19/23 12:09:26.69
  Aug 19 12:09:26.707: INFO: Creating new exec pod
  Aug 19 12:09:28.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2230 exec execpodn87t9 -- /bin/sh -x -c nslookup clusterip-service.services-2230.svc.cluster.local'
  Aug 19 12:09:28.905: INFO: stderr: "+ nslookup clusterip-service.services-2230.svc.cluster.local\n"
  Aug 19 12:09:28.905: INFO: stdout: "Server:\t\t10.152.183.120\nAddress:\t10.152.183.120#53\n\nclusterip-service.services-2230.svc.cluster.local\tcanonical name = externalsvc.services-2230.svc.cluster.local.\nName:\texternalsvc.services-2230.svc.cluster.local\nAddress: 10.152.183.215\n\n"
  Aug 19 12:09:28.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2230, will wait for the garbage collector to delete the pods @ 08/19/23 12:09:28.909
  Aug 19 12:09:28.973: INFO: Deleting ReplicationController externalsvc took: 9.758569ms
  Aug 19 12:09:29.073: INFO: Terminating ReplicationController externalsvc pods took: 100.668513ms
  Aug 19 12:09:33.895: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-2230" for this suite. @ 08/19/23 12:09:33.915
• [13.353 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/19/23 12:09:33.923
  Aug 19 12:09:33.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:09:33.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:33.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:09:33.946
  STEP: set up a multi version CRD @ 08/19/23 12:09:33.949
  Aug 19 12:09:33.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: rename a version @ 08/19/23 12:09:37.446
  STEP: check the new version name is served @ 08/19/23 12:09:37.467
  STEP: check the old version name is removed @ 08/19/23 12:09:38.685
  STEP: check the other version is not changed @ 08/19/23 12:09:39.371
  Aug 19 12:09:41.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4259" for this suite. @ 08/19/23 12:09:41.989
• [8.074 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/19/23 12:09:41.998
  Aug 19 12:09:41.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:09:41.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:09:42.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:09:42.018
  STEP: Creating pod busybox-4516d0db-ace8-4804-9b36-94e615fb9e7c in namespace container-probe-8154 @ 08/19/23 12:09:42.021
  Aug 19 12:09:44.038: INFO: Started pod busybox-4516d0db-ace8-4804-9b36-94e615fb9e7c in namespace container-probe-8154
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:09:44.038
  Aug 19 12:09:44.042: INFO: Initial restart count of pod busybox-4516d0db-ace8-4804-9b36-94e615fb9e7c is 0
  Aug 19 12:10:34.168: INFO: Restart count of pod container-probe-8154/busybox-4516d0db-ace8-4804-9b36-94e615fb9e7c is now 1 (50.12637636s elapsed)
  Aug 19 12:10:34.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:10:34.173
  STEP: Destroying namespace "container-probe-8154" for this suite. @ 08/19/23 12:10:34.189
• [52.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/19/23 12:10:34.199
  Aug 19 12:10:34.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:10:34.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:10:34.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:10:34.22
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/19/23 12:10:34.224
  STEP: Saw pod success @ 08/19/23 12:10:38.249
  Aug 19 12:10:38.253: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-8d1047eb-2e44-437a-b410-9625e53baa4c container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:10:38.272
  Aug 19 12:10:38.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7402" for this suite. @ 08/19/23 12:10:38.297
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/19/23 12:10:38.305
  Aug 19 12:10:38.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:10:38.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:10:38.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:10:38.325
  STEP: Setting up server cert @ 08/19/23 12:10:38.35
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:10:38.75
  STEP: Deploying the webhook pod @ 08/19/23 12:10:38.76
  STEP: Wait for the deployment to be ready @ 08/19/23 12:10:38.773
  Aug 19 12:10:38.781: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Aug 19 12:10:40.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 10, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 10, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 10, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 08/19/23 12:10:42.801
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:10:42.812
  Aug 19 12:10:43.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/19/23 12:10:43.817
  STEP: create a namespace for the webhook @ 08/19/23 12:10:43.834
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/19/23 12:10:43.848
  Aug 19 12:10:43.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3499" for this suite. @ 08/19/23 12:10:43.967
  STEP: Destroying namespace "webhook-markers-7048" for this suite. @ 08/19/23 12:10:43.975
  STEP: Destroying namespace "fail-closed-namespace-3659" for this suite. @ 08/19/23 12:10:43.983
• [5.686 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/19/23 12:10:43.996
  Aug 19 12:10:43.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:10:43.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:10:44.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:10:44.022
  STEP: Creating projection with secret that has name projected-secret-test-1f50a0ea-4030-4aea-b7e7-aa2e80cade1e @ 08/19/23 12:10:44.025
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:10:44.031
  STEP: Saw pod success @ 08/19/23 12:10:48.056
  Aug 19 12:10:48.061: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-12f06055-f68b-4893-80c5-25b69ccb739b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:10:48.069
  Aug 19 12:10:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2010" for this suite. @ 08/19/23 12:10:48.089
• [4.100 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/19/23 12:10:48.096
  Aug 19 12:10:48.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:10:48.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:10:48.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:10:48.117
  STEP: Creating pod liveness-5cf6b49a-5519-4325-b709-b8f7056c22aa in namespace container-probe-3413 @ 08/19/23 12:10:48.12
  Aug 19 12:10:50.139: INFO: Started pod liveness-5cf6b49a-5519-4325-b709-b8f7056c22aa in namespace container-probe-3413
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:10:50.139
  Aug 19 12:10:50.143: INFO: Initial restart count of pod liveness-5cf6b49a-5519-4325-b709-b8f7056c22aa is 0
  Aug 19 12:14:50.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:14:50.751
  STEP: Destroying namespace "container-probe-3413" for this suite. @ 08/19/23 12:14:50.766
• [242.678 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/19/23 12:14:50.775
  Aug 19 12:14:50.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 12:14:50.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:14:50.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:14:50.793
  STEP: creating a Deployment @ 08/19/23 12:14:50.801
  Aug 19 12:14:50.801: INFO: Creating simple deployment test-deployment-cbs4c
  Aug 19 12:14:50.817: INFO: deployment "test-deployment-cbs4c" doesn't have the required revision set
  Aug 19 12:14:52.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-cbs4c-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Getting /status @ 08/19/23 12:14:54.838
  Aug 19 12:14:54.844: INFO: Deployment test-deployment-cbs4c has Conditions: [{Available True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cbs4c-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/19/23 12:14:54.844
  Aug 19 12:14:54.854: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 54, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 50, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cbs4c-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/19/23 12:14:54.854
  Aug 19 12:14:54.856: INFO: Observed &Deployment event: ADDED
  Aug 19 12:14:54.856: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cbs4c-5994cf9475"}
  Aug 19 12:14:54.856: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.856: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cbs4c-5994cf9475"}
  Aug 19 12:14:54.856: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 19 12:14:54.857: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cbs4c-5994cf9475" is progressing.}
  Aug 19 12:14:54.857: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cbs4c-5994cf9475" has successfully progressed.}
  Aug 19 12:14:54.857: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 19 12:14:54.857: INFO: Observed Deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cbs4c-5994cf9475" has successfully progressed.}
  Aug 19 12:14:54.857: INFO: Found Deployment test-deployment-cbs4c in namespace deployment-3330 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:14:54.857: INFO: Deployment test-deployment-cbs4c has an updated status
  STEP: patching the Statefulset Status @ 08/19/23 12:14:54.857
  Aug 19 12:14:54.857: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 19 12:14:54.866: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/19/23 12:14:54.866
  Aug 19 12:14:54.868: INFO: Observed &Deployment event: ADDED
  Aug 19 12:14:54.868: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cbs4c-5994cf9475"}
  Aug 19 12:14:54.868: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.868: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cbs4c-5994cf9475"}
  Aug 19 12:14:54.868: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 19 12:14:54.868: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.869: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 19 12:14:54.869: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:50 +0000 UTC 2023-08-19 12:14:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cbs4c-5994cf9475" is progressing.}
  Aug 19 12:14:54.869: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.869: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 19 12:14:54.869: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cbs4c-5994cf9475" has successfully progressed.}
  Aug 19 12:14:54.869: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.870: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 19 12:14:54.870: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-19 12:14:54 +0000 UTC 2023-08-19 12:14:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cbs4c-5994cf9475" has successfully progressed.}
  Aug 19 12:14:54.870: INFO: Observed deployment test-deployment-cbs4c in namespace deployment-3330 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:14:54.870: INFO: Observed &Deployment event: MODIFIED
  Aug 19 12:14:54.870: INFO: Found deployment test-deployment-cbs4c in namespace deployment-3330 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug 19 12:14:54.870: INFO: Deployment test-deployment-cbs4c has a patched status
  Aug 19 12:14:54.874: INFO: Deployment "test-deployment-cbs4c":
  &Deployment{ObjectMeta:{test-deployment-cbs4c  deployment-3330  79100c30-91d8-4b9e-98bf-55e535068a27 4583 1 2023-08-19 12:14:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-19 12:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-19 12:14:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-19 12:14:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003821cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cbs4c-5994cf9475",LastUpdateTime:2023-08-19 12:14:54 +0000 UTC,LastTransitionTime:2023-08-19 12:14:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 19 12:14:54.878: INFO: New ReplicaSet "test-deployment-cbs4c-5994cf9475" of Deployment "test-deployment-cbs4c":
  &ReplicaSet{ObjectMeta:{test-deployment-cbs4c-5994cf9475  deployment-3330  cc007d20-c1ea-404e-b89e-81a06dc052a8 4579 1 2023-08-19 12:14:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cbs4c 79100c30-91d8-4b9e-98bf-55e535068a27 0xc004cc6077 0xc004cc6078}] [] [{kube-controller-manager Update apps/v1 2023-08-19 12:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79100c30-91d8-4b9e-98bf-55e535068a27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:14:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc6128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 12:14:54.882: INFO: Pod "test-deployment-cbs4c-5994cf9475-5c7h7" is available:
  &Pod{ObjectMeta:{test-deployment-cbs4c-5994cf9475-5c7h7 test-deployment-cbs4c-5994cf9475- deployment-3330  f735562f-fa16-48e4-8598-2c3b09773424 4578 0 2023-08-19 12:14:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-cbs4c-5994cf9475 cc007d20-c1ea-404e-b89e-81a06dc052a8 0xc004cc64d7 0xc004cc64d8}] [] [{kube-controller-manager Update v1 2023-08-19 12:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc007d20-c1ea-404e-b89e-81a06dc052a8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:14:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfbjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfbjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:14:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:14:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:14:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:14:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.150,StartTime:2023-08-19 12:14:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:14:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://be8df7e9d6dbf59c02dff562e0e7cf2e8f3cc92cf4ee0fb02436e12be377e705,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.150,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:14:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3330" for this suite. @ 08/19/23 12:14:54.888
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 08/19/23 12:14:54.898
  Aug 19 12:14:54.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename aggregator @ 08/19/23 12:14:54.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:14:54.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:14:54.919
  Aug 19 12:14:54.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Registering the sample API server. @ 08/19/23 12:14:54.923
  Aug 19 12:14:55.362: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug 19 12:14:55.394: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Aug 19 12:14:57.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:14:59.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:01.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:03.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:05.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:07.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:09.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:11.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:13.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:15.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:17.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 14, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 19 12:15:19.578: INFO: Waited 115.218412ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/19/23 12:15:19.619
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/19/23 12:15:19.623
  STEP: List APIServices @ 08/19/23 12:15:19.63
  Aug 19 12:15:19.636: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/19/23 12:15:19.636
  Aug 19 12:15:19.648: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/19/23 12:15:19.648
  Aug 19 12:15:19.659: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 19, 12, 15, 19, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/19/23 12:15:19.659
  Aug 19 12:15:19.663: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-19 12:15:19 +0000 UTC Passed all checks passed}
  Aug 19 12:15:19.663: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:15:19.663: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/19/23 12:15:19.663
  Aug 19 12:15:19.673: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-609889561" @ 08/19/23 12:15:19.674
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/19/23 12:15:19.684
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/19/23 12:15:19.692
  STEP: Patch APIService Status @ 08/19/23 12:15:19.695
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/19/23 12:15:19.703
  Aug 19 12:15:19.707: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-19 12:15:19 +0000 UTC Passed all checks passed}
  Aug 19 12:15:19.707: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:15:19.707: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug 19 12:15:19.707: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/19/23 12:15:19.707
  STEP: Confirm that the generated APIService has been deleted @ 08/19/23 12:15:19.712
  Aug 19 12:15:19.712: INFO: Requesting list of APIServices to confirm quantity
  Aug 19 12:15:19.717: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug 19 12:15:19.717: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug 19 12:15:19.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-7194" for this suite. @ 08/19/23 12:15:19.857
• [24.966 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/19/23 12:15:19.865
  Aug 19 12:15:19.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:15:19.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:19.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:19.886
  STEP: validating cluster-info @ 08/19/23 12:15:19.89
  Aug 19 12:15:19.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-2791 cluster-info'
  Aug 19 12:15:19.953: INFO: stderr: ""
  Aug 19 12:15:19.953: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug 19 12:15:19.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2791" for this suite. @ 08/19/23 12:15:19.958
• [0.101 seconds]
------------------------------
S
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/19/23 12:15:19.966
  Aug 19 12:15:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename ingressclass @ 08/19/23 12:15:19.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:19.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:19.993
  STEP: getting /apis @ 08/19/23 12:15:20
  STEP: getting /apis/networking.k8s.io @ 08/19/23 12:15:20.004
  STEP: getting /apis/networking.k8s.iov1 @ 08/19/23 12:15:20.005
  STEP: creating @ 08/19/23 12:15:20.006
  STEP: getting @ 08/19/23 12:15:20.022
  STEP: listing @ 08/19/23 12:15:20.026
  STEP: watching @ 08/19/23 12:15:20.029
  Aug 19 12:15:20.029: INFO: starting watch
  STEP: patching @ 08/19/23 12:15:20.031
  STEP: updating @ 08/19/23 12:15:20.036
  Aug 19 12:15:20.041: INFO: waiting for watch events with expected annotations
  Aug 19 12:15:20.041: INFO: saw patched and updated annotations
  STEP: deleting @ 08/19/23 12:15:20.041
  STEP: deleting a collection @ 08/19/23 12:15:20.055
  Aug 19 12:15:20.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-1286" for this suite. @ 08/19/23 12:15:20.077
• [0.119 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/19/23 12:15:20.085
  Aug 19 12:15:20.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename prestop @ 08/19/23 12:15:20.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:20.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:20.102
  STEP: Creating server pod server in namespace prestop-4036 @ 08/19/23 12:15:20.106
  STEP: Waiting for pods to come up. @ 08/19/23 12:15:20.116
  STEP: Creating tester pod tester in namespace prestop-4036 @ 08/19/23 12:15:22.129
  STEP: Deleting pre-stop pod @ 08/19/23 12:15:24.147
  Aug 19 12:15:29.162: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug 19 12:15:29.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/19/23 12:15:29.167
  STEP: Destroying namespace "prestop-4036" for this suite. @ 08/19/23 12:15:29.184
• [9.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/19/23 12:15:29.195
  Aug 19 12:15:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename runtimeclass @ 08/19/23 12:15:29.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:29.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:29.223
  Aug 19 12:15:29.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5109" for this suite. @ 08/19/23 12:15:29.277
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/19/23 12:15:29.287
  Aug 19 12:15:29.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:15:29.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:29.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:29.31
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/19/23 12:15:29.314
  Aug 19 12:15:29.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-6320 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 19 12:15:29.381: INFO: stderr: ""
  Aug 19 12:15:29.381: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/19/23 12:15:29.381
  Aug 19 12:15:29.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-6320 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Aug 19 12:15:29.449: INFO: stderr: ""
  Aug 19 12:15:29.450: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/19/23 12:15:29.45
  Aug 19 12:15:29.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-6320 delete pods e2e-test-httpd-pod'
  Aug 19 12:15:39.478: INFO: stderr: ""
  Aug 19 12:15:39.478: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 19 12:15:39.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6320" for this suite. @ 08/19/23 12:15:39.483
• [10.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/19/23 12:15:39.49
  Aug 19 12:15:39.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context-test @ 08/19/23 12:15:39.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:39.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:39.513
  Aug 19 12:15:43.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1999" for this suite. @ 08/19/23 12:15:43.55
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/19/23 12:15:43.56
  Aug 19 12:15:43.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:15:43.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:15:43.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:15:43.579
  STEP: creating a replication controller @ 08/19/23 12:15:43.583
  Aug 19 12:15:43.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 create -f -'
  Aug 19 12:15:44.305: INFO: stderr: ""
  Aug 19 12:15:44.305: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/19/23 12:15:44.305
  Aug 19 12:15:44.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:15:44.371: INFO: stderr: ""
  Aug 19 12:15:44.371: INFO: stdout: "update-demo-nautilus-4zsj9 update-demo-nautilus-hf55n "
  Aug 19 12:15:44.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:44.432: INFO: stderr: ""
  Aug 19 12:15:44.432: INFO: stdout: ""
  Aug 19 12:15:44.432: INFO: update-demo-nautilus-4zsj9 is created but not running
  Aug 19 12:15:49.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:15:49.496: INFO: stderr: ""
  Aug 19 12:15:49.496: INFO: stdout: "update-demo-nautilus-4zsj9 update-demo-nautilus-hf55n "
  Aug 19 12:15:49.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:49.555: INFO: stderr: ""
  Aug 19 12:15:49.555: INFO: stdout: "true"
  Aug 19 12:15:49.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:15:49.616: INFO: stderr: ""
  Aug 19 12:15:49.616: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:15:49.616: INFO: validating pod update-demo-nautilus-4zsj9
  Aug 19 12:15:49.624: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:15:49.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:15:49.624: INFO: update-demo-nautilus-4zsj9 is verified up and running
  Aug 19 12:15:49.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-hf55n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:49.686: INFO: stderr: ""
  Aug 19 12:15:49.686: INFO: stdout: "true"
  Aug 19 12:15:49.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-hf55n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:15:49.745: INFO: stderr: ""
  Aug 19 12:15:49.745: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:15:49.745: INFO: validating pod update-demo-nautilus-hf55n
  Aug 19 12:15:49.750: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:15:49.750: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:15:49.750: INFO: update-demo-nautilus-hf55n is verified up and running
  STEP: scaling down the replication controller @ 08/19/23 12:15:49.75
  Aug 19 12:15:49.751: INFO: scanned /root for discovery docs: <nil>
  Aug 19 12:15:49.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Aug 19 12:15:50.834: INFO: stderr: ""
  Aug 19 12:15:50.834: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/19/23 12:15:50.834
  Aug 19 12:15:50.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:15:50.900: INFO: stderr: ""
  Aug 19 12:15:50.900: INFO: stdout: "update-demo-nautilus-4zsj9 update-demo-nautilus-hf55n "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 08/19/23 12:15:50.9
  Aug 19 12:15:55.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:15:55.964: INFO: stderr: ""
  Aug 19 12:15:55.964: INFO: stdout: "update-demo-nautilus-4zsj9 "
  Aug 19 12:15:55.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:56.025: INFO: stderr: ""
  Aug 19 12:15:56.025: INFO: stdout: "true"
  Aug 19 12:15:56.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:15:56.086: INFO: stderr: ""
  Aug 19 12:15:56.086: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:15:56.086: INFO: validating pod update-demo-nautilus-4zsj9
  Aug 19 12:15:56.091: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:15:56.091: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:15:56.091: INFO: update-demo-nautilus-4zsj9 is verified up and running
  STEP: scaling up the replication controller @ 08/19/23 12:15:56.091
  Aug 19 12:15:56.092: INFO: scanned /root for discovery docs: <nil>
  Aug 19 12:15:56.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Aug 19 12:15:57.170: INFO: stderr: ""
  Aug 19 12:15:57.170: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/19/23 12:15:57.17
  Aug 19 12:15:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:15:57.233: INFO: stderr: ""
  Aug 19 12:15:57.233: INFO: stdout: "update-demo-nautilus-4zsj9 update-demo-nautilus-7sbgk "
  Aug 19 12:15:57.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:57.293: INFO: stderr: ""
  Aug 19 12:15:57.293: INFO: stdout: "true"
  Aug 19 12:15:57.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:15:57.354: INFO: stderr: ""
  Aug 19 12:15:57.354: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:15:57.354: INFO: validating pod update-demo-nautilus-4zsj9
  Aug 19 12:15:57.360: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:15:57.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:15:57.360: INFO: update-demo-nautilus-4zsj9 is verified up and running
  Aug 19 12:15:57.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-7sbgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:15:57.436: INFO: stderr: ""
  Aug 19 12:15:57.436: INFO: stdout: ""
  Aug 19 12:15:57.436: INFO: update-demo-nautilus-7sbgk is created but not running
  Aug 19 12:16:02.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:16:02.505: INFO: stderr: ""
  Aug 19 12:16:02.505: INFO: stdout: "update-demo-nautilus-4zsj9 update-demo-nautilus-7sbgk "
  Aug 19 12:16:02.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:16:02.567: INFO: stderr: ""
  Aug 19 12:16:02.567: INFO: stdout: "true"
  Aug 19 12:16:02.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-4zsj9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:16:02.627: INFO: stderr: ""
  Aug 19 12:16:02.627: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:16:02.627: INFO: validating pod update-demo-nautilus-4zsj9
  Aug 19 12:16:02.632: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:16:02.632: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:16:02.632: INFO: update-demo-nautilus-4zsj9 is verified up and running
  Aug 19 12:16:02.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-7sbgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:16:02.693: INFO: stderr: ""
  Aug 19 12:16:02.693: INFO: stdout: "true"
  Aug 19 12:16:02.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods update-demo-nautilus-7sbgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:16:02.754: INFO: stderr: ""
  Aug 19 12:16:02.754: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:16:02.754: INFO: validating pod update-demo-nautilus-7sbgk
  Aug 19 12:16:02.759: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:16:02.759: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:16:02.759: INFO: update-demo-nautilus-7sbgk is verified up and running
  STEP: using delete to clean up resources @ 08/19/23 12:16:02.759
  Aug 19 12:16:02.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 delete --grace-period=0 --force -f -'
  Aug 19 12:16:02.821: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:16:02.821: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 19 12:16:02.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get rc,svc -l name=update-demo --no-headers'
  Aug 19 12:16:02.888: INFO: stderr: "No resources found in kubectl-3521 namespace.\n"
  Aug 19 12:16:02.888: INFO: stdout: ""
  Aug 19 12:16:02.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3521 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 19 12:16:02.950: INFO: stderr: ""
  Aug 19 12:16:02.950: INFO: stdout: ""
  Aug 19 12:16:02.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3521" for this suite. @ 08/19/23 12:16:02.954
• [19.402 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 08/19/23 12:16:02.963
  Aug 19 12:16:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:16:02.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:16:02.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:16:02.983
  STEP: Creating secret with name secret-test-map-6dfe11db-faed-4f6a-9404-67e66f79f3b8 @ 08/19/23 12:16:02.987
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:16:02.992
  STEP: Saw pod success @ 08/19/23 12:16:07.016
  Aug 19 12:16:07.020: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-e19d8e61-bff6-42f0-b2a6-826eaeee5c43 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:16:07.039
  Aug 19 12:16:07.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4425" for this suite. @ 08/19/23 12:16:07.065
• [4.111 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/19/23 12:16:07.074
  Aug 19 12:16:07.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:16:07.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:16:07.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:16:07.098
  STEP: creating pod @ 08/19/23 12:16:07.101
  Aug 19 12:16:09.128: INFO: Pod pod-hostip-f74d079b-ad31-40b6-a946-77a53bafff9d has hostIP: 172.31.15.214
  Aug 19 12:16:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5428" for this suite. @ 08/19/23 12:16:09.133
• [2.065 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/19/23 12:16:09.14
  Aug 19 12:16:09.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:16:09.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:16:09.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:16:09.16
  STEP: Creating service test in namespace statefulset-7754 @ 08/19/23 12:16:09.163
  STEP: Creating a new StatefulSet @ 08/19/23 12:16:09.168
  Aug 19 12:16:09.184: INFO: Found 0 stateful pods, waiting for 3
  Aug 19 12:16:19.189: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:16:19.189: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:16:19.189: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:16:19.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7754 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:16:19.343: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:16:19.343: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:16:19.343: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/19/23 12:16:29.36
  Aug 19 12:16:29.381: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/19/23 12:16:29.381
  STEP: Updating Pods in reverse ordinal order @ 08/19/23 12:16:39.398
  Aug 19 12:16:39.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7754 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:16:39.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:16:39.547: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:16:39.547: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 08/19/23 12:16:59.573
  Aug 19 12:16:59.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7754 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:16:59.719: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:16:59.719: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:16:59.719: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 12:17:09.756: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 08/19/23 12:17:19.775
  Aug 19 12:17:19.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7754 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:17:19.915: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:17:19.915: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:17:19.915: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 12:17:29.938: INFO: Deleting all statefulset in ns statefulset-7754
  Aug 19 12:17:29.942: INFO: Scaling statefulset ss2 to 0
  Aug 19 12:17:39.963: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:17:39.967: INFO: Deleting statefulset ss2
  Aug 19 12:17:39.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7754" for this suite. @ 08/19/23 12:17:39.988
• [90.855 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/19/23 12:17:39.997
  Aug 19 12:17:39.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 12:17:39.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:17:40.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:17:40.019
  STEP: apply creating a deployment @ 08/19/23 12:17:40.023
  Aug 19 12:17:40.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5667" for this suite. @ 08/19/23 12:17:40.042
• [0.053 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/19/23 12:17:40.05
  Aug 19 12:17:40.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:17:40.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:17:40.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:17:40.068
  STEP: Creating the pod @ 08/19/23 12:17:40.072
  Aug 19 12:17:42.624: INFO: Successfully updated pod "annotationupdate1087be48-31d4-4709-9d3f-911e6a29ecd1"
  Aug 19 12:17:44.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8411" for this suite. @ 08/19/23 12:17:44.648
• [4.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/19/23 12:17:44.657
  Aug 19 12:17:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:17:44.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:17:44.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:17:44.675
  STEP: Updating Namespace "namespaces-4802" @ 08/19/23 12:17:44.679
  Aug 19 12:17:44.689: INFO: Namespace "namespaces-4802" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"6912050d-e89f-476f-82af-26dee370257f", "kubernetes.io/metadata.name":"namespaces-4802", "namespaces-4802":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug 19 12:17:44.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4802" for this suite. @ 08/19/23 12:17:44.693
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/19/23 12:17:44.701
  Aug 19 12:17:44.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/19/23 12:17:44.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:17:44.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:17:44.728
  STEP: Setting up the test @ 08/19/23 12:17:44.731
  STEP: Creating hostNetwork=false pod @ 08/19/23 12:17:44.731
  STEP: Creating hostNetwork=true pod @ 08/19/23 12:17:46.756
  STEP: Running the test @ 08/19/23 12:17:48.779
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/19/23 12:17:48.779
  Aug 19 12:17:48.779: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:48.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:48.780: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:48.780: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 19 12:17:48.873: INFO: Exec stderr: ""
  Aug 19 12:17:48.874: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:48.874: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:48.874: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 19 12:17:48.957: INFO: Exec stderr: ""
  Aug 19 12:17:48.957: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:48.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:48.958: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:48.958: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 19 12:17:49.034: INFO: Exec stderr: ""
  Aug 19 12:17:49.034: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.035: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.035: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 19 12:17:49.113: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/19/23 12:17:49.114
  Aug 19 12:17:49.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.114: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.114: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 19 12:17:49.189: INFO: Exec stderr: ""
  Aug 19 12:17:49.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.190: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.190: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 19 12:17:49.261: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/19/23 12:17:49.261
  Aug 19 12:17:49.262: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.262: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.263: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 19 12:17:49.337: INFO: Exec stderr: ""
  Aug 19 12:17:49.337: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.338: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.338: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 19 12:17:49.406: INFO: Exec stderr: ""
  Aug 19 12:17:49.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.406: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.407: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 19 12:17:49.492: INFO: Exec stderr: ""
  Aug 19 12:17:49.492: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6039 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:17:49.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:17:49.493: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:17:49.493: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6039/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 19 12:17:49.565: INFO: Exec stderr: ""
  Aug 19 12:17:49.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-6039" for this suite. @ 08/19/23 12:17:49.57
• [4.877 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/19/23 12:17:49.579
  Aug 19 12:17:49.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename taint-single-pod @ 08/19/23 12:17:49.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:17:49.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:17:49.601
  Aug 19 12:17:49.604: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 19 12:18:49.619: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 12:18:49.623: INFO: Starting informer...
  STEP: Starting pod... @ 08/19/23 12:18:49.623
  Aug 19 12:18:49.839: INFO: Pod is running on ip-172-31-15-214. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/19/23 12:18:49.839
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/19/23 12:18:49.85
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/19/23 12:18:49.855
  Aug 19 12:18:49.855: INFO: Pod wasn't evicted. Proceeding
  Aug 19 12:18:49.855: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/19/23 12:18:49.87
  STEP: Waiting some time to make sure that toleration time passed. @ 08/19/23 12:18:49.89
  Aug 19 12:20:04.892: INFO: Pod wasn't evicted. Test successful
  Aug 19 12:20:04.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-5160" for this suite. @ 08/19/23 12:20:04.898
• [135.328 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/19/23 12:20:04.907
  Aug 19 12:20:04.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename containers @ 08/19/23 12:20:04.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:04.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:04.929
  STEP: Creating a pod to test override arguments @ 08/19/23 12:20:04.933
  STEP: Saw pod success @ 08/19/23 12:20:08.959
  Aug 19 12:20:08.963: INFO: Trying to get logs from node ip-172-31-15-214 pod client-containers-a998ab1e-c471-494e-9cc0-f0f88f6d3d00 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:20:08.978
  Aug 19 12:20:08.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7033" for this suite. @ 08/19/23 12:20:08.998
• [4.101 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/19/23 12:20:09.008
  Aug 19 12:20:09.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:20:09.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:09.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:09.027
  Aug 19 12:20:09.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/19/23 12:20:10.346
  Aug 19 12:20:10.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 create -f -'
  Aug 19 12:20:12.947: INFO: stderr: ""
  Aug 19 12:20:12.947: INFO: stdout: "e2e-test-crd-publish-openapi-9729-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 19 12:20:12.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 delete e2e-test-crd-publish-openapi-9729-crds test-foo'
  Aug 19 12:20:13.031: INFO: stderr: ""
  Aug 19 12:20:13.031: INFO: stdout: "e2e-test-crd-publish-openapi-9729-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug 19 12:20:13.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 apply -f -'
  Aug 19 12:20:13.571: INFO: stderr: ""
  Aug 19 12:20:13.572: INFO: stdout: "e2e-test-crd-publish-openapi-9729-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 19 12:20:13.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 delete e2e-test-crd-publish-openapi-9729-crds test-foo'
  Aug 19 12:20:13.640: INFO: stderr: ""
  Aug 19 12:20:13.640: INFO: stdout: "e2e-test-crd-publish-openapi-9729-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/19/23 12:20:13.64
  Aug 19 12:20:13.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 create -f -'
  Aug 19 12:20:13.831: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/19/23 12:20:13.831
  Aug 19 12:20:13.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 create -f -'
  Aug 19 12:20:14.023: INFO: rc: 1
  Aug 19 12:20:14.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 apply -f -'
  Aug 19 12:20:14.232: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/19/23 12:20:14.232
  Aug 19 12:20:14.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 create -f -'
  Aug 19 12:20:14.436: INFO: rc: 1
  Aug 19 12:20:14.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 --namespace=crd-publish-openapi-277 apply -f -'
  Aug 19 12:20:14.632: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/19/23 12:20:14.632
  Aug 19 12:20:14.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 explain e2e-test-crd-publish-openapi-9729-crds'
  Aug 19 12:20:14.832: INFO: stderr: ""
  Aug 19 12:20:14.832: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9729-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/19/23 12:20:14.833
  Aug 19 12:20:14.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 explain e2e-test-crd-publish-openapi-9729-crds.metadata'
  Aug 19 12:20:15.034: INFO: stderr: ""
  Aug 19 12:20:15.035: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9729-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug 19 12:20:15.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 explain e2e-test-crd-publish-openapi-9729-crds.spec'
  Aug 19 12:20:15.231: INFO: stderr: ""
  Aug 19 12:20:15.231: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9729-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug 19 12:20:15.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 explain e2e-test-crd-publish-openapi-9729-crds.spec.bars'
  Aug 19 12:20:15.441: INFO: stderr: ""
  Aug 19 12:20:15.441: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9729-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/19/23 12:20:15.442
  Aug 19 12:20:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-277 explain e2e-test-crd-publish-openapi-9729-crds.spec.bars2'
  Aug 19 12:20:15.629: INFO: rc: 1
  Aug 19 12:20:16.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-277" for this suite. @ 08/19/23 12:20:16.914
• [7.913 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/19/23 12:20:16.922
  Aug 19 12:20:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:20:16.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:16.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:16.944
  STEP: creating the pod @ 08/19/23 12:20:16.947
  Aug 19 12:20:16.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 create -f -'
  Aug 19 12:20:17.577: INFO: stderr: ""
  Aug 19 12:20:17.577: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/19/23 12:20:19.586
  Aug 19 12:20:19.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 label pods pause testing-label=testing-label-value'
  Aug 19 12:20:19.657: INFO: stderr: ""
  Aug 19 12:20:19.657: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/19/23 12:20:19.657
  Aug 19 12:20:19.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 get pod pause -L testing-label'
  Aug 19 12:20:19.721: INFO: stderr: ""
  Aug 19 12:20:19.721: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/19/23 12:20:19.721
  Aug 19 12:20:19.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 label pods pause testing-label-'
  Aug 19 12:20:19.792: INFO: stderr: ""
  Aug 19 12:20:19.792: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/19/23 12:20:19.792
  Aug 19 12:20:19.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 get pod pause -L testing-label'
  Aug 19 12:20:19.851: INFO: stderr: ""
  Aug 19 12:20:19.851: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 08/19/23 12:20:19.851
  Aug 19 12:20:19.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 delete --grace-period=0 --force -f -'
  Aug 19 12:20:19.924: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:20:19.924: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug 19 12:20:19.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 get rc,svc -l name=pause --no-headers'
  Aug 19 12:20:19.992: INFO: stderr: "No resources found in kubectl-1227 namespace.\n"
  Aug 19 12:20:19.992: INFO: stdout: ""
  Aug 19 12:20:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1227 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 19 12:20:20.052: INFO: stderr: ""
  Aug 19 12:20:20.052: INFO: stdout: ""
  Aug 19 12:20:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1227" for this suite. @ 08/19/23 12:20:20.056
• [3.139 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/19/23 12:20:20.062
  Aug 19 12:20:20.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:20:20.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:20.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:20.084
  STEP: Creating projection with secret that has name projected-secret-test-map-27ad56c6-2320-4d23-9978-f4078766598e @ 08/19/23 12:20:20.087
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:20:20.091
  STEP: Saw pod success @ 08/19/23 12:20:24.111
  Aug 19 12:20:24.114: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-4255190c-66de-409f-8e54-1e772df29dcd container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:20:24.128
  Aug 19 12:20:24.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2809" for this suite. @ 08/19/23 12:20:24.148
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/19/23 12:20:24.155
  Aug 19 12:20:24.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 12:20:24.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:24.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:24.175
  STEP: Creating a pod to test service account token:  @ 08/19/23 12:20:24.178
  STEP: Saw pod success @ 08/19/23 12:20:28.198
  Aug 19 12:20:28.201: INFO: Trying to get logs from node ip-172-31-15-214 pod test-pod-27f6a085-d8ed-43b2-9ed2-a18d80e63554 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:20:28.208
  Aug 19 12:20:28.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9581" for this suite. @ 08/19/23 12:20:28.226
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/19/23 12:20:28.234
  Aug 19 12:20:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:20:28.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:28.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:28.255
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:20:28.258
  STEP: Saw pod success @ 08/19/23 12:20:32.277
  Aug 19 12:20:32.280: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-42914518-8cf4-4078-9120-baf057925448 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:20:32.287
  Aug 19 12:20:32.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7959" for this suite. @ 08/19/23 12:20:32.306
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/19/23 12:20:32.313
  Aug 19 12:20:32.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:20:32.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:32.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:32.336
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:20:32.339
  STEP: Saw pod success @ 08/19/23 12:20:36.36
  Aug 19 12:20:36.363: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-ab8f4ade-9aa0-4b20-92c8-789a49e38527 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:20:36.369
  Aug 19 12:20:36.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7845" for this suite. @ 08/19/23 12:20:36.39
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 08/19/23 12:20:36.399
  Aug 19 12:20:36.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 12:20:36.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:36.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:36.421
  Aug 19 12:20:36.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  W0819 12:20:38.974865      18 warnings.go:70] unknown field "alpha"
  W0819 12:20:38.974885      18 warnings.go:70] unknown field "beta"
  W0819 12:20:38.974891      18 warnings.go:70] unknown field "delta"
  W0819 12:20:38.974897      18 warnings.go:70] unknown field "epsilon"
  W0819 12:20:38.974903      18 warnings.go:70] unknown field "gamma"
  Aug 19 12:20:39.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3114" for this suite. @ 08/19/23 12:20:39.524
• [3.132 seconds]
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/19/23 12:20:39.53
  Aug 19 12:20:39.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:20:39.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:39.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:39.553
  STEP: Creating projection with secret that has name secret-emptykey-test-42a87216-2cd1-48d1-93f5-e5aed00b6c26 @ 08/19/23 12:20:39.556
  Aug 19 12:20:39.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2061" for this suite. @ 08/19/23 12:20:39.562
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/19/23 12:20:39.57
  Aug 19 12:20:39.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 12:20:39.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:39.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:39.591
  STEP: create the rc @ 08/19/23 12:20:39.599
  W0819 12:20:39.604441      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 08/19/23 12:20:45.608
  STEP: wait for the rc to be deleted @ 08/19/23 12:20:45.615
  Aug 19 12:20:46.628: INFO: 80 pods remaining
  Aug 19 12:20:46.628: INFO: 80 pods has nil DeletionTimestamp
  Aug 19 12:20:46.629: INFO: 
  Aug 19 12:20:47.628: INFO: 71 pods remaining
  Aug 19 12:20:47.629: INFO: 71 pods has nil DeletionTimestamp
  Aug 19 12:20:47.629: INFO: 
  Aug 19 12:20:48.674: INFO: 60 pods remaining
  Aug 19 12:20:48.674: INFO: 60 pods has nil DeletionTimestamp
  Aug 19 12:20:48.674: INFO: 
  Aug 19 12:20:49.636: INFO: 40 pods remaining
  Aug 19 12:20:49.636: INFO: 40 pods has nil DeletionTimestamp
  Aug 19 12:20:49.636: INFO: 
  Aug 19 12:20:50.624: INFO: 31 pods remaining
  Aug 19 12:20:50.624: INFO: 31 pods has nil DeletionTimestamp
  Aug 19 12:20:50.624: INFO: 
  Aug 19 12:20:51.628: INFO: 20 pods remaining
  Aug 19 12:20:51.628: INFO: 20 pods has nil DeletionTimestamp
  Aug 19 12:20:51.628: INFO: 
  STEP: Gathering metrics @ 08/19/23 12:20:52.621
  W0819 12:20:52.625366      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 12:20:52.625: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 12:20:52.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2587" for this suite. @ 08/19/23 12:20:52.63
• [13.065 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/19/23 12:20:52.636
  Aug 19 12:20:52.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:20:52.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:20:52.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:20:52.655
  STEP: Creating configMap configmap-3740/configmap-test-cf41699e-68a9-4a72-930e-1b4b41c48a4d @ 08/19/23 12:20:52.659
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:20:52.663
  STEP: Saw pod success @ 08/19/23 12:21:00.691
  Aug 19 12:21:00.695: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-fbc08daf-bfd5-4fd6-ba8e-a79f723fe28b container env-test: <nil>
  STEP: delete the pod @ 08/19/23 12:21:00.709
  Aug 19 12:21:00.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3740" for this suite. @ 08/19/23 12:21:00.733
• [8.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/19/23 12:21:00.744
  Aug 19 12:21:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:21:00.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:21:00.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:21:00.767
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2068 @ 08/19/23 12:21:00.771
  STEP: changing the ExternalName service to type=ClusterIP @ 08/19/23 12:21:00.776
  STEP: creating replication controller externalname-service in namespace services-2068 @ 08/19/23 12:21:00.793
  I0819 12:21:00.803009      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2068, replica count: 2
  I0819 12:21:03.854476      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0819 12:21:06.855165      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 12:21:06.855: INFO: Creating new exec pod
  Aug 19 12:21:09.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2068 exec execpodrtc9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 19 12:21:10.020: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 19 12:21:10.020: INFO: stdout: ""
  Aug 19 12:21:11.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2068 exec execpodrtc9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 19 12:21:11.169: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 19 12:21:11.169: INFO: stdout: "externalname-service-ct4jj"
  Aug 19 12:21:11.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2068 exec execpodrtc9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.241 80'
  Aug 19 12:21:11.308: INFO: stderr: "+ nc -v -t -w 2 10.152.183.241 80\nConnection to 10.152.183.241 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 12:21:11.308: INFO: stdout: ""
  Aug 19 12:21:12.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2068 exec execpodrtc9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.241 80'
  Aug 19 12:21:12.440: INFO: stderr: "+ nc -v -t -w 2 10.152.183.241 80\nConnection to 10.152.183.241 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 12:21:12.440: INFO: stdout: ""
  Aug 19 12:21:13.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-2068 exec execpodrtc9z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.241 80'
  Aug 19 12:21:13.445: INFO: stderr: "+ nc -v -t -w 2 10.152.183.241 80\nConnection to 10.152.183.241 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 12:21:13.445: INFO: stdout: "externalname-service-ct4jj"
  Aug 19 12:21:13.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 12:21:13.449: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-2068" for this suite. @ 08/19/23 12:21:13.465
• [12.727 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/19/23 12:21:13.474
  Aug 19 12:21:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:21:13.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:21:13.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:21:13.55
  STEP: Creating configMap with name configmap-test-volume-8dcf3531-f03d-454f-989c-0eae1a248b01 @ 08/19/23 12:21:13.553
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:21:13.558
  STEP: Saw pod success @ 08/19/23 12:21:17.58
  Aug 19 12:21:17.583: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-91307ba9-aaed-40d6-843e-8e5355fab0a4 container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:21:17.593
  Aug 19 12:21:17.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3972" for this suite. @ 08/19/23 12:21:17.612
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/19/23 12:21:17.625
  Aug 19 12:21:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:21:17.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:21:17.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:21:17.648
  STEP: Setting up server cert @ 08/19/23 12:21:17.688
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:21:18.464
  STEP: Deploying the webhook pod @ 08/19/23 12:21:18.474
  STEP: Wait for the deployment to be ready @ 08/19/23 12:21:18.488
  Aug 19 12:21:18.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/19/23 12:21:20.507
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:21:20.517
  Aug 19 12:21:21.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/19/23 12:21:21.521
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/19/23 12:21:21.523
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/19/23 12:21:21.523
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/19/23 12:21:21.523
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/19/23 12:21:21.524
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/19/23 12:21:21.524
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/19/23 12:21:21.525
  Aug 19 12:21:21.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5687" for this suite. @ 08/19/23 12:21:21.565
  STEP: Destroying namespace "webhook-markers-9796" for this suite. @ 08/19/23 12:21:21.572
• [3.954 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/19/23 12:21:21.58
  Aug 19 12:21:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption @ 08/19/23 12:21:21.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:21:21.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:21:21.601
  Aug 19 12:21:21.616: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 19 12:22:21.635: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/19/23 12:22:21.64
  Aug 19 12:22:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/19/23 12:22:21.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:21.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:21.663
  STEP: Finding an available node @ 08/19/23 12:22:21.666
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/19/23 12:22:21.666
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/19/23 12:22:23.684
  Aug 19 12:22:23.697: INFO: found a healthy node: ip-172-31-15-214
  Aug 19 12:22:29.764: INFO: pods created so far: [1 1 1]
  Aug 19 12:22:29.764: INFO: length of pods created so far: 3
  Aug 19 12:22:31.773: INFO: pods created so far: [2 2 1]
  Aug 19 12:22:38.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 12:22:38.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4852" for this suite. @ 08/19/23 12:22:38.842
  STEP: Destroying namespace "sched-preemption-6005" for this suite. @ 08/19/23 12:22:38.848
• [77.274 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/19/23 12:22:38.855
  Aug 19 12:22:38.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename watch @ 08/19/23 12:22:38.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:38.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:38.877
  STEP: creating a watch on configmaps with a certain label @ 08/19/23 12:22:38.883
  STEP: creating a new configmap @ 08/19/23 12:22:38.884
  STEP: modifying the configmap once @ 08/19/23 12:22:38.888
  STEP: changing the label value of the configmap @ 08/19/23 12:22:38.897
  STEP: Expecting to observe a delete notification for the watched object @ 08/19/23 12:22:38.904
  Aug 19 12:22:38.904: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9291 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:22:38.904: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9292 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:22:38.904: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9293 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/19/23 12:22:38.904
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/19/23 12:22:38.911
  STEP: changing the label value of the configmap back @ 08/19/23 12:22:48.912
  STEP: modifying the configmap a third time @ 08/19/23 12:22:48.921
  STEP: deleting the configmap @ 08/19/23 12:22:48.928
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/19/23 12:22:48.933
  Aug 19 12:22:48.933: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9386 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:22:48.934: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9387 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:22:48.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-405  ec6d2478-cf42-46ac-aa11-78fa16ea241b 9388 0 2023-08-19 12:22:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-19 12:22:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:22:48.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-405" for this suite. @ 08/19/23 12:22:48.938
• [10.089 seconds]
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/19/23 12:22:48.944
  Aug 19 12:22:48.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename events @ 08/19/23 12:22:48.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:48.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:48.964
  STEP: Create set of events @ 08/19/23 12:22:48.967
  Aug 19 12:22:48.971: INFO: created test-event-1
  Aug 19 12:22:48.975: INFO: created test-event-2
  Aug 19 12:22:48.979: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/19/23 12:22:48.979
  STEP: delete collection of events @ 08/19/23 12:22:48.983
  Aug 19 12:22:48.983: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/19/23 12:22:49.004
  Aug 19 12:22:49.004: INFO: requesting list of events to confirm quantity
  Aug 19 12:22:49.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2395" for this suite. @ 08/19/23 12:22:49.011
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/19/23 12:22:49.018
  Aug 19 12:22:49.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:22:49.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:49.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:49.036
  STEP: Creating secret with name s-test-opt-del-e1b0b7e6-d501-4991-ace4-b0ba2068272c @ 08/19/23 12:22:49.043
  STEP: Creating secret with name s-test-opt-upd-e858a9a0-7209-49ed-8324-27520a03facb @ 08/19/23 12:22:49.048
  STEP: Creating the pod @ 08/19/23 12:22:49.053
  STEP: Deleting secret s-test-opt-del-e1b0b7e6-d501-4991-ace4-b0ba2068272c @ 08/19/23 12:22:51.099
  STEP: Updating secret s-test-opt-upd-e858a9a0-7209-49ed-8324-27520a03facb @ 08/19/23 12:22:51.104
  STEP: Creating secret with name s-test-opt-create-18e8cb00-9aa1-48b5-af1a-062ae9476f4b @ 08/19/23 12:22:51.109
  STEP: waiting to observe update in volume @ 08/19/23 12:22:51.113
  Aug 19 12:22:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4223" for this suite. @ 08/19/23 12:22:53.142
• [4.130 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/19/23 12:22:53.15
  Aug 19 12:22:53.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:22:53.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:53.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:53.17
  STEP: Creating configMap with name projected-configmap-test-volume-map-6e3c6d93-1283-4e01-9bd7-cd066874b8b9 @ 08/19/23 12:22:53.173
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:22:53.177
  STEP: Saw pod success @ 08/19/23 12:22:57.198
  Aug 19 12:22:57.202: INFO: Trying to get logs from node ip-172-31-69-13 pod pod-projected-configmaps-14af651d-6d1a-4043-bdbb-2ccf4a976a90 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:22:57.22
  Aug 19 12:22:57.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5460" for this suite. @ 08/19/23 12:22:57.238
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/19/23 12:22:57.246
  Aug 19 12:22:57.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:22:57.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:57.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:57.265
  Aug 19 12:22:57.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-6430 version'
  Aug 19 12:22:57.326: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug 19 12:22:57.326: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-20T02:05:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug 19 12:22:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6430" for this suite. @ 08/19/23 12:22:57.33
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/19/23 12:22:57.338
  Aug 19 12:22:57.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename runtimeclass @ 08/19/23 12:22:57.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:57.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:57.358
  STEP: Deleting RuntimeClass runtimeclass-5077-delete-me @ 08/19/23 12:22:57.365
  STEP: Waiting for the RuntimeClass to disappear @ 08/19/23 12:22:57.371
  Aug 19 12:22:57.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5077" for this suite. @ 08/19/23 12:22:57.384
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/19/23 12:22:57.391
  Aug 19 12:22:57.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 12:22:57.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:22:57.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:22:57.409
  STEP: Creating a job @ 08/19/23 12:22:57.412
  STEP: Ensuring active pods == parallelism @ 08/19/23 12:22:57.418
  STEP: delete a job @ 08/19/23 12:23:01.423
  STEP: deleting Job.batch foo in namespace job-8688, will wait for the garbage collector to delete the pods @ 08/19/23 12:23:01.423
  Aug 19 12:23:01.484: INFO: Deleting Job.batch foo took: 6.612276ms
  Aug 19 12:23:01.584: INFO: Terminating Job.batch foo pods took: 100.210104ms
  STEP: Ensuring job was deleted @ 08/19/23 12:23:32.884
  Aug 19 12:23:32.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8688" for this suite. @ 08/19/23 12:23:32.892
• [35.508 seconds]
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/19/23 12:23:32.899
  Aug 19 12:23:32.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:23:32.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:32.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:32.921
  STEP: Creating secret with name secret-test-16bae514-3e35-4ef1-a0a6-d1580f8c904d @ 08/19/23 12:23:32.925
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:23:32.929
  STEP: Saw pod success @ 08/19/23 12:23:36.947
  Aug 19 12:23:36.950: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-5ed182ae-c70a-4f0f-a5aa-07bf56ba6741 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:23:36.957
  Aug 19 12:23:36.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-164" for this suite. @ 08/19/23 12:23:36.976
• [4.083 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 08/19/23 12:23:36.982
  Aug 19 12:23:36.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 12:23:36.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:36.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:37.002
  Aug 19 12:23:37.022: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/19/23 12:23:37.028
  Aug 19 12:23:37.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:37.033: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/19/23 12:23:37.033
  Aug 19 12:23:37.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:37.052: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:23:38.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 19 12:23:38.056: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/19/23 12:23:38.059
  Aug 19 12:23:38.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 19 12:23:38.076: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Aug 19 12:23:39.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:39.080: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/19/23 12:23:39.08
  Aug 19 12:23:39.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:39.093: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:23:40.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:40.097: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:23:41.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 19 12:23:41.096: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 12:23:41.103
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1120, will wait for the garbage collector to delete the pods @ 08/19/23 12:23:41.103
  Aug 19 12:23:41.163: INFO: Deleting DaemonSet.extensions daemon-set took: 5.494821ms
  Aug 19 12:23:41.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.099851ms
  Aug 19 12:23:42.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:23:42.967: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 12:23:42.970: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9814"},"items":null}

  Aug 19 12:23:42.973: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9814"},"items":null}

  Aug 19 12:23:42.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1120" for this suite. @ 08/19/23 12:23:42.999
• [6.023 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/19/23 12:23:43.006
  Aug 19 12:23:43.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 12:23:43.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:43.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:43.027
  STEP: Creating ServiceAccount "e2e-sa-6nwwq"  @ 08/19/23 12:23:43.03
  Aug 19 12:23:43.036: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-6nwwq"  @ 08/19/23 12:23:43.036
  Aug 19 12:23:43.045: INFO: AutomountServiceAccountToken: true
  Aug 19 12:23:43.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8345" for this suite. @ 08/19/23 12:23:43.049
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/19/23 12:23:43.057
  Aug 19 12:23:43.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:23:43.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:43.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:43.076
  STEP: Counting existing ResourceQuota @ 08/19/23 12:23:43.079
  STEP: Creating a ResourceQuota @ 08/19/23 12:23:48.086
  STEP: Ensuring resource quota status is calculated @ 08/19/23 12:23:48.092
  Aug 19 12:23:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2587" for this suite. @ 08/19/23 12:23:50.1
• [7.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/19/23 12:23:50.108
  Aug 19 12:23:50.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:23:50.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:50.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:50.129
  STEP: Creating configMap with name configmap-test-volume-da138b55-5244-4268-96db-abadf857dcf7 @ 08/19/23 12:23:50.132
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:23:50.138
  STEP: Saw pod success @ 08/19/23 12:23:54.156
  Aug 19 12:23:54.159: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-29ed09c3-762a-4fc8-bc46-96ff0c510115 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:23:54.167
  Aug 19 12:23:54.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6293" for this suite. @ 08/19/23 12:23:54.186
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/19/23 12:23:54.195
  Aug 19 12:23:54.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename containers @ 08/19/23 12:23:54.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:54.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:54.217
  Aug 19 12:23:56.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8314" for this suite. @ 08/19/23 12:23:56.245
• [2.056 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/19/23 12:23:56.252
  Aug 19 12:23:56.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename proxy @ 08/19/23 12:23:56.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:56.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:56.272
  Aug 19 12:23:56.275: INFO: Creating pod...
  Aug 19 12:23:58.290: INFO: Creating service...
  Aug 19 12:23:58.300: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=DELETE
  Aug 19 12:23:58.305: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 19 12:23:58.306: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=OPTIONS
  Aug 19 12:23:58.310: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 19 12:23:58.310: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=PATCH
  Aug 19 12:23:58.313: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 19 12:23:58.313: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=POST
  Aug 19 12:23:58.317: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 19 12:23:58.317: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=PUT
  Aug 19 12:23:58.320: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 19 12:23:58.320: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug 19 12:23:58.325: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 19 12:23:58.325: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug 19 12:23:58.331: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 19 12:23:58.331: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug 19 12:23:58.337: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 19 12:23:58.337: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=POST
  Aug 19 12:23:58.342: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 19 12:23:58.342: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=PUT
  Aug 19 12:23:58.347: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 19 12:23:58.347: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=GET
  Aug 19 12:23:58.350: INFO: http.Client request:GET StatusCode:301
  Aug 19 12:23:58.350: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=GET
  Aug 19 12:23:58.355: INFO: http.Client request:GET StatusCode:301
  Aug 19 12:23:58.355: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/pods/agnhost/proxy?method=HEAD
  Aug 19 12:23:58.358: INFO: http.Client request:HEAD StatusCode:301
  Aug 19 12:23:58.358: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-716/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug 19 12:23:58.363: INFO: http.Client request:HEAD StatusCode:301
  Aug 19 12:23:58.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-716" for this suite. @ 08/19/23 12:23:58.367
• [2.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/19/23 12:23:58.376
  Aug 19 12:23:58.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:23:58.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:23:58.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:23:58.401
  STEP: Creating service test in namespace statefulset-580 @ 08/19/23 12:23:58.404
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/19/23 12:23:58.408
  STEP: Creating stateful set ss in namespace statefulset-580 @ 08/19/23 12:23:58.411
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-580 @ 08/19/23 12:23:58.418
  Aug 19 12:23:58.421: INFO: Found 0 stateful pods, waiting for 1
  Aug 19 12:24:08.426: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/19/23 12:24:08.426
  Aug 19 12:24:08.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:24:08.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:24:08.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:24:08.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 12:24:08.574: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Aug 19 12:24:18.578: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 12:24:18.579: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:24:18.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999848s
  Aug 19 12:24:19.600: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996631134s
  Aug 19 12:24:20.603: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992687337s
  Aug 19 12:24:21.608: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988396202s
  Aug 19 12:24:22.612: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984621048s
  Aug 19 12:24:23.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979832909s
  Aug 19 12:24:24.620: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975560972s
  Aug 19 12:24:25.625: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971296685s
  Aug 19 12:24:26.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.966862552s
  Aug 19 12:24:27.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 962.907352ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-580 @ 08/19/23 12:24:28.634
  Aug 19 12:24:28.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:24:28.785: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:24:28.785: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:24:28.785: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 12:24:28.789: INFO: Found 1 stateful pods, waiting for 3
  Aug 19 12:24:38.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:24:38.795: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:24:38.795: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/19/23 12:24:38.795
  STEP: Scale down will halt with unhealthy stateful pod @ 08/19/23 12:24:38.795
  Aug 19 12:24:38.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:24:38.948: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:24:38.948: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:24:38.948: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 12:24:38.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:24:39.086: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:24:39.086: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:24:39.086: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 12:24:39.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 12:24:39.242: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 12:24:39.242: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 12:24:39.242: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 12:24:39.242: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:24:39.250: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Aug 19 12:24:49.259: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 12:24:49.259: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 12:24:49.259: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 12:24:49.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999987s
  Aug 19 12:24:50.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993570488s
  Aug 19 12:24:51.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989684278s
  Aug 19 12:24:52.288: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985596895s
  Aug 19 12:24:53.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981102943s
  Aug 19 12:24:54.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976430183s
  Aug 19 12:24:55.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972972386s
  Aug 19 12:24:56.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969068298s
  Aug 19 12:24:57.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96452043s
  Aug 19 12:24:58.314: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.235812ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-580 @ 08/19/23 12:24:59.315
  Aug 19 12:24:59.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:24:59.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:24:59.460: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:24:59.460: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 12:24:59.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:24:59.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:24:59.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:24:59.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 12:24:59.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-580 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 12:24:59.734: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 12:24:59.734: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 12:24:59.734: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 12:24:59.734: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/19/23 12:25:09.751
  Aug 19 12:25:09.751: INFO: Deleting all statefulset in ns statefulset-580
  Aug 19 12:25:09.754: INFO: Scaling statefulset ss to 0
  Aug 19 12:25:09.764: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:25:09.767: INFO: Deleting statefulset ss
  Aug 19 12:25:09.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-580" for this suite. @ 08/19/23 12:25:09.782
• [71.413 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/19/23 12:25:09.79
  Aug 19 12:25:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:25:09.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:09.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:09.81
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:25:09.814
  STEP: Saw pod success @ 08/19/23 12:25:13.833
  Aug 19 12:25:13.837: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-fc3508a4-79e4-4202-bc9f-8f11c85ef81f container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:25:13.843
  Aug 19 12:25:13.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5515" for this suite. @ 08/19/23 12:25:13.862
• [4.079 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/19/23 12:25:13.869
  Aug 19 12:25:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 12:25:13.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:13.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:13.89
  STEP: Creating a pod to test substitution in container's args @ 08/19/23 12:25:13.893
  STEP: Saw pod success @ 08/19/23 12:25:17.913
  Aug 19 12:25:17.916: INFO: Trying to get logs from node ip-172-31-15-214 pod var-expansion-472ed29a-3e10-4aca-960c-e3d4ddaf5612 container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 12:25:17.923
  Aug 19 12:25:17.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9879" for this suite. @ 08/19/23 12:25:17.939
• [4.075 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/19/23 12:25:17.945
  Aug 19 12:25:17.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:25:17.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:17.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:17.967
  STEP: Creating configMap with name projected-configmap-test-volume-77b7b139-f9e7-4515-918f-908db81bf663 @ 08/19/23 12:25:17.97
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:25:17.975
  STEP: Saw pod success @ 08/19/23 12:25:21.994
  Aug 19 12:25:21.998: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-c737c1e1-dc2d-43d1-9d2b-45e6e0a7156f container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:25:22.004
  Aug 19 12:25:22.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9906" for this suite. @ 08/19/23 12:25:22.027
• [4.089 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/19/23 12:25:22.034
  Aug 19 12:25:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 12:25:22.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:22.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:22.054
  Aug 19 12:25:22.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  W0819 12:25:24.610992      18 warnings.go:70] unknown field "alpha"
  W0819 12:25:24.611014      18 warnings.go:70] unknown field "beta"
  W0819 12:25:24.611020      18 warnings.go:70] unknown field "delta"
  W0819 12:25:24.611026      18 warnings.go:70] unknown field "epsilon"
  W0819 12:25:24.611051      18 warnings.go:70] unknown field "gamma"
  Aug 19 12:25:25.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-269" for this suite. @ 08/19/23 12:25:25.163
• [3.136 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/19/23 12:25:25.17
  Aug 19 12:25:25.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:25:25.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:25.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:25.191
  STEP: set up a multi version CRD @ 08/19/23 12:25:25.194
  Aug 19 12:25:25.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: mark a version not serverd @ 08/19/23 12:25:28.75
  STEP: check the unserved version gets removed @ 08/19/23 12:25:28.768
  STEP: check the other version is not changed @ 08/19/23 12:25:30.001
  Aug 19 12:25:32.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7700" for this suite. @ 08/19/23 12:25:32.717
• [7.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/19/23 12:25:32.725
  Aug 19 12:25:32.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:25:32.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:32.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:32.749
  STEP: create deployment with httpd image @ 08/19/23 12:25:32.752
  Aug 19 12:25:32.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-5873 create -f -'
  Aug 19 12:25:32.983: INFO: stderr: ""
  Aug 19 12:25:32.984: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/19/23 12:25:32.984
  Aug 19 12:25:32.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-5873 diff -f -'
  Aug 19 12:25:33.191: INFO: rc: 1
  Aug 19 12:25:33.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-5873 delete -f -'
  Aug 19 12:25:33.257: INFO: stderr: ""
  Aug 19 12:25:33.257: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug 19 12:25:33.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5873" for this suite. @ 08/19/23 12:25:33.261
• [0.543 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/19/23 12:25:33.269
  Aug 19 12:25:33.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:25:33.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:33.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:33.293
  STEP: Creating Pod @ 08/19/23 12:25:33.296
  STEP: Reading file content from the nginx-container @ 08/19/23 12:25:35.31
  Aug 19 12:25:35.310: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-699 PodName:pod-sharedvolume-499cca6c-0e7b-4afb-96b0-e8aeee27009e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:25:35.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:25:35.311: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:25:35.311: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-699/pods/pod-sharedvolume-499cca6c-0e7b-4afb-96b0-e8aeee27009e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Aug 19 12:25:35.393: INFO: Exec stderr: ""
  Aug 19 12:25:35.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-699" for this suite. @ 08/19/23 12:25:35.398
• [2.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/19/23 12:25:35.405
  Aug 19 12:25:35.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:25:35.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:35.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:35.425
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4445.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4445.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/19/23 12:25:35.428
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4445.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4445.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/19/23 12:25:35.428
  STEP: creating a pod to probe /etc/hosts @ 08/19/23 12:25:35.428
  STEP: submitting the pod to kubernetes @ 08/19/23 12:25:35.429
  STEP: retrieving the pod @ 08/19/23 12:25:43.455
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:25:43.459
  Aug 19 12:25:43.475: INFO: DNS probes using dns-4445/dns-test-f3916e1a-2fde-4aa3-a6ef-54e9a14edc40 succeeded

  Aug 19 12:25:43.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:25:43.479
  STEP: Destroying namespace "dns-4445" for this suite. @ 08/19/23 12:25:43.489
• [8.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/19/23 12:25:43.497
  Aug 19 12:25:43.497: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 12:25:43.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:43.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:43.52
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/19/23 12:25:43.523
  STEP: When a replication controller with a matching selector is created @ 08/19/23 12:25:45.542
  STEP: Then the orphan pod is adopted @ 08/19/23 12:25:45.547
  Aug 19 12:25:46.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7576" for this suite. @ 08/19/23 12:25:46.557
• [3.067 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/19/23 12:25:46.565
  Aug 19 12:25:46.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pod-network-test @ 08/19/23 12:25:46.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:25:46.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:25:46.586
  STEP: Performing setup for networking test in namespace pod-network-test-2552 @ 08/19/23 12:25:46.589
  STEP: creating a selector @ 08/19/23 12:25:46.589
  STEP: Creating the service pods in kubernetes @ 08/19/23 12:25:46.589
  Aug 19 12:25:46.589: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/19/23 12:26:08.687
  Aug 19 12:26:10.702: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 19 12:26:10.702: INFO: Breadth first check of 192.168.13.166 on host 172.31.15.214...
  Aug 19 12:26:10.704: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.167:9080/dial?request=hostname&protocol=udp&host=192.168.13.166&port=8081&tries=1'] Namespace:pod-network-test-2552 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:26:10.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:26:10.705: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:26:10.705: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2552/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.13.166%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 12:26:10.778: INFO: Waiting for responses: map[]
  Aug 19 12:26:10.778: INFO: reached 192.168.13.166 after 0/1 tries
  Aug 19 12:26:10.778: INFO: Breadth first check of 192.168.232.47 on host 172.31.42.145...
  Aug 19 12:26:10.781: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.167:9080/dial?request=hostname&protocol=udp&host=192.168.232.47&port=8081&tries=1'] Namespace:pod-network-test-2552 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:26:10.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:26:10.782: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:26:10.782: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2552/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.232.47%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 12:26:10.866: INFO: Waiting for responses: map[]
  Aug 19 12:26:10.866: INFO: reached 192.168.232.47 after 0/1 tries
  Aug 19 12:26:10.866: INFO: Breadth first check of 192.168.20.46 on host 172.31.69.13...
  Aug 19 12:26:10.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.167:9080/dial?request=hostname&protocol=udp&host=192.168.20.46&port=8081&tries=1'] Namespace:pod-network-test-2552 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:26:10.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:26:10.870: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:26:10.870: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2552/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.20.46%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 12:26:10.949: INFO: Waiting for responses: map[]
  Aug 19 12:26:10.949: INFO: reached 192.168.20.46 after 0/1 tries
  Aug 19 12:26:10.949: INFO: Going to retry 0 out of 3 pods....
  Aug 19 12:26:10.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2552" for this suite. @ 08/19/23 12:26:10.952
• [24.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/19/23 12:26:10.96
  Aug 19 12:26:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:26:10.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:26:10.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:26:10.98
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:26:10.983
  STEP: Saw pod success @ 08/19/23 12:26:15.005
  Aug 19 12:26:15.009: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-f4cf94f1-1bff-461f-8ec4-611b04d08ea8 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:26:15.016
  Aug 19 12:26:15.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5148" for this suite. @ 08/19/23 12:26:15.04
• [4.090 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/19/23 12:26:15.051
  Aug 19 12:26:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:26:15.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:26:15.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:26:15.076
  STEP: Creating projection with secret that has name projected-secret-test-map-3e157efb-7426-454d-934f-055fd31b25b7 @ 08/19/23 12:26:15.079
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:26:15.085
  STEP: Saw pod success @ 08/19/23 12:26:19.113
  Aug 19 12:26:19.117: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-2758d3d0-465f-4484-8b03-0630b64a4bb8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:26:19.127
  Aug 19 12:26:19.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6792" for this suite. @ 08/19/23 12:26:19.147
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/19/23 12:26:19.154
  Aug 19 12:26:19.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:26:19.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:26:19.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:26:19.176
  STEP: Creating secret with name secret-test-map-193ac589-b9b4-43bf-bd0e-048122b9341c @ 08/19/23 12:26:19.179
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:26:19.184
  STEP: Saw pod success @ 08/19/23 12:26:23.202
  Aug 19 12:26:23.205: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-ef773fa7-4bbf-4cac-9616-1ffe2e96b888 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:26:23.212
  Aug 19 12:26:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5959" for this suite. @ 08/19/23 12:26:23.234
• [4.087 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/19/23 12:26:23.242
  Aug 19 12:26:23.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename podtemplate @ 08/19/23 12:26:23.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:26:23.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:26:23.265
  Aug 19 12:26:23.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1247" for this suite. @ 08/19/23 12:26:23.308
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/19/23 12:26:23.317
  Aug 19 12:26:23.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename init-container @ 08/19/23 12:26:23.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:26:23.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:26:23.339
  STEP: creating the pod @ 08/19/23 12:26:23.342
  Aug 19 12:26:23.342: INFO: PodSpec: initContainers in spec.initContainers
  Aug 19 12:27:05.338: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1d32bb37-d460-415b-997d-b62ca338fc79", GenerateName:"", Namespace:"init-container-6467", SelfLink:"", UID:"dc1bf4ef-af04-4782-9c39-60ca1e8507d5", ResourceVersion:"11191", Generation:0, CreationTimestamp:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"342791727"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f26eb8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 19, 12, 27, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000f26ee8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-mdk97", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003fd7f60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mdk97", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mdk97", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-mdk97", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0031794e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-15-214", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000328930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003179570)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003179590)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003179598), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00317959c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f03730), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.15.214", PodIP:"192.168.13.172", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.13.172"}}, StartTime:time.Date(2023, time.August, 19, 12, 26, 23, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000328a80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000328bd0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://0908079d65ff483132869388829a2b181ffd848af60fc680a41073b471a3ea95", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003fd7fe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003fd7fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003179614), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug 19 12:27:05.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6467" for this suite. @ 08/19/23 12:27:05.344
• [42.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/19/23 12:27:05.355
  Aug 19 12:27:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:27:05.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:05.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:05.374
  STEP: Setting up server cert @ 08/19/23 12:27:05.402
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:27:05.873
  STEP: Deploying the webhook pod @ 08/19/23 12:27:05.879
  STEP: Wait for the deployment to be ready @ 08/19/23 12:27:05.891
  Aug 19 12:27:05.898: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:27:07.908
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:27:07.919
  Aug 19 12:27:08.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/19/23 12:27:08.922
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/19/23 12:27:08.938
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/19/23 12:27:08.945
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/19/23 12:27:08.954
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/19/23 12:27:08.964
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/19/23 12:27:08.974
  Aug 19 12:27:08.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9904" for this suite. @ 08/19/23 12:27:09.022
  STEP: Destroying namespace "webhook-markers-1409" for this suite. @ 08/19/23 12:27:09.031
• [3.684 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/19/23 12:27:09.041
  Aug 19 12:27:09.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename watch @ 08/19/23 12:27:09.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:09.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:09.061
  STEP: getting a starting resourceVersion @ 08/19/23 12:27:09.064
  STEP: starting a background goroutine to produce watch events @ 08/19/23 12:27:09.067
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/19/23 12:27:09.067
  Aug 19 12:27:11.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7310" for this suite. @ 08/19/23 12:27:11.896
• [2.908 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/19/23 12:27:11.951
  Aug 19 12:27:11.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:27:11.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:11.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:11.972
  Aug 19 12:27:11.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 create -f -'
  Aug 19 12:27:12.235: INFO: stderr: ""
  Aug 19 12:27:12.235: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug 19 12:27:12.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 create -f -'
  Aug 19 12:27:12.499: INFO: stderr: ""
  Aug 19 12:27:12.499: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/19/23 12:27:12.499
  Aug 19 12:27:13.503: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:27:13.503: INFO: Found 1 / 1
  Aug 19 12:27:13.503: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 19 12:27:13.506: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:27:13.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 19 12:27:13.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 describe pod agnhost-primary-rkj7c'
  Aug 19 12:27:13.582: INFO: stderr: ""
  Aug 19 12:27:13.582: INFO: stdout: "Name:             agnhost-primary-rkj7c\nNamespace:        kubectl-971\nPriority:         0\nService Account:  default\nNode:             ip-172-31-15-214/172.31.15.214\nStart Time:       Sat, 19 Aug 2023 12:27:12 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.13.174\nIPs:\n  IP:           192.168.13.174\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4c1226b7c6f402be9f9e77381e150dc34b6d53dd9d9ce0ca89f9f3f50e872ffc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 19 Aug 2023 12:27:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ssq5r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ssq5r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-971/agnhost-primary-rkj7c to ip-172-31-15-214\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  Aug 19 12:27:13.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 describe rc agnhost-primary'
  Aug 19 12:27:13.655: INFO: stderr: ""
  Aug 19 12:27:13.655: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-971\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-rkj7c\n"
  Aug 19 12:27:13.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 describe service agnhost-primary'
  Aug 19 12:27:13.724: INFO: stderr: ""
  Aug 19 12:27:13.724: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-971\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.164\nIPs:               10.152.183.164\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.13.174:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug 19 12:27:13.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 describe node ip-172-31-15-214'
  Aug 19 12:27:13.827: INFO: stderr: ""
  Aug 19 12:27:13.827: INFO: stdout: "Name:               ip-172-31-15-214\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-15-214\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 19 Aug 2023 11:54:20 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-15-214\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 19 Aug 2023 12:27:05 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 19 Aug 2023 12:26:24 +0000   Sat, 19 Aug 2023 11:54:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 19 Aug 2023 12:26:24 +0000   Sat, 19 Aug 2023 11:54:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 19 Aug 2023 12:26:24 +0000   Sat, 19 Aug 2023 11:54:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 19 Aug 2023 12:26:24 +0000   Sat, 19 Aug 2023 11:54:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.15.214\n  Hostname:    ip-172-31-15-214\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    16069568Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16066244Ki\n  pods:                 110\nAllocatable:\n  cpu:                  2\n  ephemeral-storage:    14809713845\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               15963844Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                      ec2b636e5c88c4153248cf37a0a82f9f\n  System UUID:                     ec2b636e-5c88-c415-3248-cf37a0a82f9f\n  Boot ID:                         9dfa3168-1b61-4984-b6bf-2a125ae59bc7\n  Kernel Version:                  6.2.0-1009-aws\n  OS Image:                        Ubuntu 22.04.3 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.4\n  Kube-Proxy Version:              v1.27.4\nNon-terminated Pods:               (5 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-v7bfm           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m12s\n  kube-system                      metrics-server-v0.5.2-6cf8c8b69c-tlcp9                     5m (0%)       100m (5%)   50Mi (0%)        300Mi (1%)     8m24s\n  kubectl-5873                     httpd-deployment-5cd84d4f9-5nwpj                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         101s\n  kubectl-971                      agnhost-primary-rkj7c                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  5m (0%)    100m (5%)\n  memory               50Mi (0%)  300Mi (1%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 32m                kube-proxy       \n  Normal   Starting                 32m                kube-proxy       \n  Normal   Starting                 31m                kube-proxy       \n  Normal   Starting                 32m                kube-proxy       \n  Normal   NodeHasSufficientPID     32m (x2 over 32m)  kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasNoDiskPressure    32m (x2 over 32m)  kubelet          Node ip-172-31-15-214 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  32m (x2 over 32m)  kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      32m                kubelet          invalid capacity 0 on image filesystem\n  Normal   Starting                 32m                kubelet          Starting kubelet.\n  Normal   RegisteredNode           32m                node-controller  Node ip-172-31-15-214 event: Registered Node ip-172-31-15-214 in Controller\n  Normal   NodeReady                32m                kubelet          Node ip-172-31-15-214 status is now: NodeReady\n  Warning  InvalidDiskCapacity      32m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 32m                kubelet          Starting kubelet.\n  Normal   NodeNotReady             32m                kubelet          Node ip-172-31-15-214 status is now: NodeNotReady\n  Normal   NodeHasSufficientMemory  32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     32m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity      32m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  32m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                32m                kubelet          Node ip-172-31-15-214 status is now: NodeReady\n  Normal   Starting                 32m                kubelet          Starting kubelet.\n  Normal   Starting                 31m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      31m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  31m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    31m                kubelet          Node ip-172-31-15-214 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     31m                kubelet          Node ip-172-31-15-214 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n"
  Aug 19 12:27:13.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-971 describe namespace kubectl-971'
  Aug 19 12:27:13.899: INFO: stderr: ""
  Aug 19 12:27:13.899: INFO: stdout: "Name:         kubectl-971\nLabels:       e2e-framework=kubectl\n              e2e-run=6912050d-e89f-476f-82af-26dee370257f\n              kubernetes.io/metadata.name=kubectl-971\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug 19 12:27:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-971" for this suite. @ 08/19/23 12:27:13.903
• [1.960 seconds]
------------------------------
SS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/19/23 12:27:13.911
  Aug 19 12:27:13.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename hostport @ 08/19/23 12:27:13.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:13.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:13.94
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/19/23 12:27:13.948
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.69.13 on the node which pod1 resides and expect scheduled @ 08/19/23 12:27:15.967
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.69.13 but use UDP protocol on the node which pod2 resides @ 08/19/23 12:27:28.001
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/19/23 12:27:32.031
  Aug 19 12:27:32.031: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.69.13 http://127.0.0.1:54323/hostname] Namespace:hostport-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:27:32.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:27:32.032: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:27:32.032: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3950/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.69.13+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.69.13, port: 54323 @ 08/19/23 12:27:32.105
  Aug 19 12:27:32.105: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.69.13:54323/hostname] Namespace:hostport-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:27:32.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:27:32.106: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:27:32.106: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3950/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.69.13%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.69.13, port: 54323 UDP @ 08/19/23 12:27:32.197
  Aug 19 12:27:32.197: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.69.13 54323] Namespace:hostport-3950 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:27:32.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:27:32.197: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:27:32.197: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3950/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.69.13+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Aug 19 12:27:37.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3950" for this suite. @ 08/19/23 12:27:37.279
• [23.374 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/19/23 12:27:37.285
  Aug 19 12:27:37.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:27:37.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:37.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:37.308
  STEP: Creating configMap with name configmap-test-upd-6b264796-0695-4446-9c09-547e3b400603 @ 08/19/23 12:27:37.314
  STEP: Creating the pod @ 08/19/23 12:27:37.319
  STEP: Waiting for pod with text data @ 08/19/23 12:27:39.336
  STEP: Waiting for pod with binary data @ 08/19/23 12:27:39.343
  Aug 19 12:27:39.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4045" for this suite. @ 08/19/23 12:27:39.353
• [2.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/19/23 12:27:39.364
  Aug 19 12:27:39.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:27:39.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:27:39.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:27:39.406
  STEP: Creating service test in namespace statefulset-9394 @ 08/19/23 12:27:39.409
  STEP: Creating a new StatefulSet @ 08/19/23 12:27:39.414
  Aug 19 12:27:39.423: INFO: Found 0 stateful pods, waiting for 3
  Aug 19 12:27:49.428: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:27:49.428: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:27:49.428: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/19/23 12:27:49.44
  Aug 19 12:27:49.458: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/19/23 12:27:49.458
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/19/23 12:27:59.473
  STEP: Performing a canary update @ 08/19/23 12:27:59.473
  Aug 19 12:27:59.494: INFO: Updating stateful set ss2
  Aug 19 12:27:59.504: INFO: Waiting for Pod statefulset-9394/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/19/23 12:28:09.512
  Aug 19 12:28:09.547: INFO: Found 1 stateful pods, waiting for 3
  Aug 19 12:28:19.551: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:28:19.551: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:28:19.551: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/19/23 12:28:19.557
  Aug 19 12:28:19.577: INFO: Updating stateful set ss2
  Aug 19 12:28:19.583: INFO: Waiting for Pod statefulset-9394/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 19 12:28:29.610: INFO: Updating stateful set ss2
  Aug 19 12:28:29.616: INFO: Waiting for StatefulSet statefulset-9394/ss2 to complete update
  Aug 19 12:28:29.616: INFO: Waiting for Pod statefulset-9394/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 19 12:28:39.625: INFO: Deleting all statefulset in ns statefulset-9394
  Aug 19 12:28:39.628: INFO: Scaling statefulset ss2 to 0
  Aug 19 12:28:49.646: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:28:49.649: INFO: Deleting statefulset ss2
  Aug 19 12:28:49.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9394" for this suite. @ 08/19/23 12:28:49.664
• [70.307 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/19/23 12:28:49.672
  Aug 19 12:28:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename runtimeclass @ 08/19/23 12:28:49.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:28:49.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:28:49.698
  Aug 19 12:28:51.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1477" for this suite. @ 08/19/23 12:28:51.73
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/19/23 12:28:51.739
  Aug 19 12:28:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 12:28:51.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:28:51.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:28:51.762
  Aug 19 12:28:53.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 12:28:53.782: INFO: Deleting pod "var-expansion-6d999b41-7c3d-43f5-9bbd-89ebdce35e0a" in namespace "var-expansion-6226"
  Aug 19 12:28:53.790: INFO: Wait up to 5m0s for pod "var-expansion-6d999b41-7c3d-43f5-9bbd-89ebdce35e0a" to be fully deleted
  STEP: Destroying namespace "var-expansion-6226" for this suite. @ 08/19/23 12:28:55.797
• [4.064 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/19/23 12:28:55.804
  Aug 19 12:28:55.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:28:55.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:28:55.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:28:55.824
  STEP: creating a replication controller @ 08/19/23 12:28:55.828
  Aug 19 12:28:55.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 create -f -'
  Aug 19 12:28:56.101: INFO: stderr: ""
  Aug 19 12:28:56.101: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/19/23 12:28:56.101
  Aug 19 12:28:56.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:28:56.170: INFO: stderr: ""
  Aug 19 12:28:56.170: INFO: stdout: "update-demo-nautilus-dldvd update-demo-nautilus-ppc4m "
  Aug 19 12:28:56.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods update-demo-nautilus-dldvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:28:56.231: INFO: stderr: ""
  Aug 19 12:28:56.231: INFO: stdout: ""
  Aug 19 12:28:56.231: INFO: update-demo-nautilus-dldvd is created but not running
  Aug 19 12:29:01.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 19 12:29:01.297: INFO: stderr: ""
  Aug 19 12:29:01.297: INFO: stdout: "update-demo-nautilus-dldvd update-demo-nautilus-ppc4m "
  Aug 19 12:29:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods update-demo-nautilus-dldvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:29:01.358: INFO: stderr: ""
  Aug 19 12:29:01.358: INFO: stdout: "true"
  Aug 19 12:29:01.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods update-demo-nautilus-dldvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:29:01.433: INFO: stderr: ""
  Aug 19 12:29:01.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:29:01.433: INFO: validating pod update-demo-nautilus-dldvd
  Aug 19 12:29:01.439: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:29:01.439: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:29:01.439: INFO: update-demo-nautilus-dldvd is verified up and running
  Aug 19 12:29:01.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods update-demo-nautilus-ppc4m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 19 12:29:01.501: INFO: stderr: ""
  Aug 19 12:29:01.501: INFO: stdout: "true"
  Aug 19 12:29:01.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods update-demo-nautilus-ppc4m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 19 12:29:01.564: INFO: stderr: ""
  Aug 19 12:29:01.564: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 19 12:29:01.564: INFO: validating pod update-demo-nautilus-ppc4m
  Aug 19 12:29:01.569: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 19 12:29:01.569: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 19 12:29:01.569: INFO: update-demo-nautilus-ppc4m is verified up and running
  STEP: using delete to clean up resources @ 08/19/23 12:29:01.569
  Aug 19 12:29:01.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 delete --grace-period=0 --force -f -'
  Aug 19 12:29:01.667: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:29:01.667: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 19 12:29:01.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get rc,svc -l name=update-demo --no-headers'
  Aug 19 12:29:01.770: INFO: stderr: "No resources found in kubectl-1682 namespace.\n"
  Aug 19 12:29:01.770: INFO: stdout: ""
  Aug 19 12:29:01.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-1682 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 19 12:29:01.844: INFO: stderr: ""
  Aug 19 12:29:01.844: INFO: stdout: ""
  Aug 19 12:29:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1682" for this suite. @ 08/19/23 12:29:01.848
• [6.051 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/19/23 12:29:01.855
  Aug 19 12:29:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:29:01.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:01.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:01.885
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/19/23 12:29:01.888
  STEP: Saw pod success @ 08/19/23 12:29:05.908
  Aug 19 12:29:05.911: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-827c0ff5-4317-49e0-9315-a1678a626ca5 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:29:05.918
  Aug 19 12:29:05.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8219" for this suite. @ 08/19/23 12:29:05.937
• [4.089 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/19/23 12:29:05.945
  Aug 19 12:29:05.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:29:05.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:05.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:05.973
  STEP: Creating a test externalName service @ 08/19/23 12:29:05.976
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:05.982
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:05.982
  STEP: creating a pod to probe DNS @ 08/19/23 12:29:05.982
  STEP: submitting the pod to kubernetes @ 08/19/23 12:29:05.983
  STEP: retrieving the pod @ 08/19/23 12:29:12.007
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:29:12.011
  Aug 19 12:29:12.019: INFO: DNS probes using dns-test-01631fc5-f036-41d1-8af9-ec047e7cd2e7 succeeded

  STEP: changing the externalName to bar.example.com @ 08/19/23 12:29:12.019
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:12.027
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:12.027
  STEP: creating a second pod to probe DNS @ 08/19/23 12:29:12.027
  STEP: submitting the pod to kubernetes @ 08/19/23 12:29:12.027
  STEP: retrieving the pod @ 08/19/23 12:29:14.041
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:29:14.044
  Aug 19 12:29:14.049: INFO: File wheezy_udp@dns-test-service-3.dns-3559.svc.cluster.local from pod  dns-3559/dns-test-62f08964-2db5-4b8a-b788-47ca106d334c contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 19 12:29:14.053: INFO: File jessie_udp@dns-test-service-3.dns-3559.svc.cluster.local from pod  dns-3559/dns-test-62f08964-2db5-4b8a-b788-47ca106d334c contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 19 12:29:14.053: INFO: Lookups using dns-3559/dns-test-62f08964-2db5-4b8a-b788-47ca106d334c failed for: [wheezy_udp@dns-test-service-3.dns-3559.svc.cluster.local jessie_udp@dns-test-service-3.dns-3559.svc.cluster.local]

  Aug 19 12:29:19.061: INFO: DNS probes using dns-test-62f08964-2db5-4b8a-b788-47ca106d334c succeeded

  STEP: changing the service to type=ClusterIP @ 08/19/23 12:29:19.061
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:19.076
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3559.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3559.svc.cluster.local; sleep 1; done
   @ 08/19/23 12:29:19.076
  STEP: creating a third pod to probe DNS @ 08/19/23 12:29:19.076
  STEP: submitting the pod to kubernetes @ 08/19/23 12:29:19.08
  STEP: retrieving the pod @ 08/19/23 12:29:21.095
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:29:21.098
  Aug 19 12:29:21.107: INFO: DNS probes using dns-test-801f431b-6d82-4860-9f5d-15e186a2376e succeeded

  Aug 19 12:29:21.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:29:21.111
  STEP: deleting the pod @ 08/19/23 12:29:21.125
  STEP: deleting the pod @ 08/19/23 12:29:21.14
  STEP: deleting the test externalName service @ 08/19/23 12:29:21.151
  STEP: Destroying namespace "dns-3559" for this suite. @ 08/19/23 12:29:21.165
• [15.227 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 08/19/23 12:29:21.174
  Aug 19 12:29:21.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 12:29:21.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:21.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:21.197
  STEP: Creating simple DaemonSet "daemon-set" @ 08/19/23 12:29:21.221
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 12:29:21.227
  Aug 19 12:29:21.231: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:21.232: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:21.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:29:21.235: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:29:22.241: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:22.241: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:22.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:29:22.245: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:29:23.239: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:23.239: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:23.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 12:29:23.243: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/19/23 12:29:23.246
  Aug 19 12:29:23.260: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:23.260: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:23.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 12:29:23.263: INFO: Node ip-172-31-69-13 is running 0 daemon pod, expected 1
  Aug 19 12:29:24.268: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:24.268: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:24.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 12:29:24.271: INFO: Node ip-172-31-69-13 is running 0 daemon pod, expected 1
  Aug 19 12:29:25.267: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:25.268: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:25.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 12:29:25.271: INFO: Node ip-172-31-69-13 is running 0 daemon pod, expected 1
  Aug 19 12:29:26.268: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:26.268: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:29:26.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 12:29:26.273: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 12:29:26.276
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7947, will wait for the garbage collector to delete the pods @ 08/19/23 12:29:26.276
  Aug 19 12:29:26.336: INFO: Deleting DaemonSet.extensions daemon-set took: 5.774808ms
  Aug 19 12:29:26.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.908875ms
  Aug 19 12:29:27.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:29:27.740: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 12:29:27.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12629"},"items":null}

  Aug 19 12:29:27.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12629"},"items":null}

  Aug 19 12:29:27.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7947" for this suite. @ 08/19/23 12:29:27.762
• [6.594 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/19/23 12:29:27.768
  Aug 19 12:29:27.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:29:27.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:27.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:27.789
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:29:27.792
  STEP: Saw pod success @ 08/19/23 12:29:31.811
  Aug 19 12:29:31.814: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-99919a5e-2095-4a47-ae78-d7fef6f6ddeb container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:29:31.821
  Aug 19 12:29:31.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7777" for this suite. @ 08/19/23 12:29:31.84
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/19/23 12:29:31.85
  Aug 19 12:29:31.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:29:31.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:31.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:31.87
  STEP: Creating secret with name secret-test-855d2a2e-2d2a-4a1c-b2bd-46b1526b2792 @ 08/19/23 12:29:31.873
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:29:31.877
  STEP: Saw pod success @ 08/19/23 12:29:35.897
  Aug 19 12:29:35.900: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-c3017d4d-458c-40de-af16-d169eec15dcf container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:29:35.906
  Aug 19 12:29:35.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5651" for this suite. @ 08/19/23 12:29:35.924
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/19/23 12:29:35.932
  Aug 19 12:29:35.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:29:35.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:35.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:35.95
  STEP: Creating configMap with name configmap-test-volume-519ef7f8-cafd-42a1-85b5-0a49930bc5e4 @ 08/19/23 12:29:35.953
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:29:35.961
  STEP: Saw pod success @ 08/19/23 12:29:39.98
  Aug 19 12:29:39.983: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-f0101878-a1fa-4f7d-81b1-bf5db24641d6 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:29:39.99
  Aug 19 12:29:40.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5749" for this suite. @ 08/19/23 12:29:40.009
• [4.083 seconds]
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/19/23 12:29:40.016
  Aug 19 12:29:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:29:40.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:40.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:40.033
  STEP: Create set of pods @ 08/19/23 12:29:40.04
  Aug 19 12:29:40.047: INFO: created test-pod-1
  Aug 19 12:29:40.052: INFO: created test-pod-2
  Aug 19 12:29:40.059: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/19/23 12:29:40.059
  STEP: waiting for all pods to be deleted @ 08/19/23 12:29:42.098
  Aug 19 12:29:42.101: INFO: Pod quantity 3 is different from expected quantity 0
  Aug 19 12:29:43.106: INFO: Pod quantity 3 is different from expected quantity 0
  Aug 19 12:29:44.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3394" for this suite. @ 08/19/23 12:29:44.11
• [4.100 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/19/23 12:29:44.117
  Aug 19 12:29:44.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 12:29:44.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:44.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:44.138
  STEP: Creating a job @ 08/19/23 12:29:44.142
  STEP: Ensuring job reaches completions @ 08/19/23 12:29:44.147
  Aug 19 12:29:54.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6129" for this suite. @ 08/19/23 12:29:54.156
• [10.045 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/19/23 12:29:54.163
  Aug 19 12:29:54.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:29:54.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:54.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:54.184
  STEP: Creating a ResourceQuota @ 08/19/23 12:29:54.187
  STEP: Getting a ResourceQuota @ 08/19/23 12:29:54.192
  STEP: Updating a ResourceQuota @ 08/19/23 12:29:54.194
  STEP: Verifying a ResourceQuota was modified @ 08/19/23 12:29:54.203
  STEP: Deleting a ResourceQuota @ 08/19/23 12:29:54.206
  STEP: Verifying the deleted ResourceQuota @ 08/19/23 12:29:54.219
  Aug 19 12:29:54.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4779" for this suite. @ 08/19/23 12:29:54.225
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/19/23 12:29:54.233
  Aug 19 12:29:54.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:29:54.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:29:54.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:29:54.265
  STEP: creating service multi-endpoint-test in namespace services-8219 @ 08/19/23 12:29:54.268
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8219 to expose endpoints map[] @ 08/19/23 12:29:54.277
  Aug 19 12:29:54.281: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Aug 19 12:29:55.289: INFO: successfully validated that service multi-endpoint-test in namespace services-8219 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8219 @ 08/19/23 12:29:55.289
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8219 to expose endpoints map[pod1:[100]] @ 08/19/23 12:29:57.309
  Aug 19 12:29:57.322: INFO: successfully validated that service multi-endpoint-test in namespace services-8219 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-8219 @ 08/19/23 12:29:57.322
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8219 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/19/23 12:29:59.344
  Aug 19 12:29:59.361: INFO: successfully validated that service multi-endpoint-test in namespace services-8219 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/19/23 12:29:59.361
  Aug 19 12:29:59.361: INFO: Creating new exec pod
  Aug 19 12:30:02.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8219 exec execpodzwf5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Aug 19 12:30:02.545: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug 19 12:30:02.545: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:30:02.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8219 exec execpodzwf5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.170 80'
  Aug 19 12:30:02.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.170 80\nConnection to 10.152.183.170 80 port [tcp/http] succeeded!\n"
  Aug 19 12:30:02.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:30:02.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8219 exec execpodzwf5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Aug 19 12:30:02.841: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug 19 12:30:02.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:30:02.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8219 exec execpodzwf5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.170 81'
  Aug 19 12:30:02.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.170 81\nConnection to 10.152.183.170 81 port [tcp/*] succeeded!\n"
  Aug 19 12:30:02.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8219 @ 08/19/23 12:30:02.972
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8219 to expose endpoints map[pod2:[101]] @ 08/19/23 12:30:02.987
  Aug 19 12:30:04.020: INFO: successfully validated that service multi-endpoint-test in namespace services-8219 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-8219 @ 08/19/23 12:30:04.02
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8219 to expose endpoints map[] @ 08/19/23 12:30:04.049
  Aug 19 12:30:05.065: INFO: successfully validated that service multi-endpoint-test in namespace services-8219 exposes endpoints map[]
  Aug 19 12:30:05.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8219" for this suite. @ 08/19/23 12:30:05.086
• [10.859 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/19/23 12:30:05.093
  Aug 19 12:30:05.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 12:30:05.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:05.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:05.116
  STEP: create the rc1 @ 08/19/23 12:30:05.123
  STEP: create the rc2 @ 08/19/23 12:30:05.128
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/19/23 12:30:09.137
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/19/23 12:30:09.663
  STEP: wait for the rc to be deleted @ 08/19/23 12:30:09.675
  Aug 19 12:30:14.694: INFO: 70 pods remaining
  Aug 19 12:30:14.694: INFO: 70 pods has nil DeletionTimestamp
  Aug 19 12:30:14.694: INFO: 
  STEP: Gathering metrics @ 08/19/23 12:30:19.687
  W0819 12:30:19.690957      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 12:30:19.690: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 12:30:19.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cvfx" in namespace "gc-5701"
  Aug 19 12:30:19.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mczv" in namespace "gc-5701"
  Aug 19 12:30:19.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ttfd" in namespace "gc-5701"
  Aug 19 12:30:19.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-4krcf" in namespace "gc-5701"
  Aug 19 12:30:19.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-4krps" in namespace "gc-5701"
  Aug 19 12:30:19.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vt29" in namespace "gc-5701"
  Aug 19 12:30:19.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-59qfc" in namespace "gc-5701"
  Aug 19 12:30:19.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dlff" in namespace "gc-5701"
  Aug 19 12:30:19.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h7kv" in namespace "gc-5701"
  Aug 19 12:30:19.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jkx7" in namespace "gc-5701"
  Aug 19 12:30:19.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pcbf" in namespace "gc-5701"
  Aug 19 12:30:19.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-65xgp" in namespace "gc-5701"
  Aug 19 12:30:19.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-67lgq" in namespace "gc-5701"
  Aug 19 12:30:19.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m47s" in namespace "gc-5701"
  Aug 19 12:30:19.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n9qx" in namespace "gc-5701"
  Aug 19 12:30:19.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rx42" in namespace "gc-5701"
  Aug 19 12:30:19.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-749qn" in namespace "gc-5701"
  Aug 19 12:30:19.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qf4s" in namespace "gc-5701"
  Aug 19 12:30:19.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tvpx" in namespace "gc-5701"
  Aug 19 12:30:19.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-82wtl" in namespace "gc-5701"
  Aug 19 12:30:19.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-8689f" in namespace "gc-5701"
  Aug 19 12:30:20.004: INFO: Deleting pod "simpletest-rc-to-be-deleted-89fws" in namespace "gc-5701"
  Aug 19 12:30:20.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dx4q" in namespace "gc-5701"
  Aug 19 12:30:20.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gxd2" in namespace "gc-5701"
  Aug 19 12:30:20.046: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ngnj" in namespace "gc-5701"
  Aug 19 12:30:20.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pv85" in namespace "gc-5701"
  Aug 19 12:30:20.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-92ksw" in namespace "gc-5701"
  Aug 19 12:30:20.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-94rrk" in namespace "gc-5701"
  Aug 19 12:30:20.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8hw9" in namespace "gc-5701"
  Aug 19 12:30:20.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbm8c" in namespace "gc-5701"
  Aug 19 12:30:20.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsq4z" in namespace "gc-5701"
  Aug 19 12:30:20.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-c74tt" in namespace "gc-5701"
  Aug 19 12:30:20.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch5x2" in namespace "gc-5701"
  Aug 19 12:30:20.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckvp7" in namespace "gc-5701"
  Aug 19 12:30:20.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-crz52" in namespace "gc-5701"
  Aug 19 12:30:20.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhm4h" in namespace "gc-5701"
  Aug 19 12:30:20.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmnv7" in namespace "gc-5701"
  Aug 19 12:30:20.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnrbh" in namespace "gc-5701"
  Aug 19 12:30:20.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6mr8" in namespace "gc-5701"
  Aug 19 12:30:20.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxzd6" in namespace "gc-5701"
  Aug 19 12:30:20.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzq64" in namespace "gc-5701"
  Aug 19 12:30:20.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-fztms" in namespace "gc-5701"
  Aug 19 12:30:20.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-g67zf" in namespace "gc-5701"
  Aug 19 12:30:20.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7xss" in namespace "gc-5701"
  Aug 19 12:30:20.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnztz" in namespace "gc-5701"
  Aug 19 12:30:20.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-hggd2" in namespace "gc-5701"
  Aug 19 12:30:20.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnbx4" in namespace "gc-5701"
  Aug 19 12:30:20.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnv7f" in namespace "gc-5701"
  Aug 19 12:30:20.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqw9s" in namespace "gc-5701"
  Aug 19 12:30:20.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvrb5" in namespace "gc-5701"
  Aug 19 12:30:20.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5701" for this suite. @ 08/19/23 12:30:20.443
• [15.356 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/19/23 12:30:20.45
  Aug 19 12:30:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/19/23 12:30:20.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:20.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:20.482
  Aug 19 12:30:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:30:21.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3549" for this suite. @ 08/19/23 12:30:21.521
• [1.089 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/19/23 12:30:21.54
  Aug 19 12:30:21.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename certificates @ 08/19/23 12:30:21.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:21.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:21.568
  STEP: getting /apis @ 08/19/23 12:30:22.945
  STEP: getting /apis/certificates.k8s.io @ 08/19/23 12:30:22.95
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/19/23 12:30:22.951
  STEP: creating @ 08/19/23 12:30:22.952
  STEP: getting @ 08/19/23 12:30:22.97
  STEP: listing @ 08/19/23 12:30:22.979
  STEP: watching @ 08/19/23 12:30:22.984
  Aug 19 12:30:22.984: INFO: starting watch
  STEP: patching @ 08/19/23 12:30:22.985
  STEP: updating @ 08/19/23 12:30:22.991
  Aug 19 12:30:22.997: INFO: waiting for watch events with expected annotations
  Aug 19 12:30:22.997: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/19/23 12:30:22.998
  STEP: patching /approval @ 08/19/23 12:30:23.001
  STEP: updating /approval @ 08/19/23 12:30:23.007
  STEP: getting /status @ 08/19/23 12:30:23.014
  STEP: patching /status @ 08/19/23 12:30:23.023
  STEP: updating /status @ 08/19/23 12:30:23.032
  STEP: deleting @ 08/19/23 12:30:23.039
  STEP: deleting a collection @ 08/19/23 12:30:23.053
  Aug 19 12:30:23.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-340" for this suite. @ 08/19/23 12:30:23.071
• [1.537 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/19/23 12:30:23.08
  Aug 19 12:30:23.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subpath @ 08/19/23 12:30:23.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:23.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:23.103
  STEP: Setting up data @ 08/19/23 12:30:23.106
  STEP: Creating pod pod-subpath-test-secret-pdnp @ 08/19/23 12:30:23.116
  STEP: Creating a pod to test atomic-volume-subpath @ 08/19/23 12:30:23.116
  STEP: Saw pod success @ 08/19/23 12:30:49.194
  Aug 19 12:30:49.198: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-subpath-test-secret-pdnp container test-container-subpath-secret-pdnp: <nil>
  STEP: delete the pod @ 08/19/23 12:30:49.204
  STEP: Deleting pod pod-subpath-test-secret-pdnp @ 08/19/23 12:30:49.219
  Aug 19 12:30:49.219: INFO: Deleting pod "pod-subpath-test-secret-pdnp" in namespace "subpath-655"
  Aug 19 12:30:49.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-655" for this suite. @ 08/19/23 12:30:49.23
• [26.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/19/23 12:30:49.237
  Aug 19 12:30:49.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename server-version @ 08/19/23 12:30:49.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:49.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:49.262
  STEP: Request ServerVersion @ 08/19/23 12:30:49.269
  STEP: Confirm major version @ 08/19/23 12:30:49.271
  Aug 19 12:30:49.271: INFO: Major version: 1
  STEP: Confirm minor version @ 08/19/23 12:30:49.271
  Aug 19 12:30:49.271: INFO: cleanMinorVersion: 27
  Aug 19 12:30:49.271: INFO: Minor version: 27
  Aug 19 12:30:49.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-4504" for this suite. @ 08/19/23 12:30:49.28
• [0.051 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/19/23 12:30:49.288
  Aug 19 12:30:49.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/19/23 12:30:49.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:49.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:49.317
  STEP: creating a target pod @ 08/19/23 12:30:49.32
  STEP: adding an ephemeral container @ 08/19/23 12:30:51.338
  STEP: checking pod container endpoints @ 08/19/23 12:30:53.357
  Aug 19 12:30:53.357: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2498 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 12:30:53.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:30:53.357: INFO: ExecWithOptions: Clientset creation
  Aug 19 12:30:53.358: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-2498/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Aug 19 12:30:53.434: INFO: Exec stderr: ""
  Aug 19 12:30:53.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-2498" for this suite. @ 08/19/23 12:30:53.444
• [4.163 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/19/23 12:30:53.451
  Aug 19 12:30:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/19/23 12:30:53.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:53.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:53.471
  STEP: creating @ 08/19/23 12:30:53.474
  STEP: getting @ 08/19/23 12:30:53.492
  STEP: listing in namespace @ 08/19/23 12:30:53.496
  STEP: patching @ 08/19/23 12:30:53.499
  STEP: deleting @ 08/19/23 12:30:53.512
  Aug 19 12:30:53.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5221" for this suite. @ 08/19/23 12:30:53.527
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/19/23 12:30:53.534
  Aug 19 12:30:53.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:30:53.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:30:53.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:30:53.555
  STEP: Creating pod test-grpc-677a1ca3-f355-42d6-86e9-b6285455d814 in namespace container-probe-6644 @ 08/19/23 12:30:53.558
  Aug 19 12:30:55.574: INFO: Started pod test-grpc-677a1ca3-f355-42d6-86e9-b6285455d814 in namespace container-probe-6644
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:30:55.574
  Aug 19 12:30:55.577: INFO: Initial restart count of pod test-grpc-677a1ca3-f355-42d6-86e9-b6285455d814 is 0
  Aug 19 12:32:11.740: INFO: Restart count of pod container-probe-6644/test-grpc-677a1ca3-f355-42d6-86e9-b6285455d814 is now 1 (1m16.16261314s elapsed)
  Aug 19 12:32:11.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:32:11.744
  STEP: Destroying namespace "container-probe-6644" for this suite. @ 08/19/23 12:32:11.762
• [78.234 seconds]
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/19/23 12:32:11.768
  Aug 19 12:32:11.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:32:11.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:11.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:11.794
  STEP: Creating the pod @ 08/19/23 12:32:11.797
  Aug 19 12:32:14.342: INFO: Successfully updated pod "labelsupdate04b7e5c2-22f9-4632-a470-5a03f09e1221"
  Aug 19 12:32:16.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1990" for this suite. @ 08/19/23 12:32:16.367
• [4.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/19/23 12:32:16.377
  Aug 19 12:32:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:32:16.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:16.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:16.401
  STEP: starting the proxy server @ 08/19/23 12:32:16.405
  Aug 19 12:32:16.405: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3277 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/19/23 12:32:16.453
  Aug 19 12:32:16.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3277" for this suite. @ 08/19/23 12:32:16.466
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/19/23 12:32:16.477
  Aug 19 12:32:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption @ 08/19/23 12:32:16.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:16.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:16.504
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/19/23 12:32:16.507
  STEP: Waiting for the pdb to be processed @ 08/19/23 12:32:16.518
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/19/23 12:32:18.533
  STEP: Waiting for all pods to be running @ 08/19/23 12:32:18.534
  Aug 19 12:32:18.539: INFO: pods: 0 < 3
  Aug 19 12:32:20.546: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 08/19/23 12:32:22.546
  STEP: Updating the pdb to allow a pod to be evicted @ 08/19/23 12:32:22.556
  STEP: Waiting for the pdb to be processed @ 08/19/23 12:32:22.564
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/19/23 12:32:24.571
  STEP: Waiting for all pods to be running @ 08/19/23 12:32:24.571
  STEP: Waiting for the pdb to observed all healthy pods @ 08/19/23 12:32:24.575
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/19/23 12:32:24.596
  STEP: Waiting for the pdb to be processed @ 08/19/23 12:32:24.623
  STEP: Waiting for all pods to be running @ 08/19/23 12:32:26.631
  STEP: locating a running pod @ 08/19/23 12:32:26.635
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/19/23 12:32:26.643
  STEP: Waiting for the pdb to be deleted @ 08/19/23 12:32:26.65
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/19/23 12:32:26.652
  STEP: Waiting for all pods to be running @ 08/19/23 12:32:26.652
  Aug 19 12:32:26.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5934" for this suite. @ 08/19/23 12:32:26.675
• [10.211 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/19/23 12:32:26.689
  Aug 19 12:32:26.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:32:26.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:26.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:26.716
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/19/23 12:32:26.719
  STEP: Saw pod success @ 08/19/23 12:32:30.737
  Aug 19 12:32:30.741: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-db73fc1b-07b2-44eb-a02f-15d02c2e24bb container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:32:30.747
  Aug 19 12:32:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5626" for this suite. @ 08/19/23 12:32:30.764
• [4.081 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/19/23 12:32:30.77
  Aug 19 12:32:30.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 12:32:30.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:30.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:30.792
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/19/23 12:32:30.795
  STEP: When a replicaset with a matching selector is created @ 08/19/23 12:32:32.815
  STEP: Then the orphan pod is adopted @ 08/19/23 12:32:32.82
  STEP: When the matched label of one of its pods change @ 08/19/23 12:32:33.827
  Aug 19 12:32:33.831: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/19/23 12:32:33.841
  Aug 19 12:32:34.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2933" for this suite. @ 08/19/23 12:32:34.852
• [4.089 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/19/23 12:32:34.86
  Aug 19 12:32:34.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:32:34.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:34.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:34.882
  STEP: Discovering how many secrets are in namespace by default @ 08/19/23 12:32:34.885
  STEP: Counting existing ResourceQuota @ 08/19/23 12:32:39.889
  STEP: Creating a ResourceQuota @ 08/19/23 12:32:44.892
  STEP: Ensuring resource quota status is calculated @ 08/19/23 12:32:44.897
  STEP: Creating a Secret @ 08/19/23 12:32:46.901
  STEP: Ensuring resource quota status captures secret creation @ 08/19/23 12:32:46.915
  STEP: Deleting a secret @ 08/19/23 12:32:48.918
  STEP: Ensuring resource quota status released usage @ 08/19/23 12:32:48.924
  Aug 19 12:32:50.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5144" for this suite. @ 08/19/23 12:32:50.932
• [16.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/19/23 12:32:50.941
  Aug 19 12:32:50.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-pred @ 08/19/23 12:32:50.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:50.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:50.961
  Aug 19 12:32:50.964: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 19 12:32:50.971: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 12:32:50.974: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-214 before test
  Aug 19 12:32:50.978: INFO: pod-csi-inline-volumes from csiinlinevolumes-5221 started at 2023-08-19 12:30:53 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.978: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
  Aug 19 12:32:50.978: INFO: nginx-ingress-controller-kubernetes-worker-v7bfm from ingress-nginx-kubernetes-worker started at 2023-08-19 12:19:01 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.978: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:32:50.978: INFO: metrics-server-v0.5.2-6cf8c8b69c-tlcp9 from kube-system started at 2023-08-19 12:18:49 +0000 UTC (2 container statuses recorded)
  Aug 19 12:32:50.978: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 19 12:32:50.978: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 19 12:32:50.978: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:32:50.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:32:50.978: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:32:50.978: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-145 before test
  Aug 19 12:32:50.983: INFO: rs-6b98c from disruption-5934 started at 2023-08-19 12:32:26 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.983: INFO: 	Container donothing ready: false, restart count 0
  Aug 19 12:32:50.983: INFO: nginx-ingress-controller-kubernetes-worker-xnrcp from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: coredns-5c7f76ccb8-trw9q from kube-system started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container coredns ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: kube-state-metrics-5b95b4459c-rtf6r from kube-system started at 2023-08-19 12:18:49 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: dashboard-metrics-scraper-6b8586b5c9-lgkqw from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: kubernetes-dashboard-6869f4cd5f-gfcl6 from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-52lf7 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:32:50.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:32:50.984: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-69-13 before test
  Aug 19 12:32:50.989: INFO: default-http-backend-kubernetes-worker-65fc475d49-vjk9h from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: nginx-ingress-controller-kubernetes-worker-wb2rk from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: calico-kube-controllers-85f9fb94df-xpnfw from kube-system started at 2023-08-19 11:54:32 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: sonobuoy from sonobuoy started at 2023-08-19 12:00:13 +0000 UTC (1 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: sonobuoy-e2e-job-1e976644cf094697 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container e2e ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-vmdc9 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:32:50.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:32:50.989: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-15-214 @ 08/19/23 12:32:51.003
  STEP: verifying the node has the label node ip-172-31-42-145 @ 08/19/23 12:32:51.015
  STEP: verifying the node has the label node ip-172-31-69-13 @ 08/19/23 12:32:51.028
  Aug 19 12:32:51.039: INFO: Pod pod-csi-inline-volumes requesting resource cpu=0m on Node ip-172-31-15-214
  Aug 19 12:32:51.039: INFO: Pod rs-6b98c requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.039: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-vjk9h requesting resource cpu=10m on Node ip-172-31-69-13
  Aug 19 12:32:51.039: INFO: Pod nginx-ingress-controller-kubernetes-worker-v7bfm requesting resource cpu=0m on Node ip-172-31-15-214
  Aug 19 12:32:51.039: INFO: Pod nginx-ingress-controller-kubernetes-worker-wb2rk requesting resource cpu=0m on Node ip-172-31-69-13
  Aug 19 12:32:51.040: INFO: Pod nginx-ingress-controller-kubernetes-worker-xnrcp requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.041: INFO: Pod calico-kube-controllers-85f9fb94df-xpnfw requesting resource cpu=0m on Node ip-172-31-69-13
  Aug 19 12:32:51.041: INFO: Pod coredns-5c7f76ccb8-trw9q requesting resource cpu=100m on Node ip-172-31-42-145
  Aug 19 12:32:51.041: INFO: Pod kube-state-metrics-5b95b4459c-rtf6r requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.042: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-tlcp9 requesting resource cpu=5m on Node ip-172-31-15-214
  Aug 19 12:32:51.042: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-lgkqw requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.042: INFO: Pod kubernetes-dashboard-6869f4cd5f-gfcl6 requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.042: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-69-13
  Aug 19 12:32:51.042: INFO: Pod sonobuoy-e2e-job-1e976644cf094697 requesting resource cpu=0m on Node ip-172-31-69-13
  Aug 19 12:32:51.042: INFO: Pod sonobuoy-systemd-logs-daemon-set-5181330686b44064-52lf7 requesting resource cpu=0m on Node ip-172-31-42-145
  Aug 19 12:32:51.042: INFO: Pod sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt requesting resource cpu=0m on Node ip-172-31-15-214
  Aug 19 12:32:51.042: INFO: Pod sonobuoy-systemd-logs-daemon-set-5181330686b44064-vmdc9 requesting resource cpu=0m on Node ip-172-31-69-13
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/19/23 12:32:51.042
  Aug 19 12:32:51.042: INFO: Creating a pod which consumes cpu=1396m on Node ip-172-31-15-214
  Aug 19 12:32:51.050: INFO: Creating a pod which consumes cpu=1330m on Node ip-172-31-42-145
  Aug 19 12:32:51.055: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-69-13
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/19/23 12:32:53.078
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3f59117e-2803-4557-88f1-95042a8fe080.177cc8d3961d4c98], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9428/filler-pod-3f59117e-2803-4557-88f1-95042a8fe080 to ip-172-31-69-13] @ 08/19/23 12:32:53.081
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3f59117e-2803-4557-88f1-95042a8fe080.177cc8d3bc6e5889], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/19/23 12:32:53.081
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3f59117e-2803-4557-88f1-95042a8fe080.177cc8d3bd875ee8], Reason = [Created], Message = [Created container filler-pod-3f59117e-2803-4557-88f1-95042a8fe080] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3f59117e-2803-4557-88f1-95042a8fe080.177cc8d3c34cbc74], Reason = [Started], Message = [Started container filler-pod-3f59117e-2803-4557-88f1-95042a8fe080] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245.177cc8d396300ca3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9428/filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245 to ip-172-31-42-145] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245.177cc8d3bddbf67e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245.177cc8d3bedcc01a], Reason = [Created], Message = [Created container filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245.177cc8d3c4b2a61f], Reason = [Started], Message = [Started container filler-pod-8042cf0e-858c-4ef8-bf3f-c7ed6751e245] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e.177cc8d395a0ff88], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9428/filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e to ip-172-31-15-214] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e.177cc8d3bc454df5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e.177cc8d3bd1a6a8e], Reason = [Created], Message = [Created container filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e.177cc8d3c387e69b], Reason = [Started], Message = [Started container filler-pod-cbdf3147-3cf1-427e-8818-e63884300a6e] @ 08/19/23 12:32:53.082
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177cc8d40e9949b3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 08/19/23 12:32:53.093
  STEP: removing the label node off the node ip-172-31-15-214 @ 08/19/23 12:32:54.092
  STEP: verifying the node doesn't have the label node @ 08/19/23 12:32:54.103
  STEP: removing the label node off the node ip-172-31-42-145 @ 08/19/23 12:32:54.107
  STEP: verifying the node doesn't have the label node @ 08/19/23 12:32:54.117
  STEP: removing the label node off the node ip-172-31-69-13 @ 08/19/23 12:32:54.121
  STEP: verifying the node doesn't have the label node @ 08/19/23 12:32:54.133
  Aug 19 12:32:54.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9428" for this suite. @ 08/19/23 12:32:54.146
• [3.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/19/23 12:32:54.156
  Aug 19 12:32:54.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:32:54.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:54.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:54.178
  STEP: Creating configMap with name projected-configmap-test-volume-aad4dc18-6d08-4616-b5df-defc127ef61b @ 08/19/23 12:32:54.182
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:32:54.187
  STEP: Saw pod success @ 08/19/23 12:32:58.205
  Aug 19 12:32:58.208: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-345ae78f-90f4-4818-a2d7-4e92246d1c38 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:32:58.214
  Aug 19 12:32:58.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1331" for this suite. @ 08/19/23 12:32:58.232
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 08/19/23 12:32:58.24
  Aug 19 12:32:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 12:32:58.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:32:58.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:32:58.264
  Aug 19 12:32:58.294: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 12:32:58.299
  Aug 19 12:32:58.302: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:32:58.302: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:32:58.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:32:58.305: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:32:59.309: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:32:59.309: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:32:59.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:32:59.313: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:33:00.310: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:00.310: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:00.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 12:33:00.314: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/19/23 12:33:00.325
  STEP: Check that daemon pods images are updated. @ 08/19/23 12:33:00.335
  Aug 19 12:33:00.339: INFO: Wrong image for pod: daemon-set-qwlqp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:00.339: INFO: Wrong image for pod: daemon-set-v9dff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:00.339: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:00.342: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:00.342: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:01.346: INFO: Wrong image for pod: daemon-set-v9dff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:01.346: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:01.350: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:01.350: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:02.345: INFO: Wrong image for pod: daemon-set-v9dff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:02.345: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:02.349: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:02.349: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:03.347: INFO: Pod daemon-set-97clw is not available
  Aug 19 12:33:03.347: INFO: Wrong image for pod: daemon-set-v9dff. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:03.347: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:03.350: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:03.350: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:04.349: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:04.353: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:04.353: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:05.346: INFO: Pod daemon-set-bktw5 is not available
  Aug 19 12:33:05.346: INFO: Wrong image for pod: daemon-set-x8vct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 19 12:33:05.350: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:05.350: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:06.346: INFO: Pod daemon-set-xnfjq is not available
  Aug 19 12:33:06.350: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:06.350: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/19/23 12:33:06.35
  Aug 19 12:33:06.353: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:06.354: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:06.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 12:33:06.357: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:33:07.362: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:07.362: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:07.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 12:33:07.366: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  Aug 19 12:33:08.361: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:08.361: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 12:33:08.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 12:33:08.365: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 12:33:08.38
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1877, will wait for the garbage collector to delete the pods @ 08/19/23 12:33:08.38
  Aug 19 12:33:08.440: INFO: Deleting DaemonSet.extensions daemon-set took: 5.590577ms
  Aug 19 12:33:08.540: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.493704ms
  Aug 19 12:33:11.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 12:33:11.045: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 12:33:11.048: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16742"},"items":null}

  Aug 19 12:33:11.051: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16742"},"items":null}

  Aug 19 12:33:11.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1877" for this suite. @ 08/19/23 12:33:11.067
• [12.833 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/19/23 12:33:11.075
  Aug 19 12:33:11.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:33:11.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:33:11.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:33:11.096
  STEP: creating service nodeport-test with type=NodePort in namespace services-8611 @ 08/19/23 12:33:11.099
  STEP: creating replication controller nodeport-test in namespace services-8611 @ 08/19/23 12:33:11.111
  I0819 12:33:11.117601      18 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-8611, replica count: 2
  I0819 12:33:14.168553      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 12:33:14.168: INFO: Creating new exec pod
  Aug 19 12:33:17.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug 19 12:33:17.351: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 19 12:33:17.351: INFO: stdout: ""
  Aug 19 12:33:18.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug 19 12:33:18.501: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 19 12:33:18.501: INFO: stdout: "nodeport-test-7t5t8"
  Aug 19 12:33:18.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.187 80'
  Aug 19 12:33:18.655: INFO: stderr: "+ nc -v -t -w 2 10.152.183.187 80\n+ echo hostName\nConnection to 10.152.183.187 80 port [tcp/http] succeeded!\n"
  Aug 19 12:33:18.655: INFO: stdout: "nodeport-test-7t5t8"
  Aug 19 12:33:18.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.145 31668'
  Aug 19 12:33:18.791: INFO: stderr: "+ nc -v -t -w 2 172.31.42.145 31668\n+ echo hostName\nConnection to 172.31.42.145 31668 port [tcp/*] succeeded!\n"
  Aug 19 12:33:18.791: INFO: stdout: "nodeport-test-7t5t8"
  Aug 19 12:33:18.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.214 31668'
  Aug 19 12:33:18.924: INFO: stderr: "+ nc -v -t -w 2 172.31.15.214 31668\n+ echo hostName\nConnection to 172.31.15.214 31668 port [tcp/*] succeeded!\n"
  Aug 19 12:33:18.924: INFO: stdout: ""
  Aug 19 12:33:19.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.214 31668'
  Aug 19 12:33:20.073: INFO: stderr: "+ nc -v -t -w 2 172.31.15.214 31668\nConnection to 172.31.15.214 31668 port [tcp/*] succeeded!\n+ echo hostName\n"
  Aug 19 12:33:20.073: INFO: stdout: ""
  Aug 19 12:33:20.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-8611 exec execpodc6vsv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.214 31668'
  Aug 19 12:33:21.056: INFO: stderr: "+ nc -v -t -w 2 172.31.15.214 31668\nConnection to 172.31.15.214 31668 port [tcp/*] succeeded!\n+ echo hostName\n"
  Aug 19 12:33:21.056: INFO: stdout: "nodeport-test-l2hvj"
  Aug 19 12:33:21.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8611" for this suite. @ 08/19/23 12:33:21.06
• [9.990 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/19/23 12:33:21.068
  Aug 19 12:33:21.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:33:21.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:33:21.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:33:21.094
  STEP: creating a collection of services @ 08/19/23 12:33:21.096
  Aug 19 12:33:21.096: INFO: Creating e2e-svc-a-zs9wk
  Aug 19 12:33:21.108: INFO: Creating e2e-svc-b-2fm4v
  Aug 19 12:33:21.117: INFO: Creating e2e-svc-c-d6fj5
  STEP: deleting service collection @ 08/19/23 12:33:21.131
  Aug 19 12:33:21.157: INFO: Collection of services has been deleted
  Aug 19 12:33:21.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9920" for this suite. @ 08/19/23 12:33:21.161
• [0.098 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/19/23 12:33:21.167
  Aug 19 12:33:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:33:21.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:33:21.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:33:21.185
  STEP: Creating namespace "e2e-ns-kjxvb" @ 08/19/23 12:33:21.188
  Aug 19 12:33:21.205: INFO: Namespace "e2e-ns-kjxvb-6074" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-kjxvb-6074" @ 08/19/23 12:33:21.205
  Aug 19 12:33:21.212: INFO: Namespace "e2e-ns-kjxvb-6074" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-kjxvb-6074" @ 08/19/23 12:33:21.213
  Aug 19 12:33:21.221: INFO: Namespace "e2e-ns-kjxvb-6074" has []v1.FinalizerName{"kubernetes"}
  Aug 19 12:33:21.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7440" for this suite. @ 08/19/23 12:33:21.225
  STEP: Destroying namespace "e2e-ns-kjxvb-6074" for this suite. @ 08/19/23 12:33:21.231
• [0.071 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/19/23 12:33:21.238
  Aug 19 12:33:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 12:33:21.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:33:21.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:33:21.259
  STEP: reading a file in the container @ 08/19/23 12:33:23.279
  Aug 19 12:33:23.279: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1740 pod-service-account-0afc56c1-9ebc-4039-8c68-9deb2731b52b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 08/19/23 12:33:23.411
  Aug 19 12:33:23.411: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1740 pod-service-account-0afc56c1-9ebc-4039-8c68-9deb2731b52b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 08/19/23 12:33:23.543
  Aug 19 12:33:23.543: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1740 pod-service-account-0afc56c1-9ebc-4039-8c68-9deb2731b52b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug 19 12:33:23.690: INFO: Got root ca configmap in namespace "svcaccounts-1740"
  Aug 19 12:33:23.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1740" for this suite. @ 08/19/23 12:33:23.696
• [2.464 seconds]
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/19/23 12:33:23.702
  Aug 19 12:33:23.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:33:23.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:33:23.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:33:23.721
  STEP: Creating configMap with name cm-test-opt-del-6746cdbe-722e-4f5e-b2a8-28a5a9e4c219 @ 08/19/23 12:33:23.728
  STEP: Creating configMap with name cm-test-opt-upd-03ccd744-c42e-4f58-90ef-8919eb4c619f @ 08/19/23 12:33:23.733
  STEP: Creating the pod @ 08/19/23 12:33:23.737
  STEP: Deleting configmap cm-test-opt-del-6746cdbe-722e-4f5e-b2a8-28a5a9e4c219 @ 08/19/23 12:33:25.775
  STEP: Updating configmap cm-test-opt-upd-03ccd744-c42e-4f58-90ef-8919eb4c619f @ 08/19/23 12:33:25.78
  STEP: Creating configMap with name cm-test-opt-create-292b4ba1-6feb-4210-8d7c-e97b39a64ecd @ 08/19/23 12:33:25.785
  STEP: waiting to observe update in volume @ 08/19/23 12:33:25.789
  Aug 19 12:34:38.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2039" for this suite. @ 08/19/23 12:34:38.1
• [74.406 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/19/23 12:34:38.11
  Aug 19 12:34:38.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:34:38.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:34:38.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:34:38.132
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/19/23 12:34:38.135
  Aug 19 12:34:38.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/19/23 12:34:43.595
  Aug 19 12:34:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:34:44.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:34:50.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4661" for this suite. @ 08/19/23 12:34:50.494
• [12.393 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/19/23 12:34:50.504
  Aug 19 12:34:50.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:34:50.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:34:50.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:34:50.539
  STEP: Creating a test namespace @ 08/19/23 12:34:50.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:34:50.56
  STEP: Creating a service in the namespace @ 08/19/23 12:34:50.563
  STEP: Deleting the namespace @ 08/19/23 12:34:50.575
  STEP: Waiting for the namespace to be removed. @ 08/19/23 12:34:50.593
  STEP: Recreating the namespace @ 08/19/23 12:34:56.597
  STEP: Verifying there is no service in the namespace @ 08/19/23 12:34:56.616
  Aug 19 12:34:56.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7970" for this suite. @ 08/19/23 12:34:56.627
  STEP: Destroying namespace "nsdeletetest-9606" for this suite. @ 08/19/23 12:34:56.635
  Aug 19 12:34:56.640: INFO: Namespace nsdeletetest-9606 was already deleted
  STEP: Destroying namespace "nsdeletetest-5981" for this suite. @ 08/19/23 12:34:56.64
• [6.143 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/19/23 12:34:56.647
  Aug 19 12:34:56.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename podtemplate @ 08/19/23 12:34:56.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:34:56.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:34:56.668
  STEP: Create set of pod templates @ 08/19/23 12:34:56.672
  Aug 19 12:34:56.677: INFO: created test-podtemplate-1
  Aug 19 12:34:56.683: INFO: created test-podtemplate-2
  Aug 19 12:34:56.691: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/19/23 12:34:56.691
  STEP: delete collection of pod templates @ 08/19/23 12:34:56.695
  Aug 19 12:34:56.695: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/19/23 12:34:56.722
  Aug 19 12:34:56.722: INFO: requesting list of pod templates to confirm quantity
  Aug 19 12:34:56.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5353" for this suite. @ 08/19/23 12:34:56.731
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/19/23 12:34:56.741
  Aug 19 12:34:56.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:34:56.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:34:56.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:34:56.768
  STEP: Creating the pod @ 08/19/23 12:34:56.773
  Aug 19 12:34:59.327: INFO: Successfully updated pod "annotationupdatef8f9f426-803e-4059-af04-13d08d527462"
  Aug 19 12:35:01.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5067" for this suite. @ 08/19/23 12:35:01.348
• [4.615 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/19/23 12:35:01.356
  Aug 19 12:35:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:35:01.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:01.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:01.385
  Aug 19 12:35:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: creating the pod @ 08/19/23 12:35:01.39
  STEP: submitting the pod to kubernetes @ 08/19/23 12:35:01.39
  Aug 19 12:35:03.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1048" for this suite. @ 08/19/23 12:35:03.434
• [2.086 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/19/23 12:35:03.443
  Aug 19 12:35:03.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:35:03.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:03.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:03.461
  STEP: Creating configMap with name projected-configmap-test-volume-f4629f1c-d8c8-49cc-b606-0ad2b57a299b @ 08/19/23 12:35:03.465
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:35:03.473
  STEP: Saw pod success @ 08/19/23 12:35:07.503
  Aug 19 12:35:07.507: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-914b4f6e-d9cf-483a-857f-ea32cba3eca7 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:35:07.515
  Aug 19 12:35:07.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-714" for this suite. @ 08/19/23 12:35:07.534
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/19/23 12:35:07.543
  Aug 19 12:35:07.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 12:35:07.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:07.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:07.567
  Aug 19 12:35:07.571: INFO: Creating deployment "webserver-deployment"
  Aug 19 12:35:07.578: INFO: Waiting for observed generation 1
  Aug 19 12:35:09.588: INFO: Waiting for all required pods to come up
  Aug 19 12:35:09.592: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/19/23 12:35:09.592
  Aug 19 12:35:11.601: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug 19 12:35:11.609: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug 19 12:35:11.621: INFO: Updating deployment webserver-deployment
  Aug 19 12:35:11.621: INFO: Waiting for observed generation 2
  Aug 19 12:35:13.633: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug 19 12:35:13.637: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug 19 12:35:13.641: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 19 12:35:13.652: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug 19 12:35:13.652: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug 19 12:35:13.656: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 19 12:35:13.663: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug 19 12:35:13.663: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug 19 12:35:13.673: INFO: Updating deployment webserver-deployment
  Aug 19 12:35:13.673: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug 19 12:35:13.684: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Aug 19 12:35:13.687: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Aug 19 12:35:15.705: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-3309  f7cbd9b3-055a-42bf-bf89-6501ab07e643 17795 3 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049f6a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-19 12:35:13 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-19 12:35:13 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug 19 12:35:15.709: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-3309  34ef252a-5117-4b6e-8675-63d610bf39c4 17789 3 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f7cbd9b3-055a-42bf-bf89-6501ab07e643 0xc003035cc7 0xc003035cc8}] [] [{kube-controller-manager Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7cbd9b3-055a-42bf-bf89-6501ab07e643\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003035d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 12:35:15.709: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug 19 12:35:15.709: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-3309  055506a8-d216-49a1-99c5-9fe84cbf1303 17790 3 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f7cbd9b3-055a-42bf-bf89-6501ab07e643 0xc003035bd7 0xc003035bd8}] [] [{kube-controller-manager Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7cbd9b3-055a-42bf-bf89-6501ab07e643\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003035c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 12:35:15.719: INFO: Pod "webserver-deployment-67bd4bf6dc-2vp5g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2vp5g webserver-deployment-67bd4bf6dc- deployment-3309  59dc8fad-e760-4972-99dc-ded8049093ab 17805 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ea217 0xc0039ea218}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcw2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcw2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.720: INFO: Pod "webserver-deployment-67bd4bf6dc-4gjxj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4gjxj webserver-deployment-67bd4bf6dc- deployment-3309  f50ef3a0-6f90-4ecf-a361-2f5c12f20d6d 17584 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ea407 0xc0039ea408}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrvgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrvgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.185,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://eaf043e0a8d7f97ba56c62c98b01ad6c2d75600158043471cd28931b4ada4e4d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.185,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.720: INFO: Pod "webserver-deployment-67bd4bf6dc-6bq6l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6bq6l webserver-deployment-67bd4bf6dc- deployment-3309  39bb2805-65c7-4ed1-8596-1655e3dec178 17748 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ea5f7 0xc0039ea5f8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bcl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bcl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.720: INFO: Pod "webserver-deployment-67bd4bf6dc-6klhn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6klhn webserver-deployment-67bd4bf6dc- deployment-3309  01655139-fa75-4f00-a49b-454a5e0cdde3 17591 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ea7c7 0xc0039ea7c8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.232.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4flp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4flp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:192.168.232.31,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b4536b3b07e7d3f09b8d7c6b6a5211280185fc19472c92766ccb0129f8da535a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.232.31,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.721: INFO: Pod "webserver-deployment-67bd4bf6dc-9lf92" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9lf92 webserver-deployment-67bd4bf6dc- deployment-3309  3523bcb9-06dd-4b05-bc33-11d98a224885 17569 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eaaf7 0xc0039eaaf8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrqcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrqcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:192.168.20.37,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32550aa9b663c6b816b6c485dcdf107639a00655650d5996ff240580ae9165c4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.37,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.722: INFO: Pod "webserver-deployment-67bd4bf6dc-9q45z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9q45z webserver-deployment-67bd4bf6dc- deployment-3309  c1e10b31-9c61-4d01-8266-095147e444f6 17800 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eadb7 0xc0039eadb8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwkcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwkcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.724: INFO: Pod "webserver-deployment-67bd4bf6dc-b9ztc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-b9ztc webserver-deployment-67bd4bf6dc- deployment-3309  967b3690-fbed-409c-a349-fe29d9922fa7 17812 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eaf87 0xc0039eaf88}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frzvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frzvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.726: INFO: Pod "webserver-deployment-67bd4bf6dc-c72gd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-c72gd webserver-deployment-67bd4bf6dc- deployment-3309  4ee32772-4ddf-424c-8be7-fade5fd2926a 17566 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eb597 0xc0039eb598}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdkfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdkfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:192.168.20.36,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0590278ac44194460b02693e7dfb5adb7f47f234c212e41ebf44e2cb337fb97b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.36,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.729: INFO: Pod "webserver-deployment-67bd4bf6dc-ckjvl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ckjvl webserver-deployment-67bd4bf6dc- deployment-3309  b1d305d0-0426-40dc-b4d6-28761d09f39d 17813 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eb787 0xc0039eb788}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rft7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rft7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.730: INFO: Pod "webserver-deployment-67bd4bf6dc-dl8kd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dl8kd webserver-deployment-67bd4bf6dc- deployment-3309  ca50fb96-b3cf-40d2-ac1e-460d532b30e6 17788 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039eb957 0xc0039eb958}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbktz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbktz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.732: INFO: Pod "webserver-deployment-67bd4bf6dc-dxb65" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dxb65 webserver-deployment-67bd4bf6dc- deployment-3309  834f4553-92a7-4e2f-9243-ea7b6779e256 17572 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ebb27 0xc0039ebb28}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d52xc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d52xc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:192.168.20.39,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c070453e91fe601a0f96b1df5555c1b126ba7e181172b7ed3945dce1cc9f87cc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.39,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.734: INFO: Pod "webserver-deployment-67bd4bf6dc-gvbmm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gvbmm webserver-deployment-67bd4bf6dc- deployment-3309  1247e9da-e59e-4c98-98b0-1748087b0acf 17595 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0039ebd17 0xc0039ebd18}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.232.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4blmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4blmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:192.168.232.24,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f75ef414a3574ec17a44cc17becd4423d8007216dfe98de53f32fc8aa7aeb80,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.232.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.735: INFO: Pod "webserver-deployment-67bd4bf6dc-hfjl6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hfjl6 webserver-deployment-67bd4bf6dc- deployment-3309  abd5c5ca-9eee-4ff0-a522-b464826996fc 17588 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0043121b7 0xc0043121b8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.232.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8kll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8kll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:192.168.232.25,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://803e7e5d6ac26dab1ce4a8e8e83829ef45c624cde980103a9dfd31419a0eaf9c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.232.25,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.739: INFO: Pod "webserver-deployment-67bd4bf6dc-mphpn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mphpn webserver-deployment-67bd4bf6dc- deployment-3309  f805e865-47ab-4443-b981-f26654bf01f6 17786 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0043123d7 0xc0043123d8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrrqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrrqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.741: INFO: Pod "webserver-deployment-67bd4bf6dc-qmn2d" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qmn2d webserver-deployment-67bd4bf6dc- deployment-3309  b779e1df-e708-4117-b936-4b054fab4d2f 17578 0 2023-08-19 12:35:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0043125c7 0xc0043125c8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x897d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x897d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.189,StartTime:2023-08-19 12:35:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:35:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://935a86253e096ff2dbe34194b063a9d2d4e3feb3d4c51502011932a5846b113d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.189,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.742: INFO: Pod "webserver-deployment-67bd4bf6dc-rmqds" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rmqds webserver-deployment-67bd4bf6dc- deployment-3309  a5a29652-bae6-4084-82ed-6db2136c2d12 17776 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0043127d7 0xc0043127d8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qstdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qstdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.743: INFO: Pod "webserver-deployment-67bd4bf6dc-tszkk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tszkk webserver-deployment-67bd4bf6dc- deployment-3309  9b970720-6665-4363-8eb2-85e4f33be1b7 17816 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc0043129a7 0xc0043129a8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlsdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlsdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.745: INFO: Pod "webserver-deployment-67bd4bf6dc-vwscn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vwscn webserver-deployment-67bd4bf6dc- deployment-3309  70789f91-86f5-4c17-bf3e-ac41edde455f 17752 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc004312b77 0xc004312b78}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmn62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmn62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.746: INFO: Pod "webserver-deployment-67bd4bf6dc-w72rc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w72rc webserver-deployment-67bd4bf6dc- deployment-3309  adc95370-4cca-46cb-9859-bf10241ffe20 17798 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc004312d57 0xc004312d58}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvjsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvjsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.747: INFO: Pod "webserver-deployment-67bd4bf6dc-x559m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x559m webserver-deployment-67bd4bf6dc- deployment-3309  10d5372f-621d-4ae6-9a43-16b130ecbf41 17806 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 055506a8-d216-49a1-99c5-9fe84cbf1303 0xc004312f27 0xc004312f28}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"055506a8-d216-49a1-99c5-9fe84cbf1303\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96f8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96f8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.748: INFO: Pod "webserver-deployment-7b75d79cf5-29f76" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-29f76 webserver-deployment-7b75d79cf5- deployment-3309  72fea976-5271-48c7-b3dc-7347390d9b69 17713 0 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc0043130f7 0xc0043130f8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8blfx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8blfx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:192.168.20.38,StartTime:2023-08-19 12:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.38,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.751: INFO: Pod "webserver-deployment-7b75d79cf5-2dl5m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2dl5m webserver-deployment-7b75d79cf5- deployment-3309  280716fe-1205-4b8f-b950-63bd3269b17d 17807 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313317 0xc004313318}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5vzvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vzvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.752: INFO: Pod "webserver-deployment-7b75d79cf5-2fcx2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2fcx2 webserver-deployment-7b75d79cf5- deployment-3309  b66c1993-69f2-459d-a2f2-125261bfc498 17801 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313737 0xc004313738}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnpm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnpm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.753: INFO: Pod "webserver-deployment-7b75d79cf5-2vxgc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2vxgc webserver-deployment-7b75d79cf5- deployment-3309  c985f0fc-9e99-4762-8118-9575121588ba 17721 0 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313927 0xc004313928}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.232.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfgzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfgzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:192.168.232.32,StartTime:2023-08-19 12:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.232.32,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.754: INFO: Pod "webserver-deployment-7b75d79cf5-72782" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-72782 webserver-deployment-7b75d79cf5- deployment-3309  b1490079-3e74-4e91-b105-047c6ac1b267 17719 0 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313b47 0xc004313b48}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zffgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zffgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.132,StartTime:2023-08-19 12:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.132,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.755: INFO: Pod "webserver-deployment-7b75d79cf5-7nqfs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7nqfs webserver-deployment-7b75d79cf5- deployment-3309  ce6031f3-b140-4698-8a2e-bd91832f5ef6 17794 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313d67 0xc004313d68}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llgzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llgzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.756: INFO: Pod "webserver-deployment-7b75d79cf5-crj5q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-crj5q webserver-deployment-7b75d79cf5- deployment-3309  c3c9cd2d-cb2d-4b20-b477-d489756be0ef 17810 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc004313f57 0xc004313f58}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcmf5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcmf5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.757: INFO: Pod "webserver-deployment-7b75d79cf5-gkvss" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gkvss webserver-deployment-7b75d79cf5- deployment-3309  47a8e9bc-837c-4e2d-b3ae-3b985c997597 17768 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2a147 0xc003e2a148}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfv8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfv8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.758: INFO: Pod "webserver-deployment-7b75d79cf5-gt9p7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gt9p7 webserver-deployment-7b75d79cf5- deployment-3309  8d989344-4a1e-49cd-af85-76ad64d452d8 17791 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2a337 0xc003e2a338}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8dgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8dgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-42-145,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.42.145,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.759: INFO: Pod "webserver-deployment-7b75d79cf5-hdvss" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hdvss webserver-deployment-7b75d79cf5- deployment-3309  69fe8165-75de-451f-94e2-feec7bfff550 17710 0 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2a527 0xc003e2a528}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zxc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zxc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.69.13,PodIP:192.168.20.40,StartTime:2023-08-19 12:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.40,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.760: INFO: Pod "webserver-deployment-7b75d79cf5-lwjx8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lwjx8 webserver-deployment-7b75d79cf5- deployment-3309  d310fe6a-7c02-41c7-b828-dd34a1b5b1cb 17716 0 2023-08-19 12:35:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2a7e7 0xc003e2a7e8}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6r6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6r6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.188,StartTime:2023-08-19 12:35:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.188,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.761: INFO: Pod "webserver-deployment-7b75d79cf5-nptpm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nptpm webserver-deployment-7b75d79cf5- deployment-3309  c9092a18-9d5d-476d-b345-0b4f4342b6e0 17821 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2aa17 0xc003e2aa18}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zq7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zq7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.762: INFO: Pod "webserver-deployment-7b75d79cf5-tpdb6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tpdb6 webserver-deployment-7b75d79cf5- deployment-3309  125d18cb-b263-4a52-a511-d9d6f42424bb 17822 0 2023-08-19 12:35:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 34ef252a-5117-4b6e-8675-63d610bf39c4 0xc003e2ac07 0xc003e2ac08}] [] [{kube-controller-manager Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34ef252a-5117-4b6e-8675-63d610bf39c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:35:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzj74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzj74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:35:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 12:35:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:35:15.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3309" for this suite. @ 08/19/23 12:35:15.775
• [8.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/19/23 12:35:15.788
  Aug 19 12:35:15.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:35:15.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:15.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:15.81
  STEP: creating the pod @ 08/19/23 12:35:15.814
  STEP: submitting the pod to kubernetes @ 08/19/23 12:35:15.814
  STEP: verifying the pod is in kubernetes @ 08/19/23 12:35:17.839
  STEP: updating the pod @ 08/19/23 12:35:17.843
  Aug 19 12:35:18.356: INFO: Successfully updated pod "pod-update-330ce448-99be-4e35-81d8-221ed52fb591"
  STEP: verifying the updated pod is in kubernetes @ 08/19/23 12:35:18.36
  Aug 19 12:35:18.364: INFO: Pod update OK
  Aug 19 12:35:18.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2558" for this suite. @ 08/19/23 12:35:18.369
• [2.590 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/19/23 12:35:18.377
  Aug 19 12:35:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:35:18.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:18.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:18.397
  Aug 19 12:35:18.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7323" for this suite. @ 08/19/23 12:35:18.46
• [0.091 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/19/23 12:35:18.468
  Aug 19 12:35:18.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 12:35:18.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:18.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:18.486
  STEP: creating a Deployment @ 08/19/23 12:35:18.493
  STEP: waiting for Deployment to be created @ 08/19/23 12:35:18.5
  STEP: waiting for all Replicas to be Ready @ 08/19/23 12:35:18.502
  Aug 19 12:35:18.504: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.504: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.517: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.517: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.530: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.530: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.563: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:18.563: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 19 12:35:19.387: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 19 12:35:19.387: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 19 12:35:20.121: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/19/23 12:35:20.121
  W0819 12:35:20.133146      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 19 12:35:20.135: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/19/23 12:35:20.135
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 0
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.137: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.138: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.149: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.149: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.183: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.183: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:20.193: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:20.193: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:20.211: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:20.211: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:22.355: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:22.355: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:22.384: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  STEP: listing Deployments @ 08/19/23 12:35:22.384
  Aug 19 12:35:22.389: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/19/23 12:35:22.389
  Aug 19 12:35:22.402: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/19/23 12:35:22.402
  Aug 19 12:35:22.410: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:22.416: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:22.441: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:22.454: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:22.468: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:25.183: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:25.400: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:25.452: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:25.473: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 19 12:35:26.444: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/19/23 12:35:26.476
  STEP: fetching the DeploymentStatus @ 08/19/23 12:35:26.485
  Aug 19 12:35:26.491: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:26.491: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 1
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 3
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:26.492: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 2
  Aug 19 12:35:26.493: INFO: observed Deployment test-deployment in namespace deployment-5264 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/19/23 12:35:26.493
  Aug 19 12:35:26.504: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.505: INFO: observed event type MODIFIED
  Aug 19 12:35:26.510: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug 19 12:35:26.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5264" for this suite. @ 08/19/23 12:35:26.521
• [8.066 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/19/23 12:35:26.534
  Aug 19 12:35:26.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-watch @ 08/19/23 12:35:26.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:35:26.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:35:26.559
  Aug 19 12:35:26.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Creating first CR  @ 08/19/23 12:35:29.113
  Aug 19 12:35:29.119: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:29Z]] name:name1 resourceVersion:18615 uid:d67653a1-fab6-4cdb-ad82-12a0ab988ded] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 08/19/23 12:35:39.12
  Aug 19 12:35:39.128: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:39Z]] name:name2 resourceVersion:18690 uid:f2493bac-4af7-4342-ae0e-c18cbc781078] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 08/19/23 12:35:49.128
  Aug 19 12:35:49.136: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:49Z]] name:name1 resourceVersion:18718 uid:d67653a1-fab6-4cdb-ad82-12a0ab988ded] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 08/19/23 12:35:59.136
  Aug 19 12:35:59.144: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:59Z]] name:name2 resourceVersion:18738 uid:f2493bac-4af7-4342-ae0e-c18cbc781078] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 08/19/23 12:36:09.144
  Aug 19 12:36:09.153: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:49Z]] name:name1 resourceVersion:18758 uid:d67653a1-fab6-4cdb-ad82-12a0ab988ded] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 08/19/23 12:36:19.154
  Aug 19 12:36:19.164: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-19T12:35:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-19T12:35:59Z]] name:name2 resourceVersion:18779 uid:f2493bac-4af7-4342-ae0e-c18cbc781078] num:map[num1:9223372036854775807 num2:1000000]]}
  Aug 19 12:36:29.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-2961" for this suite. @ 08/19/23 12:36:29.689
• [63.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/19/23 12:36:29.701
  Aug 19 12:36:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:36:29.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:36:29.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:36:29.723
  STEP: Counting existing ResourceQuota @ 08/19/23 12:36:46.731
  STEP: Creating a ResourceQuota @ 08/19/23 12:36:51.735
  STEP: Ensuring resource quota status is calculated @ 08/19/23 12:36:51.742
  STEP: Creating a ConfigMap @ 08/19/23 12:36:53.746
  STEP: Ensuring resource quota status captures configMap creation @ 08/19/23 12:36:53.759
  STEP: Deleting a ConfigMap @ 08/19/23 12:36:55.764
  STEP: Ensuring resource quota status released usage @ 08/19/23 12:36:55.77
  Aug 19 12:36:57.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7647" for this suite. @ 08/19/23 12:36:57.781
• [28.087 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/19/23 12:36:57.789
  Aug 19 12:36:57.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:36:57.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:36:57.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:36:57.81
  STEP: Creating projection with secret that has name projected-secret-test-f740c369-2db1-448b-a650-873baab78451 @ 08/19/23 12:36:57.818
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:36:57.823
  STEP: Saw pod success @ 08/19/23 12:37:01.845
  Aug 19 12:37:01.849: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-2e226dfc-1dcf-4f86-920b-c2dac1f25073 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:37:01.865
  Aug 19 12:37:01.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-679" for this suite. @ 08/19/23 12:37:01.892
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/19/23 12:37:01.9
  Aug 19 12:37:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:37:01.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:01.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:01.922
  STEP: Setting up server cert @ 08/19/23 12:37:01.951
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:37:02.189
  STEP: Deploying the webhook pod @ 08/19/23 12:37:02.198
  STEP: Wait for the deployment to be ready @ 08/19/23 12:37:02.209
  Aug 19 12:37:02.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:37:04.234
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:37:04.244
  Aug 19 12:37:05.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 19 12:37:05.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8084-crds.webhook.example.com via the AdmissionRegistration API @ 08/19/23 12:37:05.77
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/19/23 12:37:05.787
  Aug 19 12:37:07.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7405" for this suite. @ 08/19/23 12:37:08.448
  STEP: Destroying namespace "webhook-markers-3616" for this suite. @ 08/19/23 12:37:08.456
• [6.568 seconds]
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/19/23 12:37:08.468
  Aug 19 12:37:08.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubelet-test @ 08/19/23 12:37:08.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:08.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:08.49
  STEP: Waiting for pod completion @ 08/19/23 12:37:08.501
  Aug 19 12:37:12.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6284" for this suite. @ 08/19/23 12:37:12.534
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/19/23 12:37:12.551
  Aug 19 12:37:12.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:37:12.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:12.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:12.573
  STEP: Creating configMap with name projected-configmap-test-volume-map-65961163-9934-4725-b408-cf134a1a6e0e @ 08/19/23 12:37:12.577
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:37:12.582
  STEP: Saw pod success @ 08/19/23 12:37:16.601
  Aug 19 12:37:16.606: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-03973527-1a97-4362-96c3-600887a29b2a container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:37:16.618
  Aug 19 12:37:16.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1385" for this suite. @ 08/19/23 12:37:16.639
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/19/23 12:37:16.648
  Aug 19 12:37:16.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:37:16.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:16.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:16.669
  STEP: Create a pod @ 08/19/23 12:37:16.673
  STEP: patching /status @ 08/19/23 12:37:18.695
  Aug 19 12:37:18.707: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug 19 12:37:18.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3334" for this suite. @ 08/19/23 12:37:18.714
• [2.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/19/23 12:37:18.722
  Aug 19 12:37:18.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 12:37:18.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:18.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:18.742
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/19/23 12:37:18.748
  Aug 19 12:37:18.757: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug 19 12:37:23.762: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 12:37:23.762
  STEP: getting scale subresource @ 08/19/23 12:37:23.762
  STEP: updating a scale subresource @ 08/19/23 12:37:23.766
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/19/23 12:37:23.772
  STEP: Patch a scale subresource @ 08/19/23 12:37:23.775
  Aug 19 12:37:23.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8573" for this suite. @ 08/19/23 12:37:23.798
• [5.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/19/23 12:37:23.815
  Aug 19 12:37:23.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:37:23.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:23.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:23.844
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:37:23.848
  STEP: Saw pod success @ 08/19/23 12:37:27.873
  Aug 19 12:37:27.877: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-ed3d4576-1090-4bc3-8d3e-56710c3f4b27 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:37:27.886
  Aug 19 12:37:27.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-392" for this suite. @ 08/19/23 12:37:27.914
• [4.106 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/19/23 12:37:27.922
  Aug 19 12:37:27.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:37:27.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:27.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:27.94
  STEP: Setting up server cert @ 08/19/23 12:37:27.968
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:37:28.122
  STEP: Deploying the webhook pod @ 08/19/23 12:37:28.127
  STEP: Wait for the deployment to be ready @ 08/19/23 12:37:28.142
  Aug 19 12:37:28.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:37:30.171
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:37:30.182
  Aug 19 12:37:31.183: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/19/23 12:37:31.187
  STEP: create a pod @ 08/19/23 12:37:31.206
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/19/23 12:37:33.225
  Aug 19 12:37:33.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=webhook-488 attach --namespace=webhook-488 to-be-attached-pod -i -c=container1'
  Aug 19 12:37:33.329: INFO: rc: 1
  Aug 19 12:37:33.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-488" for this suite. @ 08/19/23 12:37:33.395
  STEP: Destroying namespace "webhook-markers-533" for this suite. @ 08/19/23 12:37:33.406
• [5.492 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/19/23 12:37:33.415
  Aug 19 12:37:33.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context-test @ 08/19/23 12:37:33.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:33.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:33.434
  Aug 19 12:37:37.471: INFO: Got logs for pod "busybox-privileged-false-2d2315a7-bb18-4d33-8d05-59418106f567": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug 19 12:37:37.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-228" for this suite. @ 08/19/23 12:37:37.476
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/19/23 12:37:37.484
  Aug 19 12:37:37.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:37:37.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:37.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:37.505
  STEP: Creating configMap with name configmap-projected-all-test-volume-9ada5374-81be-4843-ad9a-e3c56383a50d @ 08/19/23 12:37:37.509
  STEP: Creating secret with name secret-projected-all-test-volume-9cb81477-7341-4148-9560-c90e909b5b76 @ 08/19/23 12:37:37.515
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/19/23 12:37:37.521
  STEP: Saw pod success @ 08/19/23 12:37:41.544
  Aug 19 12:37:41.549: INFO: Trying to get logs from node ip-172-31-15-214 pod projected-volume-97031c9f-3c58-4d14-8fbe-b4c7a619520d container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:37:41.557
  Aug 19 12:37:41.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3821" for this suite. @ 08/19/23 12:37:41.579
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/19/23 12:37:41.588
  Aug 19 12:37:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:37:41.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:41.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:41.611
  STEP: Creating secret with name projected-secret-test-5acaedf3-2404-4934-8b03-862d00157cb2 @ 08/19/23 12:37:41.615
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:37:41.619
  STEP: Saw pod success @ 08/19/23 12:37:45.643
  Aug 19 12:37:45.648: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-18e3e8d1-b21e-4b83-8ac0-bdb85e476522 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:37:45.656
  Aug 19 12:37:45.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7687" for this suite. @ 08/19/23 12:37:45.675
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/19/23 12:37:45.685
  Aug 19 12:37:45.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 12:37:45.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:45.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:45.709
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:37:45.713
  STEP: Saw pod success @ 08/19/23 12:37:49.736
  Aug 19 12:37:49.741: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-043adca3-14cf-413d-8f78-0e9d63605f23 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:37:49.749
  Aug 19 12:37:49.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1324" for this suite. @ 08/19/23 12:37:49.769
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/19/23 12:37:49.779
  Aug 19 12:37:49.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:37:49.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:49.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:49.798
  STEP: creating Agnhost RC @ 08/19/23 12:37:49.802
  Aug 19 12:37:49.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9453 create -f -'
  Aug 19 12:37:50.123: INFO: stderr: ""
  Aug 19 12:37:50.123: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/19/23 12:37:50.123
  Aug 19 12:37:51.129: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:37:51.129: INFO: Found 0 / 1
  Aug 19 12:37:52.127: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:37:52.127: INFO: Found 1 / 1
  Aug 19 12:37:52.127: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/19/23 12:37:52.127
  Aug 19 12:37:52.131: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:37:52.131: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 19 12:37:52.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9453 patch pod agnhost-primary-vhhdp -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug 19 12:37:52.198: INFO: stderr: ""
  Aug 19 12:37:52.198: INFO: stdout: "pod/agnhost-primary-vhhdp patched\n"
  STEP: checking annotations @ 08/19/23 12:37:52.198
  Aug 19 12:37:52.202: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:37:52.202: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 19 12:37:52.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9453" for this suite. @ 08/19/23 12:37:52.207
• [2.436 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/19/23 12:37:52.215
  Aug 19 12:37:52.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:37:52.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:52.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:52.234
  STEP: Setting up server cert @ 08/19/23 12:37:52.26
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:37:52.53
  STEP: Deploying the webhook pod @ 08/19/23 12:37:52.539
  STEP: Wait for the deployment to be ready @ 08/19/23 12:37:52.55
  Aug 19 12:37:52.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:37:54.57
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:37:54.583
  Aug 19 12:37:55.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/19/23 12:37:55.587
  STEP: create a pod that should be updated by the webhook @ 08/19/23 12:37:55.604
  Aug 19 12:37:55.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8273" for this suite. @ 08/19/23 12:37:55.68
  STEP: Destroying namespace "webhook-markers-9204" for this suite. @ 08/19/23 12:37:55.689
• [3.481 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/19/23 12:37:55.698
  Aug 19 12:37:55.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:37:55.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:55.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:55.717
  STEP: Read namespace status @ 08/19/23 12:37:55.721
  Aug 19 12:37:55.724: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/19/23 12:37:55.724
  Aug 19 12:37:55.731: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/19/23 12:37:55.731
  Aug 19 12:37:55.740: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug 19 12:37:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1691" for this suite. @ 08/19/23 12:37:55.743
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/19/23 12:37:55.752
  Aug 19 12:37:55.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/19/23 12:37:55.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:37:55.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:37:55.77
  STEP: create the container to handle the HTTPGet hook request. @ 08/19/23 12:37:55.779
  STEP: create the pod with lifecycle hook @ 08/19/23 12:37:57.802
  STEP: check poststart hook @ 08/19/23 12:37:59.822
  STEP: delete the pod with lifecycle hook @ 08/19/23 12:37:59.839
  Aug 19 12:38:01.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7095" for this suite. @ 08/19/23 12:38:01.861
• [6.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/19/23 12:38:01.87
  Aug 19 12:38:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename cronjob @ 08/19/23 12:38:01.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:01.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:01.895
  STEP: Creating a cronjob @ 08/19/23 12:38:01.898
  STEP: creating @ 08/19/23 12:38:01.898
  STEP: getting @ 08/19/23 12:38:01.906
  STEP: listing @ 08/19/23 12:38:01.91
  STEP: watching @ 08/19/23 12:38:01.914
  Aug 19 12:38:01.914: INFO: starting watch
  STEP: cluster-wide listing @ 08/19/23 12:38:01.915
  STEP: cluster-wide watching @ 08/19/23 12:38:01.919
  Aug 19 12:38:01.919: INFO: starting watch
  STEP: patching @ 08/19/23 12:38:01.92
  STEP: updating @ 08/19/23 12:38:01.927
  Aug 19 12:38:01.936: INFO: waiting for watch events with expected annotations
  Aug 19 12:38:01.936: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/19/23 12:38:01.936
  STEP: updating /status @ 08/19/23 12:38:01.944
  STEP: get /status @ 08/19/23 12:38:01.952
  STEP: deleting @ 08/19/23 12:38:01.956
  STEP: deleting a collection @ 08/19/23 12:38:01.975
  Aug 19 12:38:01.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2285" for this suite. @ 08/19/23 12:38:01.991
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/19/23 12:38:02
  Aug 19 12:38:02.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:38:02.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:02.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:02.024
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/19/23 12:38:02.028
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/19/23 12:38:02.028
  STEP: creating a pod to probe DNS @ 08/19/23 12:38:02.028
  STEP: submitting the pod to kubernetes @ 08/19/23 12:38:02.028
  STEP: retrieving the pod @ 08/19/23 12:38:04.051
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:38:04.054
  Aug 19 12:38:04.074: INFO: DNS probes using dns-5390/dns-test-6b4b2dfd-40fb-429a-9e28-e6f53a1ed419 succeeded

  Aug 19 12:38:04.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:38:04.078
  STEP: Destroying namespace "dns-5390" for this suite. @ 08/19/23 12:38:04.092
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/19/23 12:38:04.101
  Aug 19 12:38:04.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:38:04.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:04.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:04.124
  STEP: creating service in namespace services-7101 @ 08/19/23 12:38:04.127
  STEP: creating service affinity-clusterip in namespace services-7101 @ 08/19/23 12:38:04.127
  STEP: creating replication controller affinity-clusterip in namespace services-7101 @ 08/19/23 12:38:04.14
  I0819 12:38:04.149060      18 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7101, replica count: 3
  I0819 12:38:07.200361      18 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 12:38:07.210: INFO: Creating new exec pod
  Aug 19 12:38:10.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-7101 exec execpod-affinity7pfrq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Aug 19 12:38:10.365: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug 19 12:38:10.365: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:38:10.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-7101 exec execpod-affinity7pfrq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.75 80'
  Aug 19 12:38:10.508: INFO: stderr: "+ nc -v -t -w 2 10.152.183.75 80\nConnection to 10.152.183.75 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 12:38:10.508: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:38:10.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-7101 exec execpod-affinity7pfrq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.75:80/ ; done'
  Aug 19 12:38:10.710: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.75:80/\n"
  Aug 19 12:38:10.710: INFO: stdout: "\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf\naffinity-clusterip-k4jdf"
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Received response from host: affinity-clusterip-k4jdf
  Aug 19 12:38:10.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 12:38:10.716: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7101, will wait for the garbage collector to delete the pods @ 08/19/23 12:38:10.728
  Aug 19 12:38:10.791: INFO: Deleting ReplicationController affinity-clusterip took: 8.079731ms
  Aug 19 12:38:10.891: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.677682ms
  STEP: Destroying namespace "services-7101" for this suite. @ 08/19/23 12:38:12.921
• [8.832 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/19/23 12:38:12.935
  Aug 19 12:38:12.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:38:12.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:12.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:12.956
  STEP: Creating service test in namespace statefulset-2877 @ 08/19/23 12:38:12.96
  STEP: Creating statefulset ss in namespace statefulset-2877 @ 08/19/23 12:38:12.968
  Aug 19 12:38:12.983: INFO: Found 0 stateful pods, waiting for 1
  Aug 19 12:38:22.987: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/19/23 12:38:22.995
  STEP: Getting /status @ 08/19/23 12:38:23.005
  Aug 19 12:38:23.012: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/19/23 12:38:23.012
  Aug 19 12:38:23.023: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/19/23 12:38:23.024
  Aug 19 12:38:23.026: INFO: Observed &StatefulSet event: ADDED
  Aug 19 12:38:23.026: INFO: Found Statefulset ss in namespace statefulset-2877 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:38:23.026: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/19/23 12:38:23.026
  Aug 19 12:38:23.026: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 19 12:38:23.033: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/19/23 12:38:23.033
  Aug 19 12:38:23.035: INFO: Observed &StatefulSet event: ADDED
  Aug 19 12:38:23.035: INFO: Deleting all statefulset in ns statefulset-2877
  Aug 19 12:38:23.038: INFO: Scaling statefulset ss to 0
  Aug 19 12:38:33.060: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:38:33.064: INFO: Deleting statefulset ss
  Aug 19 12:38:33.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2877" for this suite. @ 08/19/23 12:38:33.088
• [20.163 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/19/23 12:38:33.099
  Aug 19 12:38:33.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:38:33.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:33.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:33.124
  STEP: Creating a test headless service @ 08/19/23 12:38:33.128
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_tcp@PTR;sleep 1; done
   @ 08/19/23 12:38:33.149
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_tcp@PTR;sleep 1; done
   @ 08/19/23 12:38:33.149
  STEP: creating a pod to probe DNS @ 08/19/23 12:38:33.149
  STEP: submitting the pod to kubernetes @ 08/19/23 12:38:33.149
  STEP: retrieving the pod @ 08/19/23 12:38:35.18
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:38:35.184
  Aug 19 12:38:35.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.197: INFO: Unable to read wheezy_tcp@dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.201: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.206: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.228: INFO: Unable to read jessie_udp@dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.233: INFO: Unable to read jessie_tcp@dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.237: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local from pod dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40: the server could not find the requested resource (get pods dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40)
  Aug 19 12:38:35.259: INFO: Lookups using dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40 failed for: [wheezy_udp@dns-test-service.dns-871.svc.cluster.local wheezy_tcp@dns-test-service.dns-871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local jessie_udp@dns-test-service.dns-871.svc.cluster.local jessie_tcp@dns-test-service.dns-871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-871.svc.cluster.local]

  Aug 19 12:38:40.335: INFO: DNS probes using dns-871/dns-test-01d369e4-67dc-4191-94b4-4685b5e8aa40 succeeded

  Aug 19 12:38:40.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:38:40.339
  STEP: deleting the test service @ 08/19/23 12:38:40.36
  STEP: deleting the test headless service @ 08/19/23 12:38:40.387
  STEP: Destroying namespace "dns-871" for this suite. @ 08/19/23 12:38:40.405
• [7.314 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/19/23 12:38:40.414
  Aug 19 12:38:40.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename init-container @ 08/19/23 12:38:40.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:40.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:40.439
  STEP: creating the pod @ 08/19/23 12:38:40.443
  Aug 19 12:38:40.443: INFO: PodSpec: initContainers in spec.initContainers
  Aug 19 12:38:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2675" for this suite. @ 08/19/23 12:38:43.892
• [3.487 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/19/23 12:38:43.901
  Aug 19 12:38:43.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 12:38:43.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:43.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:43.922
  STEP: create the deployment @ 08/19/23 12:38:43.925
  W0819 12:38:43.930167      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/19/23 12:38:43.93
  STEP: delete the deployment @ 08/19/23 12:38:44.438
  STEP: wait for all rs to be garbage collected @ 08/19/23 12:38:44.446
  STEP: expected 0 rs, got 1 rs @ 08/19/23 12:38:44.454
  STEP: expected 0 pods, got 2 pods @ 08/19/23 12:38:44.459
  STEP: Gathering metrics @ 08/19/23 12:38:44.971
  W0819 12:38:44.976011      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 12:38:44.976: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 12:38:44.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6688" for this suite. @ 08/19/23 12:38:44.981
• [1.087 seconds]
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/19/23 12:38:44.988
  Aug 19 12:38:44.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption @ 08/19/23 12:38:44.989
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:45.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:45.009
  STEP: Waiting for the pdb to be processed @ 08/19/23 12:38:45.018
  STEP: Waiting for all pods to be running @ 08/19/23 12:38:47.056
  Aug 19 12:38:47.061: INFO: running pods: 0 < 3
  Aug 19 12:38:49.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5637" for this suite. @ 08/19/23 12:38:49.075
• [4.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/19/23 12:38:49.084
  Aug 19 12:38:49.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename proxy @ 08/19/23 12:38:49.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:49.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:49.108
  STEP: starting an echo server on multiple ports @ 08/19/23 12:38:49.125
  STEP: creating replication controller proxy-service-tzvdx in namespace proxy-4537 @ 08/19/23 12:38:49.125
  I0819 12:38:49.134995      18 runners.go:194] Created replication controller with name: proxy-service-tzvdx, namespace: proxy-4537, replica count: 1
  I0819 12:38:50.185796      18 runners.go:194] proxy-service-tzvdx Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0819 12:38:51.186689      18 runners.go:194] proxy-service-tzvdx Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 12:38:51.191: INFO: setup took 2.078764495s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/19/23 12:38:51.191
  Aug 19 12:38:51.198: INFO: (0) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.328488ms)
  Aug 19 12:38:51.199: INFO: (0) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 8.022925ms)
  Aug 19 12:38:51.200: INFO: (0) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 8.911466ms)
  Aug 19 12:38:51.200: INFO: (0) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.646077ms)
  Aug 19 12:38:51.201: INFO: (0) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.539777ms)
  Aug 19 12:38:51.202: INFO: (0) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 11.008518ms)
  Aug 19 12:38:51.202: INFO: (0) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 11.077033ms)
  Aug 19 12:38:51.202: INFO: (0) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 11.489865ms)
  Aug 19 12:38:51.203: INFO: (0) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.665857ms)
  Aug 19 12:38:51.204: INFO: (0) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 13.006935ms)
  Aug 19 12:38:51.206: INFO: (0) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 14.962993ms)
  Aug 19 12:38:51.207: INFO: (0) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 15.881333ms)
  Aug 19 12:38:51.207: INFO: (0) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 15.985937ms)
  Aug 19 12:38:51.208: INFO: (0) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 16.672352ms)
  Aug 19 12:38:51.209: INFO: (0) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 17.94529ms)
  Aug 19 12:38:51.209: INFO: (0) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 18.401216ms)
  Aug 19 12:38:51.215: INFO: (1) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 5.140696ms)
  Aug 19 12:38:51.215: INFO: (1) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 5.384066ms)
  Aug 19 12:38:51.217: INFO: (1) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.196629ms)
  Aug 19 12:38:51.217: INFO: (1) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 7.772561ms)
  Aug 19 12:38:51.218: INFO: (1) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 7.805775ms)
  Aug 19 12:38:51.218: INFO: (1) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.949684ms)
  Aug 19 12:38:51.219: INFO: (1) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 8.992547ms)
  Aug 19 12:38:51.219: INFO: (1) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 8.866559ms)
  Aug 19 12:38:51.219: INFO: (1) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.567008ms)
  Aug 19 12:38:51.219: INFO: (1) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.377294ms)
  Aug 19 12:38:51.220: INFO: (1) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.157602ms)
  Aug 19 12:38:51.221: INFO: (1) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 11.427454ms)
  Aug 19 12:38:51.223: INFO: (1) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 13.224501ms)
  Aug 19 12:38:51.223: INFO: (1) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 13.287968ms)
  Aug 19 12:38:51.223: INFO: (1) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 13.540701ms)
  Aug 19 12:38:51.225: INFO: (1) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 15.122218ms)
  Aug 19 12:38:51.230: INFO: (2) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 4.551947ms)
  Aug 19 12:38:51.230: INFO: (2) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 4.994368ms)
  Aug 19 12:38:51.231: INFO: (2) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 6.082162ms)
  Aug 19 12:38:51.232: INFO: (2) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 6.186112ms)
  Aug 19 12:38:51.233: INFO: (2) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 7.480004ms)
  Aug 19 12:38:51.233: INFO: (2) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 7.955022ms)
  Aug 19 12:38:51.233: INFO: (2) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 7.76154ms)
  Aug 19 12:38:51.234: INFO: (2) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 8.252877ms)
  Aug 19 12:38:51.234: INFO: (2) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 8.442801ms)
  Aug 19 12:38:51.234: INFO: (2) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 8.967318ms)
  Aug 19 12:38:51.235: INFO: (2) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 9.80024ms)
  Aug 19 12:38:51.235: INFO: (2) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.488681ms)
  Aug 19 12:38:51.235: INFO: (2) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.95952ms)
  Aug 19 12:38:51.236: INFO: (2) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.19098ms)
  Aug 19 12:38:51.236: INFO: (2) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.360156ms)
  Aug 19 12:38:51.237: INFO: (2) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.336364ms)
  Aug 19 12:38:51.242: INFO: (3) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 4.994281ms)
  Aug 19 12:38:51.243: INFO: (3) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 5.65552ms)
  Aug 19 12:38:51.243: INFO: (3) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 5.741427ms)
  Aug 19 12:38:51.244: INFO: (3) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 7.132517ms)
  Aug 19 12:38:51.244: INFO: (3) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.06797ms)
  Aug 19 12:38:51.245: INFO: (3) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 8.194017ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.346393ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.283982ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.219905ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 9.547931ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 9.624229ms)
  Aug 19 12:38:51.247: INFO: (3) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.93104ms)
  Aug 19 12:38:51.248: INFO: (3) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.387226ms)
  Aug 19 12:38:51.248: INFO: (3) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 10.128449ms)
  Aug 19 12:38:51.248: INFO: (3) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 10.590479ms)
  Aug 19 12:38:51.249: INFO: (3) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.95708ms)
  Aug 19 12:38:51.254: INFO: (4) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 4.684216ms)
  Aug 19 12:38:51.255: INFO: (4) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.58531ms)
  Aug 19 12:38:51.257: INFO: (4) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 6.798426ms)
  Aug 19 12:38:51.258: INFO: (4) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 8.109279ms)
  Aug 19 12:38:51.258: INFO: (4) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 8.602946ms)
  Aug 19 12:38:51.258: INFO: (4) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.535056ms)
  Aug 19 12:38:51.259: INFO: (4) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.880262ms)
  Aug 19 12:38:51.259: INFO: (4) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 9.091182ms)
  Aug 19 12:38:51.259: INFO: (4) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.247797ms)
  Aug 19 12:38:51.260: INFO: (4) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.51218ms)
  Aug 19 12:38:51.266: INFO: (4) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 16.698453ms)
  Aug 19 12:38:51.268: INFO: (4) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 18.189161ms)
  Aug 19 12:38:51.268: INFO: (4) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 18.337122ms)
  Aug 19 12:38:51.269: INFO: (4) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 18.78378ms)
  Aug 19 12:38:51.269: INFO: (4) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 19.060325ms)
  Aug 19 12:38:51.271: INFO: (4) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 21.085952ms)
  Aug 19 12:38:51.275: INFO: (5) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 4.714659ms)
  Aug 19 12:38:51.277: INFO: (5) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 5.809065ms)
  Aug 19 12:38:51.277: INFO: (5) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 6.274069ms)
  Aug 19 12:38:51.278: INFO: (5) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.247386ms)
  Aug 19 12:38:51.279: INFO: (5) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 7.766772ms)
  Aug 19 12:38:51.279: INFO: (5) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 8.447948ms)
  Aug 19 12:38:51.279: INFO: (5) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.175741ms)
  Aug 19 12:38:51.280: INFO: (5) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.769283ms)
  Aug 19 12:38:51.280: INFO: (5) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 9.579837ms)
  Aug 19 12:38:51.281: INFO: (5) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 9.535985ms)
  Aug 19 12:38:51.281: INFO: (5) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 9.910526ms)
  Aug 19 12:38:51.281: INFO: (5) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 10.144819ms)
  Aug 19 12:38:51.281: INFO: (5) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.6169ms)
  Aug 19 12:38:51.282: INFO: (5) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.764373ms)
  Aug 19 12:38:51.284: INFO: (5) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 12.553565ms)
  Aug 19 12:38:51.284: INFO: (5) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 12.693056ms)
  Aug 19 12:38:51.289: INFO: (6) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 5.256844ms)
  Aug 19 12:38:51.290: INFO: (6) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 5.585775ms)
  Aug 19 12:38:51.290: INFO: (6) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 5.859097ms)
  Aug 19 12:38:51.292: INFO: (6) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.087858ms)
  Aug 19 12:38:51.293: INFO: (6) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.964921ms)
  Aug 19 12:38:51.293: INFO: (6) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 8.365683ms)
  Aug 19 12:38:51.293: INFO: (6) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 9.122339ms)
  Aug 19 12:38:51.294: INFO: (6) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 9.865642ms)
  Aug 19 12:38:51.294: INFO: (6) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 9.390005ms)
  Aug 19 12:38:51.294: INFO: (6) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 9.515044ms)
  Aug 19 12:38:51.294: INFO: (6) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.102338ms)
  Aug 19 12:38:51.295: INFO: (6) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 10.435885ms)
  Aug 19 12:38:51.295: INFO: (6) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.958436ms)
  Aug 19 12:38:51.295: INFO: (6) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.975066ms)
  Aug 19 12:38:51.296: INFO: (6) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.430276ms)
  Aug 19 12:38:51.296: INFO: (6) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.770545ms)
  Aug 19 12:38:51.302: INFO: (7) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 5.303891ms)
  Aug 19 12:38:51.302: INFO: (7) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.786802ms)
  Aug 19 12:38:51.302: INFO: (7) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 5.473801ms)
  Aug 19 12:38:51.303: INFO: (7) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 6.392151ms)
  Aug 19 12:38:51.304: INFO: (7) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.205579ms)
  Aug 19 12:38:51.305: INFO: (7) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.749414ms)
  Aug 19 12:38:51.305: INFO: (7) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.480824ms)
  Aug 19 12:38:51.305: INFO: (7) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.275067ms)
  Aug 19 12:38:51.306: INFO: (7) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 9.172404ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.783295ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.39766ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 9.963054ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.128391ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.600459ms)
  Aug 19 12:38:51.307: INFO: (7) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 10.367872ms)
  Aug 19 12:38:51.308: INFO: (7) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 10.902972ms)
  Aug 19 12:38:51.313: INFO: (8) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 4.998177ms)
  Aug 19 12:38:51.313: INFO: (8) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 5.165192ms)
  Aug 19 12:38:51.314: INFO: (8) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 6.341667ms)
  Aug 19 12:38:51.315: INFO: (8) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 7.129799ms)
  Aug 19 12:38:51.316: INFO: (8) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.096775ms)
  Aug 19 12:38:51.316: INFO: (8) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.449389ms)
  Aug 19 12:38:51.317: INFO: (8) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 8.606405ms)
  Aug 19 12:38:51.318: INFO: (8) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.709953ms)
  Aug 19 12:38:51.318: INFO: (8) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 9.79877ms)
  Aug 19 12:38:51.318: INFO: (8) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.991155ms)
  Aug 19 12:38:51.318: INFO: (8) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 10.130322ms)
  Aug 19 12:38:51.319: INFO: (8) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 10.363944ms)
  Aug 19 12:38:51.319: INFO: (8) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.535014ms)
  Aug 19 12:38:51.319: INFO: (8) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 10.971129ms)
  Aug 19 12:38:51.320: INFO: (8) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 11.567354ms)
  Aug 19 12:38:51.320: INFO: (8) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 12.182602ms)
  Aug 19 12:38:51.325: INFO: (9) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.106447ms)
  Aug 19 12:38:51.326: INFO: (9) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 5.8034ms)
  Aug 19 12:38:51.328: INFO: (9) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 7.789609ms)
  Aug 19 12:38:51.329: INFO: (9) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.728286ms)
  Aug 19 12:38:51.329: INFO: (9) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.08192ms)
  Aug 19 12:38:51.329: INFO: (9) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 8.317024ms)
  Aug 19 12:38:51.330: INFO: (9) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 9.315961ms)
  Aug 19 12:38:51.330: INFO: (9) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.347933ms)
  Aug 19 12:38:51.330: INFO: (9) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.30377ms)
  Aug 19 12:38:51.330: INFO: (9) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.069829ms)
  Aug 19 12:38:51.331: INFO: (9) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.213468ms)
  Aug 19 12:38:51.331: INFO: (9) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 10.62469ms)
  Aug 19 12:38:51.332: INFO: (9) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.093866ms)
  Aug 19 12:38:51.332: INFO: (9) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.956164ms)
  Aug 19 12:38:51.332: INFO: (9) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 11.642385ms)
  Aug 19 12:38:51.332: INFO: (9) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.643739ms)
  Aug 19 12:38:51.338: INFO: (10) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.372682ms)
  Aug 19 12:38:51.339: INFO: (10) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 6.22721ms)
  Aug 19 12:38:51.340: INFO: (10) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 6.815098ms)
  Aug 19 12:38:51.341: INFO: (10) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 7.496004ms)
  Aug 19 12:38:51.341: INFO: (10) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 8.720394ms)
  Aug 19 12:38:51.341: INFO: (10) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.347889ms)
  Aug 19 12:38:51.342: INFO: (10) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.755822ms)
  Aug 19 12:38:51.342: INFO: (10) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.64802ms)
  Aug 19 12:38:51.343: INFO: (10) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.458199ms)
  Aug 19 12:38:51.344: INFO: (10) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.173162ms)
  Aug 19 12:38:51.344: INFO: (10) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 11.317971ms)
  Aug 19 12:38:51.344: INFO: (10) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.171889ms)
  Aug 19 12:38:51.344: INFO: (10) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 11.331974ms)
  Aug 19 12:38:51.345: INFO: (10) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 11.9113ms)
  Aug 19 12:38:51.345: INFO: (10) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 12.310118ms)
  Aug 19 12:38:51.346: INFO: (10) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 13.209981ms)
  Aug 19 12:38:51.351: INFO: (11) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 5.373645ms)
  Aug 19 12:38:51.352: INFO: (11) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 6.349144ms)
  Aug 19 12:38:51.353: INFO: (11) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 6.449156ms)
  Aug 19 12:38:51.353: INFO: (11) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.194179ms)
  Aug 19 12:38:51.353: INFO: (11) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 6.936845ms)
  Aug 19 12:38:51.354: INFO: (11) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 7.54248ms)
  Aug 19 12:38:51.354: INFO: (11) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 8.019964ms)
  Aug 19 12:38:51.355: INFO: (11) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.325066ms)
  Aug 19 12:38:51.355: INFO: (11) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 8.836886ms)
  Aug 19 12:38:51.355: INFO: (11) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.022465ms)
  Aug 19 12:38:51.356: INFO: (11) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.284053ms)
  Aug 19 12:38:51.356: INFO: (11) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.591221ms)
  Aug 19 12:38:51.357: INFO: (11) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.259532ms)
  Aug 19 12:38:51.357: INFO: (11) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.716003ms)
  Aug 19 12:38:51.358: INFO: (11) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 11.33119ms)
  Aug 19 12:38:51.358: INFO: (11) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.898611ms)
  Aug 19 12:38:51.363: INFO: (12) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 4.951234ms)
  Aug 19 12:38:51.364: INFO: (12) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.308165ms)
  Aug 19 12:38:51.366: INFO: (12) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 7.350996ms)
  Aug 19 12:38:51.366: INFO: (12) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 7.470932ms)
  Aug 19 12:38:51.367: INFO: (12) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 7.8792ms)
  Aug 19 12:38:51.367: INFO: (12) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.83601ms)
  Aug 19 12:38:51.368: INFO: (12) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 8.922536ms)
  Aug 19 12:38:51.369: INFO: (12) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 10.136684ms)
  Aug 19 12:38:51.369: INFO: (12) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 10.514162ms)
  Aug 19 12:38:51.369: INFO: (12) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.620304ms)
  Aug 19 12:38:51.369: INFO: (12) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.548601ms)
  Aug 19 12:38:51.369: INFO: (12) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.883329ms)
  Aug 19 12:38:51.370: INFO: (12) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.756061ms)
  Aug 19 12:38:51.370: INFO: (12) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 11.549109ms)
  Aug 19 12:38:51.371: INFO: (12) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 12.321248ms)
  Aug 19 12:38:51.371: INFO: (12) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 12.503435ms)
  Aug 19 12:38:51.377: INFO: (13) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 5.683526ms)
  Aug 19 12:38:51.378: INFO: (13) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 6.791371ms)
  Aug 19 12:38:51.379: INFO: (13) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 6.969741ms)
  Aug 19 12:38:51.380: INFO: (13) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 7.255562ms)
  Aug 19 12:38:51.380: INFO: (13) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.838641ms)
  Aug 19 12:38:51.384: INFO: (13) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 12.555064ms)
  Aug 19 12:38:51.385: INFO: (13) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 13.143271ms)
  Aug 19 12:38:51.385: INFO: (13) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 12.492283ms)
  Aug 19 12:38:51.385: INFO: (13) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 13.422804ms)
  Aug 19 12:38:51.385: INFO: (13) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 13.224406ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 13.786728ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 13.127014ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 13.86241ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 14.540252ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 14.098059ms)
  Aug 19 12:38:51.386: INFO: (13) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 14.196193ms)
  Aug 19 12:38:51.396: INFO: (14) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 9.301253ms)
  Aug 19 12:38:51.396: INFO: (14) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 9.560944ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.156941ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 9.7815ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 9.842206ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.934005ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.225356ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 10.547935ms)
  Aug 19 12:38:51.397: INFO: (14) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 10.582944ms)
  Aug 19 12:38:51.398: INFO: (14) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 10.501032ms)
  Aug 19 12:38:51.398: INFO: (14) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 10.824906ms)
  Aug 19 12:38:51.398: INFO: (14) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.731022ms)
  Aug 19 12:38:51.398: INFO: (14) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.854146ms)
  Aug 19 12:38:51.398: INFO: (14) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 11.793841ms)
  Aug 19 12:38:51.399: INFO: (14) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.879796ms)
  Aug 19 12:38:51.400: INFO: (14) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 12.970849ms)
  Aug 19 12:38:51.405: INFO: (15) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 4.621638ms)
  Aug 19 12:38:51.406: INFO: (15) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 5.78443ms)
  Aug 19 12:38:51.406: INFO: (15) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 5.594051ms)
  Aug 19 12:38:51.407: INFO: (15) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 6.418225ms)
  Aug 19 12:38:51.407: INFO: (15) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 7.02103ms)
  Aug 19 12:38:51.408: INFO: (15) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.863998ms)
  Aug 19 12:38:51.409: INFO: (15) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.482035ms)
  Aug 19 12:38:51.409: INFO: (15) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.851121ms)
  Aug 19 12:38:51.410: INFO: (15) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.90614ms)
  Aug 19 12:38:51.410: INFO: (15) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.067911ms)
  Aug 19 12:38:51.410: INFO: (15) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 10.3324ms)
  Aug 19 12:38:51.411: INFO: (15) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.864973ms)
  Aug 19 12:38:51.411: INFO: (15) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.939848ms)
  Aug 19 12:38:51.411: INFO: (15) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 11.142916ms)
  Aug 19 12:38:51.412: INFO: (15) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.377966ms)
  Aug 19 12:38:51.412: INFO: (15) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.822556ms)
  Aug 19 12:38:51.418: INFO: (16) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 5.287163ms)
  Aug 19 12:38:51.419: INFO: (16) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 6.578476ms)
  Aug 19 12:38:51.421: INFO: (16) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 8.260473ms)
  Aug 19 12:38:51.422: INFO: (16) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 9.28811ms)
  Aug 19 12:38:51.422: INFO: (16) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 9.876207ms)
  Aug 19 12:38:51.423: INFO: (16) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 10.072001ms)
  Aug 19 12:38:51.423: INFO: (16) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 10.185491ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.725624ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 11.673834ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.786367ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 11.845078ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 11.702079ms)
  Aug 19 12:38:51.425: INFO: (16) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 12.226839ms)
  Aug 19 12:38:51.424: INFO: (16) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 11.881004ms)
  Aug 19 12:38:51.425: INFO: (16) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 12.740418ms)
  Aug 19 12:38:51.425: INFO: (16) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 12.831892ms)
  Aug 19 12:38:51.432: INFO: (17) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 5.450205ms)
  Aug 19 12:38:51.432: INFO: (17) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 6.190994ms)
  Aug 19 12:38:51.433: INFO: (17) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 6.795728ms)
  Aug 19 12:38:51.433: INFO: (17) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.49417ms)
  Aug 19 12:38:51.434: INFO: (17) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 7.771403ms)
  Aug 19 12:38:51.435: INFO: (17) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 9.148437ms)
  Aug 19 12:38:51.435: INFO: (17) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 8.409592ms)
  Aug 19 12:38:51.435: INFO: (17) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.567344ms)
  Aug 19 12:38:51.436: INFO: (17) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 9.419504ms)
  Aug 19 12:38:51.437: INFO: (17) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 10.137933ms)
  Aug 19 12:38:51.437: INFO: (17) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.295816ms)
  Aug 19 12:38:51.437: INFO: (17) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 10.538584ms)
  Aug 19 12:38:51.437: INFO: (17) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.697978ms)
  Aug 19 12:38:51.437: INFO: (17) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 11.182625ms)
  Aug 19 12:38:51.439: INFO: (17) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 12.377247ms)
  Aug 19 12:38:51.439: INFO: (17) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 12.629595ms)
  Aug 19 12:38:51.444: INFO: (18) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 5.123078ms)
  Aug 19 12:38:51.445: INFO: (18) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 5.913404ms)
  Aug 19 12:38:51.446: INFO: (18) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 6.289945ms)
  Aug 19 12:38:51.446: INFO: (18) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 7.043842ms)
  Aug 19 12:38:51.447: INFO: (18) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 7.369115ms)
  Aug 19 12:38:51.447: INFO: (18) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 7.93136ms)
  Aug 19 12:38:51.448: INFO: (18) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 8.461721ms)
  Aug 19 12:38:51.449: INFO: (18) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 9.904832ms)
  Aug 19 12:38:51.450: INFO: (18) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 10.264434ms)
  Aug 19 12:38:51.450: INFO: (18) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 10.546493ms)
  Aug 19 12:38:51.450: INFO: (18) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 10.612978ms)
  Aug 19 12:38:51.450: INFO: (18) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.249255ms)
  Aug 19 12:38:51.451: INFO: (18) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 11.920506ms)
  Aug 19 12:38:51.451: INFO: (18) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 11.65733ms)
  Aug 19 12:38:51.451: INFO: (18) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 11.781887ms)
  Aug 19 12:38:51.451: INFO: (18) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.626832ms)
  Aug 19 12:38:51.456: INFO: (19) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:1080/proxy/rewriteme">test<... (200; 4.899932ms)
  Aug 19 12:38:51.457: INFO: (19) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:160/proxy/: foo (200; 5.18126ms)
  Aug 19 12:38:51.457: INFO: (19) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:1080/proxy/rewriteme">... (200; 5.796884ms)
  Aug 19 12:38:51.459: INFO: (19) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:162/proxy/: bar (200; 6.768853ms)
  Aug 19 12:38:51.460: INFO: (19) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:462/proxy/: tls qux (200; 7.5298ms)
  Aug 19 12:38:51.460: INFO: (19) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:460/proxy/: tls baz (200; 7.934949ms)
  Aug 19 12:38:51.460: INFO: (19) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789/proxy/rewriteme">test</a> (200; 8.324311ms)
  Aug 19 12:38:51.460: INFO: (19) /api/v1/namespaces/proxy-4537/pods/http:proxy-service-tzvdx-kf789:160/proxy/: foo (200; 8.57776ms)
  Aug 19 12:38:51.461: INFO: (19) /api/v1/namespaces/proxy-4537/pods/proxy-service-tzvdx-kf789:162/proxy/: bar (200; 9.689138ms)
  Aug 19 12:38:51.461: INFO: (19) /api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/: <a href="/api/v1/namespaces/proxy-4537/pods/https:proxy-service-tzvdx-kf789:443/proxy/tlsrewritem... (200; 9.631819ms)
  Aug 19 12:38:51.462: INFO: (19) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname1/proxy/: foo (200; 10.125044ms)
  Aug 19 12:38:51.462: INFO: (19) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname2/proxy/: tls qux (200; 10.515523ms)
  Aug 19 12:38:51.462: INFO: (19) /api/v1/namespaces/proxy-4537/services/https:proxy-service-tzvdx:tlsportname1/proxy/: tls baz (200; 10.561144ms)
  Aug 19 12:38:51.463: INFO: (19) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname2/proxy/: bar (200; 11.174368ms)
  Aug 19 12:38:51.463: INFO: (19) /api/v1/namespaces/proxy-4537/services/http:proxy-service-tzvdx:portname1/proxy/: foo (200; 11.321746ms)
  Aug 19 12:38:51.464: INFO: (19) /api/v1/namespaces/proxy-4537/services/proxy-service-tzvdx:portname2/proxy/: bar (200; 11.575857ms)
  Aug 19 12:38:51.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-tzvdx in namespace proxy-4537, will wait for the garbage collector to delete the pods @ 08/19/23 12:38:51.468
  Aug 19 12:38:51.530: INFO: Deleting ReplicationController proxy-service-tzvdx took: 8.574113ms
  Aug 19 12:38:51.632: INFO: Terminating ReplicationController proxy-service-tzvdx pods took: 102.557154ms
  STEP: Destroying namespace "proxy-4537" for this suite. @ 08/19/23 12:38:53.934
• [4.858 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/19/23 12:38:53.943
  Aug 19 12:38:53.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:38:53.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:53.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:53.965
  STEP: Setting up server cert @ 08/19/23 12:38:54.002
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:38:54.521
  STEP: Deploying the webhook pod @ 08/19/23 12:38:54.532
  STEP: Wait for the deployment to be ready @ 08/19/23 12:38:54.543
  Aug 19 12:38:54.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/19/23 12:38:56.567
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:38:56.581
  Aug 19 12:38:57.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/19/23 12:38:57.654
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/19/23 12:38:57.696
  STEP: Deleting the collection of validation webhooks @ 08/19/23 12:38:57.731
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/19/23 12:38:57.787
  Aug 19 12:38:57.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2519" for this suite. @ 08/19/23 12:38:57.849
  STEP: Destroying namespace "webhook-markers-7437" for this suite. @ 08/19/23 12:38:57.858
• [3.921 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/19/23 12:38:57.866
  Aug 19 12:38:57.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename events @ 08/19/23 12:38:57.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:57.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:57.886
  STEP: creating a test event @ 08/19/23 12:38:57.889
  STEP: listing events in all namespaces @ 08/19/23 12:38:57.897
  STEP: listing events in test namespace @ 08/19/23 12:38:57.907
  STEP: listing events with field selection filtering on source @ 08/19/23 12:38:57.91
  STEP: listing events with field selection filtering on reportingController @ 08/19/23 12:38:57.913
  STEP: getting the test event @ 08/19/23 12:38:57.917
  STEP: patching the test event @ 08/19/23 12:38:57.921
  STEP: getting the test event @ 08/19/23 12:38:57.93
  STEP: updating the test event @ 08/19/23 12:38:57.934
  STEP: getting the test event @ 08/19/23 12:38:57.941
  STEP: deleting the test event @ 08/19/23 12:38:57.945
  STEP: listing events in all namespaces @ 08/19/23 12:38:57.953
  STEP: listing events in test namespace @ 08/19/23 12:38:57.963
  Aug 19 12:38:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5451" for this suite. @ 08/19/23 12:38:57.97
• [0.110 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/19/23 12:38:57.977
  Aug 19 12:38:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 12:38:57.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:38:57.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:38:57.998
  STEP: create the rc @ 08/19/23 12:38:58.005
  W0819 12:38:58.012909      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 08/19/23 12:39:04.022
  STEP: wait for the rc to be deleted @ 08/19/23 12:39:04.042
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/19/23 12:39:09.061
  STEP: Gathering metrics @ 08/19/23 12:39:39.074
  W0819 12:39:39.078699      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 12:39:39.078: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 12:39:39.078: INFO: Deleting pod "simpletest.rc-24vj7" in namespace "gc-5881"
  Aug 19 12:39:39.092: INFO: Deleting pod "simpletest.rc-28gxk" in namespace "gc-5881"
  Aug 19 12:39:39.105: INFO: Deleting pod "simpletest.rc-2cxbn" in namespace "gc-5881"
  Aug 19 12:39:39.117: INFO: Deleting pod "simpletest.rc-2f2ql" in namespace "gc-5881"
  Aug 19 12:39:39.132: INFO: Deleting pod "simpletest.rc-2pnvs" in namespace "gc-5881"
  Aug 19 12:39:39.148: INFO: Deleting pod "simpletest.rc-2wjfm" in namespace "gc-5881"
  Aug 19 12:39:39.160: INFO: Deleting pod "simpletest.rc-2zwmn" in namespace "gc-5881"
  Aug 19 12:39:39.176: INFO: Deleting pod "simpletest.rc-48ggd" in namespace "gc-5881"
  Aug 19 12:39:39.187: INFO: Deleting pod "simpletest.rc-4t9nd" in namespace "gc-5881"
  Aug 19 12:39:39.199: INFO: Deleting pod "simpletest.rc-4vbsx" in namespace "gc-5881"
  Aug 19 12:39:39.214: INFO: Deleting pod "simpletest.rc-5d9sm" in namespace "gc-5881"
  Aug 19 12:39:39.229: INFO: Deleting pod "simpletest.rc-5g5br" in namespace "gc-5881"
  Aug 19 12:39:39.242: INFO: Deleting pod "simpletest.rc-5h8p5" in namespace "gc-5881"
  Aug 19 12:39:39.256: INFO: Deleting pod "simpletest.rc-5n87r" in namespace "gc-5881"
  Aug 19 12:39:39.269: INFO: Deleting pod "simpletest.rc-5s76z" in namespace "gc-5881"
  Aug 19 12:39:39.286: INFO: Deleting pod "simpletest.rc-5w222" in namespace "gc-5881"
  Aug 19 12:39:39.300: INFO: Deleting pod "simpletest.rc-6mhzg" in namespace "gc-5881"
  Aug 19 12:39:39.313: INFO: Deleting pod "simpletest.rc-6xnb4" in namespace "gc-5881"
  Aug 19 12:39:39.329: INFO: Deleting pod "simpletest.rc-6zjsn" in namespace "gc-5881"
  Aug 19 12:39:39.344: INFO: Deleting pod "simpletest.rc-74d8t" in namespace "gc-5881"
  Aug 19 12:39:39.353: INFO: Deleting pod "simpletest.rc-774mg" in namespace "gc-5881"
  Aug 19 12:39:39.366: INFO: Deleting pod "simpletest.rc-7765p" in namespace "gc-5881"
  Aug 19 12:39:39.385: INFO: Deleting pod "simpletest.rc-78q6h" in namespace "gc-5881"
  Aug 19 12:39:39.400: INFO: Deleting pod "simpletest.rc-7dfvf" in namespace "gc-5881"
  Aug 19 12:39:39.421: INFO: Deleting pod "simpletest.rc-7gwbr" in namespace "gc-5881"
  Aug 19 12:39:39.440: INFO: Deleting pod "simpletest.rc-7jnjp" in namespace "gc-5881"
  Aug 19 12:39:39.455: INFO: Deleting pod "simpletest.rc-7pqqv" in namespace "gc-5881"
  Aug 19 12:39:39.468: INFO: Deleting pod "simpletest.rc-7tt74" in namespace "gc-5881"
  Aug 19 12:39:39.480: INFO: Deleting pod "simpletest.rc-9nk8n" in namespace "gc-5881"
  Aug 19 12:39:39.494: INFO: Deleting pod "simpletest.rc-9zdh8" in namespace "gc-5881"
  Aug 19 12:39:39.510: INFO: Deleting pod "simpletest.rc-bgct7" in namespace "gc-5881"
  Aug 19 12:39:39.525: INFO: Deleting pod "simpletest.rc-bls2g" in namespace "gc-5881"
  Aug 19 12:39:39.538: INFO: Deleting pod "simpletest.rc-btpsb" in namespace "gc-5881"
  Aug 19 12:39:39.550: INFO: Deleting pod "simpletest.rc-bvzwt" in namespace "gc-5881"
  Aug 19 12:39:39.566: INFO: Deleting pod "simpletest.rc-c2pnj" in namespace "gc-5881"
  Aug 19 12:39:39.580: INFO: Deleting pod "simpletest.rc-cbgnw" in namespace "gc-5881"
  Aug 19 12:39:39.594: INFO: Deleting pod "simpletest.rc-cfplp" in namespace "gc-5881"
  Aug 19 12:39:39.608: INFO: Deleting pod "simpletest.rc-cj2tk" in namespace "gc-5881"
  Aug 19 12:39:39.623: INFO: Deleting pod "simpletest.rc-ckcz6" in namespace "gc-5881"
  Aug 19 12:39:39.639: INFO: Deleting pod "simpletest.rc-cqn4b" in namespace "gc-5881"
  Aug 19 12:39:39.658: INFO: Deleting pod "simpletest.rc-ctvkn" in namespace "gc-5881"
  Aug 19 12:39:39.675: INFO: Deleting pod "simpletest.rc-dm57v" in namespace "gc-5881"
  Aug 19 12:39:39.699: INFO: Deleting pod "simpletest.rc-dwxbm" in namespace "gc-5881"
  Aug 19 12:39:39.711: INFO: Deleting pod "simpletest.rc-f2t27" in namespace "gc-5881"
  Aug 19 12:39:39.728: INFO: Deleting pod "simpletest.rc-fs4r7" in namespace "gc-5881"
  Aug 19 12:39:39.742: INFO: Deleting pod "simpletest.rc-fxgxr" in namespace "gc-5881"
  Aug 19 12:39:39.755: INFO: Deleting pod "simpletest.rc-gt2wp" in namespace "gc-5881"
  Aug 19 12:39:39.768: INFO: Deleting pod "simpletest.rc-hjbw4" in namespace "gc-5881"
  Aug 19 12:39:39.779: INFO: Deleting pod "simpletest.rc-hk6c2" in namespace "gc-5881"
  Aug 19 12:39:39.795: INFO: Deleting pod "simpletest.rc-htnln" in namespace "gc-5881"
  Aug 19 12:39:39.808: INFO: Deleting pod "simpletest.rc-j6fs2" in namespace "gc-5881"
  Aug 19 12:39:39.821: INFO: Deleting pod "simpletest.rc-jjnr4" in namespace "gc-5881"
  Aug 19 12:39:39.834: INFO: Deleting pod "simpletest.rc-jlv7z" in namespace "gc-5881"
  Aug 19 12:39:39.848: INFO: Deleting pod "simpletest.rc-kb4bl" in namespace "gc-5881"
  Aug 19 12:39:39.861: INFO: Deleting pod "simpletest.rc-kxff4" in namespace "gc-5881"
  Aug 19 12:39:39.876: INFO: Deleting pod "simpletest.rc-l5j8t" in namespace "gc-5881"
  Aug 19 12:39:39.895: INFO: Deleting pod "simpletest.rc-lcrc6" in namespace "gc-5881"
  Aug 19 12:39:39.912: INFO: Deleting pod "simpletest.rc-lscjc" in namespace "gc-5881"
  Aug 19 12:39:39.923: INFO: Deleting pod "simpletest.rc-lwdwt" in namespace "gc-5881"
  Aug 19 12:39:39.938: INFO: Deleting pod "simpletest.rc-m2k78" in namespace "gc-5881"
  Aug 19 12:39:39.954: INFO: Deleting pod "simpletest.rc-m42r2" in namespace "gc-5881"
  Aug 19 12:39:39.974: INFO: Deleting pod "simpletest.rc-mfb9p" in namespace "gc-5881"
  Aug 19 12:39:39.986: INFO: Deleting pod "simpletest.rc-mlc56" in namespace "gc-5881"
  Aug 19 12:39:39.999: INFO: Deleting pod "simpletest.rc-mnhk5" in namespace "gc-5881"
  Aug 19 12:39:40.013: INFO: Deleting pod "simpletest.rc-nf9z2" in namespace "gc-5881"
  Aug 19 12:39:40.026: INFO: Deleting pod "simpletest.rc-nsmpd" in namespace "gc-5881"
  Aug 19 12:39:40.043: INFO: Deleting pod "simpletest.rc-p4d72" in namespace "gc-5881"
  Aug 19 12:39:40.059: INFO: Deleting pod "simpletest.rc-prh9r" in namespace "gc-5881"
  Aug 19 12:39:40.076: INFO: Deleting pod "simpletest.rc-px866" in namespace "gc-5881"
  Aug 19 12:39:40.123: INFO: Deleting pod "simpletest.rc-q5hz9" in namespace "gc-5881"
  Aug 19 12:39:40.173: INFO: Deleting pod "simpletest.rc-q97hn" in namespace "gc-5881"
  Aug 19 12:39:40.227: INFO: Deleting pod "simpletest.rc-qcdml" in namespace "gc-5881"
  Aug 19 12:39:40.280: INFO: Deleting pod "simpletest.rc-qjkhq" in namespace "gc-5881"
  Aug 19 12:39:40.323: INFO: Deleting pod "simpletest.rc-qmcgn" in namespace "gc-5881"
  Aug 19 12:39:40.377: INFO: Deleting pod "simpletest.rc-qrv26" in namespace "gc-5881"
  Aug 19 12:39:40.427: INFO: Deleting pod "simpletest.rc-rfg4j" in namespace "gc-5881"
  Aug 19 12:39:40.478: INFO: Deleting pod "simpletest.rc-rkq86" in namespace "gc-5881"
  Aug 19 12:39:40.524: INFO: Deleting pod "simpletest.rc-rpxfz" in namespace "gc-5881"
  Aug 19 12:39:40.578: INFO: Deleting pod "simpletest.rc-rspfp" in namespace "gc-5881"
  Aug 19 12:39:40.625: INFO: Deleting pod "simpletest.rc-sfgbk" in namespace "gc-5881"
  Aug 19 12:39:40.678: INFO: Deleting pod "simpletest.rc-sgl26" in namespace "gc-5881"
  Aug 19 12:39:40.727: INFO: Deleting pod "simpletest.rc-snl44" in namespace "gc-5881"
  Aug 19 12:39:40.775: INFO: Deleting pod "simpletest.rc-srnsl" in namespace "gc-5881"
  Aug 19 12:39:40.835: INFO: Deleting pod "simpletest.rc-sww2d" in namespace "gc-5881"
  Aug 19 12:39:40.876: INFO: Deleting pod "simpletest.rc-sxbx5" in namespace "gc-5881"
  Aug 19 12:39:40.924: INFO: Deleting pod "simpletest.rc-tbk9m" in namespace "gc-5881"
  Aug 19 12:39:40.977: INFO: Deleting pod "simpletest.rc-tk7tf" in namespace "gc-5881"
  Aug 19 12:39:41.027: INFO: Deleting pod "simpletest.rc-tnkff" in namespace "gc-5881"
  Aug 19 12:39:41.076: INFO: Deleting pod "simpletest.rc-v6rs7" in namespace "gc-5881"
  Aug 19 12:39:41.127: INFO: Deleting pod "simpletest.rc-vbvgq" in namespace "gc-5881"
  Aug 19 12:39:41.178: INFO: Deleting pod "simpletest.rc-vknpg" in namespace "gc-5881"
  Aug 19 12:39:41.226: INFO: Deleting pod "simpletest.rc-vkqs8" in namespace "gc-5881"
  Aug 19 12:39:41.274: INFO: Deleting pod "simpletest.rc-wcdbf" in namespace "gc-5881"
  Aug 19 12:39:41.326: INFO: Deleting pod "simpletest.rc-wpln7" in namespace "gc-5881"
  Aug 19 12:39:41.381: INFO: Deleting pod "simpletest.rc-x69sp" in namespace "gc-5881"
  Aug 19 12:39:41.426: INFO: Deleting pod "simpletest.rc-xcpgn" in namespace "gc-5881"
  Aug 19 12:39:41.473: INFO: Deleting pod "simpletest.rc-xs7qk" in namespace "gc-5881"
  Aug 19 12:39:41.528: INFO: Deleting pod "simpletest.rc-zchkk" in namespace "gc-5881"
  Aug 19 12:39:41.577: INFO: Deleting pod "simpletest.rc-zh5zz" in namespace "gc-5881"
  Aug 19 12:39:41.628: INFO: Deleting pod "simpletest.rc-zmn6b" in namespace "gc-5881"
  Aug 19 12:39:41.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5881" for this suite. @ 08/19/23 12:39:41.716
• [43.793 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 08/19/23 12:39:41.77
  Aug 19 12:39:41.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 12:39:41.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:39:41.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:39:41.791
  Aug 19 12:39:41.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  W0819 12:39:41.796683      18 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0056d0180 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0819 12:39:44.349934      18 warnings.go:70] unknown field "alpha"
  W0819 12:39:44.350054      18 warnings.go:70] unknown field "beta"
  W0819 12:39:44.350113      18 warnings.go:70] unknown field "delta"
  W0819 12:39:44.350169      18 warnings.go:70] unknown field "epsilon"
  W0819 12:39:44.350217      18 warnings.go:70] unknown field "gamma"
  Aug 19 12:39:44.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7290" for this suite. @ 08/19/23 12:39:44.906
• [3.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/19/23 12:39:44.916
  Aug 19 12:39:44.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 12:39:44.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:39:44.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:39:44.934
  STEP: creating a ReplicationController @ 08/19/23 12:39:44.942
  STEP: waiting for RC to be added @ 08/19/23 12:39:44.947
  STEP: waiting for available Replicas @ 08/19/23 12:39:44.948
  STEP: patching ReplicationController @ 08/19/23 12:39:50.585
  STEP: waiting for RC to be modified @ 08/19/23 12:39:50.596
  STEP: patching ReplicationController status @ 08/19/23 12:39:50.596
  STEP: waiting for RC to be modified @ 08/19/23 12:39:50.603
  STEP: waiting for available Replicas @ 08/19/23 12:39:50.603
  STEP: fetching ReplicationController status @ 08/19/23 12:39:50.61
  STEP: patching ReplicationController scale @ 08/19/23 12:39:50.615
  STEP: waiting for RC to be modified @ 08/19/23 12:39:50.62
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/19/23 12:39:50.621
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/19/23 12:39:52.169
  STEP: updating ReplicationController status @ 08/19/23 12:39:52.173
  STEP: waiting for RC to be modified @ 08/19/23 12:39:52.179
  STEP: listing all ReplicationControllers @ 08/19/23 12:39:52.18
  STEP: checking that ReplicationController has expected values @ 08/19/23 12:39:52.183
  STEP: deleting ReplicationControllers by collection @ 08/19/23 12:39:52.183
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/19/23 12:39:52.193
  Aug 19 12:39:52.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0819 12:39:52.250598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5177" for this suite. @ 08/19/23 12:39:52.255
• [7.345 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/19/23 12:39:52.263
  Aug 19 12:39:52.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:39:52.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:39:52.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:39:52.289
  STEP: creating a secret @ 08/19/23 12:39:52.293
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/19/23 12:39:52.298
  STEP: patching the secret @ 08/19/23 12:39:52.302
  STEP: deleting the secret using a LabelSelector @ 08/19/23 12:39:52.312
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/19/23 12:39:52.321
  Aug 19 12:39:52.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7838" for this suite. @ 08/19/23 12:39:52.329
• [0.072 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/19/23 12:39:52.335
  Aug 19 12:39:52.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:39:52.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:39:52.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:39:52.359
  Aug 19 12:39:52.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3789" for this suite. @ 08/19/23 12:39:52.411
• [0.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/19/23 12:39:52.419
  Aug 19 12:39:52.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:39:52.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:39:52.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:39:52.44
  STEP: creating service in namespace services-5639 @ 08/19/23 12:39:52.444
  STEP: creating service affinity-clusterip-transition in namespace services-5639 @ 08/19/23 12:39:52.445
  STEP: creating replication controller affinity-clusterip-transition in namespace services-5639 @ 08/19/23 12:39:52.456
  I0819 12:39:52.481193      18 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-5639, replica count: 3
  E0819 12:39:53.250675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:39:54.250771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:39:55.250871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 12:39:55.532222      18 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 12:39:55.540: INFO: Creating new exec pod
  E0819 12:39:56.251762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:39:57.251821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:39:58.252310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:39:58.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-5639 exec execpod-affinitypdrd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Aug 19 12:39:58.726: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug 19 12:39:58.726: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:39:58.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-5639 exec execpod-affinitypdrd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.144 80'
  Aug 19 12:39:58.913: INFO: stderr: "+ nc -v -t -w 2 10.152.183.144 80\nConnection to 10.152.183.144 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 12:39:58.913: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 12:39:58.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-5639 exec execpod-affinitypdrd4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.144:80/ ; done'
  Aug 19 12:39:59.193: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n"
  Aug 19 12:39:59.193: INFO: stdout: "\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-8km7b\naffinity-clusterip-transition-8km7b\naffinity-clusterip-transition-8km7b\naffinity-clusterip-transition-tt8fw\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-tt8fw\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-tt8fw\naffinity-clusterip-transition-tt8fw"
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-8km7b
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-8km7b
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-8km7b
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-tt8fw
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-tt8fw
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-tt8fw
  Aug 19 12:39:59.193: INFO: Received response from host: affinity-clusterip-transition-tt8fw
  Aug 19 12:39:59.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-5639 exec execpod-affinitypdrd4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.144:80/ ; done'
  E0819 12:39:59.252853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:39:59.472: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.144:80/\n"
  Aug 19 12:39:59.472: INFO: stdout: "\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz\naffinity-clusterip-transition-rk8vz"
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Received response from host: affinity-clusterip-transition-rk8vz
  Aug 19 12:39:59.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 12:39:59.477: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5639, will wait for the garbage collector to delete the pods @ 08/19/23 12:39:59.491
  Aug 19 12:39:59.552: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.490916ms
  Aug 19 12:39:59.653: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.279275ms
  E0819 12:40:00.253593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:01.254301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5639" for this suite. @ 08/19/23 12:40:01.773
• [9.362 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/19/23 12:40:01.782
  Aug 19 12:40:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:40:01.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:01.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:01.803
  STEP: creating all guestbook components @ 08/19/23 12:40:01.807
  Aug 19 12:40:01.807: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug 19 12:40:01.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  E0819 12:40:02.254407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:02.376: INFO: stderr: ""
  Aug 19 12:40:02.376: INFO: stdout: "service/agnhost-replica created\n"
  Aug 19 12:40:02.377: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug 19 12:40:02.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  Aug 19 12:40:02.762: INFO: stderr: ""
  Aug 19 12:40:02.762: INFO: stdout: "service/agnhost-primary created\n"
  Aug 19 12:40:02.762: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug 19 12:40:02.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  Aug 19 12:40:03.158: INFO: stderr: ""
  Aug 19 12:40:03.158: INFO: stdout: "service/frontend created\n"
  Aug 19 12:40:03.158: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug 19 12:40:03.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  E0819 12:40:03.255286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:03.653: INFO: stderr: ""
  Aug 19 12:40:03.653: INFO: stdout: "deployment.apps/frontend created\n"
  Aug 19 12:40:03.654: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 19 12:40:03.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  Aug 19 12:40:03.985: INFO: stderr: ""
  Aug 19 12:40:03.985: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug 19 12:40:03.986: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 19 12:40:03.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 create -f -'
  E0819 12:40:04.256149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:04.425: INFO: stderr: ""
  Aug 19 12:40:04.425: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/19/23 12:40:04.425
  Aug 19 12:40:04.425: INFO: Waiting for all frontend pods to be Running.
  E0819 12:40:05.256485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:06.256558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:07.256696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:08.256783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:09.257190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:09.476: INFO: Waiting for frontend to serve content.
  Aug 19 12:40:09.489: INFO: Trying to add a new entry to the guestbook.
  Aug 19 12:40:09.501: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.513
  Aug 19 12:40:09.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:09.596: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:09.596: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.596
  Aug 19 12:40:09.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:09.682: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:09.682: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.682
  Aug 19 12:40:09.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:09.756: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:09.756: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.756
  Aug 19 12:40:09.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:09.819: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:09.819: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.82
  Aug 19 12:40:09.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:09.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:09.908: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/19/23 12:40:09.908
  Aug 19 12:40:09.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-9567 delete --grace-period=0 --force -f -'
  Aug 19 12:40:10.003: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 19 12:40:10.003: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug 19 12:40:10.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9567" for this suite. @ 08/19/23 12:40:10.008
• [8.234 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/19/23 12:40:10.016
  Aug 19 12:40:10.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/19/23 12:40:10.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:10.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:10.04
  Aug 19 12:40:10.048: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 12:40:10.257583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7233" for this suite. @ 08/19/23 12:40:10.594
• [0.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/19/23 12:40:10.603
  Aug 19 12:40:10.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:40:10.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:10.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:10.621
  STEP: creating a Pod with a static label @ 08/19/23 12:40:10.636
  STEP: watching for Pod to be ready @ 08/19/23 12:40:10.647
  Aug 19 12:40:10.649: INFO: observed Pod pod-test in namespace pods-2823 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug 19 12:40:10.653: INFO: observed Pod pod-test in namespace pods-2823 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC  }]
  Aug 19 12:40:10.706: INFO: observed Pod pod-test in namespace pods-2823 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC  }]
  E0819 12:40:11.257669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:11.664: INFO: Found Pod pod-test in namespace pods-2823 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 12:40:10 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/19/23 12:40:11.669
  STEP: getting the Pod and ensuring that it's patched @ 08/19/23 12:40:11.678
  STEP: replacing the Pod's status Ready condition to False @ 08/19/23 12:40:11.682
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/19/23 12:40:11.696
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/19/23 12:40:11.696
  STEP: watching for the Pod to be deleted @ 08/19/23 12:40:11.707
  Aug 19 12:40:11.709: INFO: observed event type MODIFIED
  E0819 12:40:12.257940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:13.258034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:13.680: INFO: observed event type MODIFIED
  Aug 19 12:40:13.975: INFO: observed event type MODIFIED
  E0819 12:40:14.258446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:14.684: INFO: observed event type MODIFIED
  Aug 19 12:40:14.691: INFO: observed event type MODIFIED
  Aug 19 12:40:14.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2823" for this suite. @ 08/19/23 12:40:14.706
• [4.113 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/19/23 12:40:14.716
  Aug 19 12:40:14.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 12:40:14.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:14.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:14.737
  Aug 19 12:40:14.740: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0819 12:40:15.258793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/19/23 12:40:15.752
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/19/23 12:40:15.759
  E0819 12:40:16.259595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/19/23 12:40:16.768
  Aug 19 12:40:16.777: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/19/23 12:40:16.777
  E0819 12:40:17.259620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1423" for this suite. @ 08/19/23 12:40:17.791
• [3.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/19/23 12:40:17.799
  Aug 19 12:40:17.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context-test @ 08/19/23 12:40:17.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:17.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:17.819
  E0819 12:40:18.260132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:19.260626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:20.260713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:21.260957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:40:21.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6189" for this suite. @ 08/19/23 12:40:21.886
• [4.116 seconds]
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/19/23 12:40:21.916
  Aug 19 12:40:21.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 12:40:21.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:40:21.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:40:21.971
  STEP: creating the pod with failed condition @ 08/19/23 12:40:21.978
  E0819 12:40:22.261910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:23.261984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:24.262606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:25.262690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:26.263230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:27.263317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:28.263687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:29.263762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:30.264358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:31.264443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:32.264565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:33.264823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:34.265388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:35.265551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:36.266613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:37.266658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:38.267533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:39.268286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:40.268758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:41.268837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:42.269398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:43.270158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:44.270875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:45.271503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:46.272282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:47.273092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:48.273819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:49.274754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:50.275686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:51.275829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:52.275886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:53.276036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:54.276126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:55.276426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:56.277418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:57.277642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:58.278683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:40:59.278774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:00.279543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:01.279634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:02.279718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:03.279932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:04.280083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:05.280271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:06.281092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:07.281179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:08.281659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:09.282302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:10.282869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:11.282944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:12.283233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:13.284276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:14.284534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:15.284732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:16.285065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:17.285247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:18.285783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:19.286771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:20.287268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:21.287377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:22.287953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:23.288387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:24.288551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:25.288656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:26.288683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:27.288838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:28.289385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:29.289474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:30.289778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:31.289936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:32.290685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:33.290777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:34.291366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:35.292274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:36.292885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:37.292969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:38.293778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:39.293869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:40.294152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:41.294243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:42.294994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:43.295209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:44.296274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:45.297196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:46.297811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:47.298003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:48.298345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:49.298502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:50.298834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:51.299096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:52.300101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:53.300957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:54.301656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:55.301861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:56.302203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:57.302295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:58.303359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:41:59.303447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:00.304293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:01.304548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:02.305108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:03.305258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:04.305555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:05.305740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:06.306407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:07.306517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:08.307059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:09.307238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:10.307760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:11.308284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:12.308984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:13.309480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:14.309912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:15.310007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:16.310208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:17.310306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:18.310823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:19.310907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:20.311855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:21.312439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 08/19/23 12:42:21.995
  E0819 12:42:22.312993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:42:22.509: INFO: Successfully updated pod "var-expansion-4f879c9a-8f81-45fa-bbb5-ae4dcd4dc666"
  STEP: waiting for pod running @ 08/19/23 12:42:22.509
  E0819 12:42:23.313800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:24.314033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/19/23 12:42:24.519
  Aug 19 12:42:24.519: INFO: Deleting pod "var-expansion-4f879c9a-8f81-45fa-bbb5-ae4dcd4dc666" in namespace "var-expansion-8692"
  Aug 19 12:42:24.527: INFO: Wait up to 5m0s for pod "var-expansion-4f879c9a-8f81-45fa-bbb5-ae4dcd4dc666" to be fully deleted
  E0819 12:42:25.314126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:26.314203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:27.314309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:28.314394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:29.314672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:30.314753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:31.315095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:32.315234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:33.315973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:34.316309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:35.316397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:36.316618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:37.317387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:38.317849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:39.317957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:40.318173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:41.319226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:42.319304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:43.320290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:44.320489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:45.320572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:46.320735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:47.320820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:48.320953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:49.321950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:50.322164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:51.322229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:52.322316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:53.322875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:54.323833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:55.324574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:56.324800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:42:56.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8692" for this suite. @ 08/19/23 12:42:56.616
• [154.708 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/19/23 12:42:56.624
  Aug 19 12:42:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:42:56.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:42:56.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:42:56.653
  STEP: Setting up server cert @ 08/19/23 12:42:56.677
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:42:57.104
  STEP: Deploying the webhook pod @ 08/19/23 12:42:57.113
  STEP: Wait for the deployment to be ready @ 08/19/23 12:42:57.127
  Aug 19 12:42:57.135: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0819 12:42:57.324815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:42:58.325366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 12:42:59.149
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:42:59.161
  E0819 12:42:59.325842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:00.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 19 12:43:00.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 12:43:00.326534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1448-crds.webhook.example.com via the AdmissionRegistration API @ 08/19/23 12:43:00.68
  STEP: Creating a custom resource while v1 is storage version @ 08/19/23 12:43:00.698
  E0819 12:43:01.326916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:02.327043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/19/23 12:43:02.747
  STEP: Patching the custom resource while v2 is storage version @ 08/19/23 12:43:02.765
  Aug 19 12:43:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0819 12:43:03.327239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7" for this suite. @ 08/19/23 12:43:03.397
  STEP: Destroying namespace "webhook-markers-7084" for this suite. @ 08/19/23 12:43:03.404
• [6.787 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/19/23 12:43:03.412
  Aug 19 12:43:03.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:43:03.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:03.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:03.434
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/19/23 12:43:03.437
  Aug 19 12:43:03.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-5194 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug 19 12:43:03.507: INFO: stderr: ""
  Aug 19 12:43:03.507: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/19/23 12:43:03.507
  Aug 19 12:43:03.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-5194 delete pods e2e-test-httpd-pod'
  E0819 12:43:04.327321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:05.327399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:06.122: INFO: stderr: ""
  Aug 19 12:43:06.122: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 19 12:43:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5194" for this suite. @ 08/19/23 12:43:06.127
• [2.723 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/19/23 12:43:06.135
  Aug 19 12:43:06.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:43:06.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:06.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:06.156
  STEP: Creating a test headless service @ 08/19/23 12:43:06.159
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3941.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3941.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/19/23 12:43:06.166
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3941.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3941.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/19/23 12:43:06.166
  STEP: creating a pod to probe DNS @ 08/19/23 12:43:06.166
  STEP: submitting the pod to kubernetes @ 08/19/23 12:43:06.166
  E0819 12:43:06.328105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:07.328506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/19/23 12:43:08.19
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:43:08.195
  Aug 19 12:43:08.214: INFO: DNS probes using dns-3941/dns-test-896a0ee6-7142-44cf-9ac9-4bcf3d15a444 succeeded

  Aug 19 12:43:08.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:43:08.219
  STEP: deleting the test headless service @ 08/19/23 12:43:08.236
  STEP: Destroying namespace "dns-3941" for this suite. @ 08/19/23 12:43:08.25
• [2.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/19/23 12:43:08.259
  Aug 19 12:43:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:43:08.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:08.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:08.279
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:43:08.283
  E0819 12:43:08.328909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:09.329022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:10.329799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:11.329880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:43:12.305
  Aug 19 12:43:12.309: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-44c8c1a6-edd2-4318-8291-c1362b3381bd container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:43:12.329
  E0819 12:43:12.329926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:12.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3846" for this suite. @ 08/19/23 12:43:12.352
• [4.100 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/19/23 12:43:12.36
  Aug 19 12:43:12.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:43:12.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:12.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:12.381
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:43:12.384
  E0819 12:43:13.330595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:14.330685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:15.331695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:16.331785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:43:16.413
  Aug 19 12:43:16.417: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-1f625d51-70ce-47e5-b5b5-0b99309d606f container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:43:16.426
  Aug 19 12:43:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6774" for this suite. @ 08/19/23 12:43:16.448
• [4.095 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/19/23 12:43:16.455
  Aug 19 12:43:16.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-pred @ 08/19/23 12:43:16.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:16.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:16.477
  Aug 19 12:43:16.481: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 19 12:43:16.491: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 12:43:16.494: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-214 before test
  Aug 19 12:43:16.500: INFO: nginx-ingress-controller-kubernetes-worker-v7bfm from ingress-nginx-kubernetes-worker started at 2023-08-19 12:19:01 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.500: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:43:16.500: INFO: metrics-server-v0.5.2-6cf8c8b69c-tlcp9 from kube-system started at 2023-08-19 12:18:49 +0000 UTC (2 container statuses recorded)
  Aug 19 12:43:16.500: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 19 12:43:16.500: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 19 12:43:16.500: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:43:16.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:43:16.500: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:43:16.500: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-145 before test
  Aug 19 12:43:16.506: INFO: nginx-ingress-controller-kubernetes-worker-xnrcp from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: coredns-5c7f76ccb8-trw9q from kube-system started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container coredns ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: kube-state-metrics-5b95b4459c-rtf6r from kube-system started at 2023-08-19 12:18:49 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: dashboard-metrics-scraper-6b8586b5c9-lgkqw from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: kubernetes-dashboard-6869f4cd5f-gfcl6 from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-52lf7 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:43:16.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 12:43:16.506: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-69-13 before test
  Aug 19 12:43:16.512: INFO: default-http-backend-kubernetes-worker-65fc475d49-vjk9h from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: nginx-ingress-controller-kubernetes-worker-wb2rk from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: calico-kube-controllers-85f9fb94df-xpnfw from kube-system started at 2023-08-19 11:54:32 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: sonobuoy from sonobuoy started at 2023-08-19 12:00:13 +0000 UTC (1 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: sonobuoy-e2e-job-1e976644cf094697 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container e2e ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-vmdc9 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 12:43:16.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 12:43:16.512: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/19/23 12:43:16.512
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.177cc96536caf7e9], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 08/19/23 12:43:16.538
  E0819 12:43:17.331874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:17.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-295" for this suite. @ 08/19/23 12:43:17.542
• [1.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/19/23 12:43:17.555
  Aug 19 12:43:17.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sysctl @ 08/19/23 12:43:17.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:17.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:17.576
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/19/23 12:43:17.584
  Aug 19 12:43:17.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6830" for this suite. @ 08/19/23 12:43:17.595
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/19/23 12:43:17.604
  Aug 19 12:43:17.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-runtime @ 08/19/23 12:43:17.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:17.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:17.626
  STEP: create the container @ 08/19/23 12:43:17.629
  W0819 12:43:17.637868      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/19/23 12:43:17.638
  E0819 12:43:18.332214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:19.332863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:20.333811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/19/23 12:43:20.653
  STEP: the container should be terminated @ 08/19/23 12:43:20.658
  STEP: the termination message should be set @ 08/19/23 12:43:20.658
  Aug 19 12:43:20.658: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/19/23 12:43:20.658
  Aug 19 12:43:20.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2253" for this suite. @ 08/19/23 12:43:20.679
• [3.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/19/23 12:43:20.692
  Aug 19 12:43:20.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/19/23 12:43:20.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:20.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:20.714
  STEP: Creating 50 configmaps @ 08/19/23 12:43:20.718
  STEP: Creating RC which spawns configmap-volume pods @ 08/19/23 12:43:20.958
  Aug 19 12:43:21.077: INFO: Pod name wrapped-volume-race-94c6c2a1-f759-4f8b-8444-17b74cd5e755: Found 3 pods out of 5
  E0819 12:43:21.334403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:22.334722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:23.335433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:24.334908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:25.335226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:26.085: INFO: Pod name wrapped-volume-race-94c6c2a1-f759-4f8b-8444-17b74cd5e755: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/19/23 12:43:26.085
  STEP: Creating RC which spawns configmap-volume pods @ 08/19/23 12:43:26.115
  Aug 19 12:43:26.130: INFO: Pod name wrapped-volume-race-1b0923bc-917b-483c-b553-d206bb2adf29: Found 0 pods out of 5
  E0819 12:43:26.336205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:27.336443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:28.336523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:29.336639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:30.336698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:31.138: INFO: Pod name wrapped-volume-race-1b0923bc-917b-483c-b553-d206bb2adf29: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/19/23 12:43:31.138
  STEP: Creating RC which spawns configmap-volume pods @ 08/19/23 12:43:31.164
  Aug 19 12:43:31.180: INFO: Pod name wrapped-volume-race-7eb98372-580c-4c60-98c0-2cf5932dd013: Found 0 pods out of 5
  E0819 12:43:31.337715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:32.337955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:33.338152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:34.338296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:35.338352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:36.191: INFO: Pod name wrapped-volume-race-7eb98372-580c-4c60-98c0-2cf5932dd013: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/19/23 12:43:36.191
  Aug 19 12:43:36.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-7eb98372-580c-4c60-98c0-2cf5932dd013 in namespace emptydir-wrapper-6550, will wait for the garbage collector to delete the pods @ 08/19/23 12:43:36.218
  Aug 19 12:43:36.281: INFO: Deleting ReplicationController wrapped-volume-race-7eb98372-580c-4c60-98c0-2cf5932dd013 took: 8.439571ms
  E0819 12:43:36.338399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:36.382: INFO: Terminating ReplicationController wrapped-volume-race-7eb98372-580c-4c60-98c0-2cf5932dd013 pods took: 100.740447ms
  E0819 12:43:37.338651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:38.338954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-1b0923bc-917b-483c-b553-d206bb2adf29 in namespace emptydir-wrapper-6550, will wait for the garbage collector to delete the pods @ 08/19/23 12:43:38.383
  Aug 19 12:43:38.447: INFO: Deleting ReplicationController wrapped-volume-race-1b0923bc-917b-483c-b553-d206bb2adf29 took: 8.54069ms
  Aug 19 12:43:38.547: INFO: Terminating ReplicationController wrapped-volume-race-1b0923bc-917b-483c-b553-d206bb2adf29 pods took: 100.488829ms
  E0819 12:43:39.339935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:40.340802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-94c6c2a1-f759-4f8b-8444-17b74cd5e755 in namespace emptydir-wrapper-6550, will wait for the garbage collector to delete the pods @ 08/19/23 12:43:40.947
  Aug 19 12:43:41.012: INFO: Deleting ReplicationController wrapped-volume-race-94c6c2a1-f759-4f8b-8444-17b74cd5e755 took: 8.330852ms
  Aug 19 12:43:41.112: INFO: Terminating ReplicationController wrapped-volume-race-94c6c2a1-f759-4f8b-8444-17b74cd5e755 pods took: 100.150403ms
  E0819 12:43:41.341112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:42.341435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 08/19/23 12:43:43.113
  E0819 12:43:43.342132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-6550" for this suite. @ 08/19/23 12:43:43.439
• [22.753 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/19/23 12:43:43.445
  Aug 19 12:43:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:43:43.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:43.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:43.467
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/19/23 12:43:43.47
  E0819 12:43:44.342247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:45.342406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:46.342498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:47.342591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:43:47.497
  Aug 19 12:43:47.501: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-ae07a1e9-a6ba-44b6-b8ef-b547b7c28942 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:43:47.508
  Aug 19 12:43:47.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7067" for this suite. @ 08/19/23 12:43:47.53
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/19/23 12:43:47.54
  Aug 19 12:43:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context-test @ 08/19/23 12:43:47.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:47.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:47.561
  E0819 12:43:48.343086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:49.343258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:50.344302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:51.344405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:51.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5284" for this suite. @ 08/19/23 12:43:51.604
• [4.073 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/19/23 12:43:51.613
  Aug 19 12:43:51.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename endpointslice @ 08/19/23 12:43:51.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:51.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:51.633
  Aug 19 12:43:51.650: INFO: Endpoints addresses: [172.31.23.154 172.31.43.67] , ports: [6443]
  Aug 19 12:43:51.650: INFO: EndpointSlices addresses: [172.31.23.154 172.31.43.67] , ports: [6443]
  Aug 19 12:43:51.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7404" for this suite. @ 08/19/23 12:43:51.655
• [0.048 seconds]
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/19/23 12:43:51.662
  Aug 19 12:43:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 12:43:51.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:51.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:51.681
  STEP: Creating a job @ 08/19/23 12:43:51.684
  STEP: Ensuring active pods == parallelism @ 08/19/23 12:43:51.691
  E0819 12:43:52.345249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:53.345623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 08/19/23 12:43:53.696
  Aug 19 12:43:54.213: INFO: Successfully updated pod "adopt-release-7rv5r"
  STEP: Checking that the Job readopts the Pod @ 08/19/23 12:43:54.213
  E0819 12:43:54.346425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:55.346619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 08/19/23 12:43:56.222
  E0819 12:43:56.347634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:56.737: INFO: Successfully updated pod "adopt-release-7rv5r"
  STEP: Checking that the Job releases the Pod @ 08/19/23 12:43:56.738
  E0819 12:43:57.347749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:43:58.348067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:43:58.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8139" for this suite. @ 08/19/23 12:43:58.752
• [7.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/19/23 12:43:58.765
  Aug 19 12:43:58.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:43:58.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:43:58.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:43:58.786
  STEP: Creating pod liveness-d7617254-9200-4ce8-822a-c671b7d6214a in namespace container-probe-9811 @ 08/19/23 12:43:58.789
  E0819 12:43:59.348270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:00.348286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:44:00.809: INFO: Started pod liveness-d7617254-9200-4ce8-822a-c671b7d6214a in namespace container-probe-9811
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:44:00.809
  Aug 19 12:44:00.813: INFO: Initial restart count of pod liveness-d7617254-9200-4ce8-822a-c671b7d6214a is 0
  E0819 12:44:01.349303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:02.349318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:03.350307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:04.350403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:05.350483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:06.350667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:07.351425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:08.351968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:09.352266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:10.352465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:11.352953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:12.353019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:13.353627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:14.353734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:15.354587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:16.354680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:17.355064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:18.355219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:19.356299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:20.356393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:44:20.865: INFO: Restart count of pod container-probe-9811/liveness-d7617254-9200-4ce8-822a-c671b7d6214a is now 1 (20.051926873s elapsed)
  Aug 19 12:44:20.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:44:20.87
  STEP: Destroying namespace "container-probe-9811" for this suite. @ 08/19/23 12:44:20.886
• [22.128 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/19/23 12:44:20.894
  Aug 19 12:44:20.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:44:20.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:44:20.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:44:20.916
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-8215b8d2-d3a9-4b43-960c-123c349187f6 @ 08/19/23 12:44:20.924
  STEP: Creating the pod @ 08/19/23 12:44:20.929
  E0819 12:44:21.357005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:22.357276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-8215b8d2-d3a9-4b43-960c-123c349187f6 @ 08/19/23 12:44:22.96
  STEP: waiting to observe update in volume @ 08/19/23 12:44:22.966
  E0819 12:44:23.358131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:24.358236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:25.358760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:26.358867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:27.359459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:28.359919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:29.360830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:30.361033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:31.361123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:32.361901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:33.361991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:34.362086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:35.362862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:36.362972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:37.363944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:38.364127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:39.365040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:40.365541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:41.365610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:42.365701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:43.365787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:44.365884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:45.365963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:46.367031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:47.367688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:48.367986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:49.368296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:50.368492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:51.369506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:52.370033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:53.370478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:54.370568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:55.371522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:56.372306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:57.373224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:58.373424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:44:59.373979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:00.374070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:01.374651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:02.374750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:03.375744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:04.375830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:05.375914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:06.376300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:07.376374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:08.376489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:09.376614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:10.376798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:11.377766      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:12.377842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:13.377895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:14.377987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:15.378072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:16.378167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:17.379087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:18.379676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:19.380617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:20.380712      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:21.381598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:22.382387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:23.382609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:24.382723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:25.383461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:26.383507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:27.384272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:28.384362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:29.385085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:30.385173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:31.385327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:32.385505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:33.385586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:34.385763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:35.386246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:36.387206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:37.387589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:38.387683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:39.388733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:40.388832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:45:41.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2099" for this suite. @ 08/19/23 12:45:41.349
• [80.463 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/19/23 12:45:41.359
  Aug 19 12:45:41.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-runtime @ 08/19/23 12:45:41.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:45:41.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:45:41.383
  E0819 12:45:41.388924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the container @ 08/19/23 12:45:41.391
  W0819 12:45:41.400518      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/19/23 12:45:41.4
  E0819 12:45:42.389601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:43.389922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:44.390007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/19/23 12:45:44.419
  STEP: the container should be terminated @ 08/19/23 12:45:44.424
  STEP: the termination message should be set @ 08/19/23 12:45:44.424
  Aug 19 12:45:44.424: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/19/23 12:45:44.424
  Aug 19 12:45:44.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9766" for this suite. @ 08/19/23 12:45:44.449
• [3.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/19/23 12:45:44.459
  Aug 19 12:45:44.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:45:44.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:45:44.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:45:44.485
  STEP: Creating a ResourceQuota with best effort scope @ 08/19/23 12:45:44.49
  STEP: Ensuring ResourceQuota status is calculated @ 08/19/23 12:45:44.495
  E0819 12:45:45.390564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:46.390678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 08/19/23 12:45:46.5
  STEP: Ensuring ResourceQuota status is calculated @ 08/19/23 12:45:46.508
  E0819 12:45:47.391243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:48.392299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 08/19/23 12:45:48.513
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/19/23 12:45:48.531
  E0819 12:45:49.392433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:50.392519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/19/23 12:45:50.537
  E0819 12:45:51.392581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:52.392645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/19/23 12:45:52.541
  STEP: Ensuring resource quota status released the pod usage @ 08/19/23 12:45:52.557
  E0819 12:45:53.392791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:54.392870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 08/19/23 12:45:54.563
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/19/23 12:45:54.577
  E0819 12:45:55.392955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:56.393366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/19/23 12:45:56.582
  E0819 12:45:57.393447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:45:58.393638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/19/23 12:45:58.587
  STEP: Ensuring resource quota status released the pod usage @ 08/19/23 12:45:58.603
  E0819 12:45:59.393989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:00.394082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:00.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8574" for this suite. @ 08/19/23 12:46:00.612
• [16.162 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/19/23 12:46:00.621
  Aug 19 12:46:00.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:46:00.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:00.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:00.647
  STEP: Creating configMap with name configmap-test-volume-a7cea040-c911-49c8-9d3c-6ea5cd8c4262 @ 08/19/23 12:46:00.651
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:46:00.656
  E0819 12:46:01.394162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:02.394415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:03.395404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:04.396308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:46:04.678
  Aug 19 12:46:04.682: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-433c7cd9-f0ae-4014-9e1e-875a5a2ff339 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:46:04.69
  Aug 19 12:46:04.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6740" for this suite. @ 08/19/23 12:46:04.715
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/19/23 12:46:04.723
  Aug 19 12:46:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 12:46:04.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:04.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:04.743
  STEP: creating Agnhost RC @ 08/19/23 12:46:04.747
  Aug 19 12:46:04.747: INFO: namespace kubectl-286
  Aug 19 12:46:04.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-286 create -f -'
  Aug 19 12:46:05.241: INFO: stderr: ""
  Aug 19 12:46:05.241: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/19/23 12:46:05.241
  E0819 12:46:05.396406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:06.246: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:46:06.246: INFO: Found 0 / 1
  E0819 12:46:06.397423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:07.245: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:46:07.245: INFO: Found 1 / 1
  Aug 19 12:46:07.245: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 19 12:46:07.249: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 19 12:46:07.249: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 19 12:46:07.249: INFO: wait on agnhost-primary startup in kubectl-286 
  Aug 19 12:46:07.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-286 logs agnhost-primary-4fnkf agnhost-primary'
  Aug 19 12:46:07.325: INFO: stderr: ""
  Aug 19 12:46:07.325: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/19/23 12:46:07.325
  Aug 19 12:46:07.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-286 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  E0819 12:46:07.397761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:07.402: INFO: stderr: ""
  Aug 19 12:46:07.402: INFO: stdout: "service/rm2 exposed\n"
  Aug 19 12:46:07.412: INFO: Service rm2 in namespace kubectl-286 found.
  E0819 12:46:08.397945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:09.398038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 08/19/23 12:46:09.42
  Aug 19 12:46:09.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-286 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Aug 19 12:46:09.491: INFO: stderr: ""
  Aug 19 12:46:09.491: INFO: stdout: "service/rm3 exposed\n"
  Aug 19 12:46:09.497: INFO: Service rm3 in namespace kubectl-286 found.
  E0819 12:46:10.398117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:11.398578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:11.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-286" for this suite. @ 08/19/23 12:46:11.511
• [6.795 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/19/23 12:46:11.518
  Aug 19 12:46:11.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:46:11.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:11.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:11.54
  STEP: Creating a test headless service @ 08/19/23 12:46:11.544
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5564.svc.cluster.local;sleep 1; done
   @ 08/19/23 12:46:11.55
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5564.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5564.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5564.svc.cluster.local;sleep 1; done
   @ 08/19/23 12:46:11.55
  STEP: creating a pod to probe DNS @ 08/19/23 12:46:11.55
  STEP: submitting the pod to kubernetes @ 08/19/23 12:46:11.55
  E0819 12:46:12.399482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:13.399589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/19/23 12:46:13.575
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:46:13.578
  Aug 19 12:46:13.584: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.589: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.593: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.598: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.602: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.607: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.611: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.615: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5564.svc.cluster.local from pod dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d: the server could not find the requested resource (get pods dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d)
  Aug 19 12:46:13.615: INFO: Lookups using dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5564.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5564.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5564.svc.cluster.local jessie_udp@dns-test-service-2.dns-5564.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5564.svc.cluster.local]

  E0819 12:46:14.400283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:15.400480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:16.400568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:17.400831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:18.400922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:18.651: INFO: DNS probes using dns-5564/dns-test-92e9b8c0-b44a-4812-89c7-fe6f2892915d succeeded

  Aug 19 12:46:18.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:46:18.655
  STEP: deleting the test headless service @ 08/19/23 12:46:18.671
  STEP: Destroying namespace "dns-5564" for this suite. @ 08/19/23 12:46:18.698
• [7.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/19/23 12:46:18.712
  Aug 19 12:46:18.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubelet-test @ 08/19/23 12:46:18.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:18.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:18.731
  E0819 12:46:19.401998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:20.402155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:20.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4303" for this suite. @ 08/19/23 12:46:20.768
• [2.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/19/23 12:46:20.778
  Aug 19 12:46:20.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:46:20.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:20.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:20.799
  STEP: Creating service test in namespace statefulset-6994 @ 08/19/23 12:46:20.803
  STEP: Looking for a node to schedule stateful set and pod @ 08/19/23 12:46:20.807
  STEP: Creating pod with conflicting port in namespace statefulset-6994 @ 08/19/23 12:46:20.812
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6994 @ 08/19/23 12:46:20.824
  E0819 12:46:21.402261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:22.402319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-6994 @ 08/19/23 12:46:22.832
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6994 @ 08/19/23 12:46:22.838
  Aug 19 12:46:22.850: INFO: Observed stateful pod in namespace: statefulset-6994, name: ss-0, uid: c5ffbb88-4da4-49f7-b30a-ac3d250f8b9f, status phase: Pending. Waiting for statefulset controller to delete.
  Aug 19 12:46:22.866: INFO: Observed stateful pod in namespace: statefulset-6994, name: ss-0, uid: c5ffbb88-4da4-49f7-b30a-ac3d250f8b9f, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 19 12:46:22.890: INFO: Observed stateful pod in namespace: statefulset-6994, name: ss-0, uid: c5ffbb88-4da4-49f7-b30a-ac3d250f8b9f, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 19 12:46:22.895: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6994
  STEP: Removing pod with conflicting port in namespace statefulset-6994 @ 08/19/23 12:46:22.895
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6994 and will be in running state @ 08/19/23 12:46:22.909
  E0819 12:46:23.402936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:24.403238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:24.918: INFO: Deleting all statefulset in ns statefulset-6994
  Aug 19 12:46:24.922: INFO: Scaling statefulset ss to 0
  E0819 12:46:25.403898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:26.404313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:27.404520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:28.404929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:29.405017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:30.405113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:31.405316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:32.405510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:33.405771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:34.405954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:34.941: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 12:46:34.945: INFO: Deleting statefulset ss
  Aug 19 12:46:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6994" for this suite. @ 08/19/23 12:46:34.965
• [14.194 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/19/23 12:46:34.972
  Aug 19 12:46:34.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:46:34.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:34.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:35.002
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 12:46:35.006
  E0819 12:46:35.406897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:36.407112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:37.407909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:38.407966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:46:39.032
  Aug 19 12:46:39.036: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-a788f71c-e5d7-4899-80f4-6a1dc9e1eb40 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 12:46:39.044
  Aug 19 12:46:39.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4748" for this suite. @ 08/19/23 12:46:39.068
• [4.103 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/19/23 12:46:39.075
  Aug 19 12:46:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 12:46:39.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:39.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:39.101
  STEP: Creating service test in namespace statefulset-6017 @ 08/19/23 12:46:39.104
  Aug 19 12:46:39.124: INFO: Found 0 stateful pods, waiting for 1
  E0819 12:46:39.408944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:40.409043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:41.409220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:42.409466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:43.409764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:44.409951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:45.410043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:46.410248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:47.410443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:48.410711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:49.128: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/19/23 12:46:49.136
  W0819 12:46:49.143770      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 19 12:46:49.152: INFO: Found 1 stateful pods, waiting for 2
  E0819 12:46:49.411250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:50.411322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:51.411384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:52.412283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:53.412590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:54.412678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:55.412895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:56.412991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:57.413083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:46:58.413866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:46:59.158: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 12:46:59.158: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/19/23 12:46:59.165
  STEP: Delete all of the StatefulSets @ 08/19/23 12:46:59.169
  STEP: Verify that StatefulSets have been deleted @ 08/19/23 12:46:59.179
  Aug 19 12:46:59.183: INFO: Deleting all statefulset in ns statefulset-6017
  Aug 19 12:46:59.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6017" for this suite. @ 08/19/23 12:46:59.205
• [20.137 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/19/23 12:46:59.213
  Aug 19 12:46:59.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename tables @ 08/19/23 12:46:59.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:59.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:59.235
  Aug 19 12:46:59.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-9002" for this suite. @ 08/19/23 12:46:59.247
• [0.040 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/19/23 12:46:59.253
  Aug 19 12:46:59.253: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:46:59.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:46:59.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:46:59.273
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/19/23 12:46:59.277
  E0819 12:46:59.413969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:00.414015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:01.414100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:02.414280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:47:03.3
  Aug 19 12:47:03.305: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-0936423d-95fa-4c27-bafb-b9a2f64437f7 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:47:03.313
  Aug 19 12:47:03.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5731" for this suite. @ 08/19/23 12:47:03.334
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/19/23 12:47:03.343
  Aug 19 12:47:03.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:47:03.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:47:03.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:47:03.36
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/19/23 12:47:03.37
  E0819 12:47:03.414939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:04.415307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:05.416276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:06.416368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:47:07.392
  Aug 19 12:47:07.397: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-7b01adef-dcf7-430b-a956-62060f512d86 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:47:07.406
  E0819 12:47:07.417221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:47:07.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2970" for this suite. @ 08/19/23 12:47:07.428
• [4.094 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/19/23 12:47:07.437
  Aug 19 12:47:07.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename limitrange @ 08/19/23 12:47:07.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:47:07.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:47:07.46
  STEP: Creating LimitRange "e2e-limitrange-85bgf" in namespace "limitrange-4929" @ 08/19/23 12:47:07.464
  STEP: Creating another limitRange in another namespace @ 08/19/23 12:47:07.469
  Aug 19 12:47:07.491: INFO: Namespace "e2e-limitrange-85bgf-1967" created
  Aug 19 12:47:07.491: INFO: Creating LimitRange "e2e-limitrange-85bgf" in namespace "e2e-limitrange-85bgf-1967"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-85bgf" @ 08/19/23 12:47:07.499
  Aug 19 12:47:07.503: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-85bgf" in "limitrange-4929" namespace @ 08/19/23 12:47:07.503
  Aug 19 12:47:07.510: INFO: LimitRange "e2e-limitrange-85bgf" has been patched
  STEP: Delete LimitRange "e2e-limitrange-85bgf" by Collection with labelSelector: "e2e-limitrange-85bgf=patched" @ 08/19/23 12:47:07.51
  STEP: Confirm that the limitRange "e2e-limitrange-85bgf" has been deleted @ 08/19/23 12:47:07.519
  Aug 19 12:47:07.519: INFO: Requesting list of LimitRange to confirm quantity
  Aug 19 12:47:07.523: INFO: Found 0 LimitRange with label "e2e-limitrange-85bgf=patched"
  Aug 19 12:47:07.524: INFO: LimitRange "e2e-limitrange-85bgf" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-85bgf" @ 08/19/23 12:47:07.524
  Aug 19 12:47:07.527: INFO: Found 1 limitRange
  Aug 19 12:47:07.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4929" for this suite. @ 08/19/23 12:47:07.532
  STEP: Destroying namespace "e2e-limitrange-85bgf-1967" for this suite. @ 08/19/23 12:47:07.542
• [0.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/19/23 12:47:07.554
  Aug 19 12:47:07.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 12:47:07.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:47:07.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:47:07.585
  STEP: Creating pod test-grpc-b79f6dec-08b3-4826-a664-df45ecad7f0b in namespace container-probe-8126 @ 08/19/23 12:47:07.59
  E0819 12:47:08.418027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:09.418226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:47:09.609: INFO: Started pod test-grpc-b79f6dec-08b3-4826-a664-df45ecad7f0b in namespace container-probe-8126
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 12:47:09.609
  Aug 19 12:47:09.614: INFO: Initial restart count of pod test-grpc-b79f6dec-08b3-4826-a664-df45ecad7f0b is 0
  E0819 12:47:10.418320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:11.419257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:12.419314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:13.419381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:14.419488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:15.420282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:16.420605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:17.420668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:18.421534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:19.421624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:20.422679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:21.422944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:22.423925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:23.424328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:24.424739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:25.424807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:26.424889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:27.425055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:28.425952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:29.426060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:30.426569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:31.427531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:32.427621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:33.427840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:34.428277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:35.428370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:36.429251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:37.429340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:38.429463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:39.429730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:40.429886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:41.429974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:42.430209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:43.430456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:44.431494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:45.432269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:46.432362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:47.432495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:48.433171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:49.433245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:50.433346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:51.434418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:52.434557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:53.434826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:54.435739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:55.436284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:56.436384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:57.436502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:58.437168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:47:59.437357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:00.438420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:01.438513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:02.438623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:03.438690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:04.439490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:05.439616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:06.439700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:07.439788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:08.440425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:09.440613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:10.441269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:11.441452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:12.441916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:13.442122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:14.442635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:15.442842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:16.443216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:17.443304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:18.444186      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:19.444276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:20.444368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:21.444601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:22.445400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:23.445993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:24.446659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:25.447207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:26.448278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:27.448372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:28.448978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:29.449480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:30.449556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:31.450073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:32.450793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:33.451092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:34.451853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:35.451945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:36.452621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:37.452809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:38.452906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:39.453819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:40.453917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:41.454887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:42.455803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:43.456280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:44.456372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:45.456567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:46.457611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:47.458387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:48.458480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:49.458567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:50.459398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:51.460277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:52.460364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:53.460451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:54.461466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:55.461554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:56.462037      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:57.462224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:58.462351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:48:59.462617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:00.462878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:01.463039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:02.463144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:03.464096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:04.465068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:05.465286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:06.465533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:07.465629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:08.466063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:09.466165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:10.466176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:11.466326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:12.467133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:13.468097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:14.468255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:15.468332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:16.468428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:17.468603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:18.469621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:19.469707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:20.469955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:21.470165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:22.470472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:23.470868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:24.471282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:25.472282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:26.472540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:27.472632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:28.472730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:29.473399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:30.473504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:31.474062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:32.474138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:33.474351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:34.474721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:35.474811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:36.475537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:37.476278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:38.476535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:39.476660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:40.477594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:41.477721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:42.477917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:43.478158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:44.479079      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:45.479212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:46.479484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:47.479572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:48.480283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:49.480574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:50.481201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:51.481898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:52.482840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:53.483879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:54.484132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:55.484476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:56.484563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:57.484924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:58.485013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:49:59.485253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:00.486241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:01.486432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:02.486512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:03.486942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:04.487477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:05.488280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:06.488576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:07.488814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:08.489105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:09.490172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:10.490685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:11.490879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:12.491424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:13.491510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:14.491718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:15.491870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:16.492252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:17.492918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:18.493180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:19.494103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:20.494915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:21.495088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:22.495208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:23.496253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:24.496390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:25.496486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:26.497273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:27.497467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:28.498208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:29.498300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:30.499201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:31.499226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:32.499310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:33.500276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:34.501080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:35.501670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:36.502282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:37.502384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:38.503062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:39.503211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:40.503802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:41.504292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:42.504480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:43.504556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:44.505471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:45.505569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:46.506271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:47.506533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:48.507360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:49.508276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:50.508366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:51.508558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:52.509273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:53.509542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:54.509733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:55.510341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:56.510417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:57.510505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:58.510599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:50:59.510791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:00.511160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:01.511219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:02.511900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:03.512110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:04.513053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:05.513144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:06.513724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:07.514304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:08.514566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:09.514727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:51:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:51:10.196
  STEP: Destroying namespace "container-probe-8126" for this suite. @ 08/19/23 12:51:10.209
• [242.663 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/19/23 12:51:10.217
  Aug 19 12:51:10.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 12:51:10.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:10.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:10.238
  STEP: Creating configMap with name configmap-test-volume-map-0d790a9e-d3c0-4d6c-baff-05fa850ee67d @ 08/19/23 12:51:10.242
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:51:10.248
  E0819 12:51:10.514885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:11.515003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:12.515828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:13.515926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:51:14.273
  Aug 19 12:51:14.278: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-b0d79ab5-f40f-43ae-bd03-20cbddfadcab container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 12:51:14.297
  Aug 19 12:51:14.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9848" for this suite. @ 08/19/23 12:51:14.32
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/19/23 12:51:14.332
  Aug 19 12:51:14.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 12:51:14.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:14.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:14.354
  Aug 19 12:51:14.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2356" for this suite. @ 08/19/23 12:51:14.367
• [0.042 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/19/23 12:51:14.374
  Aug 19 12:51:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 12:51:14.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:14.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:14.392
  STEP: Create a Replicaset @ 08/19/23 12:51:14.404
  STEP: Verify that the required pods have come up. @ 08/19/23 12:51:14.41
  Aug 19 12:51:14.415: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0819 12:51:14.516303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:15.516395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:16.516707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:17.516798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:18.517086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:51:19.420: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 12:51:19.42
  STEP: Getting /status @ 08/19/23 12:51:19.42
  Aug 19 12:51:19.424: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/19/23 12:51:19.424
  Aug 19 12:51:19.438: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/19/23 12:51:19.438
  Aug 19 12:51:19.440: INFO: Observed &ReplicaSet event: ADDED
  Aug 19 12:51:19.441: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.441: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.441: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.441: INFO: Found replicaset test-rs in namespace replicaset-2684 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 19 12:51:19.441: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/19/23 12:51:19.441
  Aug 19 12:51:19.441: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 19 12:51:19.450: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/19/23 12:51:19.45
  Aug 19 12:51:19.453: INFO: Observed &ReplicaSet event: ADDED
  Aug 19 12:51:19.453: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.453: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.454: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.454: INFO: Observed replicaset test-rs in namespace replicaset-2684 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 19 12:51:19.454: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 19 12:51:19.454: INFO: Found replicaset test-rs in namespace replicaset-2684 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug 19 12:51:19.454: INFO: Replicaset test-rs has a patched status
  Aug 19 12:51:19.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2684" for this suite. @ 08/19/23 12:51:19.459
• [5.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/19/23 12:51:19.469
  Aug 19 12:51:19.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 12:51:19.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:19.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:19.495
  Aug 19 12:51:19.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 12:51:19.517408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:20.518229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/19/23 12:51:20.92
  Aug 19 12:51:20.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 create -f -'
  E0819 12:51:21.518530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:51:21.563: INFO: stderr: ""
  Aug 19 12:51:21.563: INFO: stdout: "e2e-test-crd-publish-openapi-7204-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 19 12:51:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 delete e2e-test-crd-publish-openapi-7204-crds test-cr'
  Aug 19 12:51:21.650: INFO: stderr: ""
  Aug 19 12:51:21.650: INFO: stdout: "e2e-test-crd-publish-openapi-7204-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug 19 12:51:21.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 apply -f -'
  Aug 19 12:51:21.859: INFO: stderr: ""
  Aug 19 12:51:21.859: INFO: stdout: "e2e-test-crd-publish-openapi-7204-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 19 12:51:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-4521 --namespace=crd-publish-openapi-4521 delete e2e-test-crd-publish-openapi-7204-crds test-cr'
  Aug 19 12:51:21.945: INFO: stderr: ""
  Aug 19 12:51:21.945: INFO: stdout: "e2e-test-crd-publish-openapi-7204-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/19/23 12:51:21.945
  Aug 19 12:51:21.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-4521 explain e2e-test-crd-publish-openapi-7204-crds'
  Aug 19 12:51:22.147: INFO: stderr: ""
  Aug 19 12:51:22.147: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-7204-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0819 12:51:22.519326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:51:23.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4521" for this suite. @ 08/19/23 12:51:23.438
• [3.977 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/19/23 12:51:23.447
  Aug 19 12:51:23.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 12:51:23.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:23.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:23.468
  STEP: Creating Indexed job @ 08/19/23 12:51:23.471
  STEP: Ensuring job reaches completions @ 08/19/23 12:51:23.478
  E0819 12:51:23.519761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:24.519844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:25.520694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:26.520793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:27.520936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:28.521610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:29.522421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:30.523325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:31.523654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:32.523782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 08/19/23 12:51:33.483
  Aug 19 12:51:33.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9672" for this suite. @ 08/19/23 12:51:33.492
• [10.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/19/23 12:51:33.502
  Aug 19 12:51:33.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename cronjob @ 08/19/23 12:51:33.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:51:33.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:51:33.523
  E0819 12:51:33.524068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a cronjob @ 08/19/23 12:51:33.526
  STEP: Ensuring more than one job is running at a time @ 08/19/23 12:51:33.531
  E0819 12:51:34.524130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:35.524468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:36.524542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:37.524617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:38.524872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:39.525054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:40.525159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:41.525251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:42.525350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:43.525666      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:44.525749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:45.525968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:46.526961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:47.527247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:48.528315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:49.528407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:50.528470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:51.528604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:52.528685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:53.528816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:54.528933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:55.529113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:56.529231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:57.529315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:58.530306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:51:59.530397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:00.531234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:01.532328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:02.532733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:03.533238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:04.533366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:05.534377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:06.534510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:07.535405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:08.535589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:09.536541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:10.537432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:11.538119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:12.538268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:13.538794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:14.538933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:15.539566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:16.540307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:17.541021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:18.541895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:19.542942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:20.543648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:21.543650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:22.543762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:23.543886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:24.544018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:25.544204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:26.544396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:27.544411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:28.544651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:29.545668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:30.545933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:31.546050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:32.546830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:33.547320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:34.547453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:35.548003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:36.548499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:37.548905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:38.549829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:39.550595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:40.550680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:41.551699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:42.552293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:43.552838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:44.552986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:45.553826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:46.554041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:47.554023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:48.554287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:49.554789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:50.555006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:51.555593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:52.556309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:53.556985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:54.557087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:55.557838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:56.558040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:57.558498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:58.558688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:52:59.559041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:00.559236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/19/23 12:53:01.535
  STEP: Removing cronjob @ 08/19/23 12:53:01.54
  Aug 19 12:53:01.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2722" for this suite. @ 08/19/23 12:53:01.553
  E0819 12:53:01.559844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [88.060 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/19/23 12:53:01.562
  Aug 19 12:53:01.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 12:53:01.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:01.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:01.599
  STEP: Creating configMap with name projected-configmap-test-volume-133837d1-a30c-46a3-b838-9a99d74eb194 @ 08/19/23 12:53:01.603
  STEP: Creating a pod to test consume configMaps @ 08/19/23 12:53:01.608
  E0819 12:53:02.560304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:03.560685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:04.560794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:05.561602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:53:05.635
  Aug 19 12:53:05.639: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-37e245f4-6724-4c07-9583-f4777ad8abd3 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:53:05.655
  Aug 19 12:53:05.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6538" for this suite. @ 08/19/23 12:53:05.68
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/19/23 12:53:05.688
  Aug 19 12:53:05.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/19/23 12:53:05.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:05.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:05.707
  Aug 19 12:53:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 12:53:06.562453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:07.562536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:08.563093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:53:08.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9899" for this suite. @ 08/19/23 12:53:08.937
• [3.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/19/23 12:53:08.947
  Aug 19 12:53:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 12:53:08.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:09.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:09.018
  STEP: Creating a test headless service @ 08/19/23 12:53:09.021
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8655 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8655;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8655 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8655;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8655.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8655.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8655.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8655.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8655.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8655.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8655.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8655.svc;check="$$(dig +notcp +noall +answer +search 193.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.193_tcp@PTR;sleep 1; done
   @ 08/19/23 12:53:09.04
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8655 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8655;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8655 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8655;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8655.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8655.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8655.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8655.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8655.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8655.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8655.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8655.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8655.svc;check="$$(dig +notcp +noall +answer +search 193.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.193_tcp@PTR;sleep 1; done
   @ 08/19/23 12:53:09.04
  STEP: creating a pod to probe DNS @ 08/19/23 12:53:09.041
  STEP: submitting the pod to kubernetes @ 08/19/23 12:53:09.041
  E0819 12:53:09.563954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:10.564298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/19/23 12:53:11.072
  STEP: looking for the results for each expected name from probers @ 08/19/23 12:53:11.076
  Aug 19 12:53:11.085: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.091: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.098: INFO: Unable to read wheezy_udp@dns-test-service.dns-8655 from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.102: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8655 from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.112: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.116: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.121: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.144: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.148: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.153: INFO: Unable to read jessie_udp@dns-test-service.dns-8655 from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.157: INFO: Unable to read jessie_tcp@dns-test-service.dns-8655 from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.161: INFO: Unable to read jessie_udp@dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.166: INFO: Unable to read jessie_tcp@dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.170: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.175: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8655.svc from pod dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff: the server could not find the requested resource (get pods dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff)
  Aug 19 12:53:11.193: INFO: Lookups using dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8655 wheezy_tcp@dns-test-service.dns-8655 wheezy_udp@dns-test-service.dns-8655.svc wheezy_tcp@dns-test-service.dns-8655.svc wheezy_udp@_http._tcp.dns-test-service.dns-8655.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8655.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8655 jessie_tcp@dns-test-service.dns-8655 jessie_udp@dns-test-service.dns-8655.svc jessie_tcp@dns-test-service.dns-8655.svc jessie_udp@_http._tcp.dns-test-service.dns-8655.svc jessie_tcp@_http._tcp.dns-test-service.dns-8655.svc]

  E0819 12:53:11.564515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:12.564583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:13.564680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:14.564761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:15.565410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:53:16.311: INFO: DNS probes using dns-8655/dns-test-f14c109c-dade-4c85-a4aa-229d99f7cfff succeeded

  Aug 19 12:53:16.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 12:53:16.316
  STEP: deleting the test service @ 08/19/23 12:53:16.332
  STEP: deleting the test headless service @ 08/19/23 12:53:16.36
  STEP: Destroying namespace "dns-8655" for this suite. @ 08/19/23 12:53:16.373
• [7.434 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/19/23 12:53:16.382
  Aug 19 12:53:16.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename runtimeclass @ 08/19/23 12:53:16.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:16.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:16.406
  STEP: getting /apis @ 08/19/23 12:53:16.41
  STEP: getting /apis/node.k8s.io @ 08/19/23 12:53:16.414
  STEP: getting /apis/node.k8s.io/v1 @ 08/19/23 12:53:16.415
  STEP: creating @ 08/19/23 12:53:16.416
  STEP: watching @ 08/19/23 12:53:16.441
  Aug 19 12:53:16.441: INFO: starting watch
  STEP: getting @ 08/19/23 12:53:16.449
  STEP: listing @ 08/19/23 12:53:16.455
  STEP: patching @ 08/19/23 12:53:16.461
  STEP: updating @ 08/19/23 12:53:16.465
  Aug 19 12:53:16.471: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/19/23 12:53:16.471
  STEP: deleting a collection @ 08/19/23 12:53:16.486
  Aug 19 12:53:16.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9735" for this suite. @ 08/19/23 12:53:16.508
• [0.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/19/23 12:53:16.517
  Aug 19 12:53:16.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 12:53:16.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:16.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:16.536
  STEP: Creating a suspended job @ 08/19/23 12:53:16.543
  STEP: Patching the Job @ 08/19/23 12:53:16.551
  E0819 12:53:16.566378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Watching for Job to be patched @ 08/19/23 12:53:16.57
  Aug 19 12:53:16.571: INFO: Event ADDED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 19 12:53:16.571: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 19 12:53:16.571: INFO: Event MODIFIED found for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/19/23 12:53:16.572
  STEP: Watching for Job to be updated @ 08/19/23 12:53:16.584
  Aug 19 12:53:16.585: INFO: Event MODIFIED found for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:16.585: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/19/23 12:53:16.586
  Aug 19 12:53:16.590: INFO: Job: e2e-m79ms as labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched]
  STEP: Waiting for job to complete @ 08/19/23 12:53:16.59
  E0819 12:53:17.566752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:18.567146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:19.567239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:20.567313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:21.567419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:22.567516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:23.568291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:24.568491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 08/19/23 12:53:24.594
  STEP: Watching for Job to be deleted @ 08/19/23 12:53:24.605
  Aug 19 12:53:24.607: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:24.608: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:24.608: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:24.608: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:24.608: INFO: Event MODIFIED observed for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 19 12:53:24.608: INFO: Event DELETED found for Job e2e-m79ms in namespace job-1034 with labels: map[e2e-job-label:e2e-m79ms e2e-m79ms:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/19/23 12:53:24.609
  Aug 19 12:53:24.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1034" for this suite. @ 08/19/23 12:53:24.625
• [8.120 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/19/23 12:53:24.637
  Aug 19 12:53:24.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/19/23 12:53:24.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:24.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:24.66
  STEP: create the container to handle the HTTPGet hook request. @ 08/19/23 12:53:24.668
  E0819 12:53:25.568500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:26.568597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/19/23 12:53:26.694
  E0819 12:53:27.568667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:28.569756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/19/23 12:53:28.718
  STEP: delete the pod with lifecycle hook @ 08/19/23 12:53:28.726
  E0819 12:53:29.569840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:30.569933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:31.570930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:32.571162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:53:32.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8104" for this suite. @ 08/19/23 12:53:32.75
• [8.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/19/23 12:53:32.761
  Aug 19 12:53:32.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption @ 08/19/23 12:53:32.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:53:32.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:53:32.782
  Aug 19 12:53:32.801: INFO: Waiting up to 1m0s for all nodes to be ready
  E0819 12:53:33.571171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:34.571245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:35.571332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:36.572304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:37.573246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:38.573570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:39.574075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:40.574172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:41.574254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:42.574460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:43.575456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:44.575547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:45.576309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:46.576608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:47.576695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:48.577129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:49.577228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:50.577416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:51.578465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:52.579228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:53.580252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:54.580350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:55.581397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:56.581474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:57.582544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:58.582655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:53:59.583231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:00.583326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:01.583414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:02.584287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:03.584331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:04.584526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:05.584572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:06.584661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:07.585033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:08.585457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:09.585546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:10.585642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:11.585690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:12.586368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:13.586561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:14.586589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:15.586669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:16.586881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:17.587285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:18.587470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:19.588029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:20.588297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:21.588391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:22.588571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:23.588657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:24.589721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:25.589818      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:26.590029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:27.590970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:28.591296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:29.592293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:30.592398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:31.592460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:32.593350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:54:32.821: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/19/23 12:54:32.824
  Aug 19 12:54:32.845: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 19 12:54:32.852: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 19 12:54:32.872: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 19 12:54:32.881: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 19 12:54:32.898: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 19 12:54:32.907: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/19/23 12:54:32.907
  E0819 12:54:33.593431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:34.593505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/19/23 12:54:34.938
  E0819 12:54:35.593605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:36.594465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:37.594550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:38.594629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:54:39.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1631" for this suite. @ 08/19/23 12:54:39.051
• [66.298 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/19/23 12:54:39.059
  Aug 19 12:54:39.059: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename watch @ 08/19/23 12:54:39.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:39.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:39.087
  STEP: creating a watch on configmaps @ 08/19/23 12:54:39.09
  STEP: creating a new configmap @ 08/19/23 12:54:39.092
  STEP: modifying the configmap once @ 08/19/23 12:54:39.098
  STEP: closing the watch once it receives two notifications @ 08/19/23 12:54:39.107
  Aug 19 12:54:39.107: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1648  e9667101-8562-4660-8698-005200347933 28173 0 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:54:39.107: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1648  e9667101-8562-4660-8698-005200347933 28174 0 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/19/23 12:54:39.108
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/19/23 12:54:39.116
  STEP: deleting the configmap @ 08/19/23 12:54:39.118
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/19/23 12:54:39.124
  Aug 19 12:54:39.124: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1648  e9667101-8562-4660-8698-005200347933 28175 0 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:54:39.124: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1648  e9667101-8562-4660-8698-005200347933 28176 0 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 12:54:39.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1648" for this suite. @ 08/19/23 12:54:39.129
• [0.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/19/23 12:54:39.136
  Aug 19 12:54:39.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 12:54:39.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:39.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:39.155
  Aug 19 12:54:39.162: INFO: Creating simple deployment test-new-deployment
  Aug 19 12:54:39.182: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0819 12:54:39.595490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:40.595579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 08/19/23 12:54:41.199
  STEP: updating a scale subresource @ 08/19/23 12:54:41.203
  STEP: verifying the deployment Spec.Replicas was modified @ 08/19/23 12:54:41.208
  STEP: Patch a scale subresource @ 08/19/23 12:54:41.212
  Aug 19 12:54:41.236: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-2148  762a826a-84a7-4180-9f3c-13aab79a9ea5 28220 3 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004de3a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-19 12:54:40 +0000 UTC,LastTransitionTime:2023-08-19 12:54:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-19 12:54:40 +0000 UTC,LastTransitionTime:2023-08-19 12:54:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 19 12:54:41.241: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-2148  cf1be9e1-739d-462c-91a0-1693d2879907 28223 2 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 762a826a-84a7-4180-9f3c-13aab79a9ea5 0xc00556a457 0xc00556a458}] [] [{kube-controller-manager Update apps/v1 2023-08-19 12:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"762a826a-84a7-4180-9f3c-13aab79a9ea5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 12:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00556a4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 12:54:41.246: INFO: Pod "test-new-deployment-67bd4bf6dc-5n8dx" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-5n8dx test-new-deployment-67bd4bf6dc- deployment-2148  4aa4a9c8-8bba-4201-86b1-66ea1fde9209 28224 0 2023-08-19 12:54:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc cf1be9e1-739d-462c-91a0-1693d2879907 0xc004de3e77 0xc004de3e78}] [] [{kube-controller-manager Update v1 2023-08-19 12:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf1be9e1-739d-462c-91a0-1693d2879907\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4bp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4bp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-69-13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:54:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:54:41.249: INFO: Pod "test-new-deployment-67bd4bf6dc-78sj7" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-78sj7 test-new-deployment-67bd4bf6dc- deployment-2148  5f84ed6c-dde7-472b-9672-c246301801a9 28211 0 2023-08-19 12:54:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc cf1be9e1-739d-462c-91a0-1693d2879907 0xc004de3fe0 0xc004de3fe1}] [] [{kube-controller-manager Update v1 2023-08-19 12:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf1be9e1-739d-462c-91a0-1693d2879907\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 12:54:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhdjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhdjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:54:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 12:54:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.153,StartTime:2023-08-19 12:54:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 12:54:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://59277c9382a464cd9b7370807ded3209b464aeeb9c9a13384560d2be4d36c135,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 12:54:41.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2148" for this suite. @ 08/19/23 12:54:41.26
• [2.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/19/23 12:54:41.276
  Aug 19 12:54:41.276: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:54:41.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:41.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:41.301
  STEP: creating secret secrets-9430/secret-test-3b3bafc5-ae58-4473-b01c-863c369ce812 @ 08/19/23 12:54:41.304
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:54:41.309
  E0819 12:54:41.595782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:42.595788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:43.596174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:44.596353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:54:45.331
  Aug 19 12:54:45.334: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-bac344ef-602b-4448-9114-9096746bfcd5 container env-test: <nil>
  STEP: delete the pod @ 08/19/23 12:54:45.343
  Aug 19 12:54:45.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9430" for this suite. @ 08/19/23 12:54:45.366
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/19/23 12:54:45.375
  Aug 19 12:54:45.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:54:45.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:45.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:45.395
  STEP: Counting existing ResourceQuota @ 08/19/23 12:54:45.399
  E0819 12:54:45.597348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:46.597485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:47.597952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:48.598575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:49.598670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/19/23 12:54:50.403
  STEP: Ensuring resource quota status is calculated @ 08/19/23 12:54:50.411
  E0819 12:54:50.599542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:51.600368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/19/23 12:54:52.415
  STEP: Ensuring resource quota status captures replication controller creation @ 08/19/23 12:54:52.43
  E0819 12:54:52.601112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:53.601230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/19/23 12:54:54.435
  STEP: Ensuring resource quota status released usage @ 08/19/23 12:54:54.442
  E0819 12:54:54.602216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:55.602490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:54:56.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5583" for this suite. @ 08/19/23 12:54:56.452
• [11.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/19/23 12:54:56.464
  Aug 19 12:54:56.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 12:54:56.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:56.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:56.484
  STEP: Creating a ResourceQuota @ 08/19/23 12:54:56.487
  STEP: Getting a ResourceQuota @ 08/19/23 12:54:56.492
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/19/23 12:54:56.497
  STEP: Patching the ResourceQuota @ 08/19/23 12:54:56.501
  STEP: Deleting a Collection of ResourceQuotas @ 08/19/23 12:54:56.508
  STEP: Verifying the deleted ResourceQuota @ 08/19/23 12:54:56.519
  Aug 19 12:54:56.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4549" for this suite. @ 08/19/23 12:54:56.527
• [0.071 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/19/23 12:54:56.535
  Aug 19 12:54:56.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:54:56.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:54:56.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:54:56.554
  STEP: Setting up server cert @ 08/19/23 12:54:56.579
  E0819 12:54:56.602816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:54:57.052
  STEP: Deploying the webhook pod @ 08/19/23 12:54:57.057
  STEP: Wait for the deployment to be ready @ 08/19/23 12:54:57.075
  Aug 19 12:54:57.091: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 12:54:57.603522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:54:58.603596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:54:59.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 19, 12, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 54, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 12, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 12, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 12:54:59.603718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:00.604700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 12:55:01.109
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:55:01.12
  E0819 12:55:01.605359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:55:02.122: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/19/23 12:55:02.125
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/19/23 12:55:02.142
  Aug 19 12:55:02.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 12:55:02.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9096" for this suite. @ 08/19/23 12:55:02.213
  STEP: Destroying namespace "webhook-markers-3685" for this suite. @ 08/19/23 12:55:02.222
• [5.694 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/19/23 12:55:02.231
  Aug 19 12:55:02.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 12:55:02.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:02.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:02.252
  STEP: Setting up server cert @ 08/19/23 12:55:02.276
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 12:55:02.535
  STEP: Deploying the webhook pod @ 08/19/23 12:55:02.541
  STEP: Wait for the deployment to be ready @ 08/19/23 12:55:02.553
  Aug 19 12:55:02.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 12:55:02.605858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:03.605957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 12:55:04.573
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 12:55:04.584
  E0819 12:55:04.606951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:55:05.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/19/23 12:55:05.59
  E0819 12:55:05.607600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/19/23 12:55:05.609
  STEP: Creating a configMap that should not be mutated @ 08/19/23 12:55:05.619
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/19/23 12:55:05.633
  STEP: Creating a configMap that should be mutated @ 08/19/23 12:55:05.641
  Aug 19 12:55:05.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4753" for this suite. @ 08/19/23 12:55:05.712
  STEP: Destroying namespace "webhook-markers-7633" for this suite. @ 08/19/23 12:55:05.726
• [3.503 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/19/23 12:55:05.734
  Aug 19 12:55:05.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename namespaces @ 08/19/23 12:55:05.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:05.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:05.755
  STEP: creating a Namespace @ 08/19/23 12:55:05.758
  STEP: patching the Namespace @ 08/19/23 12:55:05.774
  STEP: get the Namespace and ensuring it has the label @ 08/19/23 12:55:05.781
  Aug 19 12:55:05.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-784" for this suite. @ 08/19/23 12:55:05.791
  STEP: Destroying namespace "nspatchtest-9ef9066c-7b2a-4d51-9807-15c488f16a1c-2930" for this suite. @ 08/19/23 12:55:05.798
• [0.072 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/19/23 12:55:05.807
  Aug 19 12:55:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 12:55:05.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:05.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:05.829
  STEP: Creating secret with name secret-test-3f90a02c-63a8-4d47-b2a2-e41ceb8851b2 @ 08/19/23 12:55:05.852
  STEP: Creating a pod to test consume secrets @ 08/19/23 12:55:05.859
  E0819 12:55:06.607738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:07.608298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:08.608389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:09.608581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:55:09.889
  Aug 19 12:55:09.894: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-a5f25dea-a374-41a6-9caa-079e3dc63845 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 12:55:09.903
  Aug 19 12:55:09.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8274" for this suite. @ 08/19/23 12:55:09.928
  STEP: Destroying namespace "secret-namespace-5586" for this suite. @ 08/19/23 12:55:09.935
• [4.137 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/19/23 12:55:09.947
  Aug 19 12:55:09.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 12:55:09.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:09.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:09.975
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/19/23 12:55:09.979
  E0819 12:55:10.609406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:11.609584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:12.609624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:13.609654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:55:14.004
  Aug 19 12:55:14.009: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-785da066-d01c-4b25-ad4c-a21e68b26454 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 12:55:14.019
  Aug 19 12:55:14.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-692" for this suite. @ 08/19/23 12:55:14.039
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/19/23 12:55:14.05
  Aug 19 12:55:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename endpointslice @ 08/19/23 12:55:14.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:14.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:14.089
  E0819 12:55:14.609900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:15.610086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:55:16.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4377" for this suite. @ 08/19/23 12:55:16.183
• [2.141 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/19/23 12:55:16.192
  Aug 19 12:55:16.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 12:55:16.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:16.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:16.213
  STEP: create the deployment @ 08/19/23 12:55:16.217
  W0819 12:55:16.223590      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/19/23 12:55:16.223
  E0819 12:55:16.610500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 08/19/23 12:55:16.737
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/19/23 12:55:16.747
  STEP: Gathering metrics @ 08/19/23 12:55:17.274
  W0819 12:55:17.280213      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 12:55:17.280: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 12:55:17.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2644" for this suite. @ 08/19/23 12:55:17.285
• [1.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/19/23 12:55:17.294
  Aug 19 12:55:17.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:55:17.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:17.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:17.316
  STEP: creating the pod @ 08/19/23 12:55:17.322
  STEP: submitting the pod to kubernetes @ 08/19/23 12:55:17.322
  W0819 12:55:17.333767      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0819 12:55:17.610573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:18.610679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/19/23 12:55:19.347
  STEP: updating the pod @ 08/19/23 12:55:19.351
  E0819 12:55:19.611545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:55:19.868: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b5382863-96e3-4c51-86c3-cf200c1f829f"
  E0819 12:55:20.611569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:21.611652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:22.611771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:23.612080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 12:55:23.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9256" for this suite. @ 08/19/23 12:55:23.889
• [6.601 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/19/23 12:55:23.898
  Aug 19 12:55:23.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 12:55:23.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:23.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:23.921
  E0819 12:55:24.612192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:25.612280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:26.612347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:27.612468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:28.613101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:29.613287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 12:55:29.984
  Aug 19 12:55:29.989: INFO: Trying to get logs from node ip-172-31-15-214 pod client-envvars-a1650e97-d4cb-4cac-9ed0-4fbc224180bd container env3cont: <nil>
  STEP: delete the pod @ 08/19/23 12:55:29.998
  Aug 19 12:55:30.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2372" for this suite. @ 08/19/23 12:55:30.017
• [6.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/19/23 12:55:30.026
  Aug 19 12:55:30.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename cronjob @ 08/19/23 12:55:30.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 12:55:30.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 12:55:30.044
  STEP: Creating a ForbidConcurrent cronjob @ 08/19/23 12:55:30.048
  STEP: Ensuring a job is scheduled @ 08/19/23 12:55:30.054
  E0819 12:55:30.613521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:31.614102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:32.614194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:33.614206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:34.614293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:35.614420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:36.615216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:37.616284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:38.616339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:39.616525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:40.616590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:41.616694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:42.616785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:43.617064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:44.617169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:45.617362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:46.617455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:47.617651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:48.617723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:49.617919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:50.618324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:51.618409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:52.618677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:53.618984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:54.619955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:55.620299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:56.621355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:57.621510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:58.622445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:55:59.622537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:00.622625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:01.622766      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/19/23 12:56:02.059
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/19/23 12:56:02.063
  STEP: Ensuring no more jobs are scheduled @ 08/19/23 12:56:02.066
  E0819 12:56:02.623411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:03.624293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:04.624387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:05.624768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:06.624866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:07.625076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:08.625850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:09.626807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:10.626906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:11.627138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:12.628143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:13.628379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:14.629010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:15.629107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:16.629834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:17.630278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:18.630743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:19.631230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:20.631924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:21.632332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:22.632405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:23.632699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:24.633774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:25.633962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:26.634948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:27.635484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:28.636296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:29.636472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:30.636561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:31.636709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:32.636941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:33.637217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:34.637858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:35.638115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:36.638217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:37.638404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:38.639361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:39.640281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:40.641219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:41.641506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:42.642202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:43.642248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:44.642960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:45.643218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:46.644280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:47.644460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:48.644551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:49.645053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:50.645086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:51.645746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:52.646582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:53.646674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:54.647687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:55.648345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:56.648385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:57.648573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:58.649435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:56:59.649522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:00.650037      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:01.650107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:02.650297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:03.650564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:04.651601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:05.651656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:06.652237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:07.652334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:08.652488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:09.652641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:10.653034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:11.653146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:12.653677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:13.653749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:14.654773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:15.654943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:16.655482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:17.655571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:18.656278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:19.656368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:20.657289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:21.657449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:22.658261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:23.658461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:24.658665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:25.658826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:26.659212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:27.659289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:28.659990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:29.660277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:30.660371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:31.661164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:32.661980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:33.662094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:34.663070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:35.663209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:36.664269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:37.664749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:38.665656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:39.665755      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:40.665839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:41.666003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:42.666578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:43.666877      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:44.666923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:45.667151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:46.667224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:47.668278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:48.668764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:49.668947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:50.669749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:51.669908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:52.670858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:53.671176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:54.671226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:55.672274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:56.673302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:57.673442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:58.674451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:57:59.674543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:00.675568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:01.676508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:02.676528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:03.676803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:04.676892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:05.677059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:06.677294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:07.678043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:08.679005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:09.679116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:10.679216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:11.680283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:12.680377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:13.680730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:14.680775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:15.680982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:16.681491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:17.681687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:18.681777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:19.681976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:20.681998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:21.682172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:22.682496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:23.682584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:24.683179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:25.683229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:26.684278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:27.684509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:28.684581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:29.684682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:30.685584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:31.685794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:32.686779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:33.687069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:34.687211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:35.688281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:36.689324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:37.689512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:38.690114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:39.690305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:40.690681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:41.691066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:42.691230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:43.692275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:44.692309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:45.692390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:46.693261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:47.693341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:48.693377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:49.693457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:50.694317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:51.694496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:52.694583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:53.694671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:54.694828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:55.695044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:56.695204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:57.696276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:58.696631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:58:59.696815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:00.697305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:01.697392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:02.697473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:03.697836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:04.697901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:05.698088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:06.699170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:07.699228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:08.700075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:09.700232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:10.700659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:11.700834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:12.701860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:13.701955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:14.702920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:15.703011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:16.703328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:17.704282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:18.704368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:19.704533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:20.705573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:21.705673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:22.705754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:23.705830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:24.706436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:25.706638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:26.707211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:27.708281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:28.708833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:29.708929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:30.709715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:31.710237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:32.710330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:33.710724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:34.711550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:35.712283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:36.712697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:37.713731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:38.713821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:39.713916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:40.714931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:41.715994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:42.716580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:43.717380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:44.717982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:45.718141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:46.719207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:47.719224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:48.720290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:49.720381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:50.721315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:51.721501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:52.722032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:53.722708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:54.722750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:55.723750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:56.723838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:57.724283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:58.724372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 12:59:59.724619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:00.725162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:01.725291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:02.725821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:03.725911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:04.726002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:05.726091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:06.726146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:07.726190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:08.726268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:09.726465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:10.726563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:11.726655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:12.727622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:13.727710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:14.728106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:15.728939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:16.729032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:17.729289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:18.729914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:19.730116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:20.730573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:21.731656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:22.732279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:23.732515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:24.733054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:25.733144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:26.733228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:27.734201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:28.734292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:29.734486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:30.735506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:31.735597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:32.735948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:33.736048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:34.736446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:35.736527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:36.737506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:37.737667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:38.737776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:39.737967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:40.738131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:41.745422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:42.745812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:43.745893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:44.746327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:45.747277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:46.749203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:47.749279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:48.749481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:49.749668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:50.750502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:51.750688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:52.751212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:53.752280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:54.753057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:55.753146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:56.753300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:57.753393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:58.753494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:00:59.754464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:00.754540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:01.754749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/19/23 13:01:02.075
  Aug 19 13:01:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7961" for this suite. @ 08/19/23 13:01:02.087
• [332.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/19/23 13:01:02.099
  Aug 19 13:01:02.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context @ 08/19/23 13:01:02.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:02.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:02.124
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/19/23 13:01:02.128
  E0819 13:01:02.755686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:03.755768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:04.756291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:05.756380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:01:06.153
  Aug 19 13:01:06.156: INFO: Trying to get logs from node ip-172-31-15-214 pod security-context-4cd34bbc-a944-4f8b-a25d-80f78066013f container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:01:06.177
  Aug 19 13:01:06.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-2185" for this suite. @ 08/19/23 13:01:06.199
• [4.107 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/19/23 13:01:06.207
  Aug 19 13:01:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:01:06.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:06.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:06.226
  STEP: creating service in namespace services-872 @ 08/19/23 13:01:06.229
  STEP: creating service affinity-nodeport in namespace services-872 @ 08/19/23 13:01:06.229
  STEP: creating replication controller affinity-nodeport in namespace services-872 @ 08/19/23 13:01:06.244
  I0819 13:01:06.254920      18 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-872, replica count: 3
  E0819 13:01:06.757029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:07.757057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:08.758081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:01:09.306777      18 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 13:01:09.318: INFO: Creating new exec pod
  E0819 13:01:09.758099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:10.758183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:11.758553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:01:12.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-872 exec execpod-affinityh4h8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug 19 13:01:12.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug 19 13:01:12.486: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:01:12.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-872 exec execpod-affinityh4h8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.239 80'
  Aug 19 13:01:12.620: INFO: stderr: "+ nc -v -t -w 2 10.152.183.239 80\n+ echo hostName\nConnection to 10.152.183.239 80 port [tcp/http] succeeded!\n"
  Aug 19 13:01:12.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:01:12.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-872 exec execpod-affinityh4h8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.145 30030'
  E0819 13:01:12.759014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:01:12.761: INFO: stderr: "+ nc -v -t -w 2 172.31.42.145 30030\n+ echo hostName\nConnection to 172.31.42.145 30030 port [tcp/*] succeeded!\n"
  Aug 19 13:01:12.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:01:12.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-872 exec execpod-affinityh4h8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.69.13 30030'
  Aug 19 13:01:12.897: INFO: stderr: "+ nc -v -t -w 2 172.31.69.13 30030\n+ echo hostName\nConnection to 172.31.69.13 30030 port [tcp/*] succeeded!\n"
  Aug 19 13:01:12.897: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:01:12.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-872 exec execpod-affinityh4h8f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.214:30030/ ; done'
  Aug 19 13:01:13.109: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30030/\n"
  Aug 19 13:01:13.109: INFO: stdout: "\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l\naffinity-nodeport-s9b5l"
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.109: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Received response from host: affinity-nodeport-s9b5l
  Aug 19 13:01:13.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:01:13.115: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-872, will wait for the garbage collector to delete the pods @ 08/19/23 13:01:13.129
  Aug 19 13:01:13.191: INFO: Deleting ReplicationController affinity-nodeport took: 8.364476ms
  Aug 19 13:01:13.292: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.825835ms
  E0819 13:01:13.759844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:14.760327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-872" for this suite. @ 08/19/23 13:01:15.421
• [9.222 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/19/23 13:01:15.43
  Aug 19 13:01:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename ingress @ 08/19/23 13:01:15.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:15.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:15.468
  STEP: getting /apis @ 08/19/23 13:01:15.472
  STEP: getting /apis/networking.k8s.io @ 08/19/23 13:01:15.476
  STEP: getting /apis/networking.k8s.iov1 @ 08/19/23 13:01:15.477
  STEP: creating @ 08/19/23 13:01:15.479
  STEP: getting @ 08/19/23 13:01:15.529
  STEP: listing @ 08/19/23 13:01:15.536
  STEP: watching @ 08/19/23 13:01:15.54
  Aug 19 13:01:15.540: INFO: starting watch
  STEP: cluster-wide listing @ 08/19/23 13:01:15.542
  STEP: cluster-wide watching @ 08/19/23 13:01:15.545
  Aug 19 13:01:15.545: INFO: starting watch
  STEP: patching @ 08/19/23 13:01:15.546
  STEP: updating @ 08/19/23 13:01:15.552
  Aug 19 13:01:15.561: INFO: waiting for watch events with expected annotations
  Aug 19 13:01:15.561: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/19/23 13:01:15.562
  STEP: updating /status @ 08/19/23 13:01:15.571
  STEP: get /status @ 08/19/23 13:01:15.581
  STEP: deleting @ 08/19/23 13:01:15.586
  STEP: deleting a collection @ 08/19/23 13:01:15.602
  Aug 19 13:01:15.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-4495" for this suite. @ 08/19/23 13:01:15.623
• [0.201 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/19/23 13:01:15.631
  Aug 19 13:01:15.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 13:01:15.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:15.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:15.651
  E0819 13:01:15.760553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:16.760660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:01:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:01:17.679: INFO: Deleting pod "var-expansion-8c7de117-4858-4f16-bb88-3608cdeb086b" in namespace "var-expansion-3753"
  Aug 19 13:01:17.688: INFO: Wait up to 5m0s for pod "var-expansion-8c7de117-4858-4f16-bb88-3608cdeb086b" to be fully deleted
  E0819 13:01:17.761655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:18.762167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3753" for this suite. @ 08/19/23 13:01:19.696
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/19/23 13:01:19.707
  Aug 19 13:01:19.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:01:19.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:19.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:19.73
  STEP: Creating a pod to test downward api env vars @ 08/19/23 13:01:19.733
  E0819 13:01:19.762504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:20.762639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:21.763324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:22.763453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:01:23.757
  Aug 19 13:01:23.761: INFO: Trying to get logs from node ip-172-31-15-214 pod downward-api-990e7402-d931-4045-a66d-1a2ac14c148b container dapi-container: <nil>
  E0819 13:01:23.764001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 08/19/23 13:01:23.77
  Aug 19 13:01:23.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3670" for this suite. @ 08/19/23 13:01:23.789
• [4.090 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/19/23 13:01:23.797
  Aug 19 13:01:23.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-runtime @ 08/19/23 13:01:23.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:23.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:23.821
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/19/23 13:01:23.839
  E0819 13:01:24.764136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:25.764184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:26.764291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:27.764363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:28.764858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:29.764935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:30.765046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:31.765120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:32.765227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:33.765315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:34.765679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:35.765784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:36.765816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:37.766253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:38.766554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:39.767247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/19/23 13:01:39.915
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/19/23 13:01:39.918
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/19/23 13:01:39.926
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/19/23 13:01:39.926
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/19/23 13:01:39.951
  E0819 13:01:40.768286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:41.768789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:42.768858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/19/23 13:01:42.967
  E0819 13:01:43.769326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/19/23 13:01:43.976
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/19/23 13:01:43.984
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/19/23 13:01:43.984
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/19/23 13:01:44.01
  E0819 13:01:44.769952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/19/23 13:01:45.018
  E0819 13:01:45.770900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:46.771246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/19/23 13:01:47.033
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/19/23 13:01:47.04
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/19/23 13:01:47.04
  Aug 19 13:01:47.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5082" for this suite. @ 08/19/23 13:01:47.074
• [23.283 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/19/23 13:01:47.08
  Aug 19 13:01:47.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:01:47.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:01:47.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:01:47.101
  STEP: Setting up server cert @ 08/19/23 13:01:47.13
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:01:47.469
  STEP: Deploying the webhook pod @ 08/19/23 13:01:47.477
  STEP: Wait for the deployment to be ready @ 08/19/23 13:01:47.489
  Aug 19 13:01:47.496: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:01:47.771820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:48.772237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:01:49.508
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:01:49.518
  E0819 13:01:49.772447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:01:50.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/19/23 13:01:50.522
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/19/23 13:01:50.522
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/19/23 13:01:50.538
  E0819 13:01:50.773295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/19/23 13:01:51.548
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/19/23 13:01:51.548
  E0819 13:01:51.773810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 08/19/23 13:01:52.579
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/19/23 13:01:52.579
  E0819 13:01:52.774631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:53.774844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:54.774945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:55.775133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:56.775292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/19/23 13:01:57.615
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/19/23 13:01:57.615
  E0819 13:01:57.775811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:58.776230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:01:59.776301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:00.776407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:01.776597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:02.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8393" for this suite. @ 08/19/23 13:02:02.722
  STEP: Destroying namespace "webhook-markers-6581" for this suite. @ 08/19/23 13:02:02.729
• [15.657 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/19/23 13:02:02.739
  Aug 19 13:02:02.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 13:02:02.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:02.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:02.761
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/19/23 13:02:02.765
  Aug 19 13:02:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:02:02.776901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:03.777344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:02:04.777829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:05.778304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:06.778490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:07.778790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:08.779124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4528" for this suite. @ 08/19/23 13:02:09.519
• [6.787 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/19/23 13:02:09.526
  Aug 19 13:02:09.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:02:09.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:09.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:09.549
  STEP: Setting up server cert @ 08/19/23 13:02:09.577
  E0819 13:02:09.779509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:02:09.814
  STEP: Deploying the webhook pod @ 08/19/23 13:02:09.821
  STEP: Wait for the deployment to be ready @ 08/19/23 13:02:09.835
  Aug 19 13:02:09.845: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:02:10.779581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:11.779669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:02:11.855
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:02:11.865
  E0819 13:02:12.779746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:12.866: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/19/23 13:02:12.869
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/19/23 13:02:12.884
  STEP: Creating a dummy validating-webhook-configuration object @ 08/19/23 13:02:12.898
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/19/23 13:02:12.906
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/19/23 13:02:12.912
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/19/23 13:02:12.919
  Aug 19 13:02:12.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3955" for this suite. @ 08/19/23 13:02:12.974
  STEP: Destroying namespace "webhook-markers-5245" for this suite. @ 08/19/23 13:02:12.98
• [3.460 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/19/23 13:02:12.987
  Aug 19 13:02:12.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 13:02:12.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:13.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:13.007
  STEP: create the rc @ 08/19/23 13:02:13.01
  W0819 13:02:13.015486      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0819 13:02:13.780288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:14.780384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:15.780452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:16.781131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:17.781330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/19/23 13:02:18.019
  STEP: wait for all pods to be garbage collected @ 08/19/23 13:02:18.026
  E0819 13:02:18.782372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:19.782488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:20.782552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:21.782709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:22.782818      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/19/23 13:02:23.033
  W0819 13:02:23.037338      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 19 13:02:23.037: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 19 13:02:23.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3254" for this suite. @ 08/19/23 13:02:23.041
• [10.060 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/19/23 13:02:23.047
  Aug 19 13:02:23.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename endpointslice @ 08/19/23 13:02:23.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:23.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:23.069
  STEP: getting /apis @ 08/19/23 13:02:23.072
  STEP: getting /apis/discovery.k8s.io @ 08/19/23 13:02:23.075
  STEP: getting /apis/discovery.k8s.iov1 @ 08/19/23 13:02:23.077
  STEP: creating @ 08/19/23 13:02:23.078
  STEP: getting @ 08/19/23 13:02:23.091
  STEP: listing @ 08/19/23 13:02:23.094
  STEP: watching @ 08/19/23 13:02:23.098
  Aug 19 13:02:23.098: INFO: starting watch
  STEP: cluster-wide listing @ 08/19/23 13:02:23.099
  STEP: cluster-wide watching @ 08/19/23 13:02:23.102
  Aug 19 13:02:23.102: INFO: starting watch
  STEP: patching @ 08/19/23 13:02:23.103
  STEP: updating @ 08/19/23 13:02:23.109
  Aug 19 13:02:23.116: INFO: waiting for watch events with expected annotations
  Aug 19 13:02:23.116: INFO: saw patched and updated annotations
  STEP: deleting @ 08/19/23 13:02:23.116
  STEP: deleting a collection @ 08/19/23 13:02:23.128
  Aug 19 13:02:23.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5803" for this suite. @ 08/19/23 13:02:23.146
• [0.105 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/19/23 13:02:23.152
  Aug 19 13:02:23.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 13:02:23.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:23.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:23.175
  Aug 19 13:02:23.178: INFO: Creating ReplicaSet my-hostname-basic-d9687ede-4e9f-4095-9cbb-4395cae0b31f
  Aug 19 13:02:23.187: INFO: Pod name my-hostname-basic-d9687ede-4e9f-4095-9cbb-4395cae0b31f: Found 0 pods out of 1
  E0819 13:02:23.783557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:24.784316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:25.784391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:26.784483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:27.784787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:28.192: INFO: Pod name my-hostname-basic-d9687ede-4e9f-4095-9cbb-4395cae0b31f: Found 1 pods out of 1
  Aug 19 13:02:28.192: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d9687ede-4e9f-4095-9cbb-4395cae0b31f" is running
  Aug 19 13:02:28.195: INFO: Pod "my-hostname-basic-d9687ede-4e9f-4095-9cbb-4395cae0b31f-flfbr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:02:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:02:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:02:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:02:23 +0000 UTC Reason: Message:}])
  Aug 19 13:02:28.195: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/19/23 13:02:28.195
  Aug 19 13:02:28.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6963" for this suite. @ 08/19/23 13:02:28.21
• [5.064 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/19/23 13:02:28.216
  Aug 19 13:02:28.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/19/23 13:02:28.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:28.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:28.24
  E0819 13:02:28.785480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:29.785576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:30.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/19/23 13:02:30.272
  STEP: Cleaning up the configmap @ 08/19/23 13:02:30.278
  STEP: Cleaning up the pod @ 08/19/23 13:02:30.284
  STEP: Destroying namespace "emptydir-wrapper-4252" for this suite. @ 08/19/23 13:02:30.3
• [2.090 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/19/23 13:02:30.309
  Aug 19 13:02:30.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-webhook @ 08/19/23 13:02:30.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:30.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:30.33
  STEP: Setting up server cert @ 08/19/23 13:02:30.333
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/19/23 13:02:30.691
  STEP: Deploying the custom resource conversion webhook pod @ 08/19/23 13:02:30.696
  STEP: Wait for the deployment to be ready @ 08/19/23 13:02:30.708
  Aug 19 13:02:30.717: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0819 13:02:30.786281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:31.786377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:02:32.727
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:02:32.736
  E0819 13:02:32.786721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:33.736: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 19 13:02:33.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:02:33.786861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:34.787842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:35.787914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/19/23 13:02:36.306
  STEP: Create a v2 custom resource @ 08/19/23 13:02:36.326
  STEP: List CRs in v1 @ 08/19/23 13:02:36.404
  STEP: List CRs in v2 @ 08/19/23 13:02:36.418
  Aug 19 13:02:36.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0819 13:02:36.788693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-4376" for this suite. @ 08/19/23 13:02:36.976
• [6.674 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/19/23 13:02:36.986
  Aug 19 13:02:36.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption @ 08/19/23 13:02:36.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:37.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:37.01
  STEP: Creating a kubernetes client @ 08/19/23 13:02:37.014
  Aug 19 13:02:37.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption-2 @ 08/19/23 13:02:37.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:37.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:37.034
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:02:37.041
  E0819 13:02:37.788802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:38.789465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:02:39.052
  E0819 13:02:39.789557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:40.789764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:02:41.063
  E0819 13:02:41.789874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:42.790240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 08/19/23 13:02:43.069
  STEP: listing a collection of PDBs in namespace disruption-6394 @ 08/19/23 13:02:43.073
  STEP: deleting a collection of PDBs @ 08/19/23 13:02:43.076
  STEP: Waiting for the PDB collection to be deleted @ 08/19/23 13:02:43.089
  Aug 19 13:02:43.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:02:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-7881" for this suite. @ 08/19/23 13:02:43.099
  STEP: Destroying namespace "disruption-6394" for this suite. @ 08/19/23 13:02:43.105
• [6.125 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 08/19/23 13:02:43.112
  Aug 19 13:02:43.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 13:02:43.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:43.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:43.135
  STEP: Creating simple DaemonSet "daemon-set" @ 08/19/23 13:02:43.163
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 13:02:43.168
  Aug 19 13:02:43.172: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:43.172: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:43.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:02:43.175: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:02:43.791056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:44.179: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:44.179: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:44.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 19 13:02:44.182: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:02:44.792111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:45.179: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:45.179: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:02:45.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 13:02:45.182: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 08/19/23 13:02:45.185
  Aug 19 13:02:45.189: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/19/23 13:02:45.189
  Aug 19 13:02:45.197: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/19/23 13:02:45.197
  Aug 19 13:02:45.199: INFO: Observed &DaemonSet event: ADDED
  Aug 19 13:02:45.199: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.199: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.200: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.200: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.200: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.200: INFO: Found daemon set daemon-set in namespace daemonsets-6440 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 19 13:02:45.200: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/19/23 13:02:45.2
  STEP: watching for the daemon set status to be patched @ 08/19/23 13:02:45.208
  Aug 19 13:02:45.209: INFO: Observed &DaemonSet event: ADDED
  Aug 19 13:02:45.209: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.210: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.210: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.210: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.210: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.210: INFO: Observed daemon set daemon-set in namespace daemonsets-6440 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 19 13:02:45.211: INFO: Observed &DaemonSet event: MODIFIED
  Aug 19 13:02:45.211: INFO: Found daemon set daemon-set in namespace daemonsets-6440 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug 19 13:02:45.211: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 13:02:45.214
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6440, will wait for the garbage collector to delete the pods @ 08/19/23 13:02:45.214
  Aug 19 13:02:45.274: INFO: Deleting DaemonSet.extensions daemon-set took: 6.705002ms
  Aug 19 13:02:45.374: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.168945ms
  E0819 13:02:45.793072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:46.793898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:47.794428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:48.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:02:48.078: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 13:02:48.081: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30823"},"items":null}

  Aug 19 13:02:48.084: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30823"},"items":null}

  Aug 19 13:02:48.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6440" for this suite. @ 08/19/23 13:02:48.1
• [4.994 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/19/23 13:02:48.107
  Aug 19 13:02:48.108: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-webhook @ 08/19/23 13:02:48.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:48.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:48.13
  STEP: Setting up server cert @ 08/19/23 13:02:48.133
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/19/23 13:02:48.374
  STEP: Deploying the custom resource conversion webhook pod @ 08/19/23 13:02:48.384
  STEP: Wait for the deployment to be ready @ 08/19/23 13:02:48.395
  Aug 19 13:02:48.403: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0819 13:02:48.794728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:49.794829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:02:50.413
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:02:50.423
  E0819 13:02:50.794921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:02:51.425: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 19 13:02:51.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:02:51.795306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:52.796200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:53.796292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/19/23 13:02:53.995
  STEP: v2 custom resource should be converted @ 08/19/23 13:02:54
  Aug 19 13:02:54.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5378" for this suite. @ 08/19/23 13:02:54.559
• [6.458 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/19/23 13:02:54.569
  Aug 19 13:02:54.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 13:02:54.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:02:54.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:02:54.592
  STEP: Creating a ResourceQuota with terminating scope @ 08/19/23 13:02:54.596
  STEP: Ensuring ResourceQuota status is calculated @ 08/19/23 13:02:54.599
  E0819 13:02:54.797300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:55.797391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 08/19/23 13:02:56.603
  STEP: Ensuring ResourceQuota status is calculated @ 08/19/23 13:02:56.609
  E0819 13:02:56.798001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:57.798178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 08/19/23 13:02:58.613
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/19/23 13:02:58.626
  E0819 13:02:58.798945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:02:59.799196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/19/23 13:03:00.631
  E0819 13:03:00.799246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:01.800285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/19/23 13:03:02.634
  STEP: Ensuring resource quota status released the pod usage @ 08/19/23 13:03:02.647
  E0819 13:03:02.801073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:03.801340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 08/19/23 13:03:04.651
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/19/23 13:03:04.662
  E0819 13:03:04.801438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:05.801636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/19/23 13:03:06.665
  E0819 13:03:06.801996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:07.802184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/19/23 13:03:08.669
  STEP: Ensuring resource quota status released the pod usage @ 08/19/23 13:03:08.682
  E0819 13:03:08.803004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:09.803215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:03:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-676" for this suite. @ 08/19/23 13:03:10.69
• [16.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/19/23 13:03:10.699
  Aug 19 13:03:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 13:03:10.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:10.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:10.724
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/19/23 13:03:10.726
  Aug 19 13:03:10.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3632 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 19 13:03:10.793: INFO: stderr: ""
  Aug 19 13:03:10.793: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/19/23 13:03:10.793
  E0819 13:03:10.804052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:11.804461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:12.804555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:13.804607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:14.804712      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:15.805038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/19/23 13:03:15.845
  Aug 19 13:03:15.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3632 get pod e2e-test-httpd-pod -o json'
  Aug 19 13:03:15.909: INFO: stderr: ""
  Aug 19 13:03:15.909: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-19T13:03:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3632\",\n        \"resourceVersion\": \"31029\",\n        \"uid\": \"c2375b6f-3d93-4172-b342-a37ae883ae96\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-wdw7n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-15-214\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-wdw7n\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-19T13:03:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-19T13:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-19T13:03:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-19T13:03:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e708d57d31beec7bcaec5ce34bbdfe708a722060c8a396b1dc9333e0063937ed\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-19T13:03:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.15.214\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.13.161\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.13.161\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-19T13:03:10Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/19/23 13:03:15.909
  Aug 19 13:03:15.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3632 replace -f -'
  Aug 19 13:03:16.343: INFO: stderr: ""
  Aug 19 13:03:16.343: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/19/23 13:03:16.344
  Aug 19 13:03:16.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-3632 delete pods e2e-test-httpd-pod'
  E0819 13:03:16.805674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:03:17.661: INFO: stderr: ""
  Aug 19 13:03:17.661: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 19 13:03:17.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3632" for this suite. @ 08/19/23 13:03:17.666
• [6.972 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/19/23 13:03:17.672
  Aug 19 13:03:17.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 13:03:17.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:17.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:17.694
  STEP: Creating service test in namespace statefulset-6145 @ 08/19/23 13:03:17.698
  STEP: Creating statefulset ss in namespace statefulset-6145 @ 08/19/23 13:03:17.703
  Aug 19 13:03:17.713: INFO: Found 0 stateful pods, waiting for 1
  E0819 13:03:17.806074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:18.806399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:19.807203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:20.807245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:21.807329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:22.808304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:23.808743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:24.808915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:25.809111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:26.809307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:03:27.719: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/19/23 13:03:27.725
  STEP: updating a scale subresource @ 08/19/23 13:03:27.729
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/19/23 13:03:27.735
  STEP: Patch a scale subresource @ 08/19/23 13:03:27.738
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/19/23 13:03:27.746
  Aug 19 13:03:27.752: INFO: Deleting all statefulset in ns statefulset-6145
  Aug 19 13:03:27.755: INFO: Scaling statefulset ss to 0
  E0819 13:03:27.810048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:28.811033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:29.811154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:30.811259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:31.812302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:32.812499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:33.812768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:34.812864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:35.813058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:36.813993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:03:37.775: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 13:03:37.777: INFO: Deleting statefulset ss
  Aug 19 13:03:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6145" for this suite. @ 08/19/23 13:03:37.795
• [20.130 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/19/23 13:03:37.803
  Aug 19 13:03:37.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl @ 08/19/23 13:03:37.803
  E0819 13:03:37.814876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:37.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:37.825
  STEP: validating api versions @ 08/19/23 13:03:37.827
  Aug 19 13:03:37.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-6092 api-versions'
  Aug 19 13:03:37.887: INFO: stderr: ""
  Aug 19 13:03:37.887: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug 19 13:03:37.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6092" for this suite. @ 08/19/23 13:03:37.891
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/19/23 13:03:37.899
  Aug 19 13:03:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubelet-test @ 08/19/23 13:03:37.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:37.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:37.92
  E0819 13:03:38.815246      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:39.815329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:03:39.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8058" for this suite. @ 08/19/23 13:03:39.958
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/19/23 13:03:39.965
  Aug 19 13:03:39.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:03:39.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:39.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:39.99
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:03:39.993
  E0819 13:03:40.815589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:41.816276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:42.816344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:43.816648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:03:44.013
  Aug 19 13:03:44.016: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-035a3f9b-9f20-4f61-8487-533b2fc363fe container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:03:44.023
  Aug 19 13:03:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7772" for this suite. @ 08/19/23 13:03:44.05
• [4.095 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/19/23 13:03:44.06
  Aug 19 13:03:44.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename runtimeclass @ 08/19/23 13:03:44.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:44.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:44.083
  Aug 19 13:03:44.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5786" for this suite. @ 08/19/23 13:03:44.097
• [0.050 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/19/23 13:03:44.111
  Aug 19 13:03:44.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption @ 08/19/23 13:03:44.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:03:44.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:03:44.132
  Aug 19 13:03:44.152: INFO: Waiting up to 1m0s for all nodes to be ready
  E0819 13:03:44.816753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:45.817009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:46.817912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:47.817993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:48.818697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:49.818784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:50.819063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:51.819227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:52.819322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:53.819435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:54.819513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:55.820299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:56.820403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:57.820726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:58.820819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:03:59.821031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:00.821115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:01.821334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:02.821418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:03.821716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:04.821916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:05.822303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:06.822391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:07.822561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:08.822659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:09.822888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:10.823413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:11.823519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:12.823594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:13.823843      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:14.823944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:15.824636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:16.824699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:17.824890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:18.824973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:19.825589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:20.826378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:21.826688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:22.826760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:23.827577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:24.828268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:25.828357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:26.828657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:27.828733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:28.829675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:29.829860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:30.830238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:31.831245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:32.831581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:33.831645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:34.831750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:35.832299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:36.832342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:37.832406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:38.832736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:39.832971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:40.833065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:41.833978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:42.834856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:43.835232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:04:44.170: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/19/23 13:04:44.173
  Aug 19 13:04:44.191: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 19 13:04:44.199: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 19 13:04:44.213: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 19 13:04:44.221: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 19 13:04:44.233: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 19 13:04:44.243: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/19/23 13:04:44.243
  E0819 13:04:44.839086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:45.839542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/19/23 13:04:46.266
  E0819 13:04:46.839643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:47.839684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:48.840296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:49.840390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:04:50.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2252" for this suite. @ 08/19/23 13:04:50.35
• [66.245 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/19/23 13:04:50.357
  Aug 19 13:04:50.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subjectreview @ 08/19/23 13:04:50.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:04:50.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:04:50.379
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2863" @ 08/19/23 13:04:50.386
  Aug 19 13:04:50.390: INFO: saUsername: "system:serviceaccount:subjectreview-2863:e2e"
  Aug 19 13:04:50.390: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2863"}
  Aug 19 13:04:50.390: INFO: saUID: "cc004c4a-322e-4b44-9c90-9775cba07f9c"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2863:e2e" @ 08/19/23 13:04:50.39
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2863:e2e" @ 08/19/23 13:04:50.391
  Aug 19 13:04:50.392: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2863:e2e" api 'list' configmaps in "subjectreview-2863" namespace @ 08/19/23 13:04:50.392
  Aug 19 13:04:50.394: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2863:e2e" @ 08/19/23 13:04:50.394
  Aug 19 13:04:50.397: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug 19 13:04:50.397: INFO: LocalSubjectAccessReview has been verified
  Aug 19 13:04:50.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2863" for this suite. @ 08/19/23 13:04:50.4
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/19/23 13:04:50.41
  Aug 19 13:04:50.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 13:04:50.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:04:50.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:04:50.429
  STEP: Creating configMap with name configmap-test-volume-map-90922b9f-908b-434d-b581-767d1078a0ad @ 08/19/23 13:04:50.432
  STEP: Creating a pod to test consume configMaps @ 08/19/23 13:04:50.438
  E0819 13:04:50.841370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:51.841488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:52.842142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:53.842322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:04:54.457
  Aug 19 13:04:54.460: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-9ab8bf76-90e8-4134-91bf-4c580c6084d1 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 13:04:54.467
  Aug 19 13:04:54.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4396" for this suite. @ 08/19/23 13:04:54.493
• [4.090 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/19/23 13:04:54.5
  Aug 19 13:04:54.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:04:54.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:04:54.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:04:54.523
  STEP: Setting up server cert @ 08/19/23 13:04:54.549
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:04:54.806
  STEP: Deploying the webhook pod @ 08/19/23 13:04:54.813
  STEP: Wait for the deployment to be ready @ 08/19/23 13:04:54.824
  Aug 19 13:04:54.831: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:04:54.843181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:55.843380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:04:56.84
  E0819 13:04:56.843728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:04:56.849
  E0819 13:04:57.844677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:04:57.849: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/19/23 13:04:57.853
  STEP: create a configmap that should be updated by the webhook @ 08/19/23 13:04:57.867
  Aug 19 13:04:57.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9308" for this suite. @ 08/19/23 13:04:57.921
  STEP: Destroying namespace "webhook-markers-5717" for this suite. @ 08/19/23 13:04:57.93
• [3.436 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/19/23 13:04:57.937
  Aug 19 13:04:57.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 13:04:57.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:04:57.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:04:57.958
  Aug 19 13:04:57.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: creating the pod @ 08/19/23 13:04:57.962
  STEP: submitting the pod to kubernetes @ 08/19/23 13:04:57.962
  E0819 13:04:58.845667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:04:59.845790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:00.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8768" for this suite. @ 08/19/23 13:05:00.073
• [2.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/19/23 13:05:00.083
  Aug 19 13:05:00.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename lease-test @ 08/19/23 13:05:00.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:00.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:00.105
  Aug 19 13:05:00.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-989" for this suite. @ 08/19/23 13:05:00.161
• [0.087 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/19/23 13:05:00.17
  Aug 19 13:05:00.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:05:00.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:00.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:00.194
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/19/23 13:05:00.197
  E0819 13:05:00.846552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:01.846614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:02.847212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:03.847259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:05:04.216
  Aug 19 13:05:04.219: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-e9bd6215-8275-4e17-8486-02f79d4f6021 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:05:04.226
  Aug 19 13:05:04.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4118" for this suite. @ 08/19/23 13:05:04.245
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/19/23 13:05:04.255
  Aug 19 13:05:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename init-container @ 08/19/23 13:05:04.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:04.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:04.276
  STEP: creating the pod @ 08/19/23 13:05:04.279
  Aug 19 13:05:04.279: INFO: PodSpec: initContainers in spec.initContainers
  E0819 13:05:04.848031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:05.848112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:06.848214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:07.849074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:08.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3815" for this suite. @ 08/19/23 13:05:08.221
• [3.972 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 08/19/23 13:05:08.231
  Aug 19 13:05:08.231: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 13:05:08.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:08.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:08.259
  Aug 19 13:05:08.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:05:08.849925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:09.850005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:10.850563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4240" for this suite. @ 08/19/23 13:05:11.362
• [3.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/19/23 13:05:11.379
  Aug 19 13:05:11.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:05:11.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:11.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:11.402
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/19/23 13:05:11.404
  E0819 13:05:11.851378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:12.852325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:13.852942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:14.853385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:05:15.425
  Aug 19 13:05:15.429: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-5bf3f3cb-6c15-483b-9d3f-a7614531b832 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:05:15.435
  Aug 19 13:05:15.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9346" for this suite. @ 08/19/23 13:05:15.453
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/19/23 13:05:15.461
  Aug 19 13:05:15.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename conformance-tests @ 08/19/23 13:05:15.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:15.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:15.481
  STEP: Getting node addresses @ 08/19/23 13:05:15.485
  Aug 19 13:05:15.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug 19 13:05:15.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-8223" for this suite. @ 08/19/23 13:05:15.493
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/19/23 13:05:15.502
  Aug 19 13:05:15.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 13:05:15.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:15.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:15.522
  Aug 19 13:05:15.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:05:15.853728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/19/23 13:05:16.851
  Aug 19 13:05:16.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2452 --namespace=crd-publish-openapi-2452 create -f -'
  E0819 13:05:16.874112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:17.371: INFO: stderr: ""
  Aug 19 13:05:17.371: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 19 13:05:17.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2452 --namespace=crd-publish-openapi-2452 delete e2e-test-crd-publish-openapi-4291-crds test-cr'
  Aug 19 13:05:17.441: INFO: stderr: ""
  Aug 19 13:05:17.441: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug 19 13:05:17.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2452 --namespace=crd-publish-openapi-2452 apply -f -'
  E0819 13:05:17.875177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:18.010: INFO: stderr: ""
  Aug 19 13:05:18.010: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 19 13:05:18.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2452 --namespace=crd-publish-openapi-2452 delete e2e-test-crd-publish-openapi-4291-crds test-cr'
  Aug 19 13:05:18.080: INFO: stderr: ""
  Aug 19 13:05:18.080: INFO: stdout: "e2e-test-crd-publish-openapi-4291-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/19/23 13:05:18.08
  Aug 19 13:05:18.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=crd-publish-openapi-2452 explain e2e-test-crd-publish-openapi-4291-crds'
  Aug 19 13:05:18.358: INFO: stderr: ""
  Aug 19 13:05:18.358: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-4291-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0819 13:05:18.875275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:19.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2452" for this suite. @ 08/19/23 13:05:19.662
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/19/23 13:05:19.67
  Aug 19 13:05:19.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/19/23 13:05:19.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:19.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:19.689
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/19/23 13:05:19.692
  Aug 19 13:05:19.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:05:19.876318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:20.876819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:20.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:05:21.877334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:22.877462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:23.877542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:24.877939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:25.878063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:26.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1052" for this suite. @ 08/19/23 13:05:26.333
• [6.671 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/19/23 13:05:26.341
  Aug 19 13:05:26.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:05:26.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:26.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:26.363
  STEP: Creating a pod to test downward api env vars @ 08/19/23 13:05:26.366
  E0819 13:05:26.878177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:27.878358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:28.878620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:29.878817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:05:30.385
  Aug 19 13:05:30.388: INFO: Trying to get logs from node ip-172-31-15-214 pod downward-api-329c070c-c83c-4806-93b6-e2d72d544eba container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:05:30.395
  Aug 19 13:05:30.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-246" for this suite. @ 08/19/23 13:05:30.416
• [4.081 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/19/23 13:05:30.422
  Aug 19 13:05:30.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubectl-logs @ 08/19/23 13:05:30.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:30.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:30.444
  STEP: creating an pod @ 08/19/23 13:05:30.447
  Aug 19 13:05:30.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug 19 13:05:30.515: INFO: stderr: ""
  Aug 19 13:05:30.515: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/19/23 13:05:30.515
  Aug 19 13:05:30.515: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0819 13:05:30.879379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:31.879459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:32.522: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/19/23 13:05:32.522
  Aug 19 13:05:32.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator'
  Aug 19 13:05:32.589: INFO: stderr: ""
  Aug 19 13:05:32.589: INFO: stdout: "I0819 13:05:31.278883       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9hlg 550\nI0819 13:05:31.478982       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/6tmv 416\nI0819 13:05:31.679395       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/g6d 515\nI0819 13:05:31.879680       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/mmgr 502\nI0819 13:05:32.079136       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5nc6 531\nI0819 13:05:32.279426       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/8kf 441\nI0819 13:05:32.479761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6kfg 595\n"
  STEP: limiting log lines @ 08/19/23 13:05:32.589
  Aug 19 13:05:32.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator --tail=1'
  Aug 19 13:05:32.666: INFO: stderr: ""
  Aug 19 13:05:32.666: INFO: stdout: "I0819 13:05:32.479761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6kfg 595\n"
  Aug 19 13:05:32.666: INFO: got output "I0819 13:05:32.479761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6kfg 595\n"
  STEP: limiting log bytes @ 08/19/23 13:05:32.666
  Aug 19 13:05:32.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator --limit-bytes=1'
  Aug 19 13:05:32.732: INFO: stderr: ""
  Aug 19 13:05:32.732: INFO: stdout: "I"
  Aug 19 13:05:32.732: INFO: got output "I"
  STEP: exposing timestamps @ 08/19/23 13:05:32.732
  Aug 19 13:05:32.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug 19 13:05:32.799: INFO: stderr: ""
  Aug 19 13:05:32.799: INFO: stdout: "2023-08-19T13:05:32.679480989Z I0819 13:05:32.679331       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/st7t 406\n"
  Aug 19 13:05:32.799: INFO: got output "2023-08-19T13:05:32.679480989Z I0819 13:05:32.679331       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/st7t 406\n"
  STEP: restricting to a time range @ 08/19/23 13:05:32.799
  E0819 13:05:32.880266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:33.880779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:34.880883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:35.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator --since=1s'
  Aug 19 13:05:35.372: INFO: stderr: ""
  Aug 19 13:05:35.372: INFO: stdout: "I0819 13:05:34.478950       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/jc9 294\nI0819 13:05:34.679460       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/xlz 297\nI0819 13:05:34.879754       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/bz4 366\nI0819 13:05:35.079031       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2pvt 541\nI0819 13:05:35.283065       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/77tg 333\n"
  Aug 19 13:05:35.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 logs logs-generator logs-generator --since=24h'
  Aug 19 13:05:35.449: INFO: stderr: ""
  Aug 19 13:05:35.449: INFO: stdout: "I0819 13:05:31.278883       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9hlg 550\nI0819 13:05:31.478982       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/6tmv 416\nI0819 13:05:31.679395       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/g6d 515\nI0819 13:05:31.879680       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/mmgr 502\nI0819 13:05:32.079136       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/5nc6 531\nI0819 13:05:32.279426       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/8kf 441\nI0819 13:05:32.479761       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/6kfg 595\nI0819 13:05:32.679331       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/st7t 406\nI0819 13:05:32.879619       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/zjc 278\nI0819 13:05:33.079819       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/frh 512\nI0819 13:05:33.279101       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/t7kx 563\nI0819 13:05:33.479379       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/6xgh 507\nI0819 13:05:33.679692       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/w5q 550\nI0819 13:05:33.879909       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/6jm 451\nI0819 13:05:34.079407       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vhk 237\nI0819 13:05:34.279720       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/jxn 284\nI0819 13:05:34.478950       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/jc9 294\nI0819 13:05:34.679460       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/xlz 297\nI0819 13:05:34.879754       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/bz4 366\nI0819 13:05:35.079031       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/2pvt 541\nI0819 13:05:35.283065       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/77tg 333\n"
  Aug 19 13:05:35.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=kubectl-logs-2182 delete pod logs-generator'
  E0819 13:05:35.880965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:36.881748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:37.061: INFO: stderr: ""
  Aug 19 13:05:37.061: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug 19 13:05:37.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2182" for this suite. @ 08/19/23 13:05:37.065
• [6.649 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/19/23 13:05:37.072
  Aug 19 13:05:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename security-context @ 08/19/23 13:05:37.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:37.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:37.095
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/19/23 13:05:37.097
  E0819 13:05:37.881837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:38.882283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:39.882366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:40.882449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:05:41.116
  Aug 19 13:05:41.119: INFO: Trying to get logs from node ip-172-31-15-214 pod security-context-5831ba76-8d94-4c2f-8604-51e0eeeaa204 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:05:41.126
  Aug 19 13:05:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9967" for this suite. @ 08/19/23 13:05:41.145
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/19/23 13:05:41.153
  Aug 19 13:05:41.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/19/23 13:05:41.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:41.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:41.174
  STEP: mirroring a new custom Endpoint @ 08/19/23 13:05:41.185
  Aug 19 13:05:41.195: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0819 13:05:41.882565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:42.882752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 08/19/23 13:05:43.199
  Aug 19 13:05:43.206: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0819 13:05:43.883595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:44.883682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 08/19/23 13:05:45.21
  Aug 19 13:05:45.219: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0819 13:05:45.883773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:46.884310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:47.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-1658" for this suite. @ 08/19/23 13:05:47.227
• [6.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/19/23 13:05:47.236
  Aug 19 13:05:47.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:05:47.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:47.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:47.257
  STEP: Creating configMap with name projected-configmap-test-volume-map-e1b41384-4112-4208-ae16-a934fdfd342d @ 08/19/23 13:05:47.26
  STEP: Creating a pod to test consume configMaps @ 08/19/23 13:05:47.265
  E0819 13:05:47.884383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:48.884831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:49.884907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:50.884995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:05:51.284
  Aug 19 13:05:51.287: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-configmaps-356e878d-0a98-4e2a-9e4e-0c7d043b775b container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 13:05:51.294
  Aug 19 13:05:51.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6252" for this suite. @ 08/19/23 13:05:51.312
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/19/23 13:05:51.32
  Aug 19 13:05:51.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/19/23 13:05:51.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:51.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:51.346
  Aug 19 13:05:51.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:05:51.885795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:52.886677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:53.887445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:54.888279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:55.889074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:56.889236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:05:57.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6623" for this suite. @ 08/19/23 13:05:57.542
• [6.228 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/19/23 13:05:57.549
  Aug 19 13:05:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 13:05:57.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:57.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:57.569
  Aug 19 13:05:57.590: INFO: created pod pod-service-account-defaultsa
  Aug 19 13:05:57.590: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug 19 13:05:57.596: INFO: created pod pod-service-account-mountsa
  Aug 19 13:05:57.596: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug 19 13:05:57.600: INFO: created pod pod-service-account-nomountsa
  Aug 19 13:05:57.600: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Aug 19 13:05:57.608: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug 19 13:05:57.608: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug 19 13:05:57.613: INFO: created pod pod-service-account-mountsa-mountspec
  Aug 19 13:05:57.613: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug 19 13:05:57.621: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug 19 13:05:57.621: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug 19 13:05:57.626: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug 19 13:05:57.626: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug 19 13:05:57.634: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug 19 13:05:57.635: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug 19 13:05:57.642: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug 19 13:05:57.643: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug 19 13:05:57.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8006" for this suite. @ 08/19/23 13:05:57.649
• [0.106 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/19/23 13:05:57.656
  Aug 19 13:05:57.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubelet-test @ 08/19/23 13:05:57.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:05:57.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:05:57.686
  E0819 13:05:57.889985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:58.890538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:05:59.890727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:00.890990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:06:01.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2980" for this suite. @ 08/19/23 13:06:01.711
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/19/23 13:06:01.721
  Aug 19 13:06:01.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:06:01.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:01.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:01.742
  STEP: creating a Service @ 08/19/23 13:06:01.751
  STEP: watching for the Service to be added @ 08/19/23 13:06:01.76
  Aug 19 13:06:01.762: INFO: Found Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug 19 13:06:01.763: INFO: Service test-service-vr8g9 created
  STEP: Getting /status @ 08/19/23 13:06:01.763
  Aug 19 13:06:01.768: INFO: Service test-service-vr8g9 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/19/23 13:06:01.768
  STEP: watching for the Service to be patched @ 08/19/23 13:06:01.773
  Aug 19 13:06:01.774: INFO: observed Service test-service-vr8g9 in namespace services-8461 with annotations: map[] & LoadBalancer: {[]}
  Aug 19 13:06:01.775: INFO: Found Service test-service-vr8g9 in namespace services-8461 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug 19 13:06:01.775: INFO: Service test-service-vr8g9 has service status patched
  STEP: updating the ServiceStatus @ 08/19/23 13:06:01.775
  Aug 19 13:06:01.784: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/19/23 13:06:01.784
  Aug 19 13:06:01.785: INFO: Observed Service test-service-vr8g9 in namespace services-8461 with annotations: map[] & Conditions: {[]}
  Aug 19 13:06:01.785: INFO: Observed event: &Service{ObjectMeta:{test-service-vr8g9  services-8461  2d00902a-4631-4df6-8b44-946b568da5fa 32482 0 2023-08-19 13:06:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-19 13:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-19 13:06:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.22,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.22],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug 19 13:06:01.786: INFO: Found Service test-service-vr8g9 in namespace services-8461 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 19 13:06:01.786: INFO: Service test-service-vr8g9 has service status updated
  STEP: patching the service @ 08/19/23 13:06:01.786
  STEP: watching for the Service to be patched @ 08/19/23 13:06:01.799
  Aug 19 13:06:01.802: INFO: observed Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service-static:true]
  Aug 19 13:06:01.802: INFO: observed Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service-static:true]
  Aug 19 13:06:01.803: INFO: observed Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service-static:true]
  Aug 19 13:06:01.803: INFO: Found Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service:patched test-service-static:true]
  Aug 19 13:06:01.803: INFO: Service test-service-vr8g9 patched
  STEP: deleting the service @ 08/19/23 13:06:01.804
  STEP: watching for the Service to be deleted @ 08/19/23 13:06:01.816
  Aug 19 13:06:01.817: INFO: Observed event: ADDED
  Aug 19 13:06:01.817: INFO: Observed event: MODIFIED
  Aug 19 13:06:01.817: INFO: Observed event: MODIFIED
  Aug 19 13:06:01.817: INFO: Observed event: MODIFIED
  Aug 19 13:06:01.817: INFO: Found Service test-service-vr8g9 in namespace services-8461 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug 19 13:06:01.817: INFO: Service test-service-vr8g9 deleted
  Aug 19 13:06:01.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8461" for this suite. @ 08/19/23 13:06:01.821
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/19/23 13:06:01.829
  Aug 19 13:06:01.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 13:06:01.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:01.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:01.848
  STEP: creating the pod @ 08/19/23 13:06:01.851
  STEP: setting up watch @ 08/19/23 13:06:01.851
  E0819 13:06:01.891264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: submitting the pod to kubernetes @ 08/19/23 13:06:01.955
  STEP: verifying the pod is in kubernetes @ 08/19/23 13:06:01.964
  STEP: verifying pod creation was observed @ 08/19/23 13:06:01.967
  E0819 13:06:02.891756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:03.892278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/19/23 13:06:03.979
  STEP: verifying pod deletion was observed @ 08/19/23 13:06:03.986
  E0819 13:06:04.892877      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:06:05.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1474" for this suite. @ 08/19/23 13:06:05.392
• [3.569 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/19/23 13:06:05.398
  Aug 19 13:06:05.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subpath @ 08/19/23 13:06:05.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:05.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:05.42
  STEP: Setting up data @ 08/19/23 13:06:05.423
  STEP: Creating pod pod-subpath-test-configmap-rtts @ 08/19/23 13:06:05.431
  STEP: Creating a pod to test atomic-volume-subpath @ 08/19/23 13:06:05.432
  E0819 13:06:05.893078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:06.893171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:07.893662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:08.894063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:09.894157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:10.894308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:11.894837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:12.895084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:13.895841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:14.895921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:15.896027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:16.896127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:17.896430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:18.896841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:19.897032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:20.897252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:21.898008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:22.898175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:23.898820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:24.898914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:25.898996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:26.899096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:27.899634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:28.900292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:06:29.492
  Aug 19 13:06:29.495: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-subpath-test-configmap-rtts container test-container-subpath-configmap-rtts: <nil>
  STEP: delete the pod @ 08/19/23 13:06:29.505
  STEP: Deleting pod pod-subpath-test-configmap-rtts @ 08/19/23 13:06:29.519
  Aug 19 13:06:29.519: INFO: Deleting pod "pod-subpath-test-configmap-rtts" in namespace "subpath-8523"
  Aug 19 13:06:29.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8523" for this suite. @ 08/19/23 13:06:29.526
• [24.134 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/19/23 13:06:29.533
  Aug 19 13:06:29.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 13:06:29.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:29.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:29.552
  STEP: Create a ReplicaSet @ 08/19/23 13:06:29.557
  STEP: Verify that the required pods have come up @ 08/19/23 13:06:29.562
  Aug 19 13:06:29.565: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0819 13:06:29.900978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:30.901071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:31.901765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:32.901859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:33.902114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:06:34.569: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/19/23 13:06:34.569
  Aug 19 13:06:34.572: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/19/23 13:06:34.572
  STEP: DeleteCollection of the ReplicaSets @ 08/19/23 13:06:34.575
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/19/23 13:06:34.583
  Aug 19 13:06:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1953" for this suite. @ 08/19/23 13:06:34.59
• [5.063 seconds]
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/19/23 13:06:34.596
  Aug 19 13:06:34.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename watch @ 08/19/23 13:06:34.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:34.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:34.629
  STEP: creating a new configmap @ 08/19/23 13:06:34.631
  STEP: modifying the configmap once @ 08/19/23 13:06:34.636
  STEP: modifying the configmap a second time @ 08/19/23 13:06:34.643
  STEP: deleting the configmap @ 08/19/23 13:06:34.651
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/19/23 13:06:34.658
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/19/23 13:06:34.66
  Aug 19 13:06:34.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7676  558ca5e1-faec-407a-88de-6bc609f4b6ba 32759 0 2023-08-19 13:06:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-19 13:06:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:06:34.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7676  558ca5e1-faec-407a-88de-6bc609f4b6ba 32761 0 2023-08-19 13:06:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-19 13:06:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:06:34.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7676" for this suite. @ 08/19/23 13:06:34.664
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/19/23 13:06:34.671
  Aug 19 13:06:34.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption @ 08/19/23 13:06:34.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:06:34.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:06:34.693
  Aug 19 13:06:34.715: INFO: Waiting up to 1m0s for all nodes to be ready
  E0819 13:06:34.902075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:35.902704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:36.903768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:37.904282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:38.905166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:39.906185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:40.907200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:41.907301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:42.907386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:43.908283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:44.908354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:45.908615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:46.908892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:47.909997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:48.910540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:49.910773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:50.911781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:51.911957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:52.912051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:53.912910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:54.913502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:55.913623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:56.914649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:57.914756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:58.914851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:06:59.915018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:00.915399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:01.915476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:02.915625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:03.915627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:04.916049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:05.916141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:06.917202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:07.917391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:08.917784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:09.917878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:10.918860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:11.919024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:12.919523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:13.920533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:14.920850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:15.920946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:16.921761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:17.921860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:18.922435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:19.923277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:20.923637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:21.924320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:22.924310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:23.924710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:24.925195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:25.925366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:26.925461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:27.926419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:28.927298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:29.927393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:30.928270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:31.928365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:32.928458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:33.928787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:07:34.734: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/19/23 13:07:34.737
  Aug 19 13:07:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/19/23 13:07:34.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:07:34.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:07:34.758
  Aug 19 13:07:34.774: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug 19 13:07:34.777: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug 19 13:07:34.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:07:34.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2503" for this suite. @ 08/19/23 13:07:34.845
  STEP: Destroying namespace "sched-preemption-8362" for this suite. @ 08/19/23 13:07:34.851
• [60.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/19/23 13:07:34.861
  Aug 19 13:07:34.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 13:07:34.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:07:34.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:07:34.879
  STEP: Creating secret with name secret-test-9d68a4b4-640e-482e-82db-37de78627155 @ 08/19/23 13:07:34.883
  STEP: Creating a pod to test consume secrets @ 08/19/23 13:07:34.887
  E0819 13:07:34.929440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:35.929559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:36.930449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:37.930657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:07:38.904
  Aug 19 13:07:38.907: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-fc51873a-ee32-4281-8afa-c37d9be7898e container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 13:07:38.914
  Aug 19 13:07:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0819 13:07:38.930754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "secrets-9653" for this suite. @ 08/19/23 13:07:38.931
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/19/23 13:07:38.941
  Aug 19 13:07:38.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:07:38.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:07:38.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:07:38.962
  STEP: Setting up server cert @ 08/19/23 13:07:38.988
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:07:39.339
  STEP: Deploying the webhook pod @ 08/19/23 13:07:39.346
  STEP: Wait for the deployment to be ready @ 08/19/23 13:07:39.358
  Aug 19 13:07:39.366: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:07:39.931399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:40.931499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:07:41.376
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:07:41.388
  E0819 13:07:41.931663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:07:42.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/19/23 13:07:42.392
  STEP: create a pod that should be denied by the webhook @ 08/19/23 13:07:42.406
  STEP: create a pod that causes the webhook to hang @ 08/19/23 13:07:42.418
  E0819 13:07:42.932284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:43.932636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:44.932747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:45.932893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:46.933071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:47.933282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:48.934095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:49.934394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:50.935175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:51.935223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 08/19/23 13:07:52.427
  STEP: create a configmap that should be admitted by the webhook @ 08/19/23 13:07:52.473
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/19/23 13:07:52.483
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/19/23 13:07:52.492
  STEP: create a namespace that bypass the webhook @ 08/19/23 13:07:52.498
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/19/23 13:07:52.519
  Aug 19 13:07:52.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8285" for this suite. @ 08/19/23 13:07:52.575
  STEP: Destroying namespace "webhook-markers-9525" for this suite. @ 08/19/23 13:07:52.584
  STEP: Destroying namespace "exempted-namespace-1549" for this suite. @ 08/19/23 13:07:52.589
• [13.653 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/19/23 13:07:52.595
  Aug 19 13:07:52.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 13:07:52.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:07:52.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:07:52.614
  E0819 13:07:52.935707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:53.936348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:54.936785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:55.937339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:56.937944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:57.938506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:58.939156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:07:59.939289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:00.939677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:01.940302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:02.940983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:03.941173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:04.941991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:05.942073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:06.942861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:07.943556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:08.944026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:09.944116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:10.944808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:11.945330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:12.946294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:13.946494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:14.676: INFO: Container started at 2023-08-19 13:07:53 +0000 UTC, pod became ready at 2023-08-19 13:08:12 +0000 UTC
  Aug 19 13:08:14.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5836" for this suite. @ 08/19/23 13:08:14.68
• [22.091 seconds]
------------------------------
S
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/19/23 13:08:14.687
  Aug 19 13:08:14.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svc-latency @ 08/19/23 13:08:14.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:08:14.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:08:14.706
  Aug 19 13:08:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7827 @ 08/19/23 13:08:14.71
  I0819 13:08:14.715530      18 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7827, replica count: 1
  E0819 13:08:14.947180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:08:15.767250      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0819 13:08:15.947603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:08:16.767912      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 13:08:16.881: INFO: Created: latency-svc-67wfs
  Aug 19 13:08:16.888: INFO: Got endpoints: latency-svc-67wfs [19.887427ms]
  Aug 19 13:08:16.898: INFO: Created: latency-svc-h4mwc
  Aug 19 13:08:16.907: INFO: Created: latency-svc-2h69j
  Aug 19 13:08:16.908: INFO: Got endpoints: latency-svc-h4mwc [19.819636ms]
  Aug 19 13:08:16.915: INFO: Got endpoints: latency-svc-2h69j [26.110494ms]
  Aug 19 13:08:16.918: INFO: Created: latency-svc-98txv
  Aug 19 13:08:16.924: INFO: Created: latency-svc-4zhj2
  Aug 19 13:08:16.925: INFO: Got endpoints: latency-svc-98txv [36.804814ms]
  Aug 19 13:08:16.932: INFO: Created: latency-svc-v9sx9
  Aug 19 13:08:16.938: INFO: Got endpoints: latency-svc-4zhj2 [48.605078ms]
  Aug 19 13:08:16.941: INFO: Created: latency-svc-mfkkh
  Aug 19 13:08:16.945: INFO: Created: latency-svc-6lrdk
  E0819 13:08:16.948437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:16.951: INFO: Got endpoints: latency-svc-v9sx9 [61.538347ms]
  Aug 19 13:08:16.952: INFO: Created: latency-svc-96pqp
  Aug 19 13:08:16.956: INFO: Got endpoints: latency-svc-mfkkh [65.601924ms]
  Aug 19 13:08:16.956: INFO: Got endpoints: latency-svc-6lrdk [65.045676ms]
  Aug 19 13:08:16.959: INFO: Got endpoints: latency-svc-96pqp [67.933248ms]
  Aug 19 13:08:16.962: INFO: Created: latency-svc-4nnkp
  Aug 19 13:08:16.964: INFO: Created: latency-svc-xpb5v
  Aug 19 13:08:16.968: INFO: Got endpoints: latency-svc-4nnkp [77.581948ms]
  Aug 19 13:08:16.973: INFO: Got endpoints: latency-svc-xpb5v [82.579268ms]
  Aug 19 13:08:16.974: INFO: Created: latency-svc-6gzvx
  Aug 19 13:08:16.979: INFO: Got endpoints: latency-svc-6gzvx [88.074155ms]
  Aug 19 13:08:16.980: INFO: Created: latency-svc-9fzsf
  Aug 19 13:08:16.985: INFO: Created: latency-svc-ksk6g
  Aug 19 13:08:16.987: INFO: Got endpoints: latency-svc-9fzsf [96.588571ms]
  Aug 19 13:08:16.995: INFO: Got endpoints: latency-svc-ksk6g [104.063485ms]
  Aug 19 13:08:16.995: INFO: Created: latency-svc-dqrqm
  Aug 19 13:08:17.000: INFO: Created: latency-svc-g9bfg
  Aug 19 13:08:17.001: INFO: Got endpoints: latency-svc-dqrqm [110.36275ms]
  Aug 19 13:08:17.005: INFO: Created: latency-svc-wpsrf
  Aug 19 13:08:17.011: INFO: Got endpoints: latency-svc-g9bfg [122.035833ms]
  Aug 19 13:08:17.013: INFO: Got endpoints: latency-svc-wpsrf [104.543025ms]
  Aug 19 13:08:17.015: INFO: Created: latency-svc-6nlrj
  Aug 19 13:08:17.021: INFO: Got endpoints: latency-svc-6nlrj [106.191428ms]
  Aug 19 13:08:17.023: INFO: Created: latency-svc-4svgw
  Aug 19 13:08:17.027: INFO: Created: latency-svc-xcnqb
  Aug 19 13:08:17.031: INFO: Got endpoints: latency-svc-4svgw [105.411453ms]
  Aug 19 13:08:17.034: INFO: Created: latency-svc-sfz58
  Aug 19 13:08:17.039: INFO: Created: latency-svc-5x8cj
  Aug 19 13:08:17.041: INFO: Got endpoints: latency-svc-sfz58 [89.292566ms]
  Aug 19 13:08:17.041: INFO: Got endpoints: latency-svc-xcnqb [102.570721ms]
  Aug 19 13:08:17.048: INFO: Created: latency-svc-5ldsh
  Aug 19 13:08:17.049: INFO: Got endpoints: latency-svc-5x8cj [93.965479ms]
  Aug 19 13:08:17.055: INFO: Got endpoints: latency-svc-5ldsh [99.717327ms]
  Aug 19 13:08:17.056: INFO: Created: latency-svc-cl46w
  Aug 19 13:08:17.062: INFO: Got endpoints: latency-svc-cl46w [103.501601ms]
  Aug 19 13:08:17.063: INFO: Created: latency-svc-vbkb4
  Aug 19 13:08:17.067: INFO: Created: latency-svc-4mm6d
  Aug 19 13:08:17.073: INFO: Got endpoints: latency-svc-vbkb4 [32.521088ms]
  Aug 19 13:08:17.075: INFO: Got endpoints: latency-svc-4mm6d [106.894822ms]
  Aug 19 13:08:17.078: INFO: Created: latency-svc-24njd
  Aug 19 13:08:17.083: INFO: Got endpoints: latency-svc-24njd [109.480448ms]
  Aug 19 13:08:17.084: INFO: Created: latency-svc-k2wfh
  Aug 19 13:08:17.087: INFO: Created: latency-svc-djhfj
  Aug 19 13:08:17.092: INFO: Got endpoints: latency-svc-k2wfh [113.034614ms]
  Aug 19 13:08:17.096: INFO: Got endpoints: latency-svc-djhfj [109.373832ms]
  Aug 19 13:08:17.097: INFO: Created: latency-svc-qblt6
  Aug 19 13:08:17.103: INFO: Got endpoints: latency-svc-qblt6 [108.439052ms]
  Aug 19 13:08:17.107: INFO: Created: latency-svc-5kn94
  Aug 19 13:08:17.112: INFO: Created: latency-svc-nkcsr
  Aug 19 13:08:17.113: INFO: Got endpoints: latency-svc-5kn94 [111.353329ms]
  Aug 19 13:08:17.121: INFO: Got endpoints: latency-svc-nkcsr [109.898812ms]
  Aug 19 13:08:17.121: INFO: Created: latency-svc-cm5mg
  Aug 19 13:08:17.127: INFO: Created: latency-svc-7n7c7
  Aug 19 13:08:17.128: INFO: Got endpoints: latency-svc-cm5mg [114.758696ms]
  Aug 19 13:08:17.132: INFO: Created: latency-svc-fpcfq
  Aug 19 13:08:17.136: INFO: Got endpoints: latency-svc-7n7c7 [114.548168ms]
  Aug 19 13:08:17.139: INFO: Got endpoints: latency-svc-fpcfq [107.61313ms]
  Aug 19 13:08:17.141: INFO: Created: latency-svc-2dw7f
  Aug 19 13:08:17.145: INFO: Created: latency-svc-vsnh9
  Aug 19 13:08:17.150: INFO: Created: latency-svc-bdkfm
  Aug 19 13:08:17.157: INFO: Created: latency-svc-lxljz
  Aug 19 13:08:17.161: INFO: Created: latency-svc-vj7r9
  Aug 19 13:08:17.166: INFO: Created: latency-svc-2rtxl
  Aug 19 13:08:17.171: INFO: Created: latency-svc-rzrkx
  Aug 19 13:08:17.177: INFO: Created: latency-svc-s9vqp
  Aug 19 13:08:17.181: INFO: Created: latency-svc-czqxn
  Aug 19 13:08:17.185: INFO: Got endpoints: latency-svc-2dw7f [144.568653ms]
  Aug 19 13:08:17.189: INFO: Created: latency-svc-sq567
  Aug 19 13:08:17.192: INFO: Created: latency-svc-xggq9
  Aug 19 13:08:17.198: INFO: Created: latency-svc-66w5g
  Aug 19 13:08:17.204: INFO: Created: latency-svc-p92gs
  Aug 19 13:08:17.210: INFO: Created: latency-svc-mvp6s
  Aug 19 13:08:17.216: INFO: Created: latency-svc-gj72z
  Aug 19 13:08:17.223: INFO: Created: latency-svc-tlrpk
  Aug 19 13:08:17.237: INFO: Got endpoints: latency-svc-vsnh9 [187.413459ms]
  Aug 19 13:08:17.247: INFO: Created: latency-svc-nl458
  Aug 19 13:08:17.288: INFO: Got endpoints: latency-svc-bdkfm [232.746042ms]
  Aug 19 13:08:17.297: INFO: Created: latency-svc-2r6b2
  Aug 19 13:08:17.336: INFO: Got endpoints: latency-svc-lxljz [274.241622ms]
  Aug 19 13:08:17.345: INFO: Created: latency-svc-ssj8t
  Aug 19 13:08:17.387: INFO: Got endpoints: latency-svc-vj7r9 [313.70475ms]
  Aug 19 13:08:17.397: INFO: Created: latency-svc-hpqlq
  Aug 19 13:08:17.439: INFO: Got endpoints: latency-svc-2rtxl [364.232478ms]
  Aug 19 13:08:17.448: INFO: Created: latency-svc-vtdms
  Aug 19 13:08:17.487: INFO: Got endpoints: latency-svc-rzrkx [404.584574ms]
  Aug 19 13:08:17.496: INFO: Created: latency-svc-bms57
  Aug 19 13:08:17.537: INFO: Got endpoints: latency-svc-s9vqp [445.123187ms]
  Aug 19 13:08:17.546: INFO: Created: latency-svc-mlfz8
  Aug 19 13:08:17.588: INFO: Got endpoints: latency-svc-czqxn [491.3977ms]
  Aug 19 13:08:17.596: INFO: Created: latency-svc-w8lbf
  Aug 19 13:08:17.636: INFO: Got endpoints: latency-svc-sq567 [532.3828ms]
  Aug 19 13:08:17.648: INFO: Created: latency-svc-gqc2l
  Aug 19 13:08:17.687: INFO: Got endpoints: latency-svc-xggq9 [574.305219ms]
  Aug 19 13:08:17.698: INFO: Created: latency-svc-zh5dn
  Aug 19 13:08:17.736: INFO: Got endpoints: latency-svc-66w5g [615.059884ms]
  Aug 19 13:08:17.744: INFO: Created: latency-svc-trk66
  Aug 19 13:08:17.787: INFO: Got endpoints: latency-svc-p92gs [659.236319ms]
  Aug 19 13:08:17.797: INFO: Created: latency-svc-xp897
  Aug 19 13:08:17.840: INFO: Got endpoints: latency-svc-mvp6s [703.991894ms]
  Aug 19 13:08:17.849: INFO: Created: latency-svc-sggnf
  Aug 19 13:08:17.888: INFO: Got endpoints: latency-svc-gj72z [749.497508ms]
  Aug 19 13:08:17.900: INFO: Created: latency-svc-7mz9f
  Aug 19 13:08:17.938: INFO: Got endpoints: latency-svc-tlrpk [752.428465ms]
  E0819 13:08:17.949349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:17.956: INFO: Created: latency-svc-km4c2
  Aug 19 13:08:17.987: INFO: Got endpoints: latency-svc-nl458 [750.216816ms]
  Aug 19 13:08:17.997: INFO: Created: latency-svc-hr8lc
  Aug 19 13:08:18.039: INFO: Got endpoints: latency-svc-2r6b2 [750.662742ms]
  Aug 19 13:08:18.048: INFO: Created: latency-svc-pqpjn
  Aug 19 13:08:18.088: INFO: Got endpoints: latency-svc-ssj8t [751.668507ms]
  Aug 19 13:08:18.098: INFO: Created: latency-svc-fl6bw
  Aug 19 13:08:18.139: INFO: Got endpoints: latency-svc-hpqlq [751.813758ms]
  Aug 19 13:08:18.152: INFO: Created: latency-svc-kfqcn
  Aug 19 13:08:18.189: INFO: Got endpoints: latency-svc-vtdms [749.658509ms]
  Aug 19 13:08:18.198: INFO: Created: latency-svc-thnzk
  Aug 19 13:08:18.239: INFO: Got endpoints: latency-svc-bms57 [751.415824ms]
  Aug 19 13:08:18.250: INFO: Created: latency-svc-xlghb
  Aug 19 13:08:18.288: INFO: Got endpoints: latency-svc-mlfz8 [750.808047ms]
  Aug 19 13:08:18.298: INFO: Created: latency-svc-zrxd8
  Aug 19 13:08:18.337: INFO: Got endpoints: latency-svc-w8lbf [749.751986ms]
  Aug 19 13:08:18.346: INFO: Created: latency-svc-xzn7p
  Aug 19 13:08:18.387: INFO: Got endpoints: latency-svc-gqc2l [751.118597ms]
  Aug 19 13:08:18.396: INFO: Created: latency-svc-s44qk
  Aug 19 13:08:18.438: INFO: Got endpoints: latency-svc-zh5dn [750.043121ms]
  Aug 19 13:08:18.451: INFO: Created: latency-svc-wzspb
  Aug 19 13:08:18.488: INFO: Got endpoints: latency-svc-trk66 [752.16827ms]
  Aug 19 13:08:18.496: INFO: Created: latency-svc-c2fn2
  Aug 19 13:08:18.538: INFO: Got endpoints: latency-svc-xp897 [750.805366ms]
  Aug 19 13:08:18.546: INFO: Created: latency-svc-cn77x
  Aug 19 13:08:18.586: INFO: Got endpoints: latency-svc-sggnf [746.702957ms]
  Aug 19 13:08:18.596: INFO: Created: latency-svc-rjqfw
  Aug 19 13:08:18.637: INFO: Got endpoints: latency-svc-7mz9f [748.583972ms]
  Aug 19 13:08:18.645: INFO: Created: latency-svc-7nshr
  Aug 19 13:08:18.686: INFO: Got endpoints: latency-svc-km4c2 [747.634391ms]
  Aug 19 13:08:18.695: INFO: Created: latency-svc-g8swj
  Aug 19 13:08:18.737: INFO: Got endpoints: latency-svc-hr8lc [749.443897ms]
  Aug 19 13:08:18.747: INFO: Created: latency-svc-595dx
  Aug 19 13:08:18.788: INFO: Got endpoints: latency-svc-pqpjn [749.512748ms]
  Aug 19 13:08:18.798: INFO: Created: latency-svc-b4rqx
  Aug 19 13:08:18.849: INFO: Got endpoints: latency-svc-fl6bw [760.127923ms]
  Aug 19 13:08:18.859: INFO: Created: latency-svc-fjk29
  Aug 19 13:08:18.887: INFO: Got endpoints: latency-svc-kfqcn [748.199823ms]
  Aug 19 13:08:18.901: INFO: Created: latency-svc-n6nf2
  Aug 19 13:08:18.937: INFO: Got endpoints: latency-svc-thnzk [748.592228ms]
  Aug 19 13:08:18.947: INFO: Created: latency-svc-rw9wr
  E0819 13:08:18.950101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:18.986: INFO: Got endpoints: latency-svc-xlghb [747.238767ms]
  Aug 19 13:08:18.996: INFO: Created: latency-svc-l4ts2
  Aug 19 13:08:19.040: INFO: Got endpoints: latency-svc-zrxd8 [752.184324ms]
  Aug 19 13:08:19.050: INFO: Created: latency-svc-smrkb
  Aug 19 13:08:19.087: INFO: Got endpoints: latency-svc-xzn7p [749.201486ms]
  Aug 19 13:08:19.095: INFO: Created: latency-svc-f58j8
  Aug 19 13:08:19.137: INFO: Got endpoints: latency-svc-s44qk [749.891732ms]
  Aug 19 13:08:19.147: INFO: Created: latency-svc-fp9ch
  Aug 19 13:08:19.187: INFO: Got endpoints: latency-svc-wzspb [749.889489ms]
  Aug 19 13:08:19.203: INFO: Created: latency-svc-z9q22
  Aug 19 13:08:19.236: INFO: Got endpoints: latency-svc-c2fn2 [747.860156ms]
  Aug 19 13:08:19.247: INFO: Created: latency-svc-f7hzj
  Aug 19 13:08:19.287: INFO: Got endpoints: latency-svc-cn77x [749.478088ms]
  Aug 19 13:08:19.296: INFO: Created: latency-svc-hggqn
  Aug 19 13:08:19.338: INFO: Got endpoints: latency-svc-rjqfw [751.109346ms]
  Aug 19 13:08:19.347: INFO: Created: latency-svc-gv86g
  Aug 19 13:08:19.387: INFO: Got endpoints: latency-svc-7nshr [750.063258ms]
  Aug 19 13:08:19.395: INFO: Created: latency-svc-28572
  Aug 19 13:08:19.437: INFO: Got endpoints: latency-svc-g8swj [751.264354ms]
  Aug 19 13:08:19.448: INFO: Created: latency-svc-nnq2h
  Aug 19 13:08:19.487: INFO: Got endpoints: latency-svc-595dx [749.991331ms]
  Aug 19 13:08:19.498: INFO: Created: latency-svc-szkjv
  Aug 19 13:08:19.536: INFO: Got endpoints: latency-svc-b4rqx [747.407131ms]
  Aug 19 13:08:19.546: INFO: Created: latency-svc-x7b7j
  Aug 19 13:08:19.592: INFO: Got endpoints: latency-svc-fjk29 [743.680908ms]
  Aug 19 13:08:19.603: INFO: Created: latency-svc-zlxld
  Aug 19 13:08:19.637: INFO: Got endpoints: latency-svc-n6nf2 [749.308549ms]
  Aug 19 13:08:19.651: INFO: Created: latency-svc-c75h7
  Aug 19 13:08:19.687: INFO: Got endpoints: latency-svc-rw9wr [749.989712ms]
  Aug 19 13:08:19.699: INFO: Created: latency-svc-c9m6z
  Aug 19 13:08:19.737: INFO: Got endpoints: latency-svc-l4ts2 [750.485198ms]
  Aug 19 13:08:19.746: INFO: Created: latency-svc-d22l4
  Aug 19 13:08:19.791: INFO: Got endpoints: latency-svc-smrkb [750.389184ms]
  Aug 19 13:08:19.799: INFO: Created: latency-svc-rj6jv
  Aug 19 13:08:19.840: INFO: Got endpoints: latency-svc-f58j8 [752.920234ms]
  Aug 19 13:08:19.849: INFO: Created: latency-svc-lgrj9
  Aug 19 13:08:19.888: INFO: Got endpoints: latency-svc-fp9ch [750.686437ms]
  Aug 19 13:08:19.899: INFO: Created: latency-svc-6chrn
  Aug 19 13:08:19.939: INFO: Got endpoints: latency-svc-z9q22 [751.574097ms]
  Aug 19 13:08:19.947: INFO: Created: latency-svc-qc9hc
  E0819 13:08:19.950851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:19.989: INFO: Got endpoints: latency-svc-f7hzj [752.525278ms]
  Aug 19 13:08:19.999: INFO: Created: latency-svc-9x8b5
  Aug 19 13:08:20.037: INFO: Got endpoints: latency-svc-hggqn [749.285141ms]
  Aug 19 13:08:20.047: INFO: Created: latency-svc-dt568
  Aug 19 13:08:20.086: INFO: Got endpoints: latency-svc-gv86g [748.691492ms]
  Aug 19 13:08:20.095: INFO: Created: latency-svc-tzn4q
  Aug 19 13:08:20.137: INFO: Got endpoints: latency-svc-28572 [750.07134ms]
  Aug 19 13:08:20.149: INFO: Created: latency-svc-wj9bt
  Aug 19 13:08:20.187: INFO: Got endpoints: latency-svc-nnq2h [750.200325ms]
  Aug 19 13:08:20.196: INFO: Created: latency-svc-wq4l2
  Aug 19 13:08:20.237: INFO: Got endpoints: latency-svc-szkjv [749.549408ms]
  Aug 19 13:08:20.246: INFO: Created: latency-svc-d8slq
  Aug 19 13:08:20.288: INFO: Got endpoints: latency-svc-x7b7j [752.074531ms]
  Aug 19 13:08:20.299: INFO: Created: latency-svc-q7qhd
  Aug 19 13:08:20.337: INFO: Got endpoints: latency-svc-zlxld [744.204218ms]
  Aug 19 13:08:20.347: INFO: Created: latency-svc-pzv8j
  Aug 19 13:08:20.386: INFO: Got endpoints: latency-svc-c75h7 [748.866457ms]
  Aug 19 13:08:20.394: INFO: Created: latency-svc-qw4kk
  Aug 19 13:08:20.438: INFO: Got endpoints: latency-svc-c9m6z [750.922107ms]
  Aug 19 13:08:20.447: INFO: Created: latency-svc-qvmdf
  Aug 19 13:08:20.487: INFO: Got endpoints: latency-svc-d22l4 [750.288754ms]
  Aug 19 13:08:20.498: INFO: Created: latency-svc-92vms
  Aug 19 13:08:20.537: INFO: Got endpoints: latency-svc-rj6jv [746.148493ms]
  Aug 19 13:08:20.547: INFO: Created: latency-svc-zvncw
  Aug 19 13:08:20.588: INFO: Got endpoints: latency-svc-lgrj9 [748.307333ms]
  Aug 19 13:08:20.599: INFO: Created: latency-svc-gfgrx
  Aug 19 13:08:20.636: INFO: Got endpoints: latency-svc-6chrn [748.194299ms]
  Aug 19 13:08:20.646: INFO: Created: latency-svc-lv9s8
  Aug 19 13:08:20.687: INFO: Got endpoints: latency-svc-qc9hc [748.324928ms]
  Aug 19 13:08:20.697: INFO: Created: latency-svc-wff4g
  Aug 19 13:08:20.737: INFO: Got endpoints: latency-svc-9x8b5 [748.295ms]
  Aug 19 13:08:20.748: INFO: Created: latency-svc-krlkq
  Aug 19 13:08:20.785: INFO: Got endpoints: latency-svc-dt568 [748.139804ms]
  Aug 19 13:08:20.795: INFO: Created: latency-svc-hz6fw
  Aug 19 13:08:20.839: INFO: Got endpoints: latency-svc-tzn4q [752.572688ms]
  Aug 19 13:08:20.848: INFO: Created: latency-svc-pdtxd
  Aug 19 13:08:20.890: INFO: Got endpoints: latency-svc-wj9bt [752.192968ms]
  Aug 19 13:08:20.902: INFO: Created: latency-svc-grtm5
  Aug 19 13:08:20.938: INFO: Got endpoints: latency-svc-wq4l2 [750.707894ms]
  E0819 13:08:20.951087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:20.951: INFO: Created: latency-svc-jr457
  Aug 19 13:08:20.988: INFO: Got endpoints: latency-svc-d8slq [751.129377ms]
  Aug 19 13:08:20.998: INFO: Created: latency-svc-c8wfm
  Aug 19 13:08:21.039: INFO: Got endpoints: latency-svc-q7qhd [750.662124ms]
  Aug 19 13:08:21.050: INFO: Created: latency-svc-n6w56
  Aug 19 13:08:21.086: INFO: Got endpoints: latency-svc-pzv8j [749.923223ms]
  Aug 19 13:08:21.098: INFO: Created: latency-svc-mj7pg
  Aug 19 13:08:21.138: INFO: Got endpoints: latency-svc-qw4kk [751.66419ms]
  Aug 19 13:08:21.147: INFO: Created: latency-svc-d9b2h
  Aug 19 13:08:21.186: INFO: Got endpoints: latency-svc-qvmdf [747.745798ms]
  Aug 19 13:08:21.198: INFO: Created: latency-svc-lf489
  Aug 19 13:08:21.237: INFO: Got endpoints: latency-svc-92vms [749.83133ms]
  Aug 19 13:08:21.247: INFO: Created: latency-svc-v24xc
  Aug 19 13:08:21.286: INFO: Got endpoints: latency-svc-zvncw [749.226566ms]
  Aug 19 13:08:21.296: INFO: Created: latency-svc-wtk4s
  Aug 19 13:08:21.339: INFO: Got endpoints: latency-svc-gfgrx [751.316326ms]
  Aug 19 13:08:21.350: INFO: Created: latency-svc-6zlq8
  Aug 19 13:08:21.387: INFO: Got endpoints: latency-svc-lv9s8 [750.873967ms]
  Aug 19 13:08:21.396: INFO: Created: latency-svc-zjm8t
  Aug 19 13:08:21.438: INFO: Got endpoints: latency-svc-wff4g [750.436302ms]
  Aug 19 13:08:21.447: INFO: Created: latency-svc-kk8qn
  Aug 19 13:08:21.487: INFO: Got endpoints: latency-svc-krlkq [749.702073ms]
  Aug 19 13:08:21.497: INFO: Created: latency-svc-pjjxq
  Aug 19 13:08:21.537: INFO: Got endpoints: latency-svc-hz6fw [751.912959ms]
  Aug 19 13:08:21.550: INFO: Created: latency-svc-jtbkt
  Aug 19 13:08:21.587: INFO: Got endpoints: latency-svc-pdtxd [747.637198ms]
  Aug 19 13:08:21.595: INFO: Created: latency-svc-dwr6k
  Aug 19 13:08:21.640: INFO: Got endpoints: latency-svc-grtm5 [749.236776ms]
  Aug 19 13:08:21.649: INFO: Created: latency-svc-44757
  Aug 19 13:08:21.687: INFO: Got endpoints: latency-svc-jr457 [749.00549ms]
  Aug 19 13:08:21.697: INFO: Created: latency-svc-pdplf
  Aug 19 13:08:21.736: INFO: Got endpoints: latency-svc-c8wfm [747.905234ms]
  Aug 19 13:08:21.744: INFO: Created: latency-svc-wc76c
  Aug 19 13:08:21.787: INFO: Got endpoints: latency-svc-n6w56 [748.511541ms]
  Aug 19 13:08:21.800: INFO: Created: latency-svc-8j56r
  Aug 19 13:08:21.838: INFO: Got endpoints: latency-svc-mj7pg [751.213555ms]
  Aug 19 13:08:21.847: INFO: Created: latency-svc-ldnxv
  Aug 19 13:08:21.886: INFO: Got endpoints: latency-svc-d9b2h [748.169017ms]
  Aug 19 13:08:21.898: INFO: Created: latency-svc-knzls
  Aug 19 13:08:21.940: INFO: Got endpoints: latency-svc-lf489 [753.584398ms]
  Aug 19 13:08:21.949: INFO: Created: latency-svc-hxqdg
  E0819 13:08:21.951890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:21.986: INFO: Got endpoints: latency-svc-v24xc [748.807658ms]
  Aug 19 13:08:21.995: INFO: Created: latency-svc-qtlx5
  Aug 19 13:08:22.038: INFO: Got endpoints: latency-svc-wtk4s [751.455285ms]
  Aug 19 13:08:22.047: INFO: Created: latency-svc-rcdss
  Aug 19 13:08:22.085: INFO: Got endpoints: latency-svc-6zlq8 [746.100315ms]
  Aug 19 13:08:22.095: INFO: Created: latency-svc-7vl4m
  Aug 19 13:08:22.137: INFO: Got endpoints: latency-svc-zjm8t [749.506618ms]
  Aug 19 13:08:22.146: INFO: Created: latency-svc-dhpqj
  Aug 19 13:08:22.187: INFO: Got endpoints: latency-svc-kk8qn [748.295834ms]
  Aug 19 13:08:22.198: INFO: Created: latency-svc-blv97
  Aug 19 13:08:22.236: INFO: Got endpoints: latency-svc-pjjxq [748.923416ms]
  Aug 19 13:08:22.247: INFO: Created: latency-svc-8jrhh
  Aug 19 13:08:22.286: INFO: Got endpoints: latency-svc-jtbkt [748.901625ms]
  Aug 19 13:08:22.295: INFO: Created: latency-svc-lptt2
  Aug 19 13:08:22.336: INFO: Got endpoints: latency-svc-dwr6k [749.338778ms]
  Aug 19 13:08:22.346: INFO: Created: latency-svc-5smqs
  Aug 19 13:08:22.387: INFO: Got endpoints: latency-svc-44757 [747.284863ms]
  Aug 19 13:08:22.397: INFO: Created: latency-svc-txlkq
  Aug 19 13:08:22.438: INFO: Got endpoints: latency-svc-pdplf [750.520147ms]
  Aug 19 13:08:22.448: INFO: Created: latency-svc-zlnrq
  Aug 19 13:08:22.486: INFO: Got endpoints: latency-svc-wc76c [749.547896ms]
  Aug 19 13:08:22.494: INFO: Created: latency-svc-9nqc6
  Aug 19 13:08:22.537: INFO: Got endpoints: latency-svc-8j56r [749.623841ms]
  Aug 19 13:08:22.548: INFO: Created: latency-svc-7h6pz
  Aug 19 13:08:22.586: INFO: Got endpoints: latency-svc-ldnxv [748.706976ms]
  Aug 19 13:08:22.595: INFO: Created: latency-svc-fpr57
  Aug 19 13:08:22.635: INFO: Got endpoints: latency-svc-knzls [749.443084ms]
  Aug 19 13:08:22.646: INFO: Created: latency-svc-rx5tn
  Aug 19 13:08:22.687: INFO: Got endpoints: latency-svc-hxqdg [747.158507ms]
  Aug 19 13:08:22.696: INFO: Created: latency-svc-z6bs5
  Aug 19 13:08:22.738: INFO: Got endpoints: latency-svc-qtlx5 [751.574535ms]
  Aug 19 13:08:22.747: INFO: Created: latency-svc-9q8xp
  Aug 19 13:08:22.787: INFO: Got endpoints: latency-svc-rcdss [749.09841ms]
  Aug 19 13:08:22.795: INFO: Created: latency-svc-jqtl5
  Aug 19 13:08:22.840: INFO: Got endpoints: latency-svc-7vl4m [753.999612ms]
  Aug 19 13:08:22.849: INFO: Created: latency-svc-86dsm
  Aug 19 13:08:22.886: INFO: Got endpoints: latency-svc-dhpqj [749.395738ms]
  Aug 19 13:08:22.899: INFO: Created: latency-svc-tzm42
  Aug 19 13:08:22.936: INFO: Got endpoints: latency-svc-blv97 [749.188014ms]
  Aug 19 13:08:22.944: INFO: Created: latency-svc-hvq2n
  E0819 13:08:22.952854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:22.987: INFO: Got endpoints: latency-svc-8jrhh [750.587316ms]
  Aug 19 13:08:22.995: INFO: Created: latency-svc-x4f5t
  Aug 19 13:08:23.038: INFO: Got endpoints: latency-svc-lptt2 [751.988583ms]
  Aug 19 13:08:23.047: INFO: Created: latency-svc-2dwj6
  Aug 19 13:08:23.087: INFO: Got endpoints: latency-svc-5smqs [750.526828ms]
  Aug 19 13:08:23.096: INFO: Created: latency-svc-rs9x9
  Aug 19 13:08:23.137: INFO: Got endpoints: latency-svc-txlkq [750.164523ms]
  Aug 19 13:08:23.147: INFO: Created: latency-svc-n5xk5
  Aug 19 13:08:23.187: INFO: Got endpoints: latency-svc-zlnrq [748.97231ms]
  Aug 19 13:08:23.195: INFO: Created: latency-svc-p89hc
  Aug 19 13:08:23.236: INFO: Got endpoints: latency-svc-9nqc6 [749.758277ms]
  Aug 19 13:08:23.245: INFO: Created: latency-svc-k24qt
  Aug 19 13:08:23.287: INFO: Got endpoints: latency-svc-7h6pz [750.147968ms]
  Aug 19 13:08:23.298: INFO: Created: latency-svc-cxhsd
  Aug 19 13:08:23.338: INFO: Got endpoints: latency-svc-fpr57 [751.349082ms]
  Aug 19 13:08:23.347: INFO: Created: latency-svc-bp8cs
  Aug 19 13:08:23.388: INFO: Got endpoints: latency-svc-rx5tn [752.176781ms]
  Aug 19 13:08:23.398: INFO: Created: latency-svc-p5bpj
  Aug 19 13:08:23.436: INFO: Got endpoints: latency-svc-z6bs5 [748.772701ms]
  Aug 19 13:08:23.455: INFO: Created: latency-svc-khmxf
  Aug 19 13:08:23.487: INFO: Got endpoints: latency-svc-9q8xp [748.290981ms]
  Aug 19 13:08:23.495: INFO: Created: latency-svc-zg992
  Aug 19 13:08:23.536: INFO: Got endpoints: latency-svc-jqtl5 [749.546126ms]
  Aug 19 13:08:23.546: INFO: Created: latency-svc-sxhtg
  Aug 19 13:08:23.588: INFO: Got endpoints: latency-svc-86dsm [747.608778ms]
  Aug 19 13:08:23.598: INFO: Created: latency-svc-5cjd6
  Aug 19 13:08:23.636: INFO: Got endpoints: latency-svc-tzm42 [749.602096ms]
  Aug 19 13:08:23.649: INFO: Created: latency-svc-5bpmj
  Aug 19 13:08:23.685: INFO: Got endpoints: latency-svc-hvq2n [749.038419ms]
  Aug 19 13:08:23.694: INFO: Created: latency-svc-bnzxb
  Aug 19 13:08:23.742: INFO: Got endpoints: latency-svc-x4f5t [755.316617ms]
  Aug 19 13:08:23.758: INFO: Created: latency-svc-5wrfn
  Aug 19 13:08:23.788: INFO: Got endpoints: latency-svc-2dwj6 [749.530563ms]
  Aug 19 13:08:23.797: INFO: Created: latency-svc-wvz9r
  Aug 19 13:08:23.839: INFO: Got endpoints: latency-svc-rs9x9 [752.165439ms]
  Aug 19 13:08:23.849: INFO: Created: latency-svc-7t47w
  Aug 19 13:08:23.887: INFO: Got endpoints: latency-svc-n5xk5 [749.685595ms]
  Aug 19 13:08:23.901: INFO: Created: latency-svc-dc4nl
  Aug 19 13:08:23.937: INFO: Got endpoints: latency-svc-p89hc [750.703376ms]
  Aug 19 13:08:23.946: INFO: Created: latency-svc-gt265
  E0819 13:08:23.953245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:23.987: INFO: Got endpoints: latency-svc-k24qt [751.483567ms]
  Aug 19 13:08:23.996: INFO: Created: latency-svc-kkdsz
  Aug 19 13:08:24.037: INFO: Got endpoints: latency-svc-cxhsd [749.712524ms]
  Aug 19 13:08:24.046: INFO: Created: latency-svc-lt4t2
  Aug 19 13:08:24.087: INFO: Got endpoints: latency-svc-bp8cs [748.952427ms]
  Aug 19 13:08:24.098: INFO: Created: latency-svc-hkm84
  Aug 19 13:08:24.137: INFO: Got endpoints: latency-svc-p5bpj [748.815331ms]
  Aug 19 13:08:24.146: INFO: Created: latency-svc-2lmz8
  Aug 19 13:08:24.186: INFO: Got endpoints: latency-svc-khmxf [749.977433ms]
  Aug 19 13:08:24.197: INFO: Created: latency-svc-t9lfb
  Aug 19 13:08:24.238: INFO: Got endpoints: latency-svc-zg992 [751.153726ms]
  Aug 19 13:08:24.246: INFO: Created: latency-svc-ppdxl
  Aug 19 13:08:24.288: INFO: Got endpoints: latency-svc-sxhtg [751.46135ms]
  Aug 19 13:08:24.297: INFO: Created: latency-svc-jrmbt
  Aug 19 13:08:24.337: INFO: Got endpoints: latency-svc-5cjd6 [749.345862ms]
  Aug 19 13:08:24.346: INFO: Created: latency-svc-k7skp
  Aug 19 13:08:24.390: INFO: Got endpoints: latency-svc-5bpmj [753.695543ms]
  Aug 19 13:08:24.401: INFO: Created: latency-svc-gkzsw
  Aug 19 13:08:24.437: INFO: Got endpoints: latency-svc-bnzxb [752.213978ms]
  Aug 19 13:08:24.446: INFO: Created: latency-svc-ff78f
  Aug 19 13:08:24.489: INFO: Got endpoints: latency-svc-5wrfn [746.481095ms]
  Aug 19 13:08:24.497: INFO: Created: latency-svc-jnhb9
  Aug 19 13:08:24.537: INFO: Got endpoints: latency-svc-wvz9r [748.712186ms]
  Aug 19 13:08:24.546: INFO: Created: latency-svc-k5vhb
  Aug 19 13:08:24.587: INFO: Got endpoints: latency-svc-7t47w [748.12594ms]
  Aug 19 13:08:24.597: INFO: Created: latency-svc-7tfjl
  Aug 19 13:08:24.636: INFO: Got endpoints: latency-svc-dc4nl [748.074497ms]
  Aug 19 13:08:24.647: INFO: Created: latency-svc-26wbg
  Aug 19 13:08:24.686: INFO: Got endpoints: latency-svc-gt265 [748.679397ms]
  Aug 19 13:08:24.695: INFO: Created: latency-svc-t75rz
  Aug 19 13:08:24.737: INFO: Got endpoints: latency-svc-kkdsz [749.495874ms]
  Aug 19 13:08:24.787: INFO: Got endpoints: latency-svc-lt4t2 [749.120926ms]
  Aug 19 13:08:24.838: INFO: Got endpoints: latency-svc-hkm84 [750.442506ms]
  Aug 19 13:08:24.891: INFO: Got endpoints: latency-svc-2lmz8 [754.19421ms]
  Aug 19 13:08:24.937: INFO: Got endpoints: latency-svc-t9lfb [750.045831ms]
  E0819 13:08:24.954238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:24.988: INFO: Got endpoints: latency-svc-ppdxl [749.650713ms]
  Aug 19 13:08:25.036: INFO: Got endpoints: latency-svc-jrmbt [748.006822ms]
  Aug 19 13:08:25.088: INFO: Got endpoints: latency-svc-k7skp [750.626345ms]
  Aug 19 13:08:25.136: INFO: Got endpoints: latency-svc-gkzsw [745.99694ms]
  Aug 19 13:08:25.188: INFO: Got endpoints: latency-svc-ff78f [750.313637ms]
  Aug 19 13:08:25.239: INFO: Got endpoints: latency-svc-jnhb9 [750.514903ms]
  Aug 19 13:08:25.287: INFO: Got endpoints: latency-svc-k5vhb [750.656029ms]
  Aug 19 13:08:25.338: INFO: Got endpoints: latency-svc-7tfjl [750.840847ms]
  Aug 19 13:08:25.388: INFO: Got endpoints: latency-svc-26wbg [751.824772ms]
  Aug 19 13:08:25.436: INFO: Got endpoints: latency-svc-t75rz [749.90756ms]
  Aug 19 13:08:25.436: INFO: Latencies: [19.819636ms 26.110494ms 32.521088ms 36.804814ms 48.605078ms 61.538347ms 65.045676ms 65.601924ms 67.933248ms 77.581948ms 82.579268ms 88.074155ms 89.292566ms 93.965479ms 96.588571ms 99.717327ms 102.570721ms 103.501601ms 104.063485ms 104.543025ms 105.411453ms 106.191428ms 106.894822ms 107.61313ms 108.439052ms 109.373832ms 109.480448ms 109.898812ms 110.36275ms 111.353329ms 113.034614ms 114.548168ms 114.758696ms 122.035833ms 144.568653ms 187.413459ms 232.746042ms 274.241622ms 313.70475ms 364.232478ms 404.584574ms 445.123187ms 491.3977ms 532.3828ms 574.305219ms 615.059884ms 659.236319ms 703.991894ms 743.680908ms 744.204218ms 745.99694ms 746.100315ms 746.148493ms 746.481095ms 746.702957ms 747.158507ms 747.238767ms 747.284863ms 747.407131ms 747.608778ms 747.634391ms 747.637198ms 747.745798ms 747.860156ms 747.905234ms 748.006822ms 748.074497ms 748.12594ms 748.139804ms 748.169017ms 748.194299ms 748.199823ms 748.290981ms 748.295ms 748.295834ms 748.307333ms 748.324928ms 748.511541ms 748.583972ms 748.592228ms 748.679397ms 748.691492ms 748.706976ms 748.712186ms 748.772701ms 748.807658ms 748.815331ms 748.866457ms 748.901625ms 748.923416ms 748.952427ms 748.97231ms 749.00549ms 749.038419ms 749.09841ms 749.120926ms 749.188014ms 749.201486ms 749.226566ms 749.236776ms 749.285141ms 749.308549ms 749.338778ms 749.345862ms 749.395738ms 749.443084ms 749.443897ms 749.478088ms 749.495874ms 749.497508ms 749.506618ms 749.512748ms 749.530563ms 749.546126ms 749.547896ms 749.549408ms 749.602096ms 749.623841ms 749.650713ms 749.658509ms 749.685595ms 749.702073ms 749.712524ms 749.751986ms 749.758277ms 749.83133ms 749.889489ms 749.891732ms 749.90756ms 749.923223ms 749.977433ms 749.989712ms 749.991331ms 750.043121ms 750.045831ms 750.063258ms 750.07134ms 750.147968ms 750.164523ms 750.200325ms 750.216816ms 750.288754ms 750.313637ms 750.389184ms 750.436302ms 750.442506ms 750.485198ms 750.514903ms 750.520147ms 750.526828ms 750.587316ms 750.626345ms 750.656029ms 750.662124ms 750.662742ms 750.686437ms 750.703376ms 750.707894ms 750.805366ms 750.808047ms 750.840847ms 750.873967ms 750.922107ms 751.109346ms 751.118597ms 751.129377ms 751.153726ms 751.213555ms 751.264354ms 751.316326ms 751.349082ms 751.415824ms 751.455285ms 751.46135ms 751.483567ms 751.574097ms 751.574535ms 751.66419ms 751.668507ms 751.813758ms 751.824772ms 751.912959ms 751.988583ms 752.074531ms 752.165439ms 752.16827ms 752.176781ms 752.184324ms 752.192968ms 752.213978ms 752.428465ms 752.525278ms 752.572688ms 752.920234ms 753.584398ms 753.695543ms 753.999612ms 754.19421ms 755.316617ms 760.127923ms]
  Aug 19 13:08:25.437: INFO: 50 %ile: 749.285141ms
  Aug 19 13:08:25.437: INFO: 90 %ile: 751.824772ms
  Aug 19 13:08:25.437: INFO: 99 %ile: 755.316617ms
  Aug 19 13:08:25.437: INFO: Total sample count: 200
  Aug 19 13:08:25.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7827" for this suite. @ 08/19/23 13:08:25.442
• [10.761 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/19/23 13:08:25.448
  Aug 19 13:08:25.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 13:08:25.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:08:25.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:08:25.466
  Aug 19 13:08:25.469: INFO: Creating deployment "test-recreate-deployment"
  Aug 19 13:08:25.475: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug 19 13:08:25.481: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0819 13:08:25.955230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:26.956301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:08:27.488: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug 19 13:08:27.491: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug 19 13:08:27.499: INFO: Updating deployment test-recreate-deployment
  Aug 19 13:08:27.499: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug 19 13:08:27.584: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9987  843ee39a-798a-47f0-a605-9dc4baa02b5d 34113 2 2023-08-19 13:08:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00561bf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-19 13:08:27 +0000 UTC,LastTransitionTime:2023-08-19 13:08:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-19 13:08:27 +0000 UTC,LastTransitionTime:2023-08-19 13:08:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 19 13:08:27.587: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-9987  b4e69a89-1b81-4d0e-9543-e93ab88cefdb 34111 1 2023-08-19 13:08:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 843ee39a-798a-47f0-a605-9dc4baa02b5d 0xc005739db7 0xc005739db8}] [] [{kube-controller-manager Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"843ee39a-798a-47f0-a605-9dc4baa02b5d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005739e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:08:27.587: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug 19 13:08:27.587: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-9987  e4f49745-4598-4c4f-873e-007643d3f7cd 34101 2 2023-08-19 13:08:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 843ee39a-798a-47f0-a605-9dc4baa02b5d 0xc005739ec7 0xc005739ec8}] [] [{kube-controller-manager Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"843ee39a-798a-47f0-a605-9dc4baa02b5d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005739f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:08:27.591: INFO: Pod "test-recreate-deployment-54757ffd6c-nc4j9" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-nc4j9 test-recreate-deployment-54757ffd6c- deployment-9987  149ab5ae-6eb3-4e7d-8d2c-8a5b5d49b688 34112 0 2023-08-19 13:08:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c b4e69a89-1b81-4d0e-9543-e93ab88cefdb 0xc005944407 0xc005944408}] [] [{kube-controller-manager Update v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4e69a89-1b81-4d0e-9543-e93ab88cefdb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 13:08:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6zpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6zpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:08:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:08:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:08:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:08:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:,StartTime:2023-08-19 13:08:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 13:08:27.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9987" for this suite. @ 08/19/23 13:08:27.595
• [2.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/19/23 13:08:27.601
  Aug 19 13:08:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/19/23 13:08:27.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:08:27.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:08:27.622
  Aug 19 13:08:27.625: INFO: Waiting up to 1m0s for all nodes to be ready
  E0819 13:08:27.956323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:28.956565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:29.957586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:30.957814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:31.958269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:32.958615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:33.959522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:34.960297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:35.960402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:36.960932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:37.961191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:38.961291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:39.961741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:40.962088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:41.962637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:42.963614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:43.963715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:44.964261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:45.965236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:46.965342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:47.965662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:48.965737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:49.966698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:50.966798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:51.967733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:52.968629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:53.969542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:54.969631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:55.970553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:56.970744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:57.971744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:58.971796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:08:59.972740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:00.972987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:01.973998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:02.974316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:03.975339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:04.976279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:05.976924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:06.977013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:07.977325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:08.977521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:09.977608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:10.977868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:11.978388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:12.978681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:13.979594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:14.980275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:15.980366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:16.980801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:17.980829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:18.980999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:19.981918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:20.982174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:21.982723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:22.982879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:23.982972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:24.983073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:25.983931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:26.984278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:09:27.640: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 13:09:27.643: INFO: Starting informer...
  STEP: Starting pods... @ 08/19/23 13:09:27.643
  Aug 19 13:09:27.860: INFO: Pod1 is running on ip-172-31-15-214. Tainting Node
  E0819 13:09:27.985287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:28.985897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:29.986763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:09:30.080: INFO: Pod2 is running on ip-172-31-15-214. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/19/23 13:09:30.08
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/19/23 13:09:30.09
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/19/23 13:09:30.093
  E0819 13:09:30.986855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:31.987706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:32.988693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:33.990613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:34.990744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:35.991637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:09:36.529: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0819 13:09:36.991745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:37.991920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:38.992029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:39.992131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:40.992354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:41.993029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:42.993346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:43.994054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:44.994198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:45.994398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:46.995090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:47.995763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:48.996739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:49.996907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:50.997864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:51.998079      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:52.998527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:53.998771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:54.998932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:09:55.858: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug 19 13:09:55.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/19/23 13:09:55.872
  STEP: Destroying namespace "taint-multiple-pods-1723" for this suite. @ 08/19/23 13:09:55.875
• [88.280 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/19/23 13:09:55.882
  Aug 19 13:09:55.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption @ 08/19/23 13:09:55.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:09:55.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:09:55.911
  STEP: creating the pdb @ 08/19/23 13:09:55.914
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:09:55.92
  E0819 13:09:55.999139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:56.999232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 08/19/23 13:09:57.926
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:09:57.934
  E0819 13:09:57.999289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:09:58.999382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 08/19/23 13:09:59.94
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:09:59.951
  E0819 13:09:59.999985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:01.000381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 08/19/23 13:10:01.964
  Aug 19 13:10:01.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7432" for this suite. @ 08/19/23 13:10:01.97
• [6.096 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/19/23 13:10:01.978
  Aug 19 13:10:01.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:10:01.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:01.998
  E0819 13:10:02.000810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:02.001
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/19/23 13:10:02.004
  E0819 13:10:03.001583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:04.002448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:05.002526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:06.003585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:10:06.023
  Aug 19 13:10:06.026: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-9a87b52d-3585-4de8-961c-1d2e62b387d6 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:10:06.04
  Aug 19 13:10:06.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2947" for this suite. @ 08/19/23 13:10:06.06
• [4.087 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/19/23 13:10:06.065
  Aug 19 13:10:06.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename init-container @ 08/19/23 13:10:06.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:06.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:06.087
  STEP: creating the pod @ 08/19/23 13:10:06.09
  Aug 19 13:10:06.091: INFO: PodSpec: initContainers in spec.initContainers
  E0819 13:10:07.003953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:08.004775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:09.004850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:10.004957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9698" for this suite. @ 08/19/23 13:10:10.874
• [4.816 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 08/19/23 13:10:10.884
  Aug 19 13:10:10.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 13:10:10.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:10.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:10.91
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/19/23 13:10:10.931
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 13:10:10.937
  Aug 19 13:10:10.940: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:10.940: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:10.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:10:10.943: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:10:11.005668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:11.947: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:11.947: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:11.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 19 13:10:11.951: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:10:12.006053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:12.948: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:12.948: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:12.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 13:10:12.951: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/19/23 13:10:12.954
  Aug 19 13:10:12.968: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:12.969: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:10:12.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 13:10:12.974: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/19/23 13:10:12.974
  E0819 13:10:13.006849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 13:10:13.984
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9611, will wait for the garbage collector to delete the pods @ 08/19/23 13:10:13.984
  E0819 13:10:14.007158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:14.044: INFO: Deleting DaemonSet.extensions daemon-set took: 5.703433ms
  Aug 19 13:10:14.145: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.949826ms
  E0819 13:10:15.007456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:15.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:10:15.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 13:10:15.951: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35635"},"items":null}

  Aug 19 13:10:15.954: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35635"},"items":null}

  Aug 19 13:10:15.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9611" for this suite. @ 08/19/23 13:10:15.97
• [5.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/19/23 13:10:15.984
  Aug 19 13:10:15.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 13:10:15.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:16.006
  E0819 13:10:16.007544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:16.008
  STEP: Counting existing ResourceQuota @ 08/19/23 13:10:16.011
  E0819 13:10:17.007657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:18.008607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:19.008702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:20.009333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:21.009446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/19/23 13:10:21.015
  STEP: Ensuring resource quota status is calculated @ 08/19/23 13:10:21.021
  E0819 13:10:22.009519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:23.009624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 08/19/23 13:10:23.025
  STEP: Ensuring resource quota status captures replicaset creation @ 08/19/23 13:10:23.037
  E0819 13:10:24.009725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:25.009807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 08/19/23 13:10:25.041
  STEP: Ensuring resource quota status released usage @ 08/19/23 13:10:25.047
  E0819 13:10:26.009902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:27.010316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:27.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7899" for this suite. @ 08/19/23 13:10:27.054
• [11.076 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/19/23 13:10:27.062
  Aug 19 13:10:27.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 13:10:27.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:27.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:27.085
  STEP: Creating ReplicationController "e2e-rc-czpsq" @ 08/19/23 13:10:27.088
  Aug 19 13:10:27.093: INFO: Get Replication Controller "e2e-rc-czpsq" to confirm replicas
  E0819 13:10:28.010629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:28.097: INFO: Get Replication Controller "e2e-rc-czpsq" to confirm replicas
  Aug 19 13:10:28.101: INFO: Found 1 replicas for "e2e-rc-czpsq" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-czpsq" @ 08/19/23 13:10:28.101
  STEP: Updating a scale subresource @ 08/19/23 13:10:28.104
  STEP: Verifying replicas where modified for replication controller "e2e-rc-czpsq" @ 08/19/23 13:10:28.109
  Aug 19 13:10:28.109: INFO: Get Replication Controller "e2e-rc-czpsq" to confirm replicas
  E0819 13:10:29.011614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:29.112: INFO: Get Replication Controller "e2e-rc-czpsq" to confirm replicas
  Aug 19 13:10:29.116: INFO: Found 2 replicas for "e2e-rc-czpsq" replication controller
  Aug 19 13:10:29.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1377" for this suite. @ 08/19/23 13:10:29.12
• [2.064 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/19/23 13:10:29.128
  Aug 19 13:10:29.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 13:10:29.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:29.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:29.15
  Aug 19 13:10:29.162: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0819 13:10:30.011720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:31.012080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:32.012329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:33.012708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:34.012764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:34.173: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 13:10:34.173
  Aug 19 13:10:34.173: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0819 13:10:35.013252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:36.013411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:36.177: INFO: Creating deployment "test-rollover-deployment"
  Aug 19 13:10:36.188: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0819 13:10:37.013499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:38.013829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:38.195: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug 19 13:10:38.201: INFO: Ensure that both replica sets have 1 created replica
  Aug 19 13:10:38.207: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug 19 13:10:38.216: INFO: Updating deployment test-rollover-deployment
  Aug 19 13:10:38.216: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0819 13:10:39.014692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:40.014759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:40.223: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug 19 13:10:40.229: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug 19 13:10:40.236: INFO: all replica sets need to contain the pod-template-hash label
  Aug 19 13:10:40.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 13:10:41.014846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:42.014954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:42.243: INFO: all replica sets need to contain the pod-template-hash label
  Aug 19 13:10:42.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 13:10:43.015672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:44.015769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:44.243: INFO: all replica sets need to contain the pod-template-hash label
  Aug 19 13:10:44.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 13:10:45.016284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:46.016388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:46.243: INFO: all replica sets need to contain the pod-template-hash label
  Aug 19 13:10:46.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 13:10:47.017282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:48.017545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:48.244: INFO: all replica sets need to contain the pod-template-hash label
  Aug 19 13:10:48.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 19, 13, 10, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 19, 13, 10, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0819 13:10:49.017658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:50.017867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:10:50.244: INFO: 
  Aug 19 13:10:50.244: INFO: Ensure that both old replica sets have no replicas
  Aug 19 13:10:50.253: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2445  c3d5b183-3aa6-4f05-9908-18eae86a8d42 35926 2 2023-08-19 13:10:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-19 13:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:10:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3d508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-19 13:10:36 +0000 UTC,LastTransitionTime:2023-08-19 13:10:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-19 13:10:49 +0000 UTC,LastTransitionTime:2023-08-19 13:10:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 19 13:10:50.256: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2445  417b4302-b0a3-474f-adb5-bec60bdc0523 35916 2 2023-08-19 13:10:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c3d5b183-3aa6-4f05-9908-18eae86a8d42 0xc003c3d9e7 0xc003c3d9e8}] [] [{kube-controller-manager Update apps/v1 2023-08-19 13:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3d5b183-3aa6-4f05-9908-18eae86a8d42\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:10:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3da98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:10:50.256: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug 19 13:10:50.256: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2445  495c486d-f1b1-4408-ae1b-7201028992bb 35925 2 2023-08-19 13:10:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c3d5b183-3aa6-4f05-9908-18eae86a8d42 0xc003c3d8a7 0xc003c3d8a8}] [] [{e2e.test Update apps/v1 2023-08-19 13:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:10:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3d5b183-3aa6-4f05-9908-18eae86a8d42\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:10:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c3d968 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:10:50.257: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2445  d4530afd-3413-473a-8cbe-e6a73d2faa0c 35874 2 2023-08-19 13:10:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c3d5b183-3aa6-4f05-9908-18eae86a8d42 0xc003c3db07 0xc003c3db08}] [] [{kube-controller-manager Update apps/v1 2023-08-19 13:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3d5b183-3aa6-4f05-9908-18eae86a8d42\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:10:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c3dbb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:10:50.260: INFO: Pod "test-rollover-deployment-57777854c9-4tww5" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-4tww5 test-rollover-deployment-57777854c9- deployment-2445  fd7483fe-577c-4f68-8585-15f1589104b4 35894 0 2023-08-19 13:10:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 417b4302-b0a3-474f-adb5-bec60bdc0523 0xc0031f6117 0xc0031f6118}] [] [{kube-controller-manager Update v1 2023-08-19 13:10:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"417b4302-b0a3-474f-adb5-bec60bdc0523\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 13:10:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4rg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4rg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.178,StartTime:2023-08-19 13:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 13:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://731d800b21fd085443b46b93fc3126994c9df6d610ad7d4b63669c31e4d347e5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 13:10:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2445" for this suite. @ 08/19/23 13:10:50.264
• [21.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/19/23 13:10:50.271
  Aug 19 13:10:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:10:50.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:50.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:50.293
  STEP: Creating projection with secret that has name projected-secret-test-39aaa289-7afa-41ce-8c96-18b2f8157750 @ 08/19/23 13:10:50.296
  STEP: Creating a pod to test consume secrets @ 08/19/23 13:10:50.304
  E0819 13:10:51.018516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:52.018729      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:53.019687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:54.019797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:10:54.322
  Aug 19 13:10:54.326: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-projected-secrets-8e2e6c1a-8d87-4873-8421-808acd13cab3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 13:10:54.333
  Aug 19 13:10:54.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6301" for this suite. @ 08/19/23 13:10:54.355
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/19/23 13:10:54.367
  Aug 19 13:10:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-runtime @ 08/19/23 13:10:54.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:54.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:54.389
  STEP: create the container @ 08/19/23 13:10:54.392
  W0819 13:10:54.399006      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/19/23 13:10:54.399
  E0819 13:10:55.019889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:56.020272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:57.020413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/19/23 13:10:57.413
  STEP: the container should be terminated @ 08/19/23 13:10:57.417
  STEP: the termination message should be set @ 08/19/23 13:10:57.417
  Aug 19 13:10:57.417: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/19/23 13:10:57.417
  Aug 19 13:10:57.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3189" for this suite. @ 08/19/23 13:10:57.437
• [3.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/19/23 13:10:57.443
  Aug 19 13:10:57.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 13:10:57.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:10:57.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:10:57.462
  STEP: Creating secret with name secret-test-0f2c69fd-fdc8-4147-a1d6-d136296e2c13 @ 08/19/23 13:10:57.465
  STEP: Creating a pod to test consume secrets @ 08/19/23 13:10:57.47
  E0819 13:10:58.021311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:10:59.021397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:00.021485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:01.021586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:11:01.491
  Aug 19 13:11:01.494: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-3c32efea-da05-4a77-9045-8d9774927c51 container secret-env-test: <nil>
  STEP: delete the pod @ 08/19/23 13:11:01.501
  Aug 19 13:11:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1729" for this suite. @ 08/19/23 13:11:01.529
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/19/23 13:11:01.536
  Aug 19 13:11:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:11:01.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:11:01.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:11:01.559
  STEP: Creating the pod @ 08/19/23 13:11:01.563
  E0819 13:11:02.022351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:03.022623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:04.023034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:11:04.098: INFO: Successfully updated pod "labelsupdate8d1c3ef5-f353-496e-9c05-f308d8ea2a4c"
  E0819 13:11:05.023260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:06.024299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:11:06.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8440" for this suite. @ 08/19/23 13:11:06.116
• [4.586 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/19/23 13:11:06.122
  Aug 19 13:11:06.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 13:11:06.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:11:06.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:11:06.144
  Aug 19 13:11:06.154: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0819 13:11:07.024846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:08.025041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:09.025137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:10.025355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:11.025627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:11:11.159: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 13:11:11.159
  Aug 19 13:11:11.159: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/19/23 13:11:11.168
  Aug 19 13:11:11.179: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2949  c45dbb65-7abf-4dde-9857-fb2d7173919a 36157 1 2023-08-19 13:11:11 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-19 13:11:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062df948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 19 13:11:11.182: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Aug 19 13:11:11.182: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug 19 13:11:11.182: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2949  fd462745-646a-4b7f-9928-ea5122fe20f8 36158 1 2023-08-19 13:11:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c45dbb65-7abf-4dde-9857-fb2d7173919a 0xc0062dfcb7 0xc0062dfcb8}] [] [{e2e.test Update apps/v1 2023-08-19 13:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:11:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-19 13:11:11 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c45dbb65-7abf-4dde-9857-fb2d7173919a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0062dfd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:11:11.185: INFO: Pod "test-cleanup-controller-qdxh2" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-qdxh2 test-cleanup-controller- deployment-2949  f8617a88-8d31-40e4-acab-1842de3409b2 36146 0 2023-08-19 13:11:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller fd462745-646a-4b7f-9928-ea5122fe20f8 0xc0039ea787 0xc0039ea788}] [] [{kube-controller-manager Update v1 2023-08-19 13:11:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd462745-646a-4b7f-9928-ea5122fe20f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 13:11:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6j4mf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6j4mf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:11:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:11:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:11:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:11:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.141,StartTime:2023-08-19 13:11:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 13:11:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ef802851c9d7859ff49eefd20825a8594b696352fdf7821dce4f23648f3b7aa4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.141,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 13:11:11.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2949" for this suite. @ 08/19/23 13:11:11.189
• [5.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/19/23 13:11:11.2
  Aug 19 13:11:11.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/19/23 13:11:11.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:11:11.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:11:11.225
  STEP: create the container to handle the HTTPGet hook request. @ 08/19/23 13:11:11.235
  E0819 13:11:12.025728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:13.026722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/19/23 13:11:13.253
  E0819 13:11:14.027771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:15.028623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/19/23 13:11:15.267
  E0819 13:11:16.028717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:17.028825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/19/23 13:11:17.281
  Aug 19 13:11:17.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8942" for this suite. @ 08/19/23 13:11:17.292
• [6.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/19/23 13:11:17.303
  Aug 19 13:11:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 13:11:17.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:11:17.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:11:17.323
  STEP: Creating configMap with name configmap-test-upd-e6c0c2ce-a266-44cb-8cb7-0e93a3240dd3 @ 08/19/23 13:11:17.33
  STEP: Creating the pod @ 08/19/23 13:11:17.334
  E0819 13:11:18.029836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:19.030036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-e6c0c2ce-a266-44cb-8cb7-0e93a3240dd3 @ 08/19/23 13:11:19.356
  STEP: waiting to observe update in volume @ 08/19/23 13:11:19.36
  E0819 13:11:20.031038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:21.031227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:22.031319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:23.031408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:24.031517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:25.031606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:26.031678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:27.032307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:28.032592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:29.032687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:30.033190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:31.033285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:32.033381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:33.033717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:34.033812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:35.034019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:36.034609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:37.034845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:38.035319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:39.036300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:40.036386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:41.036483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:42.036565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:43.036638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:44.037613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:45.037812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:46.038647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:47.038913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:48.039293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:49.039386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:50.040200      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:51.040294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:52.040331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:53.040681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:54.040780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:55.040874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:56.040968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:57.041052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:58.041377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:11:59.041580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:00.041592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:01.042405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:02.042458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:03.042763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:04.042885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:05.042959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:06.043390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:07.044304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:08.044403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:09.044509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:10.044598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:11.044686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:12.044726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:13.045722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:14.045785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:15.045873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:16.046500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:17.046580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:18.047382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:19.047477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:20.048305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:21.048395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:22.048931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:23.049726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:24.050639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:25.050730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:26.051754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:27.051989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:12:27.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2818" for this suite. @ 08/19/23 13:12:27.646
• [70.349 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/19/23 13:12:27.652
  Aug 19 13:12:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:12:27.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:12:27.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:12:27.674
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:12:27.677
  E0819 13:12:28.052043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:29.052122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:30.052805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:31.053094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:12:31.698
  Aug 19 13:12:31.701: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-dd5a6b72-33a0-4b88-802b-aab4caa65382 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:12:31.708
  Aug 19 13:12:31.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9229" for this suite. @ 08/19/23 13:12:31.728
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/19/23 13:12:31.737
  Aug 19 13:12:31.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subpath @ 08/19/23 13:12:31.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:12:31.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:12:31.758
  STEP: Setting up data @ 08/19/23 13:12:31.762
  STEP: Creating pod pod-subpath-test-downwardapi-4kkj @ 08/19/23 13:12:31.77
  STEP: Creating a pod to test atomic-volume-subpath @ 08/19/23 13:12:31.77
  E0819 13:12:32.053335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:33.053681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:34.053982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:35.054746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:36.055704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:37.055794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:38.056862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:39.057057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:40.057112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:41.057293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:42.058811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:43.059742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:44.060151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:45.060248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:46.061217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:47.061342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:48.062159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:49.062254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:50.062274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:51.063077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:52.063154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:53.063549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:54.063644      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:55.064259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:12:55.827
  Aug 19 13:12:55.830: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-subpath-test-downwardapi-4kkj container test-container-subpath-downwardapi-4kkj: <nil>
  STEP: delete the pod @ 08/19/23 13:12:55.837
  STEP: Deleting pod pod-subpath-test-downwardapi-4kkj @ 08/19/23 13:12:55.853
  Aug 19 13:12:55.853: INFO: Deleting pod "pod-subpath-test-downwardapi-4kkj" in namespace "subpath-7423"
  Aug 19 13:12:55.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7423" for this suite. @ 08/19/23 13:12:55.859
• [24.129 seconds]
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/19/23 13:12:55.865
  Aug 19 13:12:55.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replicaset @ 08/19/23 13:12:55.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:12:55.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:12:55.892
  Aug 19 13:12:55.907: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0819 13:12:56.065125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:57.065497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:58.065586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:12:59.065692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:00.065761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:13:00.911: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 13:13:00.911
  STEP: Scaling up "test-rs" replicaset  @ 08/19/23 13:13:00.911
  Aug 19 13:13:00.919: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/19/23 13:13:00.919
  W0819 13:13:00.927910      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 19 13:13:00.929: INFO: observed ReplicaSet test-rs in namespace replicaset-3296 with ReadyReplicas 1, AvailableReplicas 1
  Aug 19 13:13:00.946: INFO: observed ReplicaSet test-rs in namespace replicaset-3296 with ReadyReplicas 1, AvailableReplicas 1
  Aug 19 13:13:00.964: INFO: observed ReplicaSet test-rs in namespace replicaset-3296 with ReadyReplicas 1, AvailableReplicas 1
  Aug 19 13:13:00.971: INFO: observed ReplicaSet test-rs in namespace replicaset-3296 with ReadyReplicas 1, AvailableReplicas 1
  E0819 13:13:01.066318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:02.066414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:13:02.142: INFO: observed ReplicaSet test-rs in namespace replicaset-3296 with ReadyReplicas 2, AvailableReplicas 2
  Aug 19 13:13:02.163: INFO: observed Replicaset test-rs in namespace replicaset-3296 with ReadyReplicas 3 found true
  Aug 19 13:13:02.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3296" for this suite. @ 08/19/23 13:13:02.168
• [6.309 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/19/23 13:13:02.176
  Aug 19 13:13:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:13:02.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:13:02.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:13:02.197
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/19/23 13:13:02.2
  E0819 13:13:03.066720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:04.066830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:05.067243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:06.067570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:13:06.217
  Aug 19 13:13:06.220: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-ac9219f8-36f6-45b4-9142-b7ba27fc1132 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:13:06.227
  Aug 19 13:13:06.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3554" for this suite. @ 08/19/23 13:13:06.245
• [4.075 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/19/23 13:13:06.251
  Aug 19 13:13:06.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 13:13:06.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:13:06.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:13:06.27
  STEP: Creating resourceQuota "e2e-rq-status-z4gv6" @ 08/19/23 13:13:06.276
  Aug 19 13:13:06.284: INFO: Resource quota "e2e-rq-status-z4gv6" reports spec: hard cpu limit of 500m
  Aug 19 13:13:06.284: INFO: Resource quota "e2e-rq-status-z4gv6" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-z4gv6" /status @ 08/19/23 13:13:06.284
  STEP: Confirm /status for "e2e-rq-status-z4gv6" resourceQuota via watch @ 08/19/23 13:13:06.29
  Aug 19 13:13:06.292: INFO: observed resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList(nil)
  Aug 19 13:13:06.292: INFO: Found resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 19 13:13:06.292: INFO: ResourceQuota "e2e-rq-status-z4gv6" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/19/23 13:13:06.295
  Aug 19 13:13:06.300: INFO: Resource quota "e2e-rq-status-z4gv6" reports spec: hard cpu limit of 1
  Aug 19 13:13:06.300: INFO: Resource quota "e2e-rq-status-z4gv6" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-z4gv6" /status @ 08/19/23 13:13:06.3
  STEP: Confirm /status for "e2e-rq-status-z4gv6" resourceQuota via watch @ 08/19/23 13:13:06.306
  Aug 19 13:13:06.307: INFO: observed resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 19 13:13:06.307: INFO: Found resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug 19 13:13:06.307: INFO: ResourceQuota "e2e-rq-status-z4gv6" /status was patched
  STEP: Get "e2e-rq-status-z4gv6" /status @ 08/19/23 13:13:06.307
  Aug 19 13:13:06.311: INFO: Resourcequota "e2e-rq-status-z4gv6" reports status: hard cpu of 1
  Aug 19 13:13:06.311: INFO: Resourcequota "e2e-rq-status-z4gv6" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-z4gv6" /status before checking Spec is unchanged @ 08/19/23 13:13:06.314
  Aug 19 13:13:06.331: INFO: Resourcequota "e2e-rq-status-z4gv6" reports status: hard cpu of 2
  Aug 19 13:13:06.331: INFO: Resourcequota "e2e-rq-status-z4gv6" reports status: hard memory of 2Gi
  Aug 19 13:13:06.333: INFO: observed resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug 19 13:13:06.333: INFO: Found resourceQuota "e2e-rq-status-z4gv6" in namespace "resourcequota-8066" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0819 13:13:07.067736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:08.067822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:09.068237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:10.068327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:11.068413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:12.069179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:13.069645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:14.069736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:15.069855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:16.069940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:17.070031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:18.070834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:19.071004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:20.071244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:21.071371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:22.071442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:23.071742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:24.072300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:25.072572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:26.072662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:27.073595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:28.073848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:29.073927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:30.074112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:31.074312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:32.074400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:33.074528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:34.074713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:35.074804      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:36.074960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:37.075816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:38.076837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:39.076913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:40.077005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:41.077227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:42.077322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:43.077690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:44.077884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:45.077992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:46.078178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:47.078262      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:48.078641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:49.078715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:50.078811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:51.078981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:52.079241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:53.079700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:54.079796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:55.080054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:56.080121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:57.080232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:58.080681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:13:59.080774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:00.080994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:01.081089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:14:01.340: INFO: ResourceQuota "e2e-rq-status-z4gv6" Spec was unchanged and /status reset
  Aug 19 13:14:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8066" for this suite. @ 08/19/23 13:14:01.344
• [55.100 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/19/23 13:14:01.352
  Aug 19 13:14:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename containers @ 08/19/23 13:14:01.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:14:01.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:14:01.377
  STEP: Creating a pod to test override command @ 08/19/23 13:14:01.381
  E0819 13:14:02.082050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:03.082365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:04.083085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:05.083260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:14:05.399
  Aug 19 13:14:05.402: INFO: Trying to get logs from node ip-172-31-15-214 pod client-containers-03111c26-e08a-49b0-9849-ef3d01b27acc container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 13:14:05.409
  Aug 19 13:14:05.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8221" for this suite. @ 08/19/23 13:14:05.426
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/19/23 13:14:05.435
  Aug 19 13:14:05.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 13:14:05.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:14:05.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:14:05.456
  STEP: creating a ServiceAccount @ 08/19/23 13:14:05.459
  STEP: watching for the ServiceAccount to be added @ 08/19/23 13:14:05.468
  STEP: patching the ServiceAccount @ 08/19/23 13:14:05.471
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/19/23 13:14:05.478
  STEP: deleting the ServiceAccount @ 08/19/23 13:14:05.481
  Aug 19 13:14:05.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9932" for this suite. @ 08/19/23 13:14:05.498
• [0.070 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/19/23 13:14:05.506
  Aug 19 13:14:05.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:14:05.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:14:05.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:14:05.525
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:14:05.529
  E0819 13:14:06.084309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:07.084644      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:08.085471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:09.085551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:14:09.55
  Aug 19 13:14:09.553: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-083be292-612e-4e30-bf2b-dc95a88b9401 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:14:09.56
  Aug 19 13:14:09.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3749" for this suite. @ 08/19/23 13:14:09.579
• [4.079 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/19/23 13:14:09.585
  Aug 19 13:14:09.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 13:14:09.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:14:09.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:14:09.606
  STEP: Creating configMap with name cm-test-opt-del-f7e595ff-534e-45d3-b8a2-cc4f515797bd @ 08/19/23 13:14:09.612
  STEP: Creating configMap with name cm-test-opt-upd-1eb234ab-5d4f-42b0-99f7-51ecf0faca58 @ 08/19/23 13:14:09.616
  STEP: Creating the pod @ 08/19/23 13:14:09.62
  E0819 13:14:10.086236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:11.086416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-f7e595ff-534e-45d3-b8a2-cc4f515797bd @ 08/19/23 13:14:11.657
  STEP: Updating configmap cm-test-opt-upd-1eb234ab-5d4f-42b0-99f7-51ecf0faca58 @ 08/19/23 13:14:11.666
  STEP: Creating configMap with name cm-test-opt-create-3dd3b0cb-57f7-4303-a150-907f2c621892 @ 08/19/23 13:14:11.671
  STEP: waiting to observe update in volume @ 08/19/23 13:14:11.675
  E0819 13:14:12.086875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:13.087192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:14.087237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:15.088304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:16.088621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:17.088943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:18.089905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:19.089999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:20.090094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:21.090172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:22.091037      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:23.091600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:24.091704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:25.092294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:26.093039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:27.093151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:28.093737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:29.093832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:30.094853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:31.094927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:32.095572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:33.095935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:34.096306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:35.096383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:36.096999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:37.097079      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:38.097423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:39.097622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:40.098396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:41.098678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:42.098777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:43.099064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:44.099609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:45.099700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:46.100294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:47.100401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:48.101162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:49.101244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:50.101612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:51.101805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:52.101883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:53.102269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:54.102722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:55.102853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:56.103376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:57.104278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:58.104971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:14:59.105063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:00.106088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:01.106979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:02.107602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:03.107881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:04.108885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:05.109077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:06.109628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:07.109808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:08.110717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:09.110803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:10.111583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:11.111675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:12.112529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:13.113364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:14.113677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:15.114036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:16.115078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:17.115216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:18.116087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:19.116173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:20.116256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:21.116497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:22.117513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:23.117706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:24.118615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:25.118709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:26.119190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:27.119284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:28.119367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:29.120283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:30.121148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:31.121236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:32.121347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:33.121900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:34.122641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:35.122703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:36.123480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:37.124280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:38.125279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:39.125356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:15:40.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1281" for this suite. @ 08/19/23 13:15:40.042
• [90.469 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 08/19/23 13:15:40.055
  Aug 19 13:15:40.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename field-validation @ 08/19/23 13:15:40.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:15:40.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:15:40.077
  Aug 19 13:15:40.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:15:40.126332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:41.126415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:42.126741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0819 13:15:42.629098      18 warnings.go:70] unknown field "alpha"
  W0819 13:15:42.629119      18 warnings.go:70] unknown field "beta"
  W0819 13:15:42.629126      18 warnings.go:70] unknown field "delta"
  W0819 13:15:42.629132      18 warnings.go:70] unknown field "epsilon"
  W0819 13:15:42.629138      18 warnings.go:70] unknown field "gamma"
  E0819 13:15:43.127152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:15:43.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9170" for this suite. @ 08/19/23 13:15:43.175
• [3.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/19/23 13:15:43.183
  Aug 19 13:15:43.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename containers @ 08/19/23 13:15:43.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:15:43.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:15:43.204
  STEP: Creating a pod to test override all @ 08/19/23 13:15:43.207
  E0819 13:15:44.127244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:45.127329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:46.127417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:47.127509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:15:47.228
  Aug 19 13:15:47.231: INFO: Trying to get logs from node ip-172-31-15-214 pod client-containers-75dc04e9-792b-4d97-a5b9-0543881adb80 container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 13:15:47.237
  Aug 19 13:15:47.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1845" for this suite. @ 08/19/23 13:15:47.256
• [4.079 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/19/23 13:15:47.264
  Aug 19 13:15:47.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename disruption @ 08/19/23 13:15:47.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:15:47.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:15:47.287
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:15:47.294
  E0819 13:15:48.127955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:49.128040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 08/19/23 13:15:49.3
  STEP: Waiting for all pods to be running @ 08/19/23 13:15:49.309
  Aug 19 13:15:49.313: INFO: running pods: 0 < 1
  E0819 13:15:50.128309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:51.128601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/19/23 13:15:51.317
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:15:51.329
  STEP: Patching PodDisruptionBudget status @ 08/19/23 13:15:51.336
  STEP: Waiting for the pdb to be processed @ 08/19/23 13:15:51.344
  Aug 19 13:15:51.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9436" for this suite. @ 08/19/23 13:15:51.351
• [4.093 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/19/23 13:15:51.358
  Aug 19 13:15:51.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename deployment @ 08/19/23 13:15:51.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:15:51.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:15:51.382
  Aug 19 13:15:51.388: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug 19 13:15:51.396: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0819 13:15:52.129088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:53.129792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:54.129878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:55.129982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:56.130231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:15:56.401: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/19/23 13:15:56.401
  Aug 19 13:15:56.401: INFO: Creating deployment "test-rolling-update-deployment"
  Aug 19 13:15:56.407: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug 19 13:15:56.413: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0819 13:15:57.130317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:15:58.130747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:15:58.420: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug 19 13:15:58.423: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug 19 13:15:58.433: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4480  97085785-ecb8-4571-9447-9a96e2df5ff0 37374 1 2023-08-19 13:15:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-19 13:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a99e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-19 13:15:56 +0000 UTC,LastTransitionTime:2023-08-19 13:15:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-19 13:15:57 +0000 UTC,LastTransitionTime:2023-08-19 13:15:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 19 13:15:58.436: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-4480  f4c3abb9-ba20-4a46-97bd-39a3bc7e6b3b 37363 1 2023-08-19 13:15:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 97085785-ecb8-4571-9447-9a96e2df5ff0 0xc005f04517 0xc005f04518}] [] [{kube-controller-manager Update apps/v1 2023-08-19 13:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97085785-ecb8-4571-9447-9a96e2df5ff0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:15:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f04ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:15:58.436: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug 19 13:15:58.436: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4480  fea672dd-f306-41fe-aaee-4ba113c9ed58 37373 2 2023-08-19 13:15:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 97085785-ecb8-4571-9447-9a96e2df5ff0 0xc005f04097 0xc005f04098}] [] [{e2e.test Update apps/v1 2023-08-19 13:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97085785-ecb8-4571-9447-9a96e2df5ff0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-19 13:15:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005f043e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 19 13:15:58.439: INFO: Pod "test-rolling-update-deployment-656d657cd8-jbhzr" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-jbhzr test-rolling-update-deployment-656d657cd8- deployment-4480  9ae05d6b-072a-4716-82f4-2ae3ba2765f9 37362 0 2023-08-19 13:15:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 f4c3abb9-ba20-4a46-97bd-39a3bc7e6b3b 0xc0040de217 0xc0040de218}] [] [{kube-controller-manager Update v1 2023-08-19 13:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4c3abb9-ba20-4a46-97bd-39a3bc7e6b3b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-19 13:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mq4hv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mq4hv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-19 13:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.214,PodIP:192.168.13.158,StartTime:2023-08-19 13:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-19 13:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e72982e7e55aa712d728efb2ce9b4bb41ef5b3da6f1483021e883eec50faa5d8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.13.158,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 19 13:15:58.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4480" for this suite. @ 08/19/23 13:15:58.444
• [7.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/19/23 13:15:58.453
  Aug 19 13:15:58.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:15:58.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:15:58.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:15:58.476
  STEP: Creating secret with name s-test-opt-del-b6edb337-ed07-474a-806a-a9c9391a9581 @ 08/19/23 13:15:58.483
  STEP: Creating secret with name s-test-opt-upd-b648a87d-843b-482e-a8dd-aaf664848c8b @ 08/19/23 13:15:58.488
  STEP: Creating the pod @ 08/19/23 13:15:58.492
  E0819 13:15:59.130908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:00.131002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-b6edb337-ed07-474a-806a-a9c9391a9581 @ 08/19/23 13:16:00.53
  STEP: Updating secret s-test-opt-upd-b648a87d-843b-482e-a8dd-aaf664848c8b @ 08/19/23 13:16:00.536
  STEP: Creating secret with name s-test-opt-create-efe30c49-56ba-40de-9ec8-fa4e10982601 @ 08/19/23 13:16:00.54
  STEP: waiting to observe update in volume @ 08/19/23 13:16:00.545
  E0819 13:16:01.131193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:02.131241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:03.131640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:04.132296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:04.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1965" for this suite. @ 08/19/23 13:16:04.582
• [6.135 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/19/23 13:16:04.589
  Aug 19 13:16:04.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sysctl @ 08/19/23 13:16:04.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:04.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:04.608
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/19/23 13:16:04.611
  STEP: Watching for error events or started pod @ 08/19/23 13:16:04.618
  E0819 13:16:05.133190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:06.133526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 08/19/23 13:16:06.623
  E0819 13:16:07.133604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:08.133827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 08/19/23 13:16:08.634
  STEP: Getting logs from the pod @ 08/19/23 13:16:08.634
  STEP: Checking that the sysctl is actually updated @ 08/19/23 13:16:08.641
  Aug 19 13:16:08.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9425" for this suite. @ 08/19/23 13:16:08.645
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/19/23 13:16:08.653
  Aug 19 13:16:08.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:16:08.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:08.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:08.674
  STEP: fetching services @ 08/19/23 13:16:08.677
  Aug 19 13:16:08.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9096" for this suite. @ 08/19/23 13:16:08.684
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/19/23 13:16:08.69
  Aug 19 13:16:08.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename events @ 08/19/23 13:16:08.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:08.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:08.712
  STEP: creating a test event @ 08/19/23 13:16:08.714
  STEP: listing all events in all namespaces @ 08/19/23 13:16:08.719
  STEP: patching the test event @ 08/19/23 13:16:08.723
  STEP: fetching the test event @ 08/19/23 13:16:08.73
  STEP: updating the test event @ 08/19/23 13:16:08.733
  STEP: getting the test event @ 08/19/23 13:16:08.743
  STEP: deleting the test event @ 08/19/23 13:16:08.746
  STEP: listing all events in all namespaces @ 08/19/23 13:16:08.753
  Aug 19 13:16:08.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6411" for this suite. @ 08/19/23 13:16:08.76
• [0.076 seconds]
------------------------------
SSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/19/23 13:16:08.766
  Aug 19 13:16:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/19/23 13:16:08.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:08.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:08.785
  STEP: creating @ 08/19/23 13:16:08.788
  STEP: getting @ 08/19/23 13:16:08.803
  STEP: listing @ 08/19/23 13:16:08.809
  STEP: deleting @ 08/19/23 13:16:08.812
  Aug 19 13:16:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1662" for this suite. @ 08/19/23 13:16:08.831
• [0.071 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/19/23 13:16:08.837
  Aug 19 13:16:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:16:08.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:08.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:08.856
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:16:08.859
  E0819 13:16:09.134691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:10.134776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:11.134863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:12.135237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:16:12.878
  Aug 19 13:16:12.881: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-94297040-2241-45c7-90ae-56d5f944ee84 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:16:12.888
  Aug 19 13:16:12.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6634" for this suite. @ 08/19/23 13:16:12.907
• [4.076 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/19/23 13:16:12.915
  Aug 19 13:16:12.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 13:16:12.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:12.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:12.937
  STEP: Creating configMap with name configmap-test-volume-map-36083e61-a377-48d0-abe3-54235d55ffb2 @ 08/19/23 13:16:12.939
  STEP: Creating a pod to test consume configMaps @ 08/19/23 13:16:12.945
  E0819 13:16:13.135692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:14.136282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:15.136774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:16.136857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:16:16.963
  Aug 19 13:16:16.967: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-configmaps-b03e92e4-d9c2-41ac-8676-20d055ac62ae container agnhost-container: <nil>
  STEP: delete the pod @ 08/19/23 13:16:16.974
  Aug 19 13:16:16.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1682" for this suite. @ 08/19/23 13:16:16.994
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/19/23 13:16:17.011
  Aug 19 13:16:17.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 13:16:17.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:17.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:17.035
  STEP: Counting existing ResourceQuota @ 08/19/23 13:16:17.042
  E0819 13:16:17.137380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:18.138296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:19.138879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:20.139461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:21.140433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/19/23 13:16:22.045
  STEP: Ensuring resource quota status is calculated @ 08/19/23 13:16:22.055
  E0819 13:16:22.140519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:23.140663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 08/19/23 13:16:24.059
  STEP: Creating a NodePort Service @ 08/19/23 13:16:24.075
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/19/23 13:16:24.109
  STEP: Ensuring resource quota status captures service creation @ 08/19/23 13:16:24.134
  E0819 13:16:24.141491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:25.141601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 08/19/23 13:16:26.138
  E0819 13:16:26.142143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status released usage @ 08/19/23 13:16:26.177
  E0819 13:16:27.142281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:28.142611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:28.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8829" for this suite. @ 08/19/23 13:16:28.19
• [11.185 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/19/23 13:16:28.196
  Aug 19 13:16:28.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:16:28.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:28.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:28.219
  STEP: Setting up server cert @ 08/19/23 13:16:28.247
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:16:28.514
  STEP: Deploying the webhook pod @ 08/19/23 13:16:28.521
  STEP: Wait for the deployment to be ready @ 08/19/23 13:16:28.532
  Aug 19 13:16:28.540: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:16:29.143473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:30.143867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:16:30.551
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:16:30.57
  E0819 13:16:31.143680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:31.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 19 13:16:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/19/23 13:16:32.086
  STEP: Creating a custom resource that should be denied by the webhook @ 08/19/23 13:16:32.103
  E0819 13:16:32.143942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:33.144022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/19/23 13:16:34.136
  STEP: Updating the custom resource with disallowed data should be denied @ 08/19/23 13:16:34.143
  E0819 13:16:34.144611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the custom resource should be denied @ 08/19/23 13:16:34.152
  STEP: Remove the offending key and value from the custom resource data @ 08/19/23 13:16:34.16
  STEP: Deleting the updated custom resource should be successful @ 08/19/23 13:16:34.171
  Aug 19 13:16:34.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9555" for this suite. @ 08/19/23 13:16:34.748
  STEP: Destroying namespace "webhook-markers-9689" for this suite. @ 08/19/23 13:16:34.755
• [6.565 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/19/23 13:16:34.762
  Aug 19 13:16:34.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:16:34.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:34.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:34.782
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:16:34.789
  E0819 13:16:35.144827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:36.144955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:37.145552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:38.145947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:16:38.811
  Aug 19 13:16:38.814: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-b23b6d9e-1bd9-4dd1-8cd6-e19a1fc2c8d2 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:16:38.824
  Aug 19 13:16:38.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3100" for this suite. @ 08/19/23 13:16:38.842
• [4.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/19/23 13:16:38.849
  Aug 19 13:16:38.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:16:38.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:38.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:38.876
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-9625 @ 08/19/23 13:16:38.879
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/19/23 13:16:38.895
  STEP: creating service externalsvc in namespace services-9625 @ 08/19/23 13:16:38.896
  STEP: creating replication controller externalsvc in namespace services-9625 @ 08/19/23 13:16:38.906
  I0819 13:16:38.912892      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9625, replica count: 2
  E0819 13:16:39.146311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:40.146511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:41.146616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:16:41.963957      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/19/23 13:16:41.967
  Aug 19 13:16:41.984: INFO: Creating new exec pod
  E0819 13:16:42.147192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:43.147953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:44.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9625 exec execpodww4kp -- /bin/sh -x -c nslookup nodeport-service.services-9625.svc.cluster.local'
  E0819 13:16:44.148469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:44.171: INFO: stderr: "+ nslookup nodeport-service.services-9625.svc.cluster.local\n"
  Aug 19 13:16:44.171: INFO: stdout: "Server:\t\t10.152.183.120\nAddress:\t10.152.183.120#53\n\nnodeport-service.services-9625.svc.cluster.local\tcanonical name = externalsvc.services-9625.svc.cluster.local.\nName:\texternalsvc.services-9625.svc.cluster.local\nAddress: 10.152.183.90\n\n"
  Aug 19 13:16:44.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9625, will wait for the garbage collector to delete the pods @ 08/19/23 13:16:44.175
  Aug 19 13:16:44.235: INFO: Deleting ReplicationController externalsvc took: 6.43004ms
  Aug 19 13:16:44.335: INFO: Terminating ReplicationController externalsvc pods took: 100.559005ms
  E0819 13:16:45.149359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:46.149520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:16:46.651: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-9625" for this suite. @ 08/19/23 13:16:46.664
• [7.820 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/19/23 13:16:46.675
  Aug 19 13:16:46.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 13:16:46.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:16:46.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:16:46.694
  E0819 13:16:47.150515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:48.150937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:49.151193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:50.152221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:51.152931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:52.153045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:53.153327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:54.153417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:55.153545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:56.154079      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:57.154757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:58.155130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:16:59.155765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:00.156307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:01.157319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:02.157948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:03.158430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:04.159139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:05.159795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:06.160276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:07.160329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:08.160767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:09.160828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:10.161216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:11.161915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:12.162909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:13.163686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:14.163776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:15.164772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:16.165298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:17.165995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:18.166961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:19.167242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:20.168066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:21.168911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:22.169378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:23.170067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:24.170604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:25.171610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:26.172281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:27.172913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:28.173462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:29.174511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:30.175357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:31.176270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:32.177011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:33.177435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:34.177526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:35.177604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:36.178159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:37.178849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:38.179891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:39.180000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:40.180086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:41.180697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:42.180753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:43.181742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:44.182639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:45.183318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:46.184081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:17:46.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-8251" for this suite. @ 08/19/23 13:17:46.711
• [60.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/19/23 13:17:46.719
  Aug 19 13:17:46.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:17:46.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:17:46.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:17:46.739
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/19/23 13:17:46.742
  E0819 13:17:47.184295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:48.184739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:49.184830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:50.184912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:17:50.764
  Aug 19 13:17:50.766: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-7b444301-a85b-4976-a9eb-36b03cf7c91f container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:17:50.773
  Aug 19 13:17:50.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4939" for this suite. @ 08/19/23 13:17:50.793
• [4.080 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/19/23 13:17:50.799
  Aug 19 13:17:50.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename job @ 08/19/23 13:17:50.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:17:50.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:17:50.822
  STEP: Creating a job @ 08/19/23 13:17:50.824
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/19/23 13:17:50.83
  E0819 13:17:51.185052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:52.185147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:53.185730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:54.186055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/19/23 13:17:54.835
  STEP: updating /status @ 08/19/23 13:17:54.841
  STEP: get /status @ 08/19/23 13:17:54.85
  Aug 19 13:17:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2268" for this suite. @ 08/19/23 13:17:54.857
• [4.063 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/19/23 13:17:54.864
  Aug 19 13:17:54.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename gc @ 08/19/23 13:17:54.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:17:54.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:17:54.882
  Aug 19 13:17:54.911: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"adbaac66-29fb-496d-8ac4-296081eb6429", Controller:(*bool)(0xc003e2a9a6), BlockOwnerDeletion:(*bool)(0xc003e2a9a7)}}
  Aug 19 13:17:54.918: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"32ddf231-9d2c-4846-aea5-f1733f767a8e", Controller:(*bool)(0xc004c6ac16), BlockOwnerDeletion:(*bool)(0xc004c6ac17)}}
  Aug 19 13:17:54.924: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"de8c3474-9655-4cde-bc92-46b099f905c0", Controller:(*bool)(0xc003e2abee), BlockOwnerDeletion:(*bool)(0xc003e2abef)}}
  E0819 13:17:55.186649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:56.186895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:57.187064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:58.187791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:17:59.188278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:17:59.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9136" for this suite. @ 08/19/23 13:17:59.938
• [5.080 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 08/19/23 13:17:59.945
  Aug 19 13:17:59.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 13:17:59.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:17:59.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:17:59.968
  STEP: Creating simple DaemonSet "daemon-set" @ 08/19/23 13:17:59.988
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 13:17:59.996
  Aug 19 13:17:59.999: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:17:59.999: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:00.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:18:00.002: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:18:00.188839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:01.007: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:01.007: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:01.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:18:01.010: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:18:01.188995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:02.006: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:02.007: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:02.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 13:18:02.010: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/19/23 13:18:02.013
  STEP: DeleteCollection of the DaemonSets @ 08/19/23 13:18:02.016
  STEP: Verify that ReplicaSets have been deleted @ 08/19/23 13:18:02.024
  Aug 19 13:18:02.037: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38356"},"items":null}

  Aug 19 13:18:02.041: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38358"},"items":[{"metadata":{"name":"daemon-set-7wrdg","generateName":"daemon-set-","namespace":"daemonsets-9352","uid":"43beda3c-fe8e-4781-ace6-47105771180b","resourceVersion":"38357","creationTimestamp":"2023-08-19T13:18:00Z","deletionTimestamp":"2023-08-19T13:18:32Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6477146d-9d6a-4cec-9769-e5443b420c48","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6477146d-9d6a-4cec-9769-e5443b420c48\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ckbdr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ckbdr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-69-13","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-69-13"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"}],"hostIP":"172.31.69.13","podIP":"192.168.20.19","podIPs":[{"ip":"192.168.20.19"}],"startTime":"2023-08-19T13:18:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-19T13:18:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b8bdc1e5a95e6ab36ff5cfe897fba5f95629a444fa6a2382c27c3ecad9e3d148","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wbskf","generateName":"daemon-set-","namespace":"daemonsets-9352","uid":"f5d6acaa-0441-478b-ac35-e704f63330de","resourceVersion":"38350","creationTimestamp":"2023-08-19T13:18:00Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6477146d-9d6a-4cec-9769-e5443b420c48","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6477146d-9d6a-4cec-9769-e5443b420c48\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.13.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9fdbh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9fdbh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-15-214","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-15-214"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"}],"hostIP":"172.31.15.214","podIP":"192.168.13.135","podIPs":[{"ip":"192.168.13.135"}],"startTime":"2023-08-19T13:18:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-19T13:18:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ceacf57c0255501646320e728e1b5dab683171a7902692aff334630e784682de","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xbl6n","generateName":"daemon-set-","namespace":"daemonsets-9352","uid":"5a91b1dd-af57-4edf-97b2-b5efdca3916b","resourceVersion":"38352","creationTimestamp":"2023-08-19T13:18:00Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6477146d-9d6a-4cec-9769-e5443b420c48","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6477146d-9d6a-4cec-9769-e5443b420c48\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-19T13:18:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.232.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bm765","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bm765","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-42-145","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-42-145"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:01Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-19T13:18:00Z"}],"hostIP":"172.31.42.145","podIP":"192.168.232.35","podIPs":[{"ip":"192.168.232.35"}],"startTime":"2023-08-19T13:18:00Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-19T13:18:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://96ee66ce1b57184af2306d084cf2cc5e9373955015a8aa119fa839bc650ce945","started":true}],"qosClass":"BestEffort"}}]}

  Aug 19 13:18:02.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9352" for this suite. @ 08/19/23 13:18:02.058
• [2.118 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/19/23 13:18:02.064
  Aug 19 13:18:02.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/19/23 13:18:02.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:18:02.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:18:02.085
  STEP: getting /apis @ 08/19/23 13:18:02.087
  STEP: getting /apis/storage.k8s.io @ 08/19/23 13:18:02.093
  STEP: getting /apis/storage.k8s.io/v1 @ 08/19/23 13:18:02.094
  STEP: creating @ 08/19/23 13:18:02.095
  STEP: watching @ 08/19/23 13:18:02.11
  Aug 19 13:18:02.110: INFO: starting watch
  STEP: getting @ 08/19/23 13:18:02.117
  STEP: listing in namespace @ 08/19/23 13:18:02.12
  STEP: listing across namespaces @ 08/19/23 13:18:02.122
  STEP: patching @ 08/19/23 13:18:02.125
  STEP: updating @ 08/19/23 13:18:02.132
  Aug 19 13:18:02.135: INFO: waiting for watch events with expected annotations in namespace
  Aug 19 13:18:02.135: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/19/23 13:18:02.136
  STEP: deleting a collection @ 08/19/23 13:18:02.147
  Aug 19 13:18:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1521" for this suite. @ 08/19/23 13:18:02.166
• [0.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/19/23 13:18:02.174
  Aug 19 13:18:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:18:02.175
  E0819 13:18:02.189213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:18:02.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:18:02.196
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-9916 @ 08/19/23 13:18:02.198
  STEP: changing the ExternalName service to type=NodePort @ 08/19/23 13:18:02.204
  STEP: creating replication controller externalname-service in namespace services-9916 @ 08/19/23 13:18:02.222
  I0819 13:18:02.234412      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-9916, replica count: 2
  E0819 13:18:03.190163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:04.190288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:05.190972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:18:05.285428      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 19 13:18:05.285: INFO: Creating new exec pod
  E0819 13:18:06.191650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:07.191687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:08.191753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 19 13:18:08.455: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:08.455: INFO: stdout: "externalname-service-b8k72"
  Aug 19 13:18:08.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Aug 19 13:18:08.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.50 80\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:08.593: INFO: stdout: ""
  E0819 13:18:09.191844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:09.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Aug 19 13:18:09.736: INFO: stderr: "+ nc -v -t -w 2 10.152.183.50 80\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 13:18:09.736: INFO: stdout: ""
  E0819 13:18:10.192144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:10.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Aug 19 13:18:10.734: INFO: stderr: "+ nc -v -t -w 2 10.152.183.50 80\n+ echo hostName\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:10.734: INFO: stdout: ""
  E0819 13:18:11.192792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:11.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Aug 19 13:18:11.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.50 80\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:11.758: INFO: stdout: "externalname-service-b8k72"
  Aug 19 13:18:11.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.214 32164'
  Aug 19 13:18:11.898: INFO: stderr: "+ nc -v -t -w 2 172.31.15.214 32164\n+ echo hostName\nConnection to 172.31.15.214 32164 port [tcp/*] succeeded!\n"
  Aug 19 13:18:11.898: INFO: stdout: "externalname-service-b8k72"
  Aug 19 13:18:11.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9916 exec execpod8hstn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.145 32164'
  Aug 19 13:18:12.050: INFO: stderr: "+ nc -v -t -w 2 172.31.42.145 32164\n+ echo hostName\nConnection to 172.31.42.145 32164 port [tcp/*] succeeded!\n"
  Aug 19 13:18:12.050: INFO: stdout: "externalname-service-7hl86"
  Aug 19 13:18:12.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:18:12.054: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-9916" for this suite. @ 08/19/23 13:18:12.084
• [9.916 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/19/23 13:18:12.09
  Aug 19 13:18:12.090: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:18:12.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:18:12.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:18:12.113
  STEP: creating service in namespace services-9914 @ 08/19/23 13:18:12.116
  STEP: creating service affinity-nodeport-transition in namespace services-9914 @ 08/19/23 13:18:12.116
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9914 @ 08/19/23 13:18:12.128
  I0819 13:18:12.136698      18 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9914, replica count: 3
  E0819 13:18:12.193048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:13.193898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:14.194114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0819 13:18:15.187920      18 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0819 13:18:15.194630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:15.198: INFO: Creating new exec pod
  E0819 13:18:16.195441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:17.195533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:18.195624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:18.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug 19 13:18:18.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:18.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:18:18.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.132 80'
  Aug 19 13:18:18.555: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.132 80\nConnection to 10.152.183.132 80 port [tcp/http] succeeded!\n"
  Aug 19 13:18:18.555: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:18:18.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.42.145 30564'
  Aug 19 13:18:18.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.42.145 30564\nConnection to 172.31.42.145 30564 port [tcp/*] succeeded!\n"
  Aug 19 13:18:18.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:18:18.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.69.13 30564'
  Aug 19 13:18:18.869: INFO: stderr: "+ nc -v -t -w 2 172.31.69.13 30564\n+ echo hostName\nConnection to 172.31.69.13 30564 port [tcp/*] succeeded!\n"
  Aug 19 13:18:18.869: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:18:18.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.214:30564/ ; done'
  Aug 19 13:18:19.127: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n"
  Aug 19 13:18:19.127: INFO: stdout: "\naffinity-nodeport-transition-fvr6w\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-fvr6w\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-fvr6w\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-fvr6w\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-crbl2\naffinity-nodeport-transition-2jbpx"
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-fvr6w
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-fvr6w
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-fvr6w
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-fvr6w
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-crbl2
  Aug 19 13:18:19.127: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-9914 exec execpod-affinityzrm89 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.214:30564/ ; done'
  E0819 13:18:19.196388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:19.334: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.214:30564/\n"
  Aug 19 13:18:19.334: INFO: stdout: "\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx\naffinity-nodeport-transition-2jbpx"
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Received response from host: affinity-nodeport-transition-2jbpx
  Aug 19 13:18:19.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:18:19.338: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9914, will wait for the garbage collector to delete the pods @ 08/19/23 13:18:19.354
  Aug 19 13:18:19.414: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.817662ms
  Aug 19 13:18:19.515: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.800046ms
  E0819 13:18:20.197018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:21.197992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9914" for this suite. @ 08/19/23 13:18:21.84
• [9.759 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/19/23 13:18:21.85
  Aug 19 13:18:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename controllerrevisions @ 08/19/23 13:18:21.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:18:21.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:18:21.871
  STEP: Creating DaemonSet "e2e-djpj8-daemon-set" @ 08/19/23 13:18:21.895
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/19/23 13:18:21.9
  Aug 19 13:18:21.903: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:21.903: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:21.912: INFO: Number of nodes with available pods controlled by daemonset e2e-djpj8-daemon-set: 0
  Aug 19 13:18:21.912: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:18:22.199320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:22.916: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:22.916: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:22.920: INFO: Number of nodes with available pods controlled by daemonset e2e-djpj8-daemon-set: 2
  Aug 19 13:18:22.920: INFO: Node ip-172-31-69-13 is running 0 daemon pod, expected 1
  E0819 13:18:23.199548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:23.917: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:23.917: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:18:23.920: INFO: Number of nodes with available pods controlled by daemonset e2e-djpj8-daemon-set: 3
  Aug 19 13:18:23.920: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-djpj8-daemon-set
  STEP: Confirm DaemonSet "e2e-djpj8-daemon-set" successfully created with "daemonset-name=e2e-djpj8-daemon-set" label @ 08/19/23 13:18:23.923
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-djpj8-daemon-set" @ 08/19/23 13:18:23.929
  Aug 19 13:18:23.932: INFO: Located ControllerRevision: "e2e-djpj8-daemon-set-cbcd568b9"
  STEP: Patching ControllerRevision "e2e-djpj8-daemon-set-cbcd568b9" @ 08/19/23 13:18:23.935
  Aug 19 13:18:23.942: INFO: e2e-djpj8-daemon-set-cbcd568b9 has been patched
  STEP: Create a new ControllerRevision @ 08/19/23 13:18:23.942
  Aug 19 13:18:23.947: INFO: Created ControllerRevision: e2e-djpj8-daemon-set-7fd5746877
  STEP: Confirm that there are two ControllerRevisions @ 08/19/23 13:18:23.947
  Aug 19 13:18:23.947: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 19 13:18:23.950: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-djpj8-daemon-set-cbcd568b9" @ 08/19/23 13:18:23.95
  STEP: Confirm that there is only one ControllerRevision @ 08/19/23 13:18:23.955
  Aug 19 13:18:23.955: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 19 13:18:23.958: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-djpj8-daemon-set-7fd5746877" @ 08/19/23 13:18:23.961
  Aug 19 13:18:23.970: INFO: e2e-djpj8-daemon-set-7fd5746877 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/19/23 13:18:23.97
  W0819 13:18:23.976452      18 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/19/23 13:18:23.976
  Aug 19 13:18:23.976: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0819 13:18:24.200295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:24.983: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 19 13:18:24.986: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-djpj8-daemon-set-7fd5746877=updated" @ 08/19/23 13:18:24.986
  STEP: Confirm that there is only one ControllerRevision @ 08/19/23 13:18:24.993
  Aug 19 13:18:24.993: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 19 13:18:24.996: INFO: Found 1 ControllerRevisions
  Aug 19 13:18:24.999: INFO: ControllerRevision "e2e-djpj8-daemon-set-5f4f5445f6" has revision 3
  STEP: Deleting DaemonSet "e2e-djpj8-daemon-set" @ 08/19/23 13:18:25.003
  STEP: deleting DaemonSet.extensions e2e-djpj8-daemon-set in namespace controllerrevisions-7878, will wait for the garbage collector to delete the pods @ 08/19/23 13:18:25.003
  Aug 19 13:18:25.062: INFO: Deleting DaemonSet.extensions e2e-djpj8-daemon-set took: 5.675285ms
  Aug 19 13:18:25.163: INFO: Terminating DaemonSet.extensions e2e-djpj8-daemon-set pods took: 100.881915ms
  E0819 13:18:25.200450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:26.201440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:26.866: INFO: Number of nodes with available pods controlled by daemonset e2e-djpj8-daemon-set: 0
  Aug 19 13:18:26.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-djpj8-daemon-set
  Aug 19 13:18:26.869: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38813"},"items":null}

  Aug 19 13:18:26.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38813"},"items":null}

  Aug 19 13:18:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-7878" for this suite. @ 08/19/23 13:18:26.892
• [5.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/19/23 13:18:26.905
  Aug 19 13:18:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-probe @ 08/19/23 13:18:26.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:18:26.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:18:26.927
  STEP: Creating pod test-webserver-cbf02918-f0cf-4753-8c28-c0788e2edc84 in namespace container-probe-7105 @ 08/19/23 13:18:26.929
  E0819 13:18:27.201911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:28.202585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:18:28.943: INFO: Started pod test-webserver-cbf02918-f0cf-4753-8c28-c0788e2edc84 in namespace container-probe-7105
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/19/23 13:18:28.943
  Aug 19 13:18:28.946: INFO: Initial restart count of pod test-webserver-cbf02918-f0cf-4753-8c28-c0788e2edc84 is 0
  E0819 13:18:29.203189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:30.203563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:31.204175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:32.204414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:33.204556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:34.204651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:35.205714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:36.205980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:37.206872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:38.207807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:39.208302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:40.208391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:41.208581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:42.208728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:43.208856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:44.209044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:45.209623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:46.210150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:47.210963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:48.211403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:49.211484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:50.212327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:51.213404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:52.213499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:53.214377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:54.214528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:55.214634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:56.214816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:57.214922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:58.214966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:18:59.215607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:00.216302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:01.216989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:02.217089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:03.217118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:04.217303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:05.217638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:06.218131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:07.218898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:08.218958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:09.219690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:10.220049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:11.220955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:12.221135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:13.221683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:14.221773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:15.222776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:16.222848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:17.223500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:18.223771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:19.224304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:20.224376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:21.225456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:22.226075      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:23.226387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:24.226567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:25.227064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:26.227240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:27.227703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:28.227765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:29.228458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:30.228662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:31.229172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:32.229263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:33.229699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:34.229778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:35.230147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:36.230221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:37.230880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:38.231909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:39.232778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:40.233785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:41.233829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:42.234212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:43.234813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:44.234891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:45.235156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:46.235245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:47.236292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:48.236536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:49.237320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:50.237485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:51.237724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:52.237864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:53.238179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:54.238265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:55.238728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:56.239724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:57.240624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:58.240714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:19:59.241531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:00.241597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:01.241657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:02.241746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:03.242210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:04.242301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:05.243061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:06.243258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:07.243621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:08.243944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:09.244268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:10.244366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:11.245225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:12.245305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:13.246154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:14.247212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:15.247834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:16.247942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:17.248297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:18.248765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:19.249638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:20.249727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:21.250241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:22.250340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:23.251342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:24.251438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:25.252301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:26.252389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:27.253354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:28.253858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:29.254866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:30.255146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:31.256128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:32.256345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:33.257133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:34.257346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:35.257982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:36.258336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:37.258394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:38.258819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:39.259645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:40.260278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:41.261311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:42.261414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:43.262109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:44.262203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:45.263220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:46.264276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:47.264759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:48.264853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:49.265012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:50.265190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:51.265991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:52.266272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:53.266474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:54.266587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:55.267324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:56.268277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:57.269215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:58.269443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:20:59.269925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:00.270030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:01.270585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:02.270691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:03.271486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:04.272280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:05.272940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:06.273048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:07.273494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:08.273761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:09.274124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:10.274233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:11.274493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:12.274602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:13.275015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:14.275209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:15.275302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:16.275878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:17.275966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:18.276007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:19.276156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:20.276257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:21.276443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:22.277190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:23.277952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:24.278045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:25.278254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:26.278348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:27.278438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:28.279068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:29.279260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:30.279372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:31.279453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:32.279634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:33.280280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:34.280390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:35.280690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:36.280775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:37.281482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:38.282068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:39.282254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:40.282344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:41.282553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:42.283027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:43.283252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:44.283360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:45.283415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:46.283520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:47.283615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:48.284612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:49.284799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:50.284878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:51.285150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:52.285245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:53.286085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:54.286258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:55.287077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:56.287255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:57.287366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:58.287430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:21:59.291317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:00.292297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:01.292393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:02.292477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:03.292714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:04.292789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:05.292972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:06.293774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:07.293862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:08.294679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:09.294775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:10.295083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:11.295238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:12.295357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:13.295935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:14.296364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:15.296361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:16.296990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:17.297244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:18.297890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:19.298102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:20.298192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:21.298511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:22.299285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:23.300277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:24.300393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:25.300671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:26.301060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:27.301429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:28.302436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:29.302520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/19/23 13:22:29.437
  STEP: Destroying namespace "container-probe-7105" for this suite. @ 08/19/23 13:22:29.45
• [242.551 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/19/23 13:22:29.458
  Aug 19 13:22:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:22:29.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:29.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:29.478
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/19/23 13:22:29.481
  E0819 13:22:30.302790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:31.302864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:32.303720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:33.303958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:22:33.504
  Aug 19 13:22:33.508: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-6fac0188-2ef3-445e-9ac6-b33e8419669b container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:22:33.525
  Aug 19 13:22:33.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8036" for this suite. @ 08/19/23 13:22:33.544
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/19/23 13:22:33.553
  Aug 19 13:22:33.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:22:33.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:33.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:33.574
  STEP: Creating a pod to test downward api env vars @ 08/19/23 13:22:33.576
  E0819 13:22:34.304060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:35.304927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:36.304923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:37.305167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:22:37.595
  Aug 19 13:22:37.598: INFO: Trying to get logs from node ip-172-31-15-214 pod downward-api-a9448d7b-ab1c-4d59-8058-a2c985922167 container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:22:37.606
  Aug 19 13:22:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2434" for this suite. @ 08/19/23 13:22:37.624
• [4.078 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/19/23 13:22:37.631
  Aug 19 13:22:37.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/19/23 13:22:37.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:37.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:37.652
  STEP: create the container to handle the HTTPGet hook request. @ 08/19/23 13:22:37.659
  E0819 13:22:38.305281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:39.305360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/19/23 13:22:39.678
  E0819 13:22:40.306144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:41.306257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/19/23 13:22:41.693
  E0819 13:22:42.307115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:43.307528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/19/23 13:22:43.708
  Aug 19 13:22:43.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1240" for this suite. @ 08/19/23 13:22:43.718
• [6.093 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/19/23 13:22:43.725
  Aug 19 13:22:43.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename webhook @ 08/19/23 13:22:43.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:43.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:43.748
  STEP: Setting up server cert @ 08/19/23 13:22:43.773
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/19/23 13:22:43.983
  STEP: Deploying the webhook pod @ 08/19/23 13:22:43.989
  STEP: Wait for the deployment to be ready @ 08/19/23 13:22:44.001
  Aug 19 13:22:44.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0819 13:22:44.307636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:45.307758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/19/23 13:22:46.017
  STEP: Verifying the service has paired with the endpoint @ 08/19/23 13:22:46.026
  E0819 13:22:46.307913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:47.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 19 13:22:47.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  E0819 13:22:47.308563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3370-crds.webhook.example.com via the AdmissionRegistration API @ 08/19/23 13:22:47.54
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/19/23 13:22:47.555
  E0819 13:22:48.309537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:49.309592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:49.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5687" for this suite. @ 08/19/23 13:22:50.153
  STEP: Destroying namespace "webhook-markers-4413" for this suite. @ 08/19/23 13:22:50.159
• [6.440 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 08/19/23 13:22:50.167
  Aug 19 13:22:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename daemonsets @ 08/19/23 13:22:50.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:50.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:50.188
  Aug 19 13:22:50.212: INFO: Create a RollingUpdate DaemonSet
  Aug 19 13:22:50.217: INFO: Check that daemon pods launch on every node of the cluster
  Aug 19 13:22:50.223: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:50.223: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:50.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:22:50.226: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:22:50.309923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:51.232: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:51.232: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:51.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 19 13:22:51.235: INFO: Node ip-172-31-15-214 is running 0 daemon pod, expected 1
  E0819 13:22:51.310922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:52.231: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:52.231: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:52.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 19 13:22:52.235: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Aug 19 13:22:52.235: INFO: Update the DaemonSet to trigger a rollout
  Aug 19 13:22:52.243: INFO: Updating DaemonSet daemon-set
  E0819 13:22:52.311593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:53.260: INFO: Roll back the DaemonSet before rollout is complete
  Aug 19 13:22:53.268: INFO: Updating DaemonSet daemon-set
  Aug 19 13:22:53.268: INFO: Make sure DaemonSet rollback is complete
  Aug 19 13:22:53.278: INFO: Wrong image for pod: daemon-set-l2nnj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug 19 13:22:53.278: INFO: Pod daemon-set-l2nnj is not available
  Aug 19 13:22:53.282: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:53.282: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0819 13:22:53.311580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:54.289: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:54.289: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0819 13:22:54.312449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:55.290: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:55.291: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0819 13:22:55.313095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:56.294: INFO: DaemonSet pods can't tolerate node ip-172-31-23-154 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 19 13:22:56.294: INFO: DaemonSet pods can't tolerate node ip-172-31-43-67 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 08/19/23 13:22:56.301
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1638, will wait for the garbage collector to delete the pods @ 08/19/23 13:22:56.301
  E0819 13:22:56.317531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:56.362: INFO: Deleting DaemonSet.extensions daemon-set took: 7.225396ms
  Aug 19 13:22:56.462: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.668209ms
  E0819 13:22:57.318145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:22:58.318943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:22:58.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 19 13:22:58.366: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 19 13:22:58.369: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39798"},"items":null}

  Aug 19 13:22:58.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39799"},"items":null}

  Aug 19 13:22:58.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1638" for this suite. @ 08/19/23 13:22:58.389
• [8.228 seconds]
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/19/23 13:22:58.395
  Aug 19 13:22:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pods @ 08/19/23 13:22:58.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:58.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:58.416
  STEP: creating the pod @ 08/19/23 13:22:58.419
  STEP: submitting the pod to kubernetes @ 08/19/23 13:22:58.419
  STEP: verifying QOS class is set on the pod @ 08/19/23 13:22:58.426
  Aug 19 13:22:58.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-279" for this suite. @ 08/19/23 13:22:58.435
• [0.046 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/19/23 13:22:58.442
  Aug 19 13:22:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pod-network-test @ 08/19/23 13:22:58.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:22:58.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:22:58.462
  STEP: Performing setup for networking test in namespace pod-network-test-6791 @ 08/19/23 13:22:58.465
  STEP: creating a selector @ 08/19/23 13:22:58.465
  STEP: Creating the service pods in kubernetes @ 08/19/23 13:22:58.465
  Aug 19 13:22:58.465: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0819 13:22:59.319331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:00.320353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:01.320419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:02.320496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:03.321500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:04.322189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:05.322408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:06.322656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:07.322840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:08.322928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:09.323339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:10.324387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:11.325338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:12.325817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:13.326812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:14.327091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:15.327986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:16.328298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:17.328401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:18.328802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:19.329815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:20.329997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/19/23 13:23:20.56
  E0819 13:23:21.331035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:22.331291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:22.587: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 19 13:23:22.587: INFO: Going to poll 192.168.13.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:23:22.590: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.13.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:23:22.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:23:22.591: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:23:22.591: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.13.189+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0819 13:23:23.332170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:23.676: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 19 13:23:23.676: INFO: Going to poll 192.168.232.39 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:23:23.680: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.232.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:23:23.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:23:23.681: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:23:23.681: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.232.39+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0819 13:23:24.332262      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:24.764: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 19 13:23:24.764: INFO: Going to poll 192.168.20.52 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:23:24.768: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.20.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:23:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:23:24.768: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:23:24.769: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.20.52+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0819 13:23:25.333222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:25.856: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 19 13:23:25.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6791" for this suite. @ 08/19/23 13:23:25.86
• [27.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/19/23 13:23:25.87
  Aug 19 13:23:25.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename statefulset @ 08/19/23 13:23:25.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:23:25.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:23:25.892
  STEP: Creating service test in namespace statefulset-7613 @ 08/19/23 13:23:25.895
  STEP: Creating stateful set ss in namespace statefulset-7613 @ 08/19/23 13:23:25.902
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7613 @ 08/19/23 13:23:25.908
  Aug 19 13:23:25.911: INFO: Found 0 stateful pods, waiting for 1
  E0819 13:23:26.333799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:27.334039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:28.334108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:29.334356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:30.334439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:31.335175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:32.335242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:33.335322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:34.335407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:35.335497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:35.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/19/23 13:23:35.916
  Aug 19 13:23:35.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 13:23:36.062: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 13:23:36.062: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 13:23:36.062: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 13:23:36.066: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0819 13:23:36.335736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:37.336299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:38.336815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:39.336893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:40.336982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:41.337076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:42.337191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:43.337416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:44.337502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:45.337682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:46.071: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 13:23:46.071: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 13:23:46.085: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug 19 13:23:46.085: INFO: ss-0  ip-172-31-15-214  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:25 +0000 UTC  }]
  Aug 19 13:23:46.086: INFO: 
  Aug 19 13:23:46.086: INFO: StatefulSet ss has not reached scale 3, at 1
  E0819 13:23:46.338458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:47.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996493886s
  E0819 13:23:47.338536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:48.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990993631s
  E0819 13:23:48.339034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:49.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987024587s
  E0819 13:23:49.339082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:50.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982657969s
  E0819 13:23:50.340004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:51.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978808936s
  E0819 13:23:51.340098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:52.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974511297s
  E0819 13:23:52.340178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:53.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970220164s
  E0819 13:23:53.341135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:54.120: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966465193s
  E0819 13:23:54.341224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:55.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.408118ms
  E0819 13:23:55.341838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7613 @ 08/19/23 13:23:56.123
  Aug 19 13:23:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 13:23:56.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 19 13:23:56.267: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 13:23:56.267: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 13:23:56.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0819 13:23:56.341939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:23:56.416: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 19 13:23:56.416: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 13:23:56.416: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 13:23:56.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 19 13:23:56.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 19 13:23:56.555: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 19 13:23:56.555: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 19 13:23:56.558: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 13:23:56.558: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 19 13:23:56.558: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/19/23 13:23:56.558
  Aug 19 13:23:56.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 13:23:56.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 13:23:56.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 13:23:56.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 13:23:56.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 13:23:56.845: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 13:23:56.845: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 13:23:56.845: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 13:23:56.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=statefulset-7613 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 19 13:23:56.990: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 19 13:23:56.990: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 19 13:23:56.990: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 19 13:23:56.990: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 13:23:56.993: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0819 13:23:57.342024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:58.342086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:23:59.342962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:00.343051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:01.343292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:02.343387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:03.343486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:04.343569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:05.344285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:06.344377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:07.001: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 13:24:07.001: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 13:24:07.001: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 19 13:24:07.012: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug 19 13:24:07.012: INFO: ss-0  ip-172-31-15-214  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:25 +0000 UTC  }]
  Aug 19 13:24:07.012: INFO: ss-1  ip-172-31-69-13   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  }]
  Aug 19 13:24:07.012: INFO: ss-2  ip-172-31-42-145  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  }]
  Aug 19 13:24:07.012: INFO: 
  Aug 19 13:24:07.012: INFO: StatefulSet ss has not reached scale 0, at 3
  E0819 13:24:07.344862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:08.016: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Aug 19 13:24:08.016: INFO: ss-1  ip-172-31-69-13   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  }]
  Aug 19 13:24:08.016: INFO: ss-2  ip-172-31-42-145  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:57 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-19 13:23:46 +0000 UTC  }]
  Aug 19 13:24:08.016: INFO: 
  Aug 19 13:24:08.016: INFO: StatefulSet ss has not reached scale 0, at 2
  E0819 13:24:08.344996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:09.020: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992772132s
  E0819 13:24:09.345691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:10.023: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989311078s
  E0819 13:24:10.346385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:11.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985349753s
  E0819 13:24:11.346824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:12.031: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.981439019s
  E0819 13:24:12.346911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:13.036: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.976988594s
  E0819 13:24:13.347457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:14.039: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.973371288s
  E0819 13:24:14.348061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:15.042: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.969878712s
  E0819 13:24:15.348356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:24:16.046: INFO: Verifying statefulset ss doesn't scale past 0 for another 966.458514ms
  E0819 13:24:16.348556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7613 @ 08/19/23 13:24:17.046
  Aug 19 13:24:17.051: INFO: Scaling statefulset ss to 0
  Aug 19 13:24:17.061: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 13:24:17.064: INFO: Deleting all statefulset in ns statefulset-7613
  Aug 19 13:24:17.067: INFO: Scaling statefulset ss to 0
  Aug 19 13:24:17.077: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 19 13:24:17.079: INFO: Deleting statefulset ss
  Aug 19 13:24:17.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7613" for this suite. @ 08/19/23 13:24:17.095
• [51.232 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/19/23 13:24:17.103
  Aug 19 13:24:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 13:24:17.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:24:17.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:24:17.123
  STEP: Creating a pod to test env composition @ 08/19/23 13:24:17.126
  E0819 13:24:17.348970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:18.349025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:19.349728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:20.349853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:24:21.144
  Aug 19 13:24:21.147: INFO: Trying to get logs from node ip-172-31-15-214 pod var-expansion-4b9e2999-95bd-4577-b1bd-392cd8690078 container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:24:21.161
  Aug 19 13:24:21.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1509" for this suite. @ 08/19/23 13:24:21.18
• [4.083 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/19/23 13:24:21.186
  Aug 19 13:24:21.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/19/23 13:24:21.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:24:21.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:24:21.209
  STEP: fetching the /apis discovery document @ 08/19/23 13:24:21.212
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/19/23 13:24:21.213
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/19/23 13:24:21.213
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/19/23 13:24:21.213
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/19/23 13:24:21.214
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/19/23 13:24:21.214
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/19/23 13:24:21.215
  Aug 19 13:24:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4256" for this suite. @ 08/19/23 13:24:21.219
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/19/23 13:24:21.227
  Aug 19 13:24:21.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename cronjob @ 08/19/23 13:24:21.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:24:21.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:24:21.248
  STEP: Creating a ReplaceConcurrent cronjob @ 08/19/23 13:24:21.251
  STEP: Ensuring a job is scheduled @ 08/19/23 13:24:21.257
  E0819 13:24:21.350127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:22.350391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:23.351209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:24.351254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:25.351573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:26.352342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:27.353083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:28.353975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:29.354780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:30.355215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:31.355401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:32.356291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:33.356748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:34.357065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:35.358110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:36.358337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:37.358604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:38.358695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:39.359344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:40.360282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:41.360665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:42.360824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:43.361720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:44.362351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:45.362383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:46.363425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:47.364162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:48.365158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:49.365810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:50.366038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:51.366710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:52.367031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:53.367130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:54.367213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:55.367502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:56.368272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:57.368913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:58.369006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:24:59.369090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:00.370056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/19/23 13:25:01.261
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/19/23 13:25:01.265
  STEP: Ensuring the job is replaced with a new one @ 08/19/23 13:25:01.268
  E0819 13:25:01.370543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:02.370969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:03.371655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:04.372311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:05.373088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:06.373178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:07.374014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:08.374101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:09.375133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:10.375234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:11.376179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:12.376218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:13.377052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:14.377211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:15.377984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:16.378081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:17.378617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:18.378957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:19.379021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:20.379769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:21.380556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:22.380683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:23.381685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:24.381770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:25.382118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:26.382183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:27.383024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:28.383968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:29.384633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:30.384726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:31.385320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:32.385406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:33.386255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:34.386536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:35.387463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:36.387594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:37.388587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:38.390312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:39.391006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:40.391484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:41.391858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:42.392127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:43.393134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:44.393241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:45.393335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:46.394331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:47.394700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:48.394790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:49.395051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:50.395476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:51.396206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:52.396298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:53.396755      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:54.396830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:55.397604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:56.397786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:57.398545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:58.398867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:25:59.398948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:00.399152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/19/23 13:26:01.271
  Aug 19 13:26:01.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1929" for this suite. @ 08/19/23 13:26:01.281
• [100.059 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/19/23 13:26:01.288
  Aug 19 13:26:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename svcaccounts @ 08/19/23 13:26:01.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:26:01.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:26:01.311
  Aug 19 13:26:01.317: INFO: Got root ca configmap in namespace "svcaccounts-384"
  Aug 19 13:26:01.322: INFO: Deleted root ca configmap in namespace "svcaccounts-384"
  E0819 13:26:01.399990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 08/19/23 13:26:01.823
  Aug 19 13:26:01.826: INFO: Recreated root ca configmap in namespace "svcaccounts-384"
  Aug 19 13:26:01.831: INFO: Updated root ca configmap in namespace "svcaccounts-384"
  STEP: waiting for the root ca configmap reconciled @ 08/19/23 13:26:02.331
  Aug 19 13:26:02.335: INFO: Reconciled root ca configmap in namespace "svcaccounts-384"
  Aug 19 13:26:02.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-384" for this suite. @ 08/19/23 13:26:02.339
• [1.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/19/23 13:26:02.347
  Aug 19 13:26:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename subpath @ 08/19/23 13:26:02.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:26:02.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:26:02.368
  STEP: Setting up data @ 08/19/23 13:26:02.371
  STEP: Creating pod pod-subpath-test-configmap-cq7k @ 08/19/23 13:26:02.379
  STEP: Creating a pod to test atomic-volume-subpath @ 08/19/23 13:26:02.379
  E0819 13:26:02.400526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:03.400563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:04.401386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:05.401574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:06.402554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:07.402674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:08.403389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:09.403525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:10.403772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:11.404070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:12.404993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:13.405763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:14.405869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:15.405958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:16.406554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:17.406661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:18.407074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:19.407259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:20.407333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:21.407890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:22.408298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:23.409069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:24.409257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:25.409340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:26.409409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:26:26.439
  Aug 19 13:26:26.442: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-subpath-test-configmap-cq7k container test-container-subpath-configmap-cq7k: <nil>
  STEP: delete the pod @ 08/19/23 13:26:26.456
  STEP: Deleting pod pod-subpath-test-configmap-cq7k @ 08/19/23 13:26:26.471
  Aug 19 13:26:26.471: INFO: Deleting pod "pod-subpath-test-configmap-cq7k" in namespace "subpath-1173"
  Aug 19 13:26:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1173" for this suite. @ 08/19/23 13:26:26.478
• [24.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/19/23 13:26:26.489
  Aug 19 13:26:26.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 13:26:26.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:26:26.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:26:26.512
  STEP: Given a ReplicationController is created @ 08/19/23 13:26:26.515
  STEP: When the matched label of one of its pods change @ 08/19/23 13:26:26.52
  Aug 19 13:26:26.523: INFO: Pod name pod-release: Found 0 pods out of 1
  E0819 13:26:27.409489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:28.409562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:29.409732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:30.410132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:31.410027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:26:31.528: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/19/23 13:26:31.541
  E0819 13:26:32.409983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:26:32.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3923" for this suite. @ 08/19/23 13:26:32.552
• [6.071 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/19/23 13:26:32.56
  Aug 19 13:26:32.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename sched-pred @ 08/19/23 13:26:32.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:26:32.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:26:32.582
  Aug 19 13:26:32.585: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 19 13:26:32.592: INFO: Waiting for terminating namespaces to be deleted...
  Aug 19 13:26:32.595: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-214 before test
  Aug 19 13:26:32.599: INFO: replace-28207526-t8jz7 from cronjob-1929 started at 2023-08-19 13:26:00 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.599: INFO: 	Container c ready: false, restart count 0
  Aug 19 13:26:32.599: INFO: nginx-ingress-controller-kubernetes-worker-bgp2w from ingress-nginx-kubernetes-worker started at 2023-08-19 13:09:55 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.600: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 13:26:32.600: INFO: pod-release-nrzrf from replication-controller-3923 started at 2023-08-19 13:26:31 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.600: INFO: 	Container pod-release ready: false, restart count 0
  Aug 19 13:26:32.600: INFO: pod-release-wfq96 from replication-controller-3923 started at 2023-08-19 13:26:26 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.600: INFO: 	Container pod-release ready: true, restart count 0
  Aug 19 13:26:32.600: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-n6vtt from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 13:26:32.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 13:26:32.600: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 13:26:32.600: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-42-145 before test
  Aug 19 13:26:32.604: INFO: nginx-ingress-controller-kubernetes-worker-xnrcp from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: coredns-5c7f76ccb8-trw9q from kube-system started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container coredns ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: kube-state-metrics-5b95b4459c-rtf6r from kube-system started at 2023-08-19 12:18:49 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: dashboard-metrics-scraper-6b8586b5c9-lgkqw from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: kubernetes-dashboard-6869f4cd5f-gfcl6 from kubernetes-dashboard started at 2023-08-19 11:54:20 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-52lf7 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 13:26:32.604: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 19 13:26:32.604: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-69-13 before test
  Aug 19 13:26:32.609: INFO: default-http-backend-kubernetes-worker-65fc475d49-vjk9h from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.609: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 19 13:26:32.609: INFO: nginx-ingress-controller-kubernetes-worker-wb2rk from ingress-nginx-kubernetes-worker started at 2023-08-19 11:54:27 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.609: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: calico-kube-controllers-85f9fb94df-xpnfw from kube-system started at 2023-08-19 11:54:32 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.610: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: metrics-server-v0.5.2-6cf8c8b69c-mbzlh from kube-system started at 2023-08-19 13:09:30 +0000 UTC (2 container statuses recorded)
  Aug 19 13:26:32.610: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: sonobuoy from sonobuoy started at 2023-08-19 12:00:13 +0000 UTC (1 container statuses recorded)
  Aug 19 13:26:32.610: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: sonobuoy-e2e-job-1e976644cf094697 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 13:26:32.610: INFO: 	Container e2e ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: sonobuoy-systemd-logs-daemon-set-5181330686b44064-vmdc9 from sonobuoy started at 2023-08-19 12:00:15 +0000 UTC (2 container statuses recorded)
  Aug 19 13:26:32.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 19 13:26:32.610: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/19/23 13:26:32.611
  E0819 13:26:33.410092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:34.410185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/19/23 13:26:34.626
  STEP: Trying to apply a random label on the found node. @ 08/19/23 13:26:34.641
  STEP: verifying the node has the label kubernetes.io/e2e-0d13400e-30d6-4818-9a87-8d6a6cc96715 95 @ 08/19/23 13:26:34.649
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/19/23 13:26:34.652
  E0819 13:26:35.410270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:36.410526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.15.214 on the node which pod4 resides and expect not scheduled @ 08/19/23 13:26:36.665
  E0819 13:26:37.411461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:38.412033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:39.412132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:40.412301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:41.412436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:42.412505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:43.413511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:44.414111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:45.414945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:46.415259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:47.416155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:48.416593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:49.416703      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:50.416795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:51.416882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:52.417082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:53.417922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:54.418008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:55.418111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:56.418302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:57.419286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:58.420399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:26:59.420407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:00.420490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:01.420583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:02.420682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:03.421022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:04.421219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:05.422108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:06.422975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:07.423232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:08.423922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:09.424017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:10.424570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:11.424651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:12.424857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:13.425595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:14.425696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:15.426226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:16.426319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:17.426411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:18.426590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:19.426706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:20.426813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:21.427675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:22.428300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:23.428363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:24.428558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:25.428649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:26.428737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:27.429549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:28.429659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:29.430421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:30.430516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:31.431289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:32.432318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:33.433049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:34.433417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:35.433667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:36.433751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:37.433832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:38.434306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:39.434414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:40.434595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:41.435268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:42.436266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:43.437095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:44.437202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:45.438142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:46.438211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:47.439031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:48.439292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:49.440140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:50.440346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:51.440435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:52.440647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:53.440718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:54.441424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:55.442250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:56.442344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:57.442431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:58.443031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:27:59.443287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:00.443383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:01.443640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:02.444282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:03.444326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:04.444407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:05.444500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:06.444685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:07.445309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:08.445542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:09.446481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:10.446808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:11.447230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:12.448287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:13.448989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:14.449084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:15.449458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:16.449559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:17.449650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:18.450005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:19.450784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:20.450984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:21.451206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:22.451243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:23.452290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:24.452459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:25.452560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:26.452728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:27.453646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:28.453711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:29.453815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:30.454208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:31.454884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:32.455054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:33.455139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:34.455222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:35.455915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:36.456278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:37.456947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:38.457825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:39.458494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:40.458638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:41.459418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:42.459494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:43.460214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:44.460306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:45.460659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:46.460845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:47.460932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:48.460977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:49.461241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:50.461464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:51.462455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:52.462686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:53.462781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:54.462946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:55.463040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:56.463238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:57.463317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:58.463414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:28:59.464257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:00.464400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:01.464880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:02.465072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:03.465813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:04.466432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:05.467441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:06.467567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:07.468252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:08.468586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:09.468675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:10.468883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:11.468977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:12.469244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:13.469568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:14.470173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:15.470264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:16.470445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:17.470930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:18.471208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:19.471306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:20.471399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:21.472097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:22.472198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:23.472913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:24.473009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:25.473352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:26.473608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:27.474481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:28.474578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:29.475366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:30.475458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:31.476299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:32.476501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:33.476592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:34.476970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:35.477068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:36.477264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:37.477351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:38.477775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:39.478376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:40.478710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:41.479546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:42.480286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:43.480386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:44.480481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:45.480518      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:46.481269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:47.481357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:48.481722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:49.482575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:50.482757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:51.482803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:52.482895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:53.482954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:54.483236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:55.484280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:56.484699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:57.485742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:58.485986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:29:59.486080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:00.486171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:01.487089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:02.487234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:03.487313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:04.488295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:05.488388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:06.488481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:07.489476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:08.489721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:09.489904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:10.490040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:11.490512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:12.490719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:13.491330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:14.491417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:15.491501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:16.491598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:17.492298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:18.492550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:19.492656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:20.495266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:21.496260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:22.496472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:23.496559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:24.496655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:25.496742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:26.496952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:27.497001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:28.497194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:29.497277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:30.498419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:31.499390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:32.500277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:33.500791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:34.501730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:35.501933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:36.502023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:37.502916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:38.503014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:39.503215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:40.503309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:41.503993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:42.504904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:43.505418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:44.506092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:45.506242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:46.506435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:47.506709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:48.506807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:49.507565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:50.508281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:51.509026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:52.509122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:53.509207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:54.509301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:55.509389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:56.509555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:57.510512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:58.510922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:30:59.511014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:00.511226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:01.512089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:02.512714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:03.512801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:04.512893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:05.513896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:06.513924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:07.514678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:08.514779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:09.515591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:10.515678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:11.515873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:12.516290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:13.516662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:14.516918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:15.517799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:16.517896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:17.518185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:18.518264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:19.519026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:20.519228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:21.519698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:22.520284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:23.520365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:24.520473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:25.520926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:26.521242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:27.521506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:28.522524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:29.523204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:30.523320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:31.523577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:32.523672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:33.524700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:34.524795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:35.525622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:36.525783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-0d13400e-30d6-4818-9a87-8d6a6cc96715 off the node ip-172-31-15-214 @ 08/19/23 13:31:36.672
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-0d13400e-30d6-4818-9a87-8d6a6cc96715 @ 08/19/23 13:31:36.685
  Aug 19 13:31:36.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-29" for this suite. @ 08/19/23 13:31:36.694
• [304.140 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/19/23 13:31:36.701
  Aug 19 13:31:36.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename limitrange @ 08/19/23 13:31:36.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:31:36.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:31:36.738
  STEP: Creating a LimitRange @ 08/19/23 13:31:36.743
  STEP: Setting up watch @ 08/19/23 13:31:36.743
  STEP: Submitting a LimitRange @ 08/19/23 13:31:36.847
  STEP: Verifying LimitRange creation was observed @ 08/19/23 13:31:36.853
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/19/23 13:31:36.853
  Aug 19 13:31:36.855: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 19 13:31:36.855: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/19/23 13:31:36.856
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/19/23 13:31:36.86
  Aug 19 13:31:36.864: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 19 13:31:36.864: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/19/23 13:31:36.864
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/19/23 13:31:36.869
  Aug 19 13:31:36.873: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug 19 13:31:36.873: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/19/23 13:31:36.873
  STEP: Failing to create a Pod with more than max resources @ 08/19/23 13:31:36.875
  STEP: Updating a LimitRange @ 08/19/23 13:31:36.876
  STEP: Verifying LimitRange updating is effective @ 08/19/23 13:31:36.882
  E0819 13:31:37.526157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:38.526589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 08/19/23 13:31:38.886
  STEP: Failing to create a Pod with more than max resources @ 08/19/23 13:31:38.893
  STEP: Deleting a LimitRange @ 08/19/23 13:31:38.896
  STEP: Verifying the LimitRange was deleted @ 08/19/23 13:31:38.904
  E0819 13:31:39.526888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:40.526980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:41.527229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:42.528288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:43.528387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:31:43.908: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/19/23 13:31:43.908
  Aug 19 13:31:43.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5780" for this suite. @ 08/19/23 13:31:43.92
• [7.225 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/19/23 13:31:43.927
  Aug 19 13:31:43.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename var-expansion @ 08/19/23 13:31:43.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:31:43.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:31:43.948
  STEP: Creating a pod to test substitution in container's command @ 08/19/23 13:31:43.951
  E0819 13:31:44.528615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:45.528805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:46.528891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:47.528982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:31:47.967
  Aug 19 13:31:47.971: INFO: Trying to get logs from node ip-172-31-15-214 pod var-expansion-ebd708ae-df59-41b5-b769-ef408403073e container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:31:47.987
  Aug 19 13:31:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9869" for this suite. @ 08/19/23 13:31:48.008
• [4.087 seconds]
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/19/23 13:31:48.014
  Aug 19 13:31:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:31:48.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:31:48.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:31:48.035
  STEP: Creating a pod to test downward api env vars @ 08/19/23 13:31:48.038
  E0819 13:31:48.529610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:49.529799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:50.530630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:51.530792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:31:52.055
  Aug 19 13:31:52.059: INFO: Trying to get logs from node ip-172-31-15-214 pod downward-api-a831875d-dc0a-4efd-ac37-0140ed2f8ab7 container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:31:52.066
  Aug 19 13:31:52.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6105" for this suite. @ 08/19/23 13:31:52.085
• [4.078 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/19/23 13:31:52.093
  Aug 19 13:31:52.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename endpointslice @ 08/19/23 13:31:52.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:31:52.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:31:52.113
  E0819 13:31:52.531675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:53.531994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:54.532286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:55.532390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:56.532653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/19/23 13:31:57.164
  E0819 13:31:57.533303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:58.533720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:31:59.534249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:00.534708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:01.534778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/19/23 13:32:02.171
  E0819 13:32:02.535737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:03.536040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:04.536147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:05.536243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:06.536320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/19/23 13:32:07.18
  E0819 13:32:07.537245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:08.537322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:09.537489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:10.537599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:11.537685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/19/23 13:32:12.189
  Aug 19 13:32:12.209: INFO: EndpointSlice for Service endpointslice-8139/example-named-port not found
  E0819 13:32:12.538688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:13.538776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:14.538871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:15.539929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:16.539993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:17.540286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:18.540397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:19.540488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:20.540769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:21.540881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:32:22.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8139" for this suite. @ 08/19/23 13:32:22.22
• [30.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/19/23 13:32:22.227
  Aug 19 13:32:22.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:32:22.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:32:22.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:32:22.251
  STEP: creating an Endpoint @ 08/19/23 13:32:22.257
  STEP: waiting for available Endpoint @ 08/19/23 13:32:22.261
  STEP: listing all Endpoints @ 08/19/23 13:32:22.263
  STEP: updating the Endpoint @ 08/19/23 13:32:22.266
  STEP: fetching the Endpoint @ 08/19/23 13:32:22.271
  STEP: patching the Endpoint @ 08/19/23 13:32:22.275
  STEP: fetching the Endpoint @ 08/19/23 13:32:22.281
  STEP: deleting the Endpoint by Collection @ 08/19/23 13:32:22.284
  STEP: waiting for Endpoint deletion @ 08/19/23 13:32:22.293
  STEP: fetching the Endpoint @ 08/19/23 13:32:22.294
  Aug 19 13:32:22.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3368" for this suite. @ 08/19/23 13:32:22.301
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/19/23 13:32:22.308
  Aug 19 13:32:22.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename kubelet-test @ 08/19/23 13:32:22.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:32:22.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:32:22.33
  Aug 19 13:32:22.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7643" for this suite. @ 08/19/23 13:32:22.355
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/19/23 13:32:22.361
  Aug 19 13:32:22.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename configmap @ 08/19/23 13:32:22.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:32:22.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:32:22.382
  STEP: Creating configMap that has name configmap-test-emptyKey-0309554f-2ed3-429b-a6e7-ec82745f1b35 @ 08/19/23 13:32:22.388
  Aug 19 13:32:22.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7375" for this suite. @ 08/19/23 13:32:22.394
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/19/23 13:32:22.407
  Aug 19 13:32:22.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename cronjob @ 08/19/23 13:32:22.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:32:22.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:32:22.429
  STEP: Creating a suspended cronjob @ 08/19/23 13:32:22.433
  STEP: Ensuring no jobs are scheduled @ 08/19/23 13:32:22.438
  E0819 13:32:22.540937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:23.541145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:24.541624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:25.541723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:26.542324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:27.542420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:28.543270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:29.544260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:30.544657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:31.544483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:32.544484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:33.544774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:34.545470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:35.545612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:36.545811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:37.545900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:38.546226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:39.546317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:40.546537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:41.546602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:42.546912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:43.547162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:44.547334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:45.547422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:46.547725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:47.548618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:48.549336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:49.549431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:50.550327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:51.550537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:52.551015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:53.551152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:54.551809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:55.552281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:56.553294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:57.553466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:58.554264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:32:59.554354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:00.554567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:01.554660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:02.555335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:03.555426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:04.556274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:05.556368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:06.557401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:07.557496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:08.558422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:09.558852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:10.559150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:11.559229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:12.559505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:13.560279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:14.560698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:15.560851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:16.561643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:17.561846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:18.562222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:19.562313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:20.562891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:21.563821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:22.564440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:23.564527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:24.565309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:25.565498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:26.565918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:27.566009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:28.566652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:29.566742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:30.567189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:31.567372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:32.567959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:33.568794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:34.569500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:35.569973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:36.570306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:37.571324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:38.571781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:39.572859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:40.573814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:41.573913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:42.574113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:43.575019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:44.575625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:45.576282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:46.576655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:47.576753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:48.576816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:49.576904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:50.577525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:51.577693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:52.578742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:53.578830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:54.579367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:55.581340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:56.581979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:57.582064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:58.582661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:33:59.582767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:00.582917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:01.583219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:02.583812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:03.583912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:04.584213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:05.584292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:06.584397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:07.584585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:08.585409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:09.585502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:10.585592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:11.585685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:12.586607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:13.586935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:14.587213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:15.588277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:16.589238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:17.589367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:18.589828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:19.590920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:20.591634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:21.592390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:22.592930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:23.593188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:24.593690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:25.594529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:26.595209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:27.596283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:28.596370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:29.596553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:30.596963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:31.596828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:32.596890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:33.597750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:34.598102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:35.598190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:36.598874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:37.599005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:38.599686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:39.599756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:40.600656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:41.601264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:42.601563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:43.601644      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:44.602654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:45.602866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:46.603795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:47.604282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:48.605265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:49.605371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:50.606217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:51.606275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:52.606635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:53.606745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:54.607197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:55.607239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:56.608163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:57.608256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:58.608573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:34:59.608666      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:00.609135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:01.609203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:02.609847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:03.610858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:04.610916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:05.611006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:06.611498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:07.611598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:08.612101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:09.612179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:10.612261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:11.613203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:12.613872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:13.614149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:14.615005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:15.615655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:16.616162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:17.616258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:18.616298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:19.616391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:20.616751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:21.616831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:22.617272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:23.617344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:24.617670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:25.617770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:26.617946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:27.618038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:28.619068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:29.619243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:30.620239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:31.620321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:32.620919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:33.621219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:34.621425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:35.621527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:36.622502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:37.623441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:38.623828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:39.623858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:40.624008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:41.624108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:42.624787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:43.625100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:44.625949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:45.626048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:46.626541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:47.626739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:48.627530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:49.627604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:50.628295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:51.628540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:52.629342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:53.629676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:54.630607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:55.630779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:56.631123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:57.631233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:58.631754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:35:59.631849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:00.632303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:01.633307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:02.634057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:03.634323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:04.634738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:05.634829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:06.635701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:07.636302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:08.636761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:09.636984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:10.637679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:11.637777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:12.638786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:13.638862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:14.639211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:15.639239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:16.639954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:17.640056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:18.640827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:19.640916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:20.641243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:21.641564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:22.641643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:23.641924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:24.643003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:25.643252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:26.643546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:27.643637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:28.644561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:29.644603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:30.644850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:31.644947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:32.645498      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:33.645589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:34.646129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:35.646220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:36.646310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:37.646899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:38.647207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:39.648278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:40.648878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:41.648981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:42.649768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:43.650018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:44.650470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:45.650559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:46.650875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:47.650972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:48.651985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:49.652276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:50.652794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:51.652983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:52.653375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:53.653453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:54.653501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:55.653698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:56.654408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:57.654499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:58.654700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:36:59.654902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:00.655316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:01.655415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:02.655486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:03.655701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:04.656693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:05.656787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:06.656869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:07.657030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:08.657256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:09.657462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:10.657630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:11.657819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:12.658572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:13.658654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:14.659412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:15.659514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:16.660123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:17.660317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:18.660857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:19.660988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:20.661792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:21.661878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/19/23 13:37:22.446
  STEP: Removing cronjob @ 08/19/23 13:37:22.449
  Aug 19 13:37:22.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-296" for this suite. @ 08/19/23 13:37:22.459
• [300.057 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/19/23 13:37:22.466
  Aug 19 13:37:22.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:37:22.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:37:22.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:37:22.488
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:37:22.491
  E0819 13:37:22.662285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:23.662323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:24.662771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:25.663841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:37:26.51
  Aug 19 13:37:26.513: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-3010f750-0714-4992-b24b-4f4485650eff container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:37:26.53
  Aug 19 13:37:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7068" for this suite. @ 08/19/23 13:37:26.55
• [4.091 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/19/23 13:37:26.557
  Aug 19 13:37:26.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename resourcequota @ 08/19/23 13:37:26.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:37:26.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:37:26.578
  STEP: Counting existing ResourceQuota @ 08/19/23 13:37:26.581
  E0819 13:37:26.664061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:27.664864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:28.665640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:29.666189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:30.667180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/19/23 13:37:31.584
  STEP: Ensuring resource quota status is calculated @ 08/19/23 13:37:31.589
  E0819 13:37:31.667452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:32.668308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 08/19/23 13:37:33.593
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/19/23 13:37:33.608
  E0819 13:37:33.669233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:34.669331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/19/23 13:37:35.611
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/19/23 13:37:35.613
  STEP: Ensuring a pod cannot update its resource requirements @ 08/19/23 13:37:35.615
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/19/23 13:37:35.62
  E0819 13:37:35.670303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:36.670587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/19/23 13:37:37.624
  STEP: Ensuring resource quota status released the pod usage @ 08/19/23 13:37:37.636
  E0819 13:37:37.671354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:38.672299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:37:39.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-747" for this suite. @ 08/19/23 13:37:39.645
• [13.094 seconds]
------------------------------
S
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/19/23 13:37:39.652
  Aug 19 13:37:39.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename services @ 08/19/23 13:37:39.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:37:39.671
  E0819 13:37:39.672952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:37:39.674
  STEP: creating service endpoint-test2 in namespace services-3491 @ 08/19/23 13:37:39.677
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3491 to expose endpoints map[] @ 08/19/23 13:37:39.697
  Aug 19 13:37:39.705: INFO: successfully validated that service endpoint-test2 in namespace services-3491 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-3491 @ 08/19/23 13:37:39.705
  E0819 13:37:40.673372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:41.673479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3491 to expose endpoints map[pod1:[80]] @ 08/19/23 13:37:41.723
  Aug 19 13:37:41.733: INFO: successfully validated that service endpoint-test2 in namespace services-3491 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/19/23 13:37:41.733
  Aug 19 13:37:41.733: INFO: Creating new exec pod
  E0819 13:37:42.673663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:43.673747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:44.674040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:37:44.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 19 13:37:44.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 19 13:37:44.889: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:37:44.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.101 80'
  Aug 19 13:37:45.028: INFO: stderr: "+ nc -v -t -w 2 10.152.183.101 80\nConnection to 10.152.183.101 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 19 13:37:45.028: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-3491 @ 08/19/23 13:37:45.028
  E0819 13:37:45.674129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:46.674224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3491 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/19/23 13:37:47.044
  Aug 19 13:37:47.056: INFO: successfully validated that service endpoint-test2 in namespace services-3491 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/19/23 13:37:47.056
  E0819 13:37:47.674297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:37:48.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 19 13:37:48.210: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 19 13:37:48.210: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:37:48.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.101 80'
  Aug 19 13:37:48.345: INFO: stderr: "+ nc -v -t -w 2 10.152.183.101 80\n+ echo hostName\nConnection to 10.152.183.101 80 port [tcp/http] succeeded!\n"
  Aug 19 13:37:48.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-3491 @ 08/19/23 13:37:48.345
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3491 to expose endpoints map[pod2:[80]] @ 08/19/23 13:37:48.362
  Aug 19 13:37:48.377: INFO: successfully validated that service endpoint-test2 in namespace services-3491 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/19/23 13:37:48.377
  E0819 13:37:48.675230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:37:49.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 19 13:37:49.522: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 19 13:37:49.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 19 13:37:49.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1223866462 --namespace=services-3491 exec execpodhlj6w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.101 80'
  Aug 19 13:37:49.661: INFO: stderr: "+ nc -v -t -w 2 10.152.183.101 80\n+ echo hostName\nConnection to 10.152.183.101 80 port [tcp/http] succeeded!\n"
  Aug 19 13:37:49.661: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-3491 @ 08/19/23 13:37:49.661
  E0819 13:37:49.675418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3491 to expose endpoints map[] @ 08/19/23 13:37:49.687
  Aug 19 13:37:49.697: INFO: successfully validated that service endpoint-test2 in namespace services-3491 exposes endpoints map[]
  Aug 19 13:37:49.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3491" for this suite. @ 08/19/23 13:37:49.714
• [10.068 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/19/23 13:37:49.72
  Aug 19 13:37:49.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename podtemplate @ 08/19/23 13:37:49.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:37:49.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:37:49.741
  STEP: Create a pod template @ 08/19/23 13:37:49.745
  STEP: Replace a pod template @ 08/19/23 13:37:49.75
  Aug 19 13:37:49.757: INFO: Found updated podtemplate annotation: "true"

  Aug 19 13:37:49.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8498" for this suite. @ 08/19/23 13:37:49.76
• [0.046 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/19/23 13:37:49.767
  Aug 19 13:37:49.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename watch @ 08/19/23 13:37:49.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:37:49.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:37:49.785
  STEP: creating a watch on configmaps with label A @ 08/19/23 13:37:49.788
  STEP: creating a watch on configmaps with label B @ 08/19/23 13:37:49.789
  STEP: creating a watch on configmaps with label A or B @ 08/19/23 13:37:49.79
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/19/23 13:37:49.791
  Aug 19 13:37:49.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42622 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:49.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42622 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/19/23 13:37:49.797
  Aug 19 13:37:49.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42623 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:49.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42623 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/19/23 13:37:49.805
  Aug 19 13:37:49.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42624 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:49.813: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42624 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/19/23 13:37:49.813
  Aug 19 13:37:49.818: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42625 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:49.818: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5943  06697cd2-478a-45a9-bc15-67a0d6402e17 42625 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/19/23 13:37:49.818
  Aug 19 13:37:49.823: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5943  c5a2d6d3-b5d8-451c-b555-2e0a597046f8 42626 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:49.823: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5943  c5a2d6d3-b5d8-451c-b555-2e0a597046f8 42626 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0819 13:37:50.676288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:51.676368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:52.676532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:53.676621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:54.676732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:55.676817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:56.676914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:57.677105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:58.677206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:37:59.677300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/19/23 13:37:59.824
  Aug 19 13:37:59.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5943  c5a2d6d3-b5d8-451c-b555-2e0a597046f8 42683 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 19 13:37:59.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5943  c5a2d6d3-b5d8-451c-b555-2e0a597046f8 42683 0 2023-08-19 13:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-19 13:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0819 13:38:00.677913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:01.678386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:02.678486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:03.678782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:04.678893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:05.678984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:06.679164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:07.679229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:08.679583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:09.679641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:38:09.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5943" for this suite. @ 08/19/23 13:38:09.841
• [20.080 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/19/23 13:38:09.847
  Aug 19 13:38:09.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename proxy @ 08/19/23 13:38:09.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:09.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:09.868
  Aug 19 13:38:09.871: INFO: Creating pod...
  E0819 13:38:10.679757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:11.679845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:38:11.886: INFO: Creating service...
  Aug 19 13:38:11.898: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/DELETE
  Aug 19 13:38:11.904: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 19 13:38:11.905: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/GET
  Aug 19 13:38:11.909: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 19 13:38:11.909: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/HEAD
  Aug 19 13:38:11.912: INFO: http.Client request:HEAD | StatusCode:200
  Aug 19 13:38:11.912: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug 19 13:38:11.916: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 19 13:38:11.916: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/PATCH
  Aug 19 13:38:11.919: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 19 13:38:11.919: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/POST
  Aug 19 13:38:11.923: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 19 13:38:11.923: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/pods/agnhost/proxy/some/path/with/PUT
  Aug 19 13:38:11.927: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 19 13:38:11.927: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/DELETE
  Aug 19 13:38:11.932: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 19 13:38:11.932: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/GET
  Aug 19 13:38:11.937: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 19 13:38:11.937: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/HEAD
  Aug 19 13:38:11.942: INFO: http.Client request:HEAD | StatusCode:200
  Aug 19 13:38:11.942: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/OPTIONS
  Aug 19 13:38:11.947: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 19 13:38:11.947: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/PATCH
  Aug 19 13:38:11.953: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 19 13:38:11.953: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/POST
  Aug 19 13:38:11.957: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 19 13:38:11.957: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-323/services/test-service/proxy/some/path/with/PUT
  Aug 19 13:38:11.962: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 19 13:38:11.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-323" for this suite. @ 08/19/23 13:38:11.966
• [2.126 seconds]
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/19/23 13:38:11.973
  Aug 19 13:38:11.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename dns @ 08/19/23 13:38:11.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:11.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:11.994
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/19/23 13:38:11.997
  Aug 19 13:38:12.004: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7502  1b39aed0-9924-4391-9622-6ae85fa55385 42733 0 2023-08-19 13:38:12 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-19 13:38:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8m5js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8m5js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0819 13:38:12.680333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:13.680376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/19/23 13:38:14.012
  Aug 19 13:38:14.012: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7502 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:38:14.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:38:14.013: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:38:14.013: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7502/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 08/19/23 13:38:14.106
  Aug 19 13:38:14.106: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7502 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:38:14.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:38:14.106: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:38:14.106: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7502/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 19 13:38:14.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 19 13:38:14.200: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7502" for this suite. @ 08/19/23 13:38:14.212
• [2.248 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/19/23 13:38:14.223
  Aug 19 13:38:14.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename emptydir @ 08/19/23 13:38:14.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:14.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:14.24
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/19/23 13:38:14.243
  E0819 13:38:14.681082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:15.681177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:16.681279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:17.682319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:38:18.263
  Aug 19 13:38:18.267: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-a4005ffa-96dc-45d9-8046-ac7980eba349 container test-container: <nil>
  STEP: delete the pod @ 08/19/23 13:38:18.273
  Aug 19 13:38:18.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3984" for this suite. @ 08/19/23 13:38:18.291
• [4.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/19/23 13:38:18.3
  Aug 19 13:38:18.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pod-network-test @ 08/19/23 13:38:18.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:18.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:18.319
  STEP: Performing setup for networking test in namespace pod-network-test-3030 @ 08/19/23 13:38:18.322
  STEP: creating a selector @ 08/19/23 13:38:18.322
  STEP: Creating the service pods in kubernetes @ 08/19/23 13:38:18.322
  Aug 19 13:38:18.322: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0819 13:38:18.685156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:19.685913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:20.686384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:21.686697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:22.686807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:23.687147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:24.688213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:25.688305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:26.688859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:27.689821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:28.690811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:29.690910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:30.691277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:31.691068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:32.691804      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:33.692557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:34.692945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:35.692996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:36.693984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:37.694168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:38.694716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:39.694879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/19/23 13:38:40.41
  E0819 13:38:40.695083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:41.695222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:38:42.436: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 19 13:38:42.436: INFO: Going to poll 192.168.13.147 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:38:42.438: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.13.147:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:38:42.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:38:42.439: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:38:42.439: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.13.147%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 19 13:38:42.518: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 19 13:38:42.518: INFO: Going to poll 192.168.232.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:38:42.521: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.232.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:38:42.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:38:42.522: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:38:42.522: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.232.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 19 13:38:42.599: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 19 13:38:42.599: INFO: Going to poll 192.168.20.11 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 19 13:38:42.602: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.20.11:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:38:42.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:38:42.603: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:38:42.603: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.20.11%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 19 13:38:42.682: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 19 13:38:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3030" for this suite. @ 08/19/23 13:38:42.686
• [24.392 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/19/23 13:38:42.693
  Aug 19 13:38:42.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename projected @ 08/19/23 13:38:42.694
  E0819 13:38:42.695344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:42.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:42.72
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:38:42.722
  E0819 13:38:43.696270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:44.697189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:45.697279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:46.697362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:38:46.742
  Aug 19 13:38:46.744: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-fafa4711-a390-4279-a449-7744d53d43b0 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:38:46.751
  Aug 19 13:38:46.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9097" for this suite. @ 08/19/23 13:38:46.771
• [4.083 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/19/23 13:38:46.777
  Aug 19 13:38:46.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename pod-network-test @ 08/19/23 13:38:46.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:38:46.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:38:46.796
  STEP: Performing setup for networking test in namespace pod-network-test-2567 @ 08/19/23 13:38:46.799
  STEP: creating a selector @ 08/19/23 13:38:46.799
  STEP: Creating the service pods in kubernetes @ 08/19/23 13:38:46.799
  Aug 19 13:38:46.799: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0819 13:38:47.697458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:48.697524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:49.697634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:50.697716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:51.697822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:52.698881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:53.698908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:54.698991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:55.699145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:56.699240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:57.699338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:38:58.699401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/19/23 13:38:58.866
  E0819 13:38:59.700269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:00.700485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:39:00.882: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 19 13:39:00.882: INFO: Breadth first check of 192.168.13.182 on host 172.31.15.214...
  Aug 19 13:39:00.885: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.151:9080/dial?request=hostname&protocol=http&host=192.168.13.182&port=8083&tries=1'] Namespace:pod-network-test-2567 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:39:00.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:39:00.885: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:39:00.886: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2567/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.13.182%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 13:39:00.984: INFO: Waiting for responses: map[]
  Aug 19 13:39:00.984: INFO: reached 192.168.13.182 after 0/1 tries
  Aug 19 13:39:00.984: INFO: Breadth first check of 192.168.232.63 on host 172.31.42.145...
  Aug 19 13:39:00.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.151:9080/dial?request=hostname&protocol=http&host=192.168.232.63&port=8083&tries=1'] Namespace:pod-network-test-2567 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:39:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:39:00.989: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:39:00.989: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2567/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.232.63%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 13:39:01.071: INFO: Waiting for responses: map[]
  Aug 19 13:39:01.071: INFO: reached 192.168.232.63 after 0/1 tries
  Aug 19 13:39:01.071: INFO: Breadth first check of 192.168.20.56 on host 172.31.69.13...
  Aug 19 13:39:01.074: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.13.151:9080/dial?request=hostname&protocol=http&host=192.168.20.56&port=8083&tries=1'] Namespace:pod-network-test-2567 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 19 13:39:01.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  Aug 19 13:39:01.074: INFO: ExecWithOptions: Clientset creation
  Aug 19 13:39:01.074: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2567/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.13.151%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.20.56%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 19 13:39:01.154: INFO: Waiting for responses: map[]
  Aug 19 13:39:01.154: INFO: reached 192.168.20.56 after 0/1 tries
  Aug 19 13:39:01.154: INFO: Going to retry 0 out of 3 pods....
  Aug 19 13:39:01.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2567" for this suite. @ 08/19/23 13:39:01.158
• [14.388 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/19/23 13:39:01.166
  Aug 19 13:39:01.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename secrets @ 08/19/23 13:39:01.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:39:01.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:39:01.192
  STEP: Creating secret with name secret-test-fe3cf87d-ae16-4dca-bd34-e799f0e5dfbf @ 08/19/23 13:39:01.195
  STEP: Creating a pod to test consume secrets @ 08/19/23 13:39:01.199
  E0819 13:39:01.700526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:02.700683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:03.700709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:04.701089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:39:05.217
  Aug 19 13:39:05.221: INFO: Trying to get logs from node ip-172-31-15-214 pod pod-secrets-323da1c3-c92f-46aa-93ce-ec65e61eb2be container secret-volume-test: <nil>
  STEP: delete the pod @ 08/19/23 13:39:05.228
  Aug 19 13:39:05.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3088" for this suite. @ 08/19/23 13:39:05.245
• [4.088 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/19/23 13:39:05.255
  Aug 19 13:39:05.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename replication-controller @ 08/19/23 13:39:05.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:39:05.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:39:05.276
  STEP: Creating replication controller my-hostname-basic-f471ab96-ac74-47ff-9feb-0f3878b710ad @ 08/19/23 13:39:05.279
  Aug 19 13:39:05.287: INFO: Pod name my-hostname-basic-f471ab96-ac74-47ff-9feb-0f3878b710ad: Found 0 pods out of 1
  E0819 13:39:05.701118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:06.701218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:07.701378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:08.701619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:09.701835      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:39:10.291: INFO: Pod name my-hostname-basic-f471ab96-ac74-47ff-9feb-0f3878b710ad: Found 1 pods out of 1
  Aug 19 13:39:10.291: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f471ab96-ac74-47ff-9feb-0f3878b710ad" are running
  Aug 19 13:39:10.294: INFO: Pod "my-hostname-basic-f471ab96-ac74-47ff-9feb-0f3878b710ad-zv2n9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:39:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:39:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:39:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-19 13:39:05 +0000 UTC Reason: Message:}])
  Aug 19 13:39:10.294: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/19/23 13:39:10.294
  Aug 19 13:39:10.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7178" for this suite. @ 08/19/23 13:39:10.308
• [5.059 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/19/23 13:39:10.316
  Aug 19 13:39:10.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:39:10.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:39:10.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:39:10.335
  STEP: Creating a pod to test downward api env vars @ 08/19/23 13:39:10.338
  E0819 13:39:10.702838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:11.704245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:12.704958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:13.705157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:39:14.358
  Aug 19 13:39:14.362: INFO: Trying to get logs from node ip-172-31-15-214 pod downward-api-589aed93-d317-4743-b64b-518aa7b9f791 container dapi-container: <nil>
  STEP: delete the pod @ 08/19/23 13:39:14.371
  Aug 19 13:39:14.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3332" for this suite. @ 08/19/23 13:39:14.393
• [4.085 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/19/23 13:39:14.401
  Aug 19 13:39:14.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename discovery @ 08/19/23 13:39:14.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:39:14.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:39:14.423
  STEP: Setting up server cert @ 08/19/23 13:39:14.429
  E0819 13:39:14.705322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 19 13:39:14.990: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug 19 13:39:14.991: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug 19 13:39:14.991: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug 19 13:39:14.991: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug 19 13:39:14.991: INFO: Checking APIGroup: apps
  Aug 19 13:39:14.993: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug 19 13:39:14.993: INFO: Versions found [{apps/v1 v1}]
  Aug 19 13:39:14.993: INFO: apps/v1 matches apps/v1
  Aug 19 13:39:14.993: INFO: Checking APIGroup: events.k8s.io
  Aug 19 13:39:14.998: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug 19 13:39:14.998: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug 19 13:39:14.998: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug 19 13:39:14.998: INFO: Checking APIGroup: authentication.k8s.io
  Aug 19 13:39:14.999: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug 19 13:39:14.999: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug 19 13:39:14.999: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug 19 13:39:14.999: INFO: Checking APIGroup: authorization.k8s.io
  Aug 19 13:39:15.000: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug 19 13:39:15.000: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug 19 13:39:15.000: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug 19 13:39:15.000: INFO: Checking APIGroup: autoscaling
  Aug 19 13:39:15.002: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug 19 13:39:15.002: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug 19 13:39:15.002: INFO: autoscaling/v2 matches autoscaling/v2
  Aug 19 13:39:15.002: INFO: Checking APIGroup: batch
  Aug 19 13:39:15.003: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug 19 13:39:15.003: INFO: Versions found [{batch/v1 v1}]
  Aug 19 13:39:15.003: INFO: batch/v1 matches batch/v1
  Aug 19 13:39:15.003: INFO: Checking APIGroup: certificates.k8s.io
  Aug 19 13:39:15.004: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug 19 13:39:15.004: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug 19 13:39:15.004: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug 19 13:39:15.004: INFO: Checking APIGroup: networking.k8s.io
  Aug 19 13:39:15.005: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug 19 13:39:15.005: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug 19 13:39:15.005: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug 19 13:39:15.005: INFO: Checking APIGroup: policy
  Aug 19 13:39:15.006: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug 19 13:39:15.006: INFO: Versions found [{policy/v1 v1}]
  Aug 19 13:39:15.006: INFO: policy/v1 matches policy/v1
  Aug 19 13:39:15.006: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug 19 13:39:15.008: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug 19 13:39:15.008: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug 19 13:39:15.008: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug 19 13:39:15.008: INFO: Checking APIGroup: storage.k8s.io
  Aug 19 13:39:15.009: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug 19 13:39:15.009: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug 19 13:39:15.010: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug 19 13:39:15.010: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug 19 13:39:15.011: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug 19 13:39:15.011: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug 19 13:39:15.011: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug 19 13:39:15.011: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug 19 13:39:15.012: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug 19 13:39:15.012: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug 19 13:39:15.012: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug 19 13:39:15.012: INFO: Checking APIGroup: scheduling.k8s.io
  Aug 19 13:39:15.014: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug 19 13:39:15.014: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug 19 13:39:15.014: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug 19 13:39:15.014: INFO: Checking APIGroup: coordination.k8s.io
  Aug 19 13:39:15.015: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug 19 13:39:15.015: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug 19 13:39:15.015: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug 19 13:39:15.015: INFO: Checking APIGroup: node.k8s.io
  Aug 19 13:39:15.016: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug 19 13:39:15.017: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug 19 13:39:15.017: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug 19 13:39:15.017: INFO: Checking APIGroup: discovery.k8s.io
  Aug 19 13:39:15.018: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug 19 13:39:15.018: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug 19 13:39:15.018: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug 19 13:39:15.018: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug 19 13:39:15.020: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug 19 13:39:15.020: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug 19 13:39:15.020: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug 19 13:39:15.020: INFO: Checking APIGroup: metrics.k8s.io
  Aug 19 13:39:15.021: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Aug 19 13:39:15.021: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Aug 19 13:39:15.021: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Aug 19 13:39:15.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-3142" for this suite. @ 08/19/23 13:39:15.026
• [0.633 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/19/23 13:39:15.034
  Aug 19 13:39:15.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1223866462
  STEP: Building a namespace api object, basename downward-api @ 08/19/23 13:39:15.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/19/23 13:39:15.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/19/23 13:39:15.057
  STEP: Creating a pod to test downward API volume plugin @ 08/19/23 13:39:15.063
  E0819 13:39:15.705418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:16.705500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:17.705599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0819 13:39:18.705863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/19/23 13:39:19.086
  Aug 19 13:39:19.089: INFO: Trying to get logs from node ip-172-31-15-214 pod downwardapi-volume-54e79e83-50db-4ae8-8649-44eb749d5e30 container client-container: <nil>
  STEP: delete the pod @ 08/19/23 13:39:19.095
  Aug 19 13:39:19.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8543" for this suite. @ 08/19/23 13:39:19.117
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug 19 13:39:19.126: INFO: Running AfterSuite actions on node 1
  Aug 19 13:39:19.126: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.048 seconds]
------------------------------

Ran 378 of 7207 Specs in 5930.943 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h38m51.336953021s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

