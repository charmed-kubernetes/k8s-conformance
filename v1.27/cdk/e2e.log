  I0812 12:09:45.933911      19 e2e.go:117] Starting e2e run "6291cefb-2518-4696-9a5a-b192283a03a4" on Ginkgo node 1
  Aug 12 12:09:45.968: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1691842185 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug 12 12:09:46.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:09:46.259: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug 12 12:09:46.301: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug 12 12:09:46.306: INFO: e2e test version: v1.27.4
  Aug 12 12:09:46.308: INFO: kube-apiserver version: v1.27.4
  Aug 12 12:09:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:09:46.314: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/12/23 12:09:47.02
  Aug 12 12:09:47.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:09:47.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:09:47.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:09:47.051
  STEP: Creating configMap with name configmap-test-volume-a08d7f33-4225-49e9-b483-ae0f1d986908 @ 08/12/23 12:09:47.056
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:09:47.061
  STEP: Saw pod success @ 08/12/23 12:10:11.162
  Aug 12 12:10:11.167: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-aef8d560-3d58-4af7-8001-8b532029351f container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:10:11.199
  Aug 12 12:10:11.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3686" for this suite. @ 08/12/23 12:10:11.229
• [24.220 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/12/23 12:10:11.24
  Aug 12 12:10:11.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:10:11.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:10:11.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:10:11.286
  STEP: Creating service test in namespace statefulset-4374 @ 08/12/23 12:10:11.292
  STEP: Creating a new StatefulSet @ 08/12/23 12:10:11.305
  Aug 12 12:10:11.320: INFO: Found 0 stateful pods, waiting for 3
  Aug 12 12:10:21.328: INFO: Found 2 stateful pods, waiting for 3
  Aug 12 12:10:31.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:10:31.326: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:10:31.326: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Aug 12 12:10:41.327: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:10:41.327: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:10:41.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/12/23 12:10:41.343
  Aug 12 12:10:41.365: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/12/23 12:10:41.365
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/12/23 12:10:51.386
  STEP: Performing a canary update @ 08/12/23 12:10:51.386
  Aug 12 12:10:51.410: INFO: Updating stateful set ss2
  Aug 12 12:10:51.419: INFO: Waiting for Pod statefulset-4374/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/12/23 12:11:01.431
  Aug 12 12:11:01.472: INFO: Found 2 stateful pods, waiting for 3
  Aug 12 12:11:11.480: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:11:11.480: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:11:11.480: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/12/23 12:11:11.491
  Aug 12 12:11:11.515: INFO: Updating stateful set ss2
  Aug 12 12:11:11.533: INFO: Waiting for Pod statefulset-4374/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 12 12:11:21.573: INFO: Updating stateful set ss2
  Aug 12 12:11:21.587: INFO: Waiting for StatefulSet statefulset-4374/ss2 to complete update
  Aug 12 12:11:21.588: INFO: Waiting for Pod statefulset-4374/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 12 12:11:31.601: INFO: Deleting all statefulset in ns statefulset-4374
  Aug 12 12:11:31.606: INFO: Scaling statefulset ss2 to 0
  Aug 12 12:11:41.629: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:11:41.633: INFO: Deleting statefulset ss2
  Aug 12 12:11:41.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4374" for this suite. @ 08/12/23 12:11:41.656
• [90.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/12/23 12:11:41.668
  Aug 12 12:11:41.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:11:41.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:11:41.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:11:41.7
  STEP: Creating service test in namespace statefulset-6637 @ 08/12/23 12:11:41.704
  STEP: Creating statefulset ss in namespace statefulset-6637 @ 08/12/23 12:11:41.712
  Aug 12 12:11:41.737: INFO: Found 0 stateful pods, waiting for 1
  Aug 12 12:11:51.744: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/12/23 12:11:51.757
  STEP: updating a scale subresource @ 08/12/23 12:11:51.762
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/12/23 12:11:51.77
  STEP: Patch a scale subresource @ 08/12/23 12:11:51.78
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/12/23 12:11:51.814
  Aug 12 12:11:51.820: INFO: Deleting all statefulset in ns statefulset-6637
  Aug 12 12:11:51.825: INFO: Scaling statefulset ss to 0
  Aug 12 12:12:01.867: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:12:01.871: INFO: Deleting statefulset ss
  Aug 12 12:12:01.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6637" for this suite. @ 08/12/23 12:12:01.896
• [20.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/12/23 12:12:01.92
  Aug 12 12:12:01.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename lease-test @ 08/12/23 12:12:01.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:12:01.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:12:01.958
  Aug 12 12:12:02.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-1278" for this suite. @ 08/12/23 12:12:02.043
• [0.132 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/12/23 12:12:02.052
  Aug 12 12:12:02.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:12:02.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:12:02.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:12:02.083
  STEP: Creating resourceQuota "e2e-rq-status-w8xlw" @ 08/12/23 12:12:02.091
  Aug 12 12:12:02.102: INFO: Resource quota "e2e-rq-status-w8xlw" reports spec: hard cpu limit of 500m
  Aug 12 12:12:02.102: INFO: Resource quota "e2e-rq-status-w8xlw" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-w8xlw" /status @ 08/12/23 12:12:02.102
  STEP: Confirm /status for "e2e-rq-status-w8xlw" resourceQuota via watch @ 08/12/23 12:12:02.114
  Aug 12 12:12:02.116: INFO: observed resourceQuota "e2e-rq-status-w8xlw" in namespace "resourcequota-1791" with hard status: v1.ResourceList(nil)
  Aug 12 12:12:02.116: INFO: Found resourceQuota "e2e-rq-status-w8xlw" in namespace "resourcequota-1791" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 12 12:12:02.116: INFO: ResourceQuota "e2e-rq-status-w8xlw" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/12/23 12:12:02.12
  Aug 12 12:12:02.128: INFO: Resource quota "e2e-rq-status-w8xlw" reports spec: hard cpu limit of 1
  Aug 12 12:12:02.128: INFO: Resource quota "e2e-rq-status-w8xlw" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-w8xlw" /status @ 08/12/23 12:12:02.129
  STEP: Confirm /status for "e2e-rq-status-w8xlw" resourceQuota via watch @ 08/12/23 12:12:02.141
  Aug 12 12:12:02.143: INFO: observed resourceQuota "e2e-rq-status-w8xlw" in namespace "resourcequota-1791" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 12 12:12:02.144: INFO: Found resourceQuota "e2e-rq-status-w8xlw" in namespace "resourcequota-1791" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug 12 12:12:02.144: INFO: ResourceQuota "e2e-rq-status-w8xlw" /status was patched
  STEP: Get "e2e-rq-status-w8xlw" /status @ 08/12/23 12:12:02.144
  Aug 12 12:12:02.149: INFO: Resourcequota "e2e-rq-status-w8xlw" reports status: hard cpu of 1
  Aug 12 12:12:02.149: INFO: Resourcequota "e2e-rq-status-w8xlw" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-w8xlw" /status before checking Spec is unchanged @ 08/12/23 12:12:02.153
  Aug 12 12:12:02.161: INFO: Resourcequota "e2e-rq-status-w8xlw" reports status: hard cpu of 2
  Aug 12 12:12:02.161: INFO: Resourcequota "e2e-rq-status-w8xlw" reports status: hard memory of 2Gi
  Aug 12 12:12:02.163: INFO: Found resourceQuota "e2e-rq-status-w8xlw" in namespace "resourcequota-1791" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Aug 12 12:14:17.173: INFO: ResourceQuota "e2e-rq-status-w8xlw" Spec was unchanged and /status reset
  Aug 12 12:14:17.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1791" for this suite. @ 08/12/23 12:14:17.18
• [135.137 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/12/23 12:14:17.19
  Aug 12 12:14:17.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 12:14:17.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:14:17.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:14:17.221
  STEP: Creating a test headless service @ 08/12/23 12:14:17.225
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-18 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-18;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-18 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-18;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-18.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-18.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-18.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-18.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-18.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-18.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-18.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-18.svc;check="$$(dig +notcp +noall +answer +search 136.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.136_tcp@PTR;sleep 1; done
   @ 08/12/23 12:14:17.248
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-18 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-18;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-18 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-18;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-18.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-18.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-18.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-18.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-18.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-18.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-18.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-18.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-18.svc;check="$$(dig +notcp +noall +answer +search 136.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.136_tcp@PTR;sleep 1; done
   @ 08/12/23 12:14:17.248
  STEP: creating a pod to probe DNS @ 08/12/23 12:14:17.248
  STEP: submitting the pod to kubernetes @ 08/12/23 12:14:17.248
  STEP: retrieving the pod @ 08/12/23 12:14:27.305
  STEP: looking for the results for each expected name from probers @ 08/12/23 12:14:27.309
  Aug 12 12:14:27.317: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.322: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.327: INFO: Unable to read wheezy_udp@dns-test-service.dns-18 from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.333: INFO: Unable to read wheezy_tcp@dns-test-service.dns-18 from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.338: INFO: Unable to read wheezy_udp@dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.343: INFO: Unable to read wheezy_tcp@dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.354: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.381: INFO: Unable to read jessie_udp@dns-test-service from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.386: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.391: INFO: Unable to read jessie_udp@dns-test-service.dns-18 from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.397: INFO: Unable to read jessie_tcp@dns-test-service.dns-18 from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.402: INFO: Unable to read jessie_udp@dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.408: INFO: Unable to read jessie_tcp@dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.414: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.418: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-18.svc from pod dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171: the server could not find the requested resource (get pods dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171)
  Aug 12 12:14:27.440: INFO: Lookups using dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-18 wheezy_tcp@dns-test-service.dns-18 wheezy_udp@dns-test-service.dns-18.svc wheezy_tcp@dns-test-service.dns-18.svc wheezy_udp@_http._tcp.dns-test-service.dns-18.svc wheezy_tcp@_http._tcp.dns-test-service.dns-18.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-18 jessie_tcp@dns-test-service.dns-18 jessie_udp@dns-test-service.dns-18.svc jessie_tcp@dns-test-service.dns-18.svc jessie_udp@_http._tcp.dns-test-service.dns-18.svc jessie_tcp@_http._tcp.dns-test-service.dns-18.svc]

  Aug 12 12:14:32.569: INFO: DNS probes using dns-18/dns-test-99cadc4f-c00b-4415-91b4-83faf33ff171 succeeded

  Aug 12 12:14:32.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 12:14:32.576
  STEP: deleting the test service @ 08/12/23 12:14:32.595
  STEP: deleting the test headless service @ 08/12/23 12:14:32.622
  STEP: Destroying namespace "dns-18" for this suite. @ 08/12/23 12:14:32.641
• [15.467 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/12/23 12:14:32.658
  Aug 12 12:14:32.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:14:32.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:14:32.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:14:32.688
  STEP: Saw pod success @ 08/12/23 12:14:44.78
  Aug 12 12:14:44.784: INFO: Trying to get logs from node ip-172-31-32-142 pod client-envvars-3d8ebd4b-e296-4ce6-adf6-5e87713fdedc container env3cont: <nil>
  STEP: delete the pod @ 08/12/23 12:14:44.804
  Aug 12 12:14:44.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4996" for this suite. @ 08/12/23 12:14:44.832
• [12.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/12/23 12:14:44.846
  Aug 12 12:14:44.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context-test @ 08/12/23 12:14:44.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:14:44.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:14:44.873
  Aug 12 12:14:50.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7493" for this suite. @ 08/12/23 12:14:50.932
• [6.094 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 08/12/23 12:14:50.941
  Aug 12 12:14:50.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 12:14:50.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:14:50.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:14:50.973
  Aug 12 12:14:51.003: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/12/23 12:14:51.01
  Aug 12 12:14:51.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:51.015: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/12/23 12:14:51.015
  Aug 12 12:14:51.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:51.042: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:14:52.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 12 12:14:52.048: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/12/23 12:14:52.054
  Aug 12 12:14:52.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 12 12:14:52.074: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Aug 12 12:14:53.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:53.079: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/12/23 12:14:53.079
  Aug 12 12:14:53.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:53.095: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:14:54.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:54.101: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:14:55.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 12 12:14:55.102: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 12:14:55.113
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6004, will wait for the garbage collector to delete the pods @ 08/12/23 12:14:55.113
  Aug 12 12:14:55.179: INFO: Deleting DaemonSet.extensions daemon-set took: 10.070331ms
  Aug 12 12:14:55.279: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.17649ms
  Aug 12 12:14:57.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:14:57.085: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 12:14:57.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4318"},"items":null}

  Aug 12 12:14:57.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4318"},"items":null}

  Aug 12 12:14:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6004" for this suite. @ 08/12/23 12:14:57.13
• [6.198 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/12/23 12:14:57.139
  Aug 12 12:14:57.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 12:14:57.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:14:57.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:14:57.167
  Aug 12 12:14:57.184: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Aug 12 12:15:02.191: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 12:15:02.191
  Aug 12 12:15:02.191: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/12/23 12:15:02.203
  Aug 12 12:15:02.221: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4856  bf47b209-bec3-4b29-8d34-2ed15fb06142 4357 1 2023-08-12 12:15:02 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-12 12:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f225b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 12 12:15:02.228: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Aug 12 12:15:02.228: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug 12 12:15:02.228: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4856  61ce8e2a-2aa5-4641-9912-8f0ac607112d 4360 1 2023-08-12 12:14:57 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment bf47b209-bec3-4b29-8d34-2ed15fb06142 0xc002f8288f 0xc002f828a0}] [] [{e2e.test Update apps/v1 2023-08-12 12:14:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:14:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-12 12:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"bf47b209-bec3-4b29-8d34-2ed15fb06142\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f82958 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 12:15:02.234: INFO: Pod "test-cleanup-controller-8mxm4" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-8mxm4 test-cleanup-controller- deployment-4856  16df2202-7aa5-43d1-953a-5d7b0df2c6aa 4341 0 2023-08-12 12:14:57 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 61ce8e2a-2aa5-4641-9912-8f0ac607112d 0xc002f228ef 0xc002f22900}] [] [{kube-controller-manager Update v1 2023-08-12 12:14:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ce8e2a-2aa5-4641-9912-8f0ac607112d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:14:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qfbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qfbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:14:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:14:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:14:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:14:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.139,StartTime:2023-08-12 12:14:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:14:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2ec6c89d19db35a69425e4276dfa1f35a8848424c52bdbddab201caa1b5fa4a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.139,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:15:02.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4856" for this suite. @ 08/12/23 12:15:02.248
• [5.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/12/23 12:15:02.265
  Aug 12 12:15:02.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption @ 08/12/23 12:15:02.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:15:02.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:15:02.307
  Aug 12 12:15:02.343: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 12 12:16:02.369: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/12/23 12:16:02.374
  Aug 12 12:16:02.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/12/23 12:16:02.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:02.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:02.403
  Aug 12 12:16:02.427: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug 12 12:16:02.432: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug 12 12:16:02.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:16:02.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-5538" for this suite. @ 08/12/23 12:16:02.536
  STEP: Destroying namespace "sched-preemption-1413" for this suite. @ 08/12/23 12:16:02.551
• [60.296 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/12/23 12:16:02.565
  Aug 12 12:16:02.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:16:02.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:02.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:02.591
  STEP: Creating configMap with name configmap-test-upd-72c2e4e9-acfc-4e3b-8587-d0d7cd0e20c8 @ 08/12/23 12:16:02.601
  STEP: Creating the pod @ 08/12/23 12:16:02.607
  STEP: Waiting for pod with text data @ 08/12/23 12:16:04.634
  STEP: Waiting for pod with binary data @ 08/12/23 12:16:04.648
  Aug 12 12:16:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6848" for this suite. @ 08/12/23 12:16:04.663
• [2.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/12/23 12:16:04.673
  Aug 12 12:16:04.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/12/23 12:16:04.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:04.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:04.701
  STEP: creating @ 08/12/23 12:16:04.706
  STEP: getting @ 08/12/23 12:16:04.735
  STEP: listing @ 08/12/23 12:16:04.742
  STEP: deleting @ 08/12/23 12:16:04.748
  Aug 12 12:16:04.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6649" for this suite. @ 08/12/23 12:16:04.778
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/12/23 12:16:04.799
  Aug 12 12:16:04.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename cronjob @ 08/12/23 12:16:04.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:04.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:04.827
  STEP: Creating a cronjob @ 08/12/23 12:16:04.833
  STEP: creating @ 08/12/23 12:16:04.833
  STEP: getting @ 08/12/23 12:16:04.842
  STEP: listing @ 08/12/23 12:16:04.847
  STEP: watching @ 08/12/23 12:16:04.851
  Aug 12 12:16:04.851: INFO: starting watch
  STEP: cluster-wide listing @ 08/12/23 12:16:04.853
  STEP: cluster-wide watching @ 08/12/23 12:16:04.858
  Aug 12 12:16:04.858: INFO: starting watch
  STEP: patching @ 08/12/23 12:16:04.859
  STEP: updating @ 08/12/23 12:16:04.868
  Aug 12 12:16:04.881: INFO: waiting for watch events with expected annotations
  Aug 12 12:16:04.881: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/12/23 12:16:04.881
  STEP: updating /status @ 08/12/23 12:16:04.89
  STEP: get /status @ 08/12/23 12:16:04.9
  STEP: deleting @ 08/12/23 12:16:04.905
  STEP: deleting a collection @ 08/12/23 12:16:04.924
  Aug 12 12:16:04.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2663" for this suite. @ 08/12/23 12:16:04.946
• [0.156 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/12/23 12:16:04.957
  Aug 12 12:16:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-webhook @ 08/12/23 12:16:04.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:04.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:04.987
  STEP: Setting up server cert @ 08/12/23 12:16:04.994
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/12/23 12:16:05.483
  STEP: Deploying the custom resource conversion webhook pod @ 08/12/23 12:16:05.496
  STEP: Wait for the deployment to be ready @ 08/12/23 12:16:05.514
  Aug 12 12:16:05.523: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  Aug 12 12:16:07.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 12 12:16:09.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 16, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 08/12/23 12:16:11.547
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:16:11.588
  Aug 12 12:16:12.588: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 12 12:16:12.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Creating a v1 custom resource @ 08/12/23 12:16:15.207
  STEP: Create a v2 custom resource @ 08/12/23 12:16:15.228
  STEP: List CRs in v1 @ 08/12/23 12:16:15.291
  STEP: List CRs in v2 @ 08/12/23 12:16:15.298
  Aug 12 12:16:15.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-412" for this suite. @ 08/12/23 12:16:15.883
• [10.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/12/23 12:16:15.907
  Aug 12 12:16:15.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:16:15.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:15.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:15.94
  STEP: Creating the pod @ 08/12/23 12:16:15.945
  Aug 12 12:16:18.508: INFO: Successfully updated pod "labelsupdatec6858ecb-772f-4644-9c94-6b54d37c147c"
  Aug 12 12:16:20.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6934" for this suite. @ 08/12/23 12:16:20.543
• [4.649 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 08/12/23 12:16:20.557
  Aug 12 12:16:20.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:16:20.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:20.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:20.585
  STEP: Creating secret with name secret-test-map-ab18d912-08b5-4f08-bcbd-ebda8f6e440b @ 08/12/23 12:16:20.594
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:16:20.601
  STEP: Saw pod success @ 08/12/23 12:16:24.632
  Aug 12 12:16:24.636: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-d4a33c71-09c9-4eda-ad74-7b4c74cae815 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:16:24.647
  Aug 12 12:16:24.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2701" for this suite. @ 08/12/23 12:16:24.674
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/12/23 12:16:24.688
  Aug 12 12:16:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:16:24.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:24.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:24.716
  STEP: creating a Pod with a static label @ 08/12/23 12:16:24.727
  STEP: watching for Pod to be ready @ 08/12/23 12:16:24.739
  Aug 12 12:16:24.741: INFO: observed Pod pod-test in namespace pods-4684 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug 12 12:16:24.744: INFO: observed Pod pod-test in namespace pods-4684 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC  }]
  Aug 12 12:16:24.766: INFO: observed Pod pod-test in namespace pods-4684 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC  }]
  Aug 12 12:16:26.343: INFO: Found Pod pod-test in namespace pods-4684 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:16:24 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/12/23 12:16:26.349
  STEP: getting the Pod and ensuring that it's patched @ 08/12/23 12:16:26.372
  STEP: replacing the Pod's status Ready condition to False @ 08/12/23 12:16:26.378
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/12/23 12:16:26.395
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/12/23 12:16:26.395
  STEP: watching for the Pod to be deleted @ 08/12/23 12:16:26.409
  Aug 12 12:16:26.412: INFO: observed event type MODIFIED
  Aug 12 12:16:28.303: INFO: observed event type MODIFIED
  Aug 12 12:16:28.711: INFO: observed event type MODIFIED
  Aug 12 12:16:29.308: INFO: observed event type MODIFIED
  Aug 12 12:16:29.333: INFO: observed event type MODIFIED
  Aug 12 12:16:29.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4684" for this suite. @ 08/12/23 12:16:29.351
• [4.675 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/12/23 12:16:29.365
  Aug 12 12:16:29.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:16:29.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:29.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:29.397
  STEP: Creating secret with name secret-test-2f772de8-dd85-4780-bf75-6a7d40f72596 @ 08/12/23 12:16:29.402
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:16:29.408
  STEP: Saw pod success @ 08/12/23 12:16:33.44
  Aug 12 12:16:33.445: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-212d3d58-51ad-46a8-8f25-85a47ea7ca3e container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:16:33.454
  Aug 12 12:16:33.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4861" for this suite. @ 08/12/23 12:16:33.479
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 08/12/23 12:16:33.496
  Aug 12 12:16:33.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 12:16:33.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:33.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:33.523
  Aug 12 12:16:33.554: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 12:16:33.562
  Aug 12 12:16:33.566: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:33.566: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:33.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:16:33.572: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  Aug 12 12:16:34.580: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:34.580: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:34.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:16:34.585: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  Aug 12 12:16:35.580: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:35.581: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:35.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 12:16:35.586: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/12/23 12:16:35.606
  STEP: Check that daemon pods images are updated. @ 08/12/23 12:16:35.619
  Aug 12 12:16:35.624: INFO: Wrong image for pod: daemon-set-95phg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:35.624: INFO: Wrong image for pod: daemon-set-rhlnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:35.624: INFO: Wrong image for pod: daemon-set-x7kbg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:35.629: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:35.630: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:36.637: INFO: Wrong image for pod: daemon-set-rhlnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:36.637: INFO: Wrong image for pod: daemon-set-x7kbg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:36.643: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:36.643: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:37.637: INFO: Pod daemon-set-pw2kv is not available
  Aug 12 12:16:37.637: INFO: Wrong image for pod: daemon-set-rhlnb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:37.637: INFO: Wrong image for pod: daemon-set-x7kbg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:37.643: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:37.643: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:38.636: INFO: Wrong image for pod: daemon-set-x7kbg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:38.644: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:38.644: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:39.637: INFO: Pod daemon-set-p8255 is not available
  Aug 12 12:16:39.637: INFO: Wrong image for pod: daemon-set-x7kbg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 12 12:16:39.643: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:39.643: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:40.636: INFO: Pod daemon-set-m72kt is not available
  Aug 12 12:16:40.643: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:40.643: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/12/23 12:16:40.644
  Aug 12 12:16:40.650: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:40.650: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:40.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:16:40.655: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:16:41.664: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:41.664: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:41.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:16:41.669: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:16:42.661: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:42.661: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:42.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:16:42.667: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:16:43.663: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:43.663: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:43.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:16:43.669: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:16:44.663: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:44.663: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:44.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:16:44.669: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  Aug 12 12:16:45.661: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:45.661: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:16:45.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 12:16:45.667: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 12:16:45.69
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9746, will wait for the garbage collector to delete the pods @ 08/12/23 12:16:45.69
  Aug 12 12:16:45.755: INFO: Deleting DaemonSet.extensions daemon-set took: 9.825677ms
  Aug 12 12:16:45.856: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.218131ms
  Aug 12 12:16:51.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:16:51.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 12:16:51.367: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5124"},"items":null}

  Aug 12 12:16:51.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5124"},"items":null}

  Aug 12 12:16:51.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9746" for this suite. @ 08/12/23 12:16:51.396
• [17.909 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/12/23 12:16:51.406
  Aug 12 12:16:51.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 12:16:51.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:51.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:51.437
  STEP: Setting up server cert @ 08/12/23 12:16:51.468
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 12:16:51.997
  STEP: Deploying the webhook pod @ 08/12/23 12:16:52.01
  STEP: Wait for the deployment to be ready @ 08/12/23 12:16:52.026
  Aug 12 12:16:52.038: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/12/23 12:16:54.056
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:16:54.069
  Aug 12 12:16:55.070: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/12/23 12:16:55.155
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/12/23 12:16:55.208
  STEP: Deleting the collection of validation webhooks @ 08/12/23 12:16:55.254
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/12/23 12:16:55.323
  Aug 12 12:16:55.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8398" for this suite. @ 08/12/23 12:16:55.403
  STEP: Destroying namespace "webhook-markers-38" for this suite. @ 08/12/23 12:16:55.413
• [4.017 seconds]
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/12/23 12:16:55.424
  Aug 12 12:16:55.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:16:55.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:16:55.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:16:55.451
  STEP: creating all guestbook components @ 08/12/23 12:16:55.455
  Aug 12 12:16:55.455: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug 12 12:16:55.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:56.558: INFO: stderr: ""
  Aug 12 12:16:56.558: INFO: stdout: "service/agnhost-replica created\n"
  Aug 12 12:16:56.558: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug 12 12:16:56.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:57.130: INFO: stderr: ""
  Aug 12 12:16:57.130: INFO: stdout: "service/agnhost-primary created\n"
  Aug 12 12:16:57.131: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug 12 12:16:57.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:57.973: INFO: stderr: ""
  Aug 12 12:16:57.973: INFO: stdout: "service/frontend created\n"
  Aug 12 12:16:57.973: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug 12 12:16:57.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:58.537: INFO: stderr: ""
  Aug 12 12:16:58.537: INFO: stdout: "deployment.apps/frontend created\n"
  Aug 12 12:16:58.537: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 12 12:16:58.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:59.082: INFO: stderr: ""
  Aug 12 12:16:59.082: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug 12 12:16:59.083: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 12 12:16:59.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 create -f -'
  Aug 12 12:16:59.757: INFO: stderr: ""
  Aug 12 12:16:59.757: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/12/23 12:16:59.757
  Aug 12 12:16:59.757: INFO: Waiting for all frontend pods to be Running.
  Aug 12 12:17:04.808: INFO: Waiting for frontend to serve content.
  Aug 12 12:17:04.825: INFO: Trying to add a new entry to the guestbook.
  Aug 12 12:17:04.844: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/12/23 12:17:04.855
  Aug 12 12:17:04.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:04.974: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:04.974: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/12/23 12:17:04.974
  Aug 12 12:17:04.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:05.083: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:05.083: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/12/23 12:17:05.083
  Aug 12 12:17:05.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:05.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:05.185: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/12/23 12:17:05.185
  Aug 12 12:17:05.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:05.273: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:05.273: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/12/23 12:17:05.273
  Aug 12 12:17:05.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:05.398: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:05.398: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/12/23 12:17:05.398
  Aug 12 12:17:05.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3580 delete --grace-period=0 --force -f -'
  Aug 12 12:17:05.527: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:17:05.527: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug 12 12:17:05.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3580" for this suite. @ 08/12/23 12:17:05.536
• [10.123 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/12/23 12:17:05.547
  Aug 12 12:17:05.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:17:05.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:17:05.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:17:05.584
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/12/23 12:17:05.591
  STEP: Saw pod success @ 08/12/23 12:17:09.626
  Aug 12 12:17:09.630: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-f6438edc-b47a-4fe5-a711-87d19067a57f container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:17:09.64
  Aug 12 12:17:09.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5133" for this suite. @ 08/12/23 12:17:09.669
• [4.135 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/12/23 12:17:09.683
  Aug 12 12:17:09.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 12:17:09.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:17:09.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:17:09.715
  Aug 12 12:17:09.720: INFO: Creating deployment "webserver-deployment"
  Aug 12 12:17:09.729: INFO: Waiting for observed generation 1
  Aug 12 12:17:11.740: INFO: Waiting for all required pods to come up
  Aug 12 12:17:11.749: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/12/23 12:17:11.749
  Aug 12 12:17:13.770: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug 12 12:17:13.780: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug 12 12:17:13.799: INFO: Updating deployment webserver-deployment
  Aug 12 12:17:13.799: INFO: Waiting for observed generation 2
  Aug 12 12:17:15.819: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug 12 12:17:15.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug 12 12:17:15.829: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 12 12:17:15.844: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug 12 12:17:15.844: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug 12 12:17:15.850: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 12 12:17:15.860: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug 12 12:17:15.860: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug 12 12:17:15.877: INFO: Updating deployment webserver-deployment
  Aug 12 12:17:15.877: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug 12 12:17:15.894: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Aug 12 12:17:15.900: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Aug 12 12:17:15.929: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-6034  bc848a7f-00e3-467c-b50e-043436d60290 5864 3 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eb9998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-12 12:17:13 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-12 12:17:15 +0000 UTC,LastTransitionTime:2023-08-12 12:17:15 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug 12 12:17:15.939: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-6034  aecd53f5-9fb6-4fec-884b-17ade8bf49f7 5857 3 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bc848a7f-00e3-467c-b50e-043436d60290 0xc004e7e0f7 0xc004e7e0f8}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc848a7f-00e3-467c-b50e-043436d60290\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e7e198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 12:17:15.940: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug 12 12:17:15.941: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-6034  4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 5855 3 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bc848a7f-00e3-467c-b50e-043436d60290 0xc004e7e007 0xc004e7e008}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc848a7f-00e3-467c-b50e-043436d60290\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e7e098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 12:17:15.950: INFO: Pod "webserver-deployment-67bd4bf6dc-2fht2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2fht2 webserver-deployment-67bd4bf6dc- deployment-6034  a55a2563-d729-4bc9-9900-4ca7f50354d6 5871 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7e690 0xc004e7e691}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hptkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hptkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.951: INFO: Pod "webserver-deployment-67bd4bf6dc-4pps6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4pps6 webserver-deployment-67bd4bf6dc- deployment-6034  46cdb6f6-66ec-472d-8cb2-346e719b07ca 5712 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7e7c7 0xc004e7e7c8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5lzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5lzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.153,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d3c3469b9c559c2be7c0e99902e8256950cb0eeef1e9b14c74df5db9fbdc00dc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.951: INFO: Pod "webserver-deployment-67bd4bf6dc-4qp6g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4qp6g webserver-deployment-67bd4bf6dc- deployment-6034  258460cc-4f72-4e03-9637-636e44eb6a17 5867 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7e9b7 0xc004e7e9b8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nm96h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nm96h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.952: INFO: Pod "webserver-deployment-67bd4bf6dc-6fmvr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6fmvr webserver-deployment-67bd4bf6dc- deployment-6034  50ecad58-209a-48b6-b671-013573eef065 5702 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7eaf7 0xc004e7eaf8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.182.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5m4x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5m4x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-79-233,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.79.233,PodIP:192.168.182.15,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e2436ed927e2c84b9f6c46093e582b14103462fc0e3aa6bbffc1791cb95691dd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.182.15,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.952: INFO: Pod "webserver-deployment-67bd4bf6dc-6vvr9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6vvr9 webserver-deployment-67bd4bf6dc- deployment-6034  e2ed45db-e7d0-4b02-bf75-1822127d42a4 5699 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7ece7 0xc004e7ece8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.182.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cfsx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfsx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-79-233,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.79.233,PodIP:192.168.182.17,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://eb2ccde899612a10b97f3fe7b19d95ed929058d8c29c62f2accd3e577c2d183b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.182.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.953: INFO: Pod "webserver-deployment-67bd4bf6dc-b4cjl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-b4cjl webserver-deployment-67bd4bf6dc- deployment-6034  93c71a78-8f91-46a1-a2e2-63df978e6f82 5723 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7eed7 0xc004e7eed8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6rjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6rjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.12,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d12ba28f9d9edf4885e1f1e3edd7d195d867b49b4bff8ea51a8a351b55c9e896,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.953: INFO: Pod "webserver-deployment-67bd4bf6dc-bhn5k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bhn5k webserver-deployment-67bd4bf6dc- deployment-6034  990dec74-be9d-45c8-8105-e63f50c6122f 5866 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f0c7 0xc004e7f0c8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr85f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr85f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.954: INFO: Pod "webserver-deployment-67bd4bf6dc-jt2n4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jt2n4 webserver-deployment-67bd4bf6dc- deployment-6034  9bcc1967-66b4-46d9-aae2-3e47811b7f8f 5729 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f230 0xc004e7f231}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pnd9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pnd9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.11,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d44e8cd0d800f5053b405476869fdd4df2d800509e316d150065e0d5ccc3083d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.954: INFO: Pod "webserver-deployment-67bd4bf6dc-ljswz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ljswz webserver-deployment-67bd4bf6dc- deployment-6034  21a09234-82f7-40ff-8228-539812427a2e 5719 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f417 0xc004e7f418}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h85rk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h85rk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.154,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cc6336a7d65f3c4195aef07fdeb75c09c531a418cb67e1bc74d5c781d314525b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.154,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.954: INFO: Pod "webserver-deployment-67bd4bf6dc-pbvdw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pbvdw webserver-deployment-67bd4bf6dc- deployment-6034  b6942a1f-6065-4c7b-b42c-fdc8efe6f048 5870 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f607 0xc004e7f608}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hhr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hhr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.955: INFO: Pod "webserver-deployment-67bd4bf6dc-rzr2g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rzr2g webserver-deployment-67bd4bf6dc- deployment-6034  cde39e2c-614b-4926-994d-f7e2c13f93cc 5868 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f747 0xc004e7f748}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crnqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crnqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.956: INFO: Pod "webserver-deployment-67bd4bf6dc-twvhc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-twvhc webserver-deployment-67bd4bf6dc- deployment-6034  5a8aab0f-f921-4ed0-84f9-2c50f685925a 5706 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7f887 0xc004e7f888}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.182.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8f7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8f7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-79-233,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.79.233,PodIP:192.168.182.16,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://801c2d5e70cbd493cacff551f39d5da8d81b5558cfda38ce0da886359de8e514,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.182.16,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.958: INFO: Pod "webserver-deployment-67bd4bf6dc-vpqpn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vpqpn webserver-deployment-67bd4bf6dc- deployment-6034  b101cdc2-e1ff-4a13-af53-5a0e7081a821 5865 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7fa77 0xc004e7fa78}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksrm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksrm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.959: INFO: Pod "webserver-deployment-67bd4bf6dc-vsfp4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vsfp4 webserver-deployment-67bd4bf6dc- deployment-6034  89253b9b-bbe2-4264-a9e2-2a7bfa010502 5862 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7fbf0 0xc004e7fbf1}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnqvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnqvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.961: INFO: Pod "webserver-deployment-67bd4bf6dc-wg4kn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wg4kn webserver-deployment-67bd4bf6dc- deployment-6034  e3311e2d-67cd-4e53-9ce3-72250578a4c1 5726 0 2023-08-12 12:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4 0xc004e7fd50 0xc004e7fd51}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b07fbcf-6c2b-4fa0-9d50-bf4d90ec8bc4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d5tx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d5tx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.10,StartTime:2023-08-12 12:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:17:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://014a44d56d1c378cdc0f2ce4d88bb9e7c6331b01632f73a23341f8eeb9c3cfb9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.10,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.962: INFO: Pod "webserver-deployment-7b75d79cf5-66fbg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-66fbg webserver-deployment-7b75d79cf5- deployment-6034  e2e5eb31-5144-469e-bd63-4ed9eb83de66 5874 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004e7ff47 0xc004e7ff48}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgpvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgpvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.963: INFO: Pod "webserver-deployment-7b75d79cf5-csmgr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-csmgr webserver-deployment-7b75d79cf5- deployment-6034  0f0fac87-37fa-4975-b03a-07eb48e7cf95 5875 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4a097 0xc004f4a098}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98gps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98gps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.963: INFO: Pod "webserver-deployment-7b75d79cf5-lmsk6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lmsk6 webserver-deployment-7b75d79cf5- deployment-6034  11d94616-7bde-48cf-8ec3-528f61ca055f 5826 0 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4a1e7 0xc004f4a1e8}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5p7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5p7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.13,StartTime:2023-08-12 12:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.13,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.964: INFO: Pod "webserver-deployment-7b75d79cf5-n5xjw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n5xjw webserver-deployment-7b75d79cf5- deployment-6034  e1841f65-ae23-4fca-b318-9d1d307a42da 5848 0 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4a407 0xc004f4a408}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhdwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhdwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.157,StartTime:2023-08-12 12:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.157,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.964: INFO: Pod "webserver-deployment-7b75d79cf5-q6qbd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-q6qbd webserver-deployment-7b75d79cf5- deployment-6034  1542ba58-a281-45fe-a5e9-00e92f47eaba 5845 0 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4a627 0xc004f4a628}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqxz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqxz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.156,StartTime:2023-08-12 12:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.156,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.965: INFO: Pod "webserver-deployment-7b75d79cf5-smvpd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-smvpd webserver-deployment-7b75d79cf5- deployment-6034  274dc0a6-9881-4edb-9bb8-7b8c3f92facb 5838 0 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4a847 0xc004f4a848}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.182.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpmn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpmn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-79-233,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.79.233,PodIP:192.168.182.18,StartTime:2023-08-12 12:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.182.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.965: INFO: Pod "webserver-deployment-7b75d79cf5-xckbw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xckbw webserver-deployment-7b75d79cf5- deployment-6034  8d5a379f-2a5d-4cd0-842e-9f9227659176 5823 0 2023-08-12 12:17:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4aa67 0xc004f4aa68}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snv8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snv8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.14,StartTime:2023-08-12 12:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.14,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.966: INFO: Pod "webserver-deployment-7b75d79cf5-zlh7x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zlh7x webserver-deployment-7b75d79cf5- deployment-6034  8f44c815-db82-427d-8b3f-6b45010232c0 5872 0 2023-08-12 12:17:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aecd53f5-9fb6-4fec-884b-17ade8bf49f7 0xc004f4ac87 0xc004f4ac88}] [] [{kube-controller-manager Update v1 2023-08-12 12:17:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aecd53f5-9fb6-4fec-884b-17ade8bf49f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdjp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdjp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-79-233,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:17:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:17:15.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6034" for this suite. @ 08/12/23 12:17:15.984
• [6.329 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/12/23 12:17:16.015
  Aug 12 12:17:16.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 12:17:16.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:17:16.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:17:16.117
  Aug 12 12:18:16.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9769" for this suite. @ 08/12/23 12:18:16.178
• [60.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/12/23 12:18:16.196
  Aug 12 12:18:16.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 12:18:16.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:16.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:16.229
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/12/23 12:18:16.235
  STEP: When a replication controller with a matching selector is created @ 08/12/23 12:18:18.268
  STEP: Then the orphan pod is adopted @ 08/12/23 12:18:18.276
  Aug 12 12:18:19.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-203" for this suite. @ 08/12/23 12:18:19.292
• [3.105 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/12/23 12:18:19.301
  Aug 12 12:18:19.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 12:18:19.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:19.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:19.328
  STEP: Read namespace status @ 08/12/23 12:18:19.333
  Aug 12 12:18:19.338: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/12/23 12:18:19.338
  Aug 12 12:18:19.346: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/12/23 12:18:19.346
  Aug 12 12:18:19.358: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug 12 12:18:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9804" for this suite. @ 08/12/23 12:18:19.367
• [0.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/12/23 12:18:19.376
  Aug 12 12:18:19.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 12:18:19.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:19.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:19.427
  Aug 12 12:18:19.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/12/23 12:18:20.989
  Aug 12 12:18:20.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2157 --namespace=crd-publish-openapi-2157 create -f -'
  Aug 12 12:18:21.710: INFO: stderr: ""
  Aug 12 12:18:21.710: INFO: stdout: "e2e-test-crd-publish-openapi-1108-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 12 12:18:21.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2157 --namespace=crd-publish-openapi-2157 delete e2e-test-crd-publish-openapi-1108-crds test-cr'
  Aug 12 12:18:21.814: INFO: stderr: ""
  Aug 12 12:18:21.814: INFO: stdout: "e2e-test-crd-publish-openapi-1108-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug 12 12:18:21.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2157 --namespace=crd-publish-openapi-2157 apply -f -'
  Aug 12 12:18:22.079: INFO: stderr: ""
  Aug 12 12:18:22.079: INFO: stdout: "e2e-test-crd-publish-openapi-1108-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 12 12:18:22.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2157 --namespace=crd-publish-openapi-2157 delete e2e-test-crd-publish-openapi-1108-crds test-cr'
  Aug 12 12:18:22.170: INFO: stderr: ""
  Aug 12 12:18:22.170: INFO: stdout: "e2e-test-crd-publish-openapi-1108-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/12/23 12:18:22.171
  Aug 12 12:18:22.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2157 explain e2e-test-crd-publish-openapi-1108-crds'
  Aug 12 12:18:22.794: INFO: stderr: ""
  Aug 12 12:18:22.794: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-1108-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Aug 12 12:18:24.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2157" for this suite. @ 08/12/23 12:18:24.759
• [5.391 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/12/23 12:18:24.772
  Aug 12 12:18:24.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:18:24.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:24.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:24.797
  STEP: Creating configMap with name projected-configmap-test-volume-map-9f94fffa-bc7e-4512-a5a6-7edd043e3730 @ 08/12/23 12:18:24.801
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:18:24.807
  STEP: Saw pod success @ 08/12/23 12:18:28.84
  Aug 12 12:18:28.846: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-d84e6872-d42b-4151-9530-2d71f7238414 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:18:28.866
  Aug 12 12:18:28.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7690" for this suite. @ 08/12/23 12:18:28.893
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/12/23 12:18:28.908
  Aug 12 12:18:28.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:18:28.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:28.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:28.933
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/12/23 12:18:28.937
  STEP: Saw pod success @ 08/12/23 12:18:32.967
  Aug 12 12:18:32.972: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-d6d6ab74-71d1-46eb-90c2-98c6e5159fb7 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:18:32.983
  Aug 12 12:18:33.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7512" for this suite. @ 08/12/23 12:18:33.012
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/12/23 12:18:33.023
  Aug 12 12:18:33.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sysctl @ 08/12/23 12:18:33.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:33.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:33.047
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/12/23 12:18:33.052
  STEP: Watching for error events or started pod @ 08/12/23 12:18:33.064
  STEP: Waiting for pod completion @ 08/12/23 12:18:35.07
  STEP: Checking that the pod succeeded @ 08/12/23 12:18:37.085
  STEP: Getting logs from the pod @ 08/12/23 12:18:37.086
  STEP: Checking that the sysctl is actually updated @ 08/12/23 12:18:37.097
  Aug 12 12:18:37.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-4130" for this suite. @ 08/12/23 12:18:37.103
• [4.089 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/12/23 12:18:37.113
  Aug 12 12:18:37.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:18:37.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:37.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:37.141
  STEP: Creating configMap with name projected-configmap-test-volume-e72e3b1c-7c23-4231-8d96-c3df3488833b @ 08/12/23 12:18:37.145
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:18:37.151
  STEP: Saw pod success @ 08/12/23 12:18:41.179
  Aug 12 12:18:41.183: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-db13db04-1e35-4ccd-8bad-14b5ed755875 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:18:41.193
  Aug 12 12:18:41.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7482" for this suite. @ 08/12/23 12:18:41.219
• [4.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/12/23 12:18:41.229
  Aug 12 12:18:41.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:18:41.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:41.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:41.255
  STEP: creating the pod @ 08/12/23 12:18:41.26
  STEP: submitting the pod to kubernetes @ 08/12/23 12:18:41.26
  STEP: verifying the pod is in kubernetes @ 08/12/23 12:18:43.287
  STEP: updating the pod @ 08/12/23 12:18:43.291
  Aug 12 12:18:43.809: INFO: Successfully updated pod "pod-update-9fe5f470-d038-4da3-9fde-d72135b9b7a9"
  STEP: verifying the updated pod is in kubernetes @ 08/12/23 12:18:43.813
  Aug 12 12:18:43.818: INFO: Pod update OK
  Aug 12 12:18:43.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9042" for this suite. @ 08/12/23 12:18:43.825
• [2.606 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/12/23 12:18:43.836
  Aug 12 12:18:43.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:18:43.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:43.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:43.858
  STEP: Creating projection with secret that has name projected-secret-test-785e4537-dcfb-4e3a-b035-11e1fc2988c8 @ 08/12/23 12:18:43.862
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:18:43.867
  STEP: Saw pod success @ 08/12/23 12:18:47.9
  Aug 12 12:18:47.925: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-secrets-6b29b535-7fa3-4205-b411-ae92792c9df8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:18:47.935
  Aug 12 12:18:47.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2069" for this suite. @ 08/12/23 12:18:47.962
• [4.135 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/12/23 12:18:47.974
  Aug 12 12:18:47.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption @ 08/12/23 12:18:47.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:47.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:48.002
  STEP: Waiting for the pdb to be processed @ 08/12/23 12:18:48.012
  STEP: Waiting for all pods to be running @ 08/12/23 12:18:50.06
  Aug 12 12:18:50.070: INFO: running pods: 0 < 3
  Aug 12 12:18:52.076: INFO: running pods: 2 < 3
  Aug 12 12:18:54.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4214" for this suite. @ 08/12/23 12:18:54.086
• [6.121 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/12/23 12:18:54.097
  Aug 12 12:18:54.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 12:18:54.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:54.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:54.125
  STEP: Setting up server cert @ 08/12/23 12:18:54.156
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 12:18:54.405
  STEP: Deploying the webhook pod @ 08/12/23 12:18:54.42
  STEP: Wait for the deployment to be ready @ 08/12/23 12:18:54.436
  Aug 12 12:18:54.449: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/12/23 12:18:56.465
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:18:56.481
  Aug 12 12:18:57.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/12/23 12:18:57.488
  STEP: create a pod that should be updated by the webhook @ 08/12/23 12:18:57.509
  Aug 12 12:18:57.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9524" for this suite. @ 08/12/23 12:18:57.63
  STEP: Destroying namespace "webhook-markers-4169" for this suite. @ 08/12/23 12:18:57.64
• [3.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/12/23 12:18:57.657
  Aug 12 12:18:57.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:18:57.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:18:57.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:18:57.684
  STEP: Creating configMap with name cm-test-opt-del-222617c4-543f-490a-8e8d-69cb2fd52cf7 @ 08/12/23 12:18:57.694
  STEP: Creating configMap with name cm-test-opt-upd-dea8c1d8-da26-49dc-a9bb-e4ea5817d290 @ 08/12/23 12:18:57.7
  STEP: Creating the pod @ 08/12/23 12:18:57.707
  STEP: Deleting configmap cm-test-opt-del-222617c4-543f-490a-8e8d-69cb2fd52cf7 @ 08/12/23 12:18:59.776
  STEP: Updating configmap cm-test-opt-upd-dea8c1d8-da26-49dc-a9bb-e4ea5817d290 @ 08/12/23 12:18:59.785
  STEP: Creating configMap with name cm-test-opt-create-1698d64c-5cf2-49c9-a091-9eca8618ae3a @ 08/12/23 12:18:59.792
  STEP: waiting to observe update in volume @ 08/12/23 12:18:59.798
  Aug 12 12:20:04.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2139" for this suite. @ 08/12/23 12:20:04.246
• [66.604 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/12/23 12:20:04.266
  Aug 12 12:20:04.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:20:04.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:04.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:04.295
  STEP: creating service multi-endpoint-test in namespace services-211 @ 08/12/23 12:20:04.299
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-211 to expose endpoints map[] @ 08/12/23 12:20:04.316
  Aug 12 12:20:04.328: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Aug 12 12:20:05.339: INFO: successfully validated that service multi-endpoint-test in namespace services-211 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-211 @ 08/12/23 12:20:05.339
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-211 to expose endpoints map[pod1:[100]] @ 08/12/23 12:20:07.367
  Aug 12 12:20:07.381: INFO: successfully validated that service multi-endpoint-test in namespace services-211 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-211 @ 08/12/23 12:20:07.381
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-211 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/12/23 12:20:09.407
  Aug 12 12:20:09.425: INFO: successfully validated that service multi-endpoint-test in namespace services-211 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/12/23 12:20:09.425
  Aug 12 12:20:09.425: INFO: Creating new exec pod
  Aug 12 12:20:12.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-211 exec execpod2rhnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Aug 12 12:20:12.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug 12 12:20:12.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:20:12.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-211 exec execpod2rhnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 80'
  Aug 12 12:20:12.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
  Aug 12 12:20:12.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:20:12.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-211 exec execpod2rhnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Aug 12 12:20:12.973: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug 12 12:20:12.973: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:20:12.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-211 exec execpod2rhnh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 81'
  Aug 12 12:20:13.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.58 81\nConnection to 10.152.183.58 81 port [tcp/*] succeeded!\n"
  Aug 12 12:20:13.132: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-211 @ 08/12/23 12:20:13.133
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-211 to expose endpoints map[pod2:[101]] @ 08/12/23 12:20:13.15
  Aug 12 12:20:13.173: INFO: successfully validated that service multi-endpoint-test in namespace services-211 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-211 @ 08/12/23 12:20:13.173
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-211 to expose endpoints map[] @ 08/12/23 12:20:13.202
  Aug 12 12:20:13.217: INFO: successfully validated that service multi-endpoint-test in namespace services-211 exposes endpoints map[]
  Aug 12 12:20:13.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-211" for this suite. @ 08/12/23 12:20:13.248
• [8.991 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/12/23 12:20:13.258
  Aug 12 12:20:13.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 12:20:13.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:13.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:13.282
  STEP: Setting up server cert @ 08/12/23 12:20:13.312
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 12:20:13.926
  STEP: Deploying the webhook pod @ 08/12/23 12:20:13.937
  STEP: Wait for the deployment to be ready @ 08/12/23 12:20:13.954
  Aug 12 12:20:13.964: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/12/23 12:20:15.978
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:20:15.996
  Aug 12 12:20:16.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/12/23 12:20:17.003
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/12/23 12:20:17.003
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/12/23 12:20:17.027
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/12/23 12:20:18.044
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/12/23 12:20:18.044
  STEP: Having no error when timeout is longer than webhook latency @ 08/12/23 12:20:19.084
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/12/23 12:20:19.084
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/12/23 12:20:24.132
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/12/23 12:20:24.132
  Aug 12 12:20:29.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1500" for this suite. @ 08/12/23 12:20:29.265
  STEP: Destroying namespace "webhook-markers-3035" for this suite. @ 08/12/23 12:20:29.274
• [16.026 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/12/23 12:20:29.293
  Aug 12 12:20:29.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 12:20:29.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:29.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:29.318
  STEP: Setting up server cert @ 08/12/23 12:20:29.347
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 12:20:29.582
  STEP: Deploying the webhook pod @ 08/12/23 12:20:29.589
  STEP: Wait for the deployment to be ready @ 08/12/23 12:20:29.605
  Aug 12 12:20:29.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/12/23 12:20:31.631
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:20:31.645
  Aug 12 12:20:32.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/12/23 12:20:32.65
  STEP: create a pod that should be denied by the webhook @ 08/12/23 12:20:32.673
  STEP: create a pod that causes the webhook to hang @ 08/12/23 12:20:32.687
  STEP: create a configmap that should be denied by the webhook @ 08/12/23 12:20:42.697
  STEP: create a configmap that should be admitted by the webhook @ 08/12/23 12:20:42.755
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/12/23 12:20:42.769
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/12/23 12:20:42.782
  STEP: create a namespace that bypass the webhook @ 08/12/23 12:20:42.789
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/12/23 12:20:42.857
  Aug 12 12:20:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-341" for this suite. @ 08/12/23 12:20:42.951
  STEP: Destroying namespace "webhook-markers-1329" for this suite. @ 08/12/23 12:20:42.963
  STEP: Destroying namespace "exempted-namespace-2260" for this suite. @ 08/12/23 12:20:42.972
• [13.688 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/12/23 12:20:42.986
  Aug 12 12:20:42.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 12:20:42.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:43.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:43.01
  STEP: apply creating a deployment @ 08/12/23 12:20:43.015
  Aug 12 12:20:43.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2671" for this suite. @ 08/12/23 12:20:43.043
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/12/23 12:20:43.055
  Aug 12 12:20:43.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 12:20:43.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:43.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:43.079
  STEP: Create a Replicaset @ 08/12/23 12:20:43.088
  STEP: Verify that the required pods have come up. @ 08/12/23 12:20:43.096
  Aug 12 12:20:43.101: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug 12 12:20:48.110: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 12:20:48.11
  STEP: Getting /status @ 08/12/23 12:20:48.11
  Aug 12 12:20:48.116: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/12/23 12:20:48.116
  Aug 12 12:20:48.135: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/12/23 12:20:48.135
  Aug 12 12:20:48.138: INFO: Observed &ReplicaSet event: ADDED
  Aug 12 12:20:48.138: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.138: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.139: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.139: INFO: Found replicaset test-rs in namespace replicaset-5422 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 12 12:20:48.140: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/12/23 12:20:48.14
  Aug 12 12:20:48.140: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 12 12:20:48.154: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/12/23 12:20:48.154
  Aug 12 12:20:48.158: INFO: Observed &ReplicaSet event: ADDED
  Aug 12 12:20:48.159: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.160: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.161: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.162: INFO: Observed replicaset test-rs in namespace replicaset-5422 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 12:20:48.163: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 12 12:20:48.163: INFO: Found replicaset test-rs in namespace replicaset-5422 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug 12 12:20:48.164: INFO: Replicaset test-rs has a patched status
  Aug 12 12:20:48.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5422" for this suite. @ 08/12/23 12:20:48.175
• [5.133 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/12/23 12:20:48.188
  Aug 12 12:20:48.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subpath @ 08/12/23 12:20:48.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:20:48.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:20:48.224
  STEP: Setting up data @ 08/12/23 12:20:48.23
  STEP: Creating pod pod-subpath-test-configmap-m9pl @ 08/12/23 12:20:48.248
  STEP: Creating a pod to test atomic-volume-subpath @ 08/12/23 12:20:48.248
  STEP: Saw pod success @ 08/12/23 12:21:12.348
  Aug 12 12:21:12.353: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-subpath-test-configmap-m9pl container test-container-subpath-configmap-m9pl: <nil>
  STEP: delete the pod @ 08/12/23 12:21:12.368
  STEP: Deleting pod pod-subpath-test-configmap-m9pl @ 08/12/23 12:21:12.386
  Aug 12 12:21:12.386: INFO: Deleting pod "pod-subpath-test-configmap-m9pl" in namespace "subpath-5968"
  Aug 12 12:21:12.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5968" for this suite. @ 08/12/23 12:21:12.398
• [24.219 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/12/23 12:21:12.409
  Aug 12 12:21:12.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename containers @ 08/12/23 12:21:12.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:21:12.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:21:12.434
  Aug 12 12:21:14.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8178" for this suite. @ 08/12/23 12:21:14.478
• [2.080 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/12/23 12:21:14.49
  Aug 12 12:21:14.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:21:14.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:21:14.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:21:14.513
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:21:14.517
  STEP: Saw pod success @ 08/12/23 12:21:18.551
  Aug 12 12:21:18.558: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-bf7dd527-3748-43b8-8418-2f548450dbc8 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:21:18.568
  Aug 12 12:21:18.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2498" for this suite. @ 08/12/23 12:21:18.592
• [4.112 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/12/23 12:21:18.603
  Aug 12 12:21:18.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 12:21:18.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:21:18.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:21:18.626
  STEP: Creating a test headless service @ 08/12/23 12:21:18.631
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6867.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6867.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/12/23 12:21:18.637
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6867.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6867.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/12/23 12:21:18.637
  STEP: creating a pod to probe DNS @ 08/12/23 12:21:18.637
  STEP: submitting the pod to kubernetes @ 08/12/23 12:21:18.637
  STEP: retrieving the pod @ 08/12/23 12:21:20.677
  STEP: looking for the results for each expected name from probers @ 08/12/23 12:21:20.682
  Aug 12 12:21:20.706: INFO: DNS probes using dns-6867/dns-test-73fb04ef-d180-471a-a37f-9601aa13b178 succeeded

  Aug 12 12:21:20.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 12:21:20.711
  STEP: deleting the test headless service @ 08/12/23 12:21:20.728
  STEP: Destroying namespace "dns-6867" for this suite. @ 08/12/23 12:21:20.751
• [2.158 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/12/23 12:21:20.762
  Aug 12 12:21:20.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 12:21:20.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:21:20.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:21:20.786
  Aug 12 12:21:20.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/12/23 12:21:22.277
  Aug 12 12:21:22.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-4727 --namespace=crd-publish-openapi-4727 create -f -'
  Aug 12 12:21:23.098: INFO: stderr: ""
  Aug 12 12:21:23.098: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 12 12:21:23.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-4727 --namespace=crd-publish-openapi-4727 delete e2e-test-crd-publish-openapi-3749-crds test-cr'
  Aug 12 12:21:23.236: INFO: stderr: ""
  Aug 12 12:21:23.236: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug 12 12:21:23.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-4727 --namespace=crd-publish-openapi-4727 apply -f -'
  Aug 12 12:21:24.571: INFO: stderr: ""
  Aug 12 12:21:24.571: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 12 12:21:24.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-4727 --namespace=crd-publish-openapi-4727 delete e2e-test-crd-publish-openapi-3749-crds test-cr'
  Aug 12 12:21:24.732: INFO: stderr: ""
  Aug 12 12:21:24.732: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/12/23 12:21:24.732
  Aug 12 12:21:24.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-4727 explain e2e-test-crd-publish-openapi-3749-crds'
  Aug 12 12:21:25.270: INFO: stderr: ""
  Aug 12 12:21:25.270: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-3749-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Aug 12 12:21:26.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4727" for this suite. @ 08/12/23 12:21:26.765
• [6.014 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/12/23 12:21:26.777
  Aug 12 12:21:26.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:21:26.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:21:26.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:21:26.81
  STEP: Creating service test in namespace statefulset-2500 @ 08/12/23 12:21:26.815
  STEP: Creating a new StatefulSet @ 08/12/23 12:21:26.824
  Aug 12 12:21:26.842: INFO: Found 0 stateful pods, waiting for 3
  Aug 12 12:21:36.847: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:21:36.848: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:21:36.848: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:21:36.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-2500 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 12:21:37.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:21:37.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:21:37.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/12/23 12:21:47.062
  Aug 12 12:21:47.089: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/12/23 12:21:47.089
  STEP: Updating Pods in reverse ordinal order @ 08/12/23 12:21:57.116
  Aug 12 12:21:57.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-2500 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 12:21:57.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 12:21:57.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 12:21:57.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 08/12/23 12:22:07.307
  Aug 12 12:22:07.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-2500 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 12:22:07.607: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:22:07.607: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:22:07.607: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 12:22:17.652: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 08/12/23 12:22:27.68
  Aug 12 12:22:27.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-2500 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 12:22:27.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 12:22:27.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 12:22:27.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 12:22:37.879: INFO: Deleting all statefulset in ns statefulset-2500
  Aug 12 12:22:37.882: INFO: Scaling statefulset ss2 to 0
  Aug 12 12:22:47.909: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:22:47.914: INFO: Deleting statefulset ss2
  Aug 12 12:22:47.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2500" for this suite. @ 08/12/23 12:22:47.941
• [81.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/12/23 12:22:47.957
  Aug 12 12:22:47.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:22:47.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:22:47.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:22:47.988
  STEP: Creating configMap configmap-1466/configmap-test-7d5733dd-1d81-463a-bc75-32fdf7c06d82 @ 08/12/23 12:22:47.992
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:22:48.001
  STEP: Saw pod success @ 08/12/23 12:22:52.029
  Aug 12 12:22:52.035: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-bdceb829-bfea-4009-99bc-3375066e5b58 container env-test: <nil>
  STEP: delete the pod @ 08/12/23 12:22:52.057
  Aug 12 12:22:52.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1466" for this suite. @ 08/12/23 12:22:52.107
• [4.160 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/12/23 12:22:52.119
  Aug 12 12:22:52.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:22:52.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:22:52.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:22:52.143
  Aug 12 12:22:52.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2432 version'
  Aug 12 12:22:52.222: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug 12 12:22:52.222: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-20T02:05:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug 12 12:22:52.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2432" for this suite. @ 08/12/23 12:22:52.229
• [0.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/12/23 12:22:52.243
  Aug 12 12:22:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 12:22:52.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:22:52.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:22:52.272
  STEP: Creating replication controller my-hostname-basic-ba02ff4c-f2d0-446e-8bfc-fe62ace5ad46 @ 08/12/23 12:22:52.276
  Aug 12 12:22:52.289: INFO: Pod name my-hostname-basic-ba02ff4c-f2d0-446e-8bfc-fe62ace5ad46: Found 0 pods out of 1
  Aug 12 12:22:57.296: INFO: Pod name my-hostname-basic-ba02ff4c-f2d0-446e-8bfc-fe62ace5ad46: Found 1 pods out of 1
  Aug 12 12:22:57.296: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ba02ff4c-f2d0-446e-8bfc-fe62ace5ad46" are running
  Aug 12 12:22:57.301: INFO: Pod "my-hostname-basic-ba02ff4c-f2d0-446e-8bfc-fe62ace5ad46-68ntl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:22:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:22:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:22:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:22:52 +0000 UTC Reason: Message:}])
  Aug 12 12:22:57.301: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/12/23 12:22:57.301
  Aug 12 12:22:57.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1421" for this suite. @ 08/12/23 12:22:57.444
• [5.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/12/23 12:22:57.457
  Aug 12 12:22:57.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/12/23 12:22:57.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:22:57.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:22:57.489
  Aug 12 12:22:57.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:22:58.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6116" for this suite. @ 08/12/23 12:22:58.574
• [1.126 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/12/23 12:22:58.586
  Aug 12 12:22:58.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 12:22:58.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:22:58.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:22:58.61
  STEP: Creating pod liveness-4f127abe-075f-4154-8910-a36db73e6bd5 in namespace container-probe-2022 @ 08/12/23 12:22:58.619
  Aug 12 12:23:00.643: INFO: Started pod liveness-4f127abe-075f-4154-8910-a36db73e6bd5 in namespace container-probe-2022
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 12:23:00.643
  Aug 12 12:23:00.649: INFO: Initial restart count of pod liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is 0
  Aug 12 12:23:20.715: INFO: Restart count of pod container-probe-2022/liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is now 1 (20.06621113s elapsed)
  Aug 12 12:23:40.775: INFO: Restart count of pod container-probe-2022/liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is now 2 (40.126513106s elapsed)
  Aug 12 12:24:00.833: INFO: Restart count of pod container-probe-2022/liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is now 3 (1m0.18433694s elapsed)
  Aug 12 12:24:20.897: INFO: Restart count of pod container-probe-2022/liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is now 4 (1m20.248708846s elapsed)
  Aug 12 12:25:31.114: INFO: Restart count of pod container-probe-2022/liveness-4f127abe-075f-4154-8910-a36db73e6bd5 is now 5 (2m30.465582116s elapsed)
  Aug 12 12:25:31.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 12:25:31.12
  STEP: Destroying namespace "container-probe-2022" for this suite. @ 08/12/23 12:25:31.157
• [152.580 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/12/23 12:25:31.169
  Aug 12 12:25:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subjectreview @ 08/12/23 12:25:31.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:25:31.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:25:31.194
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-8448" @ 08/12/23 12:25:31.198
  Aug 12 12:25:31.206: INFO: saUsername: "system:serviceaccount:subjectreview-8448:e2e"
  Aug 12 12:25:31.206: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-8448"}
  Aug 12 12:25:31.206: INFO: saUID: "17376ee5-3dfb-42e8-9025-d90e477d5d00"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-8448:e2e" @ 08/12/23 12:25:31.206
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-8448:e2e" @ 08/12/23 12:25:31.206
  Aug 12 12:25:31.209: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-8448:e2e" api 'list' configmaps in "subjectreview-8448" namespace @ 08/12/23 12:25:31.209
  Aug 12 12:25:31.212: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-8448:e2e" @ 08/12/23 12:25:31.213
  Aug 12 12:25:31.215: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug 12 12:25:31.215: INFO: LocalSubjectAccessReview has been verified
  Aug 12 12:25:31.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-8448" for this suite. @ 08/12/23 12:25:31.222
• [0.062 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/12/23 12:25:31.231
  Aug 12 12:25:31.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename runtimeclass @ 08/12/23 12:25:31.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:25:31.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:25:31.257
  Aug 12 12:25:31.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9470" for this suite. @ 08/12/23 12:25:31.275
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/12/23 12:25:31.292
  Aug 12 12:25:31.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption @ 08/12/23 12:25:31.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:25:31.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:25:31.321
  Aug 12 12:25:31.343: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 12 12:26:31.386: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/12/23 12:26:31.392
  Aug 12 12:26:31.420: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 12 12:26:31.431: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 12 12:26:31.454: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 12 12:26:31.463: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 12 12:26:31.487: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 12 12:26:31.497: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/12/23 12:26:31.497
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/12/23 12:26:35.545
  Aug 12 12:26:39.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2212" for this suite. @ 08/12/23 12:26:39.701
• [68.417 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/12/23 12:26:39.71
  Aug 12 12:26:39.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:26:39.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:39.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:39.732
  STEP: creating the pod @ 08/12/23 12:26:39.737
  STEP: setting up watch @ 08/12/23 12:26:39.737
  STEP: submitting the pod to kubernetes @ 08/12/23 12:26:39.843
  STEP: verifying the pod is in kubernetes @ 08/12/23 12:26:39.854
  STEP: verifying pod creation was observed @ 08/12/23 12:26:39.859
  STEP: deleting the pod gracefully @ 08/12/23 12:26:41.876
  STEP: verifying pod deletion was observed @ 08/12/23 12:26:41.886
  Aug 12 12:26:43.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-665" for this suite. @ 08/12/23 12:26:43.281
• [3.580 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/12/23 12:26:43.293
  Aug 12 12:26:43.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename runtimeclass @ 08/12/23 12:26:43.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:43.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:43.319
  STEP: getting /apis @ 08/12/23 12:26:43.324
  STEP: getting /apis/node.k8s.io @ 08/12/23 12:26:43.329
  STEP: getting /apis/node.k8s.io/v1 @ 08/12/23 12:26:43.33
  STEP: creating @ 08/12/23 12:26:43.332
  STEP: watching @ 08/12/23 12:26:43.355
  Aug 12 12:26:43.355: INFO: starting watch
  STEP: getting @ 08/12/23 12:26:43.362
  STEP: listing @ 08/12/23 12:26:43.367
  STEP: patching @ 08/12/23 12:26:43.371
  STEP: updating @ 08/12/23 12:26:43.377
  Aug 12 12:26:43.384: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/12/23 12:26:43.385
  STEP: deleting a collection @ 08/12/23 12:26:43.401
  Aug 12 12:26:43.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5439" for this suite. @ 08/12/23 12:26:43.429
• [0.145 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/12/23 12:26:43.438
  Aug 12 12:26:43.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 12:26:43.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:43.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:43.468
  STEP: apply creating a deployment @ 08/12/23 12:26:43.471
  Aug 12 12:26:43.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7693" for this suite. @ 08/12/23 12:26:43.497
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/12/23 12:26:43.511
  Aug 12 12:26:43.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:26:43.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:43.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:43.534
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:26:43.538
  STEP: Saw pod success @ 08/12/23 12:26:47.574
  Aug 12 12:26:47.579: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-0bf7e241-c529-46f1-a7c4-f94c20c21725 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:26:47.605
  Aug 12 12:26:47.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5364" for this suite. @ 08/12/23 12:26:47.633
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/12/23 12:26:47.646
  Aug 12 12:26:47.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:26:47.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:47.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:47.676
  STEP: Creating configMap with name configmap-test-volume-map-9d3b4ab9-8216-4e86-9b0d-7b5ab3425293 @ 08/12/23 12:26:47.68
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:26:47.686
  STEP: Saw pod success @ 08/12/23 12:26:51.718
  Aug 12 12:26:51.722: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-874b08f9-c33d-4132-8718-a783cea1eb14 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:26:51.733
  Aug 12 12:26:51.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-893" for this suite. @ 08/12/23 12:26:51.765
• [4.127 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/12/23 12:26:51.775
  Aug 12 12:26:51.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 12:26:51.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:51.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:51.801
  Aug 12 12:26:51.805: INFO: Creating deployment "test-recreate-deployment"
  Aug 12 12:26:51.816: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug 12 12:26:51.825: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Aug 12 12:26:53.836: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug 12 12:26:53.840: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug 12 12:26:53.853: INFO: Updating deployment test-recreate-deployment
  Aug 12 12:26:53.853: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug 12 12:26:53.967: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8955  919ed57e-3a7a-400a-8b60-bd2dceaeed66 9391 2 2023-08-12 12:26:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003066808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-12 12:26:53 +0000 UTC,LastTransitionTime:2023-08-12 12:26:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-12 12:26:53 +0000 UTC,LastTransitionTime:2023-08-12 12:26:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 12 12:26:53.975: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-8955  621b1e11-37fe-4b53-8eb3-36282281281e 9387 1 2023-08-12 12:26:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 919ed57e-3a7a-400a-8b60-bd2dceaeed66 0xc00331b987 0xc00331b988}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919ed57e-3a7a-400a-8b60-bd2dceaeed66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00331bb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 12:26:53.976: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug 12 12:26:53.976: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-8955  2a558057-680c-41f6-9ca7-2d4f2500bf6a 9379 2 2023-08-12 12:26:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 919ed57e-3a7a-400a-8b60-bd2dceaeed66 0xc00331bdf7 0xc00331bdf8}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919ed57e-3a7a-400a-8b60-bd2dceaeed66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00331bf08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 12:26:53.980: INFO: Pod "test-recreate-deployment-54757ffd6c-vkwb4" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-vkwb4 test-recreate-deployment-54757ffd6c- deployment-8955  57c78c19-5f31-4eea-aa1c-b9189444b1c7 9390 0 2023-08-12 12:26:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 621b1e11-37fe-4b53-8eb3-36282281281e 0xc003066bb7 0xc003066bb8}] [] [{kube-controller-manager Update v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"621b1e11-37fe-4b53-8eb3-36282281281e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:26:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5h9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5h9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:26:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:26:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:26:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:26:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:,StartTime:2023-08-12 12:26:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 12:26:53.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8955" for this suite. @ 08/12/23 12:26:53.986
• [2.221 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/12/23 12:26:53.998
  Aug 12 12:26:53.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:26:53.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:54.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:54.022
  STEP: Creating the pod @ 08/12/23 12:26:54.027
  Aug 12 12:26:56.599: INFO: Successfully updated pod "labelsupdatee22ea158-a225-4920-869c-960ff8e5d977"
  Aug 12 12:26:58.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6323" for this suite. @ 08/12/23 12:26:58.642
• [4.661 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/12/23 12:26:58.659
  Aug 12 12:26:58.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename watch @ 08/12/23 12:26:58.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:58.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:58.906
  STEP: creating a watch on configmaps @ 08/12/23 12:26:58.911
  STEP: creating a new configmap @ 08/12/23 12:26:58.913
  STEP: modifying the configmap once @ 08/12/23 12:26:58.919
  STEP: closing the watch once it receives two notifications @ 08/12/23 12:26:58.93
  Aug 12 12:26:58.930: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2968  04345114-ecb4-4f86-a92d-8f0b612bbd9e 9440 0 2023-08-12 12:26:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-12 12:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 12:26:58.930: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2968  04345114-ecb4-4f86-a92d-8f0b612bbd9e 9441 0 2023-08-12 12:26:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-12 12:26:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/12/23 12:26:58.93
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/12/23 12:26:58.94
  STEP: deleting the configmap @ 08/12/23 12:26:58.942
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/12/23 12:26:58.95
  Aug 12 12:26:58.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2968  04345114-ecb4-4f86-a92d-8f0b612bbd9e 9443 0 2023-08-12 12:26:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-12 12:26:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 12:26:58.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2968  04345114-ecb4-4f86-a92d-8f0b612bbd9e 9444 0 2023-08-12 12:26:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-12 12:26:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 12:26:58.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2968" for this suite. @ 08/12/23 12:26:58.956
• [0.305 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/12/23 12:26:58.969
  Aug 12 12:26:58.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:26:58.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:26:58.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:26:58.992
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-474b00da-e7e5-46ad-a659-0f79428fd30b @ 08/12/23 12:26:59.004
  STEP: Creating the pod @ 08/12/23 12:26:59.012
  STEP: Updating configmap projected-configmap-test-upd-474b00da-e7e5-46ad-a659-0f79428fd30b @ 08/12/23 12:27:01.07
  STEP: waiting to observe update in volume @ 08/12/23 12:27:01.078
  Aug 12 12:28:29.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3680" for this suite. @ 08/12/23 12:28:29.637
• [90.678 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/12/23 12:28:29.653
  Aug 12 12:28:29.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:28:29.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:28:29.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:28:29.677
  STEP: creating service in namespace services-2638 @ 08/12/23 12:28:29.682
  STEP: creating service affinity-nodeport-transition in namespace services-2638 @ 08/12/23 12:28:29.682
  STEP: creating replication controller affinity-nodeport-transition in namespace services-2638 @ 08/12/23 12:28:29.712
  I0812 12:28:29.729274      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-2638, replica count: 3
  I0812 12:28:32.780622      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 12:28:32.795: INFO: Creating new exec pod
  Aug 12 12:28:35.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug 12 12:28:35.993: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug 12 12:28:35.993: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:28:35.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.230 80'
  Aug 12 12:28:36.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.230 80\nConnection to 10.152.183.230 80 port [tcp/http] succeeded!\n"
  Aug 12 12:28:36.152: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:28:36.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.84.203 30801'
  Aug 12 12:28:36.325: INFO: stderr: "+ nc -v -t -w 2 172.31.84.203 30801\n+ echo hostName\nConnection to 172.31.84.203 30801 port [tcp/*] succeeded!\n"
  Aug 12 12:28:36.325: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:28:36.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.32.142 30801'
  Aug 12 12:28:36.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.32.142 30801\nConnection to 172.31.32.142 30801 port [tcp/*] succeeded!\n"
  Aug 12 12:28:36.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:28:36.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.32.142:30801/ ; done'
  Aug 12 12:28:36.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n"
  Aug 12 12:28:36.773: INFO: stdout: "\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-zrpc5\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-zbjxl\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-zrpc5\naffinity-nodeport-transition-zrpc5\naffinity-nodeport-transition-zbjxl\naffinity-nodeport-transition-zrpc5\naffinity-nodeport-transition-zbjxl\naffinity-nodeport-transition-zbjxl\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-zbjxl\naffinity-nodeport-transition-zbjxl"
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-zrpc5
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.773: INFO: Received response from host: affinity-nodeport-transition-zrpc5
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zrpc5
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zrpc5
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.774: INFO: Received response from host: affinity-nodeport-transition-zbjxl
  Aug 12 12:28:36.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2638 exec execpod-affinitym2g7c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.32.142:30801/ ; done'
  Aug 12 12:28:37.058: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:30801/\n"
  Aug 12 12:28:37.058: INFO: stdout: "\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc\naffinity-nodeport-transition-kbvbc"
  Aug 12 12:28:37.058: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.058: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.058: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.058: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Received response from host: affinity-nodeport-transition-kbvbc
  Aug 12 12:28:37.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:28:37.065: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2638, will wait for the garbage collector to delete the pods @ 08/12/23 12:28:37.092
  Aug 12 12:28:37.160: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.087643ms
  Aug 12 12:28:37.261: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.212008ms
  STEP: Destroying namespace "services-2638" for this suite. @ 08/12/23 12:28:39.699
• [10.055 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/12/23 12:28:39.709
  Aug 12 12:28:39.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 12:28:39.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:28:39.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:28:39.741
  STEP: creating a ReplicationController @ 08/12/23 12:28:39.75
  STEP: waiting for RC to be added @ 08/12/23 12:28:39.759
  STEP: waiting for available Replicas @ 08/12/23 12:28:39.759
  STEP: patching ReplicationController @ 08/12/23 12:28:42.618
  STEP: waiting for RC to be modified @ 08/12/23 12:28:42.63
  STEP: patching ReplicationController status @ 08/12/23 12:28:42.63
  STEP: waiting for RC to be modified @ 08/12/23 12:28:42.638
  STEP: waiting for available Replicas @ 08/12/23 12:28:42.638
  STEP: fetching ReplicationController status @ 08/12/23 12:28:42.643
  STEP: patching ReplicationController scale @ 08/12/23 12:28:42.648
  STEP: waiting for RC to be modified @ 08/12/23 12:28:42.656
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/12/23 12:28:42.656
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/12/23 12:28:44.769
  STEP: updating ReplicationController status @ 08/12/23 12:28:44.775
  STEP: waiting for RC to be modified @ 08/12/23 12:28:44.783
  STEP: listing all ReplicationControllers @ 08/12/23 12:28:44.783
  STEP: checking that ReplicationController has expected values @ 08/12/23 12:28:44.787
  STEP: deleting ReplicationControllers by collection @ 08/12/23 12:28:44.787
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/12/23 12:28:44.801
  Aug 12 12:28:44.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0812 12:28:44.842189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9731" for this suite. @ 08/12/23 12:28:44.848
• [5.148 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/12/23 12:28:44.859
  Aug 12 12:28:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:28:44.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:28:44.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:28:44.887
  E0812 12:28:45.842325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:46.842712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:47.842817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:48.842895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:49.843049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:50.843735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:51.843818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:52.843952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:53.844074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:54.844683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:55.845763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:56.846772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:57.846935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:58.847004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:28:59.847999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:00.848573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:01.848705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/12/23 12:29:01.896
  E0812 12:29:02.849779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:03.849878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:04.849987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:05.850170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:06.850227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 12:29:06.902
  STEP: Ensuring resource quota status is calculated @ 08/12/23 12:29:06.909
  E0812 12:29:07.850854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:08.851259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 08/12/23 12:29:08.915
  STEP: Ensuring resource quota status captures configMap creation @ 08/12/23 12:29:08.93
  E0812 12:29:09.851761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:10.851898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 08/12/23 12:29:10.937
  STEP: Ensuring resource quota status released usage @ 08/12/23 12:29:10.946
  E0812 12:29:11.852436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:12.852715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:12.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3493" for this suite. @ 08/12/23 12:29:12.959
• [28.110 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 08/12/23 12:29:12.971
  Aug 12 12:29:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 12:29:12.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:29:12.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:29:12.997
  STEP: Creating simple DaemonSet "daemon-set" @ 08/12/23 12:29:13.031
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 12:29:13.038
  Aug 12 12:29:13.044: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:13.044: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:13.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:29:13.049: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 12:29:13.852751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:14.056: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:14.056: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:14.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:29:14.061: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 12:29:14.852815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:15.056: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:15.056: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:29:15.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 12:29:15.061: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 08/12/23 12:29:15.065
  Aug 12 12:29:15.071: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/12/23 12:29:15.071
  Aug 12 12:29:15.084: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/12/23 12:29:15.084
  Aug 12 12:29:15.087: INFO: Observed &DaemonSet event: ADDED
  Aug 12 12:29:15.087: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.088: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.088: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.089: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.089: INFO: Found daemon set daemon-set in namespace daemonsets-8929 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 12 12:29:15.089: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/12/23 12:29:15.089
  STEP: watching for the daemon set status to be patched @ 08/12/23 12:29:15.099
  Aug 12 12:29:15.102: INFO: Observed &DaemonSet event: ADDED
  Aug 12 12:29:15.102: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.103: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.103: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.104: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.104: INFO: Observed daemon set daemon-set in namespace daemonsets-8929 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 12 12:29:15.104: INFO: Observed &DaemonSet event: MODIFIED
  Aug 12 12:29:15.104: INFO: Found daemon set daemon-set in namespace daemonsets-8929 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug 12 12:29:15.104: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 12:29:15.11
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8929, will wait for the garbage collector to delete the pods @ 08/12/23 12:29:15.11
  Aug 12 12:29:15.176: INFO: Deleting DaemonSet.extensions daemon-set took: 10.298887ms
  Aug 12 12:29:15.277: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820658ms
  E0812 12:29:15.853548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:16.853969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:16.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:29:16.882: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 12:29:16.887: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10116"},"items":null}

  Aug 12 12:29:16.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10116"},"items":null}

  Aug 12 12:29:16.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8929" for this suite. @ 08/12/23 12:29:16.916
• [3.953 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/12/23 12:29:16.926
  Aug 12 12:29:16.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:29:16.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:29:16.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:29:16.947
  STEP: Creating service test in namespace statefulset-27 @ 08/12/23 12:29:16.952
  STEP: Creating stateful set ss in namespace statefulset-27 @ 08/12/23 12:29:16.965
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-27 @ 08/12/23 12:29:16.978
  Aug 12 12:29:16.985: INFO: Found 0 stateful pods, waiting for 1
  E0812 12:29:17.854090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:18.854436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:19.854773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:20.854889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:21.855273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:22.855610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:23.855914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:24.856023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:25.856319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:26.856697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:26.993: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/12/23 12:29:26.993
  Aug 12 12:29:26.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 12:29:27.162: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:29:27.162: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:29:27.162: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 12:29:27.168: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0812 12:29:27.857688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:28.857818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:29.858068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:30.858185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:31.858907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:32.859005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:33.859140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:34.859464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:35.859574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:36.860149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:37.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 12:29:37.174: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:29:37.196: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug 12 12:29:37.196: INFO: ss-0  ip-172-31-32-142  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:16 +0000 UTC  }]
  Aug 12 12:29:37.196: INFO: 
  Aug 12 12:29:37.196: INFO: StatefulSet ss has not reached scale 3, at 1
  E0812 12:29:37.861123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:38.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994232003s
  E0812 12:29:38.861898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:39.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989451776s
  E0812 12:29:39.862323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:40.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98399124s
  E0812 12:29:40.862396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:41.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976868393s
  E0812 12:29:41.862912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:42.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970876697s
  E0812 12:29:42.863895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:43.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965383144s
  E0812 12:29:43.864856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:44.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959192368s
  E0812 12:29:44.864998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:45.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953254097s
  E0812 12:29:45.865105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:46.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.853685ms
  E0812 12:29:46.865353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-27 @ 08/12/23 12:29:47.249
  Aug 12 12:29:47.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 12:29:47.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 12:29:47.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 12:29:47.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 12:29:47.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 12:29:47.587: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 12 12:29:47.588: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 12:29:47.588: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 12:29:47.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 12:29:47.759: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 12 12:29:47.759: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 12:29:47.759: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 12:29:47.765: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0812 12:29:47.865962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:48.866789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:49.867087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:50.867183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:51.867270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:52.867412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:53.867546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:54.867816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:55.868130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:56.868663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:57.772: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:29:57.772: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:29:57.772: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/12/23 12:29:57.772
  Aug 12 12:29:57.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0812 12:29:57.869520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:29:57.943: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:29:57.943: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:29:57.943: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 12:29:57.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 12:29:58.111: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:29:58.111: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:29:58.111: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 12:29:58.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-27 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 12:29:58.288: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 12:29:58.288: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 12:29:58.288: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 12:29:58.288: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:29:58.293: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0812 12:29:58.869700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:29:59.870651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:00.870776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:01.871385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:02.871689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:03.871808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:04.872125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:05.873079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:06.873289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:07.873563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:08.305: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 12:30:08.305: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 12:30:08.305: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 12:30:08.323: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug 12 12:30:08.323: INFO: ss-0  ip-172-31-32-142  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:16 +0000 UTC  }]
  Aug 12 12:30:08.323: INFO: ss-1  ip-172-31-84-203  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC  }]
  Aug 12 12:30:08.323: INFO: ss-2  ip-172-31-79-233  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC  }]
  Aug 12 12:30:08.323: INFO: 
  Aug 12 12:30:08.323: INFO: StatefulSet ss has not reached scale 0, at 3
  E0812 12:30:08.873897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:09.330: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Aug 12 12:30:09.330: INFO: ss-0  ip-172-31-32-142  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:17 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:16 +0000 UTC  }]
  Aug 12 12:30:09.330: INFO: ss-2  ip-172-31-79-233  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-12 12:29:37 +0000 UTC  }]
  Aug 12 12:30:09.330: INFO: 
  Aug 12 12:30:09.330: INFO: StatefulSet ss has not reached scale 0, at 2
  E0812 12:30:09.874927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:10.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988190845s
  E0812 12:30:10.875253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:11.341: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.981649072s
  E0812 12:30:11.875363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:12.347: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.976107888s
  E0812 12:30:12.875487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:13.354: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.970440094s
  E0812 12:30:13.875813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:14.359: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964247156s
  E0812 12:30:14.875952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:15.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.959149295s
  E0812 12:30:15.876085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:16.371: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.953464183s
  E0812 12:30:16.876808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:17.376: INFO: Verifying statefulset ss doesn't scale past 0 for another 947.237008ms
  E0812 12:30:17.876923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-27 @ 08/12/23 12:30:18.376
  Aug 12 12:30:18.382: INFO: Scaling statefulset ss to 0
  Aug 12 12:30:18.396: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:30:18.401: INFO: Deleting all statefulset in ns statefulset-27
  Aug 12 12:30:18.406: INFO: Scaling statefulset ss to 0
  Aug 12 12:30:18.421: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:30:18.425: INFO: Deleting statefulset ss
  Aug 12 12:30:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-27" for this suite. @ 08/12/23 12:30:18.453
• [61.537 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/12/23 12:30:18.465
  Aug 12 12:30:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:30:18.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:30:18.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:30:18.489
  STEP: creating service in namespace services-160 @ 08/12/23 12:30:18.494
  STEP: creating service affinity-clusterip-transition in namespace services-160 @ 08/12/23 12:30:18.494
  STEP: creating replication controller affinity-clusterip-transition in namespace services-160 @ 08/12/23 12:30:18.508
  I0812 12:30:18.518992      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-160, replica count: 3
  E0812 12:30:18.877777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:19.878540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:20.879581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 12:30:21.570615      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 12:30:21.581: INFO: Creating new exec pod
  E0812 12:30:21.879909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:22.880024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:23.880970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:24.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-160 exec execpod-affinity2w9wb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Aug 12 12:30:24.774: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug 12 12:30:24.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:30:24.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-160 exec execpod-affinity2w9wb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.232 80'
  E0812 12:30:24.881770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:30:24.941: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.232 80\nConnection to 10.152.183.232 80 port [tcp/http] succeeded!\n"
  Aug 12 12:30:24.941: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:30:24.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-160 exec execpod-affinity2w9wb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.232:80/ ; done'
  Aug 12 12:30:25.231: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n"
  Aug 12 12:30:25.231: INFO: stdout: "\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-9b5wt\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-gtrhh\naffinity-clusterip-transition-9b5wt\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-9b5wt\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-9b5wt"
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-9b5wt
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-gtrhh
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-9b5wt
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-9b5wt
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.231: INFO: Received response from host: affinity-clusterip-transition-9b5wt
  Aug 12 12:30:25.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-160 exec execpod-affinity2w9wb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.232:80/ ; done'
  Aug 12 12:30:25.517: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.232:80/\n"
  Aug 12 12:30:25.517: INFO: stdout: "\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj\naffinity-clusterip-transition-vqjmj"
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.517: INFO: Received response from host: affinity-clusterip-transition-vqjmj
  Aug 12 12:30:25.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:30:25.524: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-160, will wait for the garbage collector to delete the pods @ 08/12/23 12:30:25.545
  Aug 12 12:30:25.612: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.386722ms
  Aug 12 12:30:25.713: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.816054ms
  E0812 12:30:25.882352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:26.883011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:27.883965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-160" for this suite. @ 08/12/23 12:30:28.14
• [9.686 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/12/23 12:30:28.152
  Aug 12 12:30:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubelet-test @ 08/12/23 12:30:28.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:30:28.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:30:28.177
  Aug 12 12:30:28.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3689" for this suite. @ 08/12/23 12:30:28.224
• [0.081 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/12/23 12:30:28.234
  Aug 12 12:30:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 12:30:28.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:30:28.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:30:28.26
  Aug 12 12:30:28.269: INFO: Got root ca configmap in namespace "svcaccounts-9911"
  Aug 12 12:30:28.278: INFO: Deleted root ca configmap in namespace "svcaccounts-9911"
  STEP: waiting for a new root ca configmap created @ 08/12/23 12:30:28.779
  Aug 12 12:30:28.786: INFO: Recreated root ca configmap in namespace "svcaccounts-9911"
  Aug 12 12:30:28.792: INFO: Updated root ca configmap in namespace "svcaccounts-9911"
  E0812 12:30:28.884179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 08/12/23 12:30:29.293
  Aug 12 12:30:29.299: INFO: Reconciled root ca configmap in namespace "svcaccounts-9911"
  Aug 12 12:30:29.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9911" for this suite. @ 08/12/23 12:30:29.305
• [1.080 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/12/23 12:30:29.319
  Aug 12 12:30:29.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename taint-single-pod @ 08/12/23 12:30:29.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:30:29.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:30:29.344
  Aug 12 12:30:29.348: INFO: Waiting up to 1m0s for all nodes to be ready
  E0812 12:30:29.885156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:30.885769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:31.886101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:32.886234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:33.887093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:34.887181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:35.887309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:36.887776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:37.887890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:38.888155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:39.888836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:40.888968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:41.889098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:42.889750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:43.890316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:44.890761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:45.890894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:46.891745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:47.892246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:48.892305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:49.893116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:50.893872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:51.894678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:52.894912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:53.895446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:54.895724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:55.896178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:56.896686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:57.897738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:58.898020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:30:59.898141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:00.898382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:01.899268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:02.899405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:03.899525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:04.900034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:05.900160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:06.900658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:07.900808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:08.901009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:09.901859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:10.902122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:11.902238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:12.902523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:13.903274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:14.903986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:15.904765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:16.904891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:17.905007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:18.905747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:19.906053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:20.906187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:21.906240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:22.906385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:23.907063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:24.907194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:25.907451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:26.907560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:27.908093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:28.908377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:31:29.368: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 12:31:29.374: INFO: Starting informer...
  STEP: Starting pod... @ 08/12/23 12:31:29.374
  Aug 12 12:31:29.594: INFO: Pod is running on ip-172-31-32-142. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/12/23 12:31:29.594
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/12/23 12:31:29.608
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/12/23 12:31:29.614
  Aug 12 12:31:29.614: INFO: Pod wasn't evicted. Proceeding
  Aug 12 12:31:29.614: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/12/23 12:31:29.632
  STEP: Waiting some time to make sure that toleration time passed. @ 08/12/23 12:31:29.641
  E0812 12:31:29.909089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:30.909264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:31.909787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:32.910079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:33.910654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:34.910778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:35.911623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:36.911981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:37.912239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:38.912494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:39.913581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:40.913854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:41.914510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:42.914620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:43.915658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:44.915942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:45.916683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:46.916715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:47.916918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:48.917741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:49.917997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:50.918183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:51.918326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:52.918427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:53.918580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:54.918894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:55.919989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:56.920457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:57.920703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:58.920801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:31:59.920928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:00.921745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:01.921875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:02.922151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:03.922276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:04.922585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:05.923125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:06.923442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:07.923566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:08.924232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:09.924673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:10.925764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:11.926429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:12.926551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:13.926823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:14.927696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:15.927970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:16.928459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:17.928785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:18.928928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:19.929049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:20.929763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:21.930197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:22.930352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:23.930655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:24.930776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:25.930977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:26.931455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:27.931526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:28.931845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:29.932941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:30.933127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:31.933687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:32.933369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:33.934059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:34.934166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:35.934907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:36.935176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:37.935437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:38.935724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:39.935979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:40.937048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:41.937181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:42.937307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:43.937976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:32:44.641: INFO: Pod wasn't evicted. Test successful
  Aug 12 12:32:44.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4263" for this suite. @ 08/12/23 12:32:44.647
• [135.337 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/12/23 12:32:44.658
  Aug 12 12:32:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 12:32:44.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:32:44.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:32:44.681
  STEP: creating a Deployment @ 08/12/23 12:32:44.689
  STEP: waiting for Deployment to be created @ 08/12/23 12:32:44.698
  STEP: waiting for all Replicas to be Ready @ 08/12/23 12:32:44.701
  Aug 12 12:32:44.703: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.703: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.717: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.717: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.735: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.735: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.781: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 12 12:32:44.782: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0812 12:32:44.938391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:45.938602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:32:46.263: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 12 12:32:46.263: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 12 12:32:46.379: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/12/23 12:32:46.379
  W0812 12:32:46.394799      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 12 12:32:46.397: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/12/23 12:32:46.398
  Aug 12 12:32:46.402: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.403: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.403: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.404: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.405: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 0
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.412: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.412: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.447: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.447: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:46.458: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:46.458: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:46.478: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:46.478: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  E0812 12:32:46.938853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:47.938962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:32:48.303: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:48.303: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:48.348: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  STEP: listing Deployments @ 08/12/23 12:32:48.348
  Aug 12 12:32:48.354: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/12/23 12:32:48.354
  Aug 12 12:32:48.369: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/12/23 12:32:48.369
  Aug 12 12:32:48.380: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:48.387: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:48.412: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:48.436: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:48.446: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0812 12:32:48.939212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:32:49.298: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:49.339: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 12 12:32:49.360: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0812 12:32:49.939293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:32:50.406: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/12/23 12:32:50.439
  STEP: fetching the DeploymentStatus @ 08/12/23 12:32:50.448
  Aug 12 12:32:50.454: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 1
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:50.455: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:50.456: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 2
  Aug 12 12:32:50.456: INFO: observed Deployment test-deployment in namespace deployment-3789 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/12/23 12:32:50.456
  Aug 12 12:32:50.471: INFO: observed event type MODIFIED
  Aug 12 12:32:50.471: INFO: observed event type MODIFIED
  Aug 12 12:32:50.471: INFO: observed event type MODIFIED
  Aug 12 12:32:50.471: INFO: observed event type MODIFIED
  Aug 12 12:32:50.471: INFO: observed event type MODIFIED
  Aug 12 12:32:50.472: INFO: observed event type MODIFIED
  Aug 12 12:32:50.472: INFO: observed event type MODIFIED
  Aug 12 12:32:50.472: INFO: observed event type MODIFIED
  Aug 12 12:32:50.473: INFO: observed event type MODIFIED
  Aug 12 12:32:50.473: INFO: observed event type MODIFIED
  Aug 12 12:32:50.481: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug 12 12:32:50.486: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-3789  97b0b717-c0f0-4ae4-a4c4-5b020d8f2a18 11087 3 2023-08-12 12:32:44 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 648c9041-1fde-42bd-a1d8-b1929a931f9b 0xc002ac7fd7 0xc002ac7fd8}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:32:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"648c9041-1fde-42bd-a1d8-b1929a931f9b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:32:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fca000 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Aug 12 12:32:50.494: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-3789  d5780630-afff-4f3b-b04f-9ab231f86f51 11193 4 2023-08-12 12:32:46 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 648c9041-1fde-42bd-a1d8-b1929a931f9b 0xc004fca067 0xc004fca068}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:32:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"648c9041-1fde-42bd-a1d8-b1929a931f9b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:32:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fca0f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Aug 12 12:32:50.500: INFO: pod: "test-deployment-5b5dcbcd95-rhqc5":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-rhqc5 test-deployment-5b5dcbcd95- deployment-3789  963778b4-ca44-427d-9ca4-35e310e6865b 11187 0 2023-08-12 12:32:46 +0000 UTC 2023-08-12 12:32:51 +0000 UTC 0xc003375b18 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 d5780630-afff-4f3b-b04f-9ab231f86f51 0xc003375b47 0xc003375b48}] [] [{kube-controller-manager Update v1 2023-08-12 12:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5780630-afff-4f3b-b04f-9ab231f86f51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nsrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nsrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.139,StartTime:2023-08-12 12:32:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://fa4a10966690d54e9672521da5c6e7a41b72c0f9fdefa446b7723e3fa4cc568c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.139,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug 12 12:32:50.500: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-3789  983064e8-0826-41bb-a7f5-17bb659468f0 11182 2 2023-08-12 12:32:48 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 648c9041-1fde-42bd-a1d8-b1929a931f9b 0xc004fca157 0xc004fca158}] [] [{kube-controller-manager Update apps/v1 2023-08-12 12:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"648c9041-1fde-42bd-a1d8-b1929a931f9b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 12:32:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fca1e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Aug 12 12:32:50.507: INFO: pod: "test-deployment-6fc78d85c6-tlkdp":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-tlkdp test-deployment-6fc78d85c6- deployment-3789  7b8de478-f574-4d4a-9127-0499965bd449 11199 0 2023-08-12 12:32:48 +0000 UTC 2023-08-12 12:32:51 +0000 UTC 0xc004fca4b8 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 983064e8-0826-41bb-a7f5-17bb659468f0 0xc004fca697 0xc004fca698}] [] [{kube-controller-manager Update v1 2023-08-12 12:32:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"983064e8-0826-41bb-a7f5-17bb659468f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:32:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvjc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvjc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.140,StartTime:2023-08-12 12:32:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:32:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5723c5780851cc91d0ba155b54e918da94666ed865b37efee5782ba082452499,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug 12 12:32:50.507: INFO: pod: "test-deployment-6fc78d85c6-vtk5l":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-vtk5l test-deployment-6fc78d85c6- deployment-3789  ce677941-912a-4ae8-89c1-17bd9e686e68 11181 0 2023-08-12 12:32:49 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 983064e8-0826-41bb-a7f5-17bb659468f0 0xc004fca9f7 0xc004fca9f8}] [] [{kube-controller-manager Update v1 2023-08-12 12:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"983064e8-0826-41bb-a7f5-17bb659468f0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 12:32:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2nsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2nsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 12:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:192.168.177.31,StartTime:2023-08-12 12:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 12:32:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a54697a99b4c2758fac6af0a151ae60b394bcc41fc3143ce188876aa4c0afc9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.177.31,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug 12 12:32:50.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3789" for this suite. @ 08/12/23 12:32:50.514
• [5.865 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/12/23 12:32:50.525
  Aug 12 12:32:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:32:50.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:32:50.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:32:50.552
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/12/23 12:32:50.558
  E0812 12:32:50.940308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:51.940749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:52.940777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:53.941768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:32:54.588
  Aug 12 12:32:54.592: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-ceeea625-979e-494d-8fdb-28a705605b2f container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:32:54.619
  Aug 12 12:32:54.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2718" for this suite. @ 08/12/23 12:32:54.646
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/12/23 12:32:54.656
  Aug 12 12:32:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename podtemplate @ 08/12/23 12:32:54.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:32:54.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:32:54.682
  Aug 12 12:32:54.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9789" for this suite. @ 08/12/23 12:32:54.733
• [0.086 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/12/23 12:32:54.744
  Aug 12 12:32:54.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:32:54.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:32:54.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:32:54.767
  STEP: Counting existing ResourceQuota @ 08/12/23 12:32:54.772
  E0812 12:32:54.941882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:55.941930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:56.942069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:57.942915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:32:58.943501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 12:32:59.777
  STEP: Ensuring resource quota status is calculated @ 08/12/23 12:32:59.783
  E0812 12:32:59.944058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:00.944879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 08/12/23 12:33:01.79
  STEP: Creating a NodePort Service @ 08/12/23 12:33:01.816
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/12/23 12:33:01.849
  STEP: Ensuring resource quota status captures service creation @ 08/12/23 12:33:01.879
  E0812 12:33:01.945810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:02.945933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 08/12/23 12:33:03.886
  STEP: Ensuring resource quota status released usage @ 08/12/23 12:33:03.942
  E0812 12:33:03.946859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:04.947023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:05.947798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:05.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3592" for this suite. @ 08/12/23 12:33:05.955
• [11.221 seconds]
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/12/23 12:33:05.965
  Aug 12 12:33:05.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:33:05.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:33:05.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:33:05.995
  STEP: creating service endpoint-test2 in namespace services-5995 @ 08/12/23 12:33:06
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5995 to expose endpoints map[] @ 08/12/23 12:33:06.021
  Aug 12 12:33:06.038: INFO: successfully validated that service endpoint-test2 in namespace services-5995 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5995 @ 08/12/23 12:33:06.038
  E0812 12:33:06.947945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:07.948304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5995 to expose endpoints map[pod1:[80]] @ 08/12/23 12:33:08.068
  Aug 12 12:33:08.084: INFO: successfully validated that service endpoint-test2 in namespace services-5995 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/12/23 12:33:08.084
  Aug 12 12:33:08.084: INFO: Creating new exec pod
  E0812 12:33:08.948806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:09.948986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:10.949294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:11.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 12 12:33:11.269: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:11.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:33:11.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.75 80'
  Aug 12 12:33:11.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.75 80\nConnection to 10.152.183.75 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:11.435: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-5995 @ 08/12/23 12:33:11.435
  E0812 12:33:11.949778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:12.950144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5995 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/12/23 12:33:13.463
  Aug 12 12:33:13.482: INFO: successfully validated that service endpoint-test2 in namespace services-5995 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/12/23 12:33:13.482
  E0812 12:33:13.950298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:14.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 12 12:33:14.658: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:14.658: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:33:14.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.75 80'
  Aug 12 12:33:14.828: INFO: stderr: "+ nc -v -t -w 2 10.152.183.75 80\n+ echo hostName\nConnection to 10.152.183.75 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:14.828: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5995 @ 08/12/23 12:33:14.828
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5995 to expose endpoints map[pod2:[80]] @ 08/12/23 12:33:14.843
  E0812 12:33:14.950960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:15.875: INFO: successfully validated that service endpoint-test2 in namespace services-5995 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/12/23 12:33:15.875
  E0812 12:33:15.951785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:16.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0812 12:33:16.951783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:17.046: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:17.046: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:33:17.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-5995 exec execpodb76jv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.75 80'
  Aug 12 12:33:17.209: INFO: stderr: "+ nc -v -t -w 2 10.152.183.75 80\n+ echo hostName\nConnection to 10.152.183.75 80 port [tcp/http] succeeded!\n"
  Aug 12 12:33:17.209: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-5995 @ 08/12/23 12:33:17.209
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5995 to expose endpoints map[] @ 08/12/23 12:33:17.237
  E0812 12:33:17.952778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:33:18.261: INFO: successfully validated that service endpoint-test2 in namespace services-5995 exposes endpoints map[]
  Aug 12 12:33:18.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5995" for this suite. @ 08/12/23 12:33:18.294
• [12.340 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/12/23 12:33:18.31
  Aug 12 12:33:18.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-pred @ 08/12/23 12:33:18.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:33:18.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:33:18.341
  Aug 12 12:33:18.344: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 12 12:33:18.354: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 12:33:18.359: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-32-142 before test
  Aug 12 12:33:18.365: INFO: nginx-ingress-controller-kubernetes-worker-5944p from ingress-nginx-kubernetes-worker started at 2023-08-12 12:31:40 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.365: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:18.365: INFO: execpodb76jv from services-5995 started at 2023-08-12 12:33:08 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.365: INFO: 	Container agnhost-container ready: true, restart count 0
  Aug 12 12:33:18.365: INFO: sonobuoy from sonobuoy started at 2023-08-12 12:09:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.365: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 12 12:33:18.365: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-5tdg5 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:18.366: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:18.366: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 12:33:18.366: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-79-233 before test
  Aug 12 12:33:18.373: INFO: default-http-backend-kubernetes-worker-65fc475d49-9qsc9 from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:29 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: nginx-ingress-controller-kubernetes-worker-9f2bf from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:28 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: coredns-5c7f76ccb8-qnk7d from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container coredns ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: kube-state-metrics-5b95b4459c-v9mbb from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: metrics-server-v0.5.2-6cf8c8b69c-zqxs2 from kube-system started at 2023-08-12 11:54:23 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: dashboard-metrics-scraper-6b8586b5c9-vtmth from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: kubernetes-dashboard-6869f4cd5f-ttr98 from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-bvmhq from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:18.373: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 12:33:18.373: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-203 before test
  Aug 12 12:33:18.381: INFO: nginx-ingress-controller-kubernetes-worker-rdq6z from ingress-nginx-kubernetes-worker started at 2023-08-12 11:59:55 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.381: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:18.381: INFO: calico-kube-controllers-5f769b769b-ttpqk from kube-system started at 2023-08-12 12:03:40 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:18.381: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 12 12:33:18.381: INFO: sonobuoy-e2e-job-d2bbfc40cf5e42c2 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:18.381: INFO: 	Container e2e ready: true, restart count 0
  Aug 12 12:33:18.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:18.381: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-r2r8l from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:18.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:18.381: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-32-142 @ 08/12/23 12:33:18.401
  STEP: verifying the node has the label node ip-172-31-79-233 @ 08/12/23 12:33:18.418
  STEP: verifying the node has the label node ip-172-31-84-203 @ 08/12/23 12:33:18.436
  Aug 12 12:33:18.455: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-9qsc9 requesting resource cpu=10m on Node ip-172-31-79-233
  Aug 12 12:33:18.455: INFO: Pod nginx-ingress-controller-kubernetes-worker-5944p requesting resource cpu=0m on Node ip-172-31-32-142
  Aug 12 12:33:18.455: INFO: Pod nginx-ingress-controller-kubernetes-worker-9f2bf requesting resource cpu=0m on Node ip-172-31-79-233
  Aug 12 12:33:18.456: INFO: Pod nginx-ingress-controller-kubernetes-worker-rdq6z requesting resource cpu=0m on Node ip-172-31-84-203
  Aug 12 12:33:18.456: INFO: Pod calico-kube-controllers-5f769b769b-ttpqk requesting resource cpu=0m on Node ip-172-31-84-203
  Aug 12 12:33:18.456: INFO: Pod coredns-5c7f76ccb8-qnk7d requesting resource cpu=100m on Node ip-172-31-79-233
  Aug 12 12:33:18.458: INFO: Pod kube-state-metrics-5b95b4459c-v9mbb requesting resource cpu=0m on Node ip-172-31-79-233
  Aug 12 12:33:18.458: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-zqxs2 requesting resource cpu=5m on Node ip-172-31-79-233
  Aug 12 12:33:18.458: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-vtmth requesting resource cpu=0m on Node ip-172-31-79-233
  Aug 12 12:33:18.458: INFO: Pod kubernetes-dashboard-6869f4cd5f-ttr98 requesting resource cpu=0m on Node ip-172-31-79-233
  Aug 12 12:33:18.459: INFO: Pod execpodb76jv requesting resource cpu=0m on Node ip-172-31-32-142
  Aug 12 12:33:18.459: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-32-142
  Aug 12 12:33:18.459: INFO: Pod sonobuoy-e2e-job-d2bbfc40cf5e42c2 requesting resource cpu=0m on Node ip-172-31-84-203
  Aug 12 12:33:18.459: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdb230855f564179-5tdg5 requesting resource cpu=0m on Node ip-172-31-32-142
  Aug 12 12:33:18.459: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdb230855f564179-bvmhq requesting resource cpu=0m on Node ip-172-31-79-233
  Aug 12 12:33:18.459: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdb230855f564179-r2r8l requesting resource cpu=0m on Node ip-172-31-84-203
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/12/23 12:33:18.46
  Aug 12 12:33:18.460: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-32-142
  Aug 12 12:33:18.473: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-79-233
  Aug 12 12:33:18.482: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-84-203
  E0812 12:33:18.953592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:19.954039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/12/23 12:33:20.515
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657.177aa2c9ff672186], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3695/filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657 to ip-172-31-32-142] @ 08/12/23 12:33:20.521
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657.177aa2ca2b928eaf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657.177aa2ca2cafd21f], Reason = [Created], Message = [Created container filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657.177aa2ca32779750], Reason = [Started], Message = [Started container filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434.177aa2c9ff67a0b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3695/filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434 to ip-172-31-79-233] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434.177aa2ca2af9a616], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434.177aa2ca2c2f31b4], Reason = [Created], Message = [Created container filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434.177aa2ca30949d82], Reason = [Started], Message = [Started container filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090.177aa2ca003f203f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3695/filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090 to ip-172-31-84-203] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090.177aa2ca2b6aaf29], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090.177aa2ca2d7e051e], Reason = [Created], Message = [Created container filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090.177aa2ca343b11b2], Reason = [Started], Message = [Started container filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090] @ 08/12/23 12:33:20.522
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177aa2ca7925ad8e], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 08/12/23 12:33:20.538
  E0812 12:33:20.954041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-79-233 @ 08/12/23 12:33:21.539
  STEP: verifying the node doesn't have the label node @ 08/12/23 12:33:21.556
  STEP: removing the label node off the node ip-172-31-84-203 @ 08/12/23 12:33:21.561
  STEP: verifying the node doesn't have the label node @ 08/12/23 12:33:21.577
  STEP: removing the label node off the node ip-172-31-32-142 @ 08/12/23 12:33:21.584
  STEP: verifying the node doesn't have the label node @ 08/12/23 12:33:21.608
  Aug 12 12:33:21.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3695" for this suite. @ 08/12/23 12:33:21.624
• [3.328 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/12/23 12:33:21.648
  Aug 12 12:33:21.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:33:21.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:33:21.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:33:21.674
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:33:21.681
  E0812 12:33:21.954557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:22.955081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:23.955264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:24.955298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:33:25.714
  Aug 12 12:33:25.720: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-570075cd-51bd-44de-80da-5db4440bd765 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:33:25.729
  Aug 12 12:33:25.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1976" for this suite. @ 08/12/23 12:33:25.754
• [4.115 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/12/23 12:33:25.765
  Aug 12 12:33:25.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-pred @ 08/12/23 12:33:25.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:33:25.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:33:25.794
  Aug 12 12:33:25.798: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 12 12:33:25.808: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 12:33:25.813: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-32-142 before test
  Aug 12 12:33:25.821: INFO: nginx-ingress-controller-kubernetes-worker-5944p from ingress-nginx-kubernetes-worker started at 2023-08-12 12:31:40 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.821: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:25.821: INFO: filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657 from sched-pred-3695 started at 2023-08-12 12:33:18 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.821: INFO: 	Container filler-pod-d2dad57b-1be4-4a05-b262-21f0260bb657 ready: true, restart count 0
  Aug 12 12:33:25.821: INFO: sonobuoy from sonobuoy started at 2023-08-12 12:09:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.821: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 12 12:33:25.821: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-5tdg5 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:25.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:25.821: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 12:33:25.821: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-79-233 before test
  Aug 12 12:33:25.828: INFO: default-http-backend-kubernetes-worker-65fc475d49-9qsc9 from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:29 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.828: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 12 12:33:25.828: INFO: nginx-ingress-controller-kubernetes-worker-9f2bf from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:28 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.828: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:25.828: INFO: coredns-5c7f76ccb8-qnk7d from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.828: INFO: 	Container coredns ready: true, restart count 0
  Aug 12 12:33:25.828: INFO: kube-state-metrics-5b95b4459c-v9mbb from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.828: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 12 12:33:25.828: INFO: metrics-server-v0.5.2-6cf8c8b69c-zqxs2 from kube-system started at 2023-08-12 11:54:23 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:25.829: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: dashboard-metrics-scraper-6b8586b5c9-vtmth from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.829: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: kubernetes-dashboard-6869f4cd5f-ttr98 from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.829: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434 from sched-pred-3695 started at 2023-08-12 12:33:18 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.829: INFO: 	Container filler-pod-e5cfbe57-c04b-4e1d-a682-745249e5b434 ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-bvmhq from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:25.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 12:33:25.829: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-203 before test
  Aug 12 12:33:25.836: INFO: nginx-ingress-controller-kubernetes-worker-rdq6z from ingress-nginx-kubernetes-worker started at 2023-08-12 11:59:55 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.836: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 12:33:25.836: INFO: calico-kube-controllers-5f769b769b-ttpqk from kube-system started at 2023-08-12 12:03:40 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.836: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 12 12:33:25.836: INFO: filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090 from sched-pred-3695 started at 2023-08-12 12:33:18 +0000 UTC (1 container statuses recorded)
  Aug 12 12:33:25.836: INFO: 	Container filler-pod-ff5058f2-bcbd-42c3-a671-8cf1b76e8090 ready: true, restart count 0
  Aug 12 12:33:25.836: INFO: sonobuoy-e2e-job-d2bbfc40cf5e42c2 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:25.836: INFO: 	Container e2e ready: true, restart count 0
  Aug 12 12:33:25.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:25.836: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-r2r8l from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 12:33:25.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 12:33:25.837: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/12/23 12:33:25.837
  E0812 12:33:25.955800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:26.956037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/12/23 12:33:27.867
  STEP: Trying to apply a random label on the found node. @ 08/12/23 12:33:27.89
  STEP: verifying the node has the label kubernetes.io/e2e-4d5936a6-f884-4154-8128-75822db3e397 95 @ 08/12/23 12:33:27.902
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/12/23 12:33:27.913
  E0812 12:33:27.956306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:28.956553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.32.142 on the node which pod4 resides and expect not scheduled @ 08/12/23 12:33:29.937
  E0812 12:33:29.957024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:30.957651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:31.958485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:32.959410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:33.959569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:34.959748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:35.960443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:36.960704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:37.961119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:38.962055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:39.962913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:40.963275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:41.963429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:42.963562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:43.964560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:44.964763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:45.965550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:46.965645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:47.966388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:48.966509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:49.966926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:50.967264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:51.967402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:52.967458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:53.967706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:54.967862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:55.968485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:56.969175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:57.969273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:58.969404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:33:59.969453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:00.970246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:01.970526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:02.970632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:03.970902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:04.970996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:05.971685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:06.971805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:07.972188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:08.973311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:09.973989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:10.974332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:11.974423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:12.975348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:13.975602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:14.975717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:15.976104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:16.976224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:17.976585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:18.976756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:19.977756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:20.978236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:21.978381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:22.978690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:23.979670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:24.979752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:25.980132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:26.980240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:27.980580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:28.980751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:29.980902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:30.981286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:31.981788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:32.982103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:33.982198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:34.982367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:35.983046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:36.983270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:37.983572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:38.984080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:39.984487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:40.985245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:41.985371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:42.985498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:43.985723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:44.985853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:45.986431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:46.987011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:47.987297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:48.987983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:49.988116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:50.988195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:51.988631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:52.988684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:53.989740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:54.990367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:55.990933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:56.991058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:57.991303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:58.992316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:34:59.992771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:00.993293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:01.993431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:02.993919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:03.994160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:04.994287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:05.994878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:06.995378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:07.995649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:08.996761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:09.997751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:10.998152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:11.998433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:12.999357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:13.999483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:14.999705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:16.000157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:17.001215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:18.001773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:19.002750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:20.003006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:21.003315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:22.003557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:23.003782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:24.003926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:25.004462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:26.005112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:27.005992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:28.006116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:29.006997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:30.007675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:31.007751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:32.007993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:33.008137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:34.008407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:35.009271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:36.009868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:37.010874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:38.011134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:39.011211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:40.011445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:41.012313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:42.012600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:43.012679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:44.012834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:45.013744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:46.014252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:47.014939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:48.015185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:49.015904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:50.016046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:51.016783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:52.017743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:53.018074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:54.018580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:55.018709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:56.019309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:57.019819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:58.020102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:35:59.020224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:00.020474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:01.021294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:02.021757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:03.021819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:04.022079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:05.022293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:06.022908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:07.023875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:08.024000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:09.024838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:10.024964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:11.025292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:12.025748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:13.026780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:14.027055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:15.027135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:16.027677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:17.027738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:18.028232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:19.028334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:20.028460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:21.028992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:22.029177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:23.029833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:24.030043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:25.030677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:26.031193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:27.031327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:28.031555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:29.032269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:30.032661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:31.033044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:32.033537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:33.033767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:34.033865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:35.034575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:36.035124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:37.035866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:38.035969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:39.036122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:40.036336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:41.037345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:42.037741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:43.038663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:44.038799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:45.038857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:46.039436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:47.040494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:48.040723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:49.040964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:50.041094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:51.041247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:52.041758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:53.042537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:54.042767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:55.043846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:56.044201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:57.044371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:58.044715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:36:59.044940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:00.045748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:01.046297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:02.046423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:03.047074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:04.047197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:05.047898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:06.048467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:07.049252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:08.049807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:09.050186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:10.050438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:11.050809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:12.050948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:13.051085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:14.051208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:15.051985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:16.052539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:17.052732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:18.052968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:19.054013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:20.054963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:21.055015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:22.055610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:23.056251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:24.056590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:25.057116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:26.058116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:27.058841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:28.058970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:29.059714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:30.060825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:31.061764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:32.061929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:33.062488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:34.062687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:35.062889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:36.063503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:37.063554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:38.063622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:39.064344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:40.064484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:41.065351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:42.065653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:43.066508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:44.066580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:45.066710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:46.067241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:47.068176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:48.068587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:49.068610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:50.068690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:51.069352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:52.069486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:53.069595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:54.069701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:55.069795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:56.070375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:57.071117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:58.071991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:37:59.073046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:00.073180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:01.074105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:02.074197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:03.075004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:04.075109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:05.076046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:06.076390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:07.077106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:08.077245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:09.077783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:10.077891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:11.078614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:12.078985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:13.080081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:14.080330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:15.081014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:16.081081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:17.082119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:18.082212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:19.083082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:20.083209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:21.083993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:22.084353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:23.085063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:24.085343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:25.086138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:26.086982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:27.087084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:28.087209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:29.087300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-4d5936a6-f884-4154-8128-75822db3e397 off the node ip-172-31-32-142 @ 08/12/23 12:38:29.948
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d5936a6-f884-4154-8128-75822db3e397 @ 08/12/23 12:38:29.966
  Aug 12 12:38:29.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-753" for this suite. @ 08/12/23 12:38:29.981
• [304.224 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/12/23 12:38:29.99
  Aug 12 12:38:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-runtime @ 08/12/23 12:38:29.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:38:30.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:38:30.017
  STEP: create the container @ 08/12/23 12:38:30.022
  W0812 12:38:30.035617      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/12/23 12:38:30.035
  E0812 12:38:30.087595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:31.088294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:32.088675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/12/23 12:38:33.059
  STEP: the container should be terminated @ 08/12/23 12:38:33.065
  STEP: the termination message should be set @ 08/12/23 12:38:33.065
  Aug 12 12:38:33.065: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/12/23 12:38:33.065
  Aug 12 12:38:33.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0812 12:38:33.088450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-runtime-1757" for this suite. @ 08/12/23 12:38:33.092
• [3.111 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 08/12/23 12:38:33.102
  Aug 12 12:38:33.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 12:38:33.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:38:33.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:38:33.124
  Aug 12 12:38:33.164: INFO: Create a RollingUpdate DaemonSet
  Aug 12 12:38:33.172: INFO: Check that daemon pods launch on every node of the cluster
  Aug 12 12:38:33.177: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:33.178: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:33.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:38:33.182: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 12:38:34.088690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:34.188: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:34.188: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:34.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 12:38:34.193: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  E0812 12:38:35.089253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:35.190: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:35.190: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:35.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 12:38:35.197: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Aug 12 12:38:35.197: INFO: Update the DaemonSet to trigger a rollout
  Aug 12 12:38:35.212: INFO: Updating DaemonSet daemon-set
  E0812 12:38:36.090248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:36.233: INFO: Roll back the DaemonSet before rollout is complete
  Aug 12 12:38:36.245: INFO: Updating DaemonSet daemon-set
  Aug 12 12:38:36.245: INFO: Make sure DaemonSet rollback is complete
  Aug 12 12:38:36.250: INFO: Wrong image for pod: daemon-set-227c8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug 12 12:38:36.250: INFO: Pod daemon-set-227c8 is not available
  Aug 12 12:38:36.255: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:36.255: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0812 12:38:37.090357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:37.270: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:37.270: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0812 12:38:38.090774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:38.267: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:38.267: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0812 12:38:39.090934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:39.263: INFO: Pod daemon-set-vwzhl is not available
  Aug 12 12:38:39.268: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 12:38:39.268: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 12:38:39.278
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5729, will wait for the garbage collector to delete the pods @ 08/12/23 12:38:39.278
  Aug 12 12:38:39.343: INFO: Deleting DaemonSet.extensions daemon-set took: 8.639574ms
  Aug 12 12:38:39.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.110823ms
  E0812 12:38:40.091252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:40.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 12:38:40.651: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 12:38:40.656: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12491"},"items":null}

  Aug 12 12:38:40.662: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12491"},"items":null}

  Aug 12 12:38:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5729" for this suite. @ 08/12/23 12:38:40.688
• [7.597 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/12/23 12:38:40.701
  Aug 12 12:38:40.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:38:40.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:38:40.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:38:40.731
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/12/23 12:38:40.735
  Aug 12 12:38:40.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2959 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug 12 12:38:40.824: INFO: stderr: ""
  Aug 12 12:38:40.824: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/12/23 12:38:40.824
  Aug 12 12:38:40.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2959 delete pods e2e-test-httpd-pod'
  E0812 12:38:41.092262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:42.092554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:43.092888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:43.180: INFO: stderr: ""
  Aug 12 12:38:43.180: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 12 12:38:43.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2959" for this suite. @ 08/12/23 12:38:43.187
• [2.495 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/12/23 12:38:43.197
  Aug 12 12:38:43.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:38:43.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:38:43.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:38:43.221
  STEP: Counting existing ResourceQuota @ 08/12/23 12:38:43.225
  E0812 12:38:44.093406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:45.093530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:46.094247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:47.094342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:48.094488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 12:38:48.231
  STEP: Ensuring resource quota status is calculated @ 08/12/23 12:38:48.237
  E0812 12:38:49.095556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:50.095952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/12/23 12:38:50.244
  STEP: Ensuring resource quota status captures replication controller creation @ 08/12/23 12:38:50.26
  E0812 12:38:51.096888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:52.097005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/12/23 12:38:52.266
  STEP: Ensuring resource quota status released usage @ 08/12/23 12:38:52.276
  E0812 12:38:53.097754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:54.097878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:38:54.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9248" for this suite. @ 08/12/23 12:38:54.29
• [11.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/12/23 12:38:54.3
  Aug 12 12:38:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/12/23 12:38:54.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:38:54.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:38:54.325
  STEP: create the container to handle the HTTPGet hook request. @ 08/12/23 12:38:54.34
  E0812 12:38:55.098855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:56.099343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/12/23 12:38:56.366
  E0812 12:38:57.099460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:38:58.099615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/12/23 12:38:58.388
  E0812 12:38:59.099709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:00.099874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/12/23 12:39:00.411
  Aug 12 12:39:00.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5435" for this suite. @ 08/12/23 12:39:00.444
• [6.154 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/12/23 12:39:00.455
  Aug 12 12:39:00.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:39:00.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:39:00.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:39:00.479
  STEP: Creating a pod to test downward api env vars @ 08/12/23 12:39:00.484
  E0812 12:39:01.100413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:02.100758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:03.101748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:04.101933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:39:04.513
  Aug 12 12:39:04.518: INFO: Trying to get logs from node ip-172-31-32-142 pod downward-api-42941d51-d2ba-4d94-88fa-abd894a607aa container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:39:04.527
  Aug 12 12:39:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1019" for this suite. @ 08/12/23 12:39:04.556
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/12/23 12:39:04.566
  Aug 12 12:39:04.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption @ 08/12/23 12:39:04.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:39:04.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:39:04.592
  Aug 12 12:39:04.616: INFO: Waiting up to 1m0s for all nodes to be ready
  E0812 12:39:05.102223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:06.103277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:07.104095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:08.104202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:09.105117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:10.105759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:11.106658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:12.106736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:13.106870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:14.107002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:15.107137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:16.107218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:17.107330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:18.107648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:19.107782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:20.108100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:21.108913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:22.109079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:23.109199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:24.109331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:25.109527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:26.110349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:27.110551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:28.110918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:29.111619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:30.111770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:31.112456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:32.112681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:33.113557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:34.113671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:35.114358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:36.114888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:37.115971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:38.116354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:39.116419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:40.116706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:41.117781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:42.118485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:43.119595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:44.119877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:45.120711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:46.121313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:47.121390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:48.121496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:49.121647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:50.121811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:51.122442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:52.122640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:53.123481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:54.124023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:55.124173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:56.124621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:57.125488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:58.125751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:39:59.126804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:00.127016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:01.127824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:02.128140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:03.129207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:04.129751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:04.639: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/12/23 12:40:04.643
  Aug 12 12:40:04.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/12/23 12:40:04.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:04.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:04.667
  STEP: Finding an available node @ 08/12/23 12:40:04.671
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/12/23 12:40:04.671
  E0812 12:40:05.130566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:06.131276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/12/23 12:40:06.701
  Aug 12 12:40:06.718: INFO: found a healthy node: ip-172-31-32-142
  E0812 12:40:07.132104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:08.132123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:09.133223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:10.134077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:11.134802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:12.135167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:12.813: INFO: pods created so far: [1 1 1]
  Aug 12 12:40:12.813: INFO: length of pods created so far: 3
  E0812 12:40:13.136150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:14.136276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:15.136397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:16.137175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:16.826: INFO: pods created so far: [2 2 1]
  E0812 12:40:17.137743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:18.137884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:19.138594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:20.138732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:21.138799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:22.139010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:23.139153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:23.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:40:23.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-259" for this suite. @ 08/12/23 12:40:24.042
  STEP: Destroying namespace "sched-preemption-857" for this suite. @ 08/12/23 12:40:24.052
• [79.496 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/12/23 12:40:24.063
  Aug 12 12:40:24.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 12:40:24.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:24.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:24.094
  Aug 12 12:40:24.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 12:40:24.139472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:25.139817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:26.140382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0812 12:40:26.668275      19 warnings.go:70] unknown field "alpha"
  W0812 12:40:26.668307      19 warnings.go:70] unknown field "beta"
  W0812 12:40:26.668318      19 warnings.go:70] unknown field "delta"
  W0812 12:40:26.668327      19 warnings.go:70] unknown field "epsilon"
  W0812 12:40:26.668337      19 warnings.go:70] unknown field "gamma"
  E0812 12:40:27.141428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:27.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5023" for this suite. @ 08/12/23 12:40:27.269
• [3.216 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/12/23 12:40:27.28
  Aug 12 12:40:27.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 12:40:27.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:27.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:27.304
  Aug 12 12:40:27.325: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0812 12:40:28.141569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:29.141696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:30.141833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:31.142470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:32.143112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:32.331: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 12:40:32.331
  STEP: Scaling up "test-rs" replicaset  @ 08/12/23 12:40:32.331
  Aug 12 12:40:32.357: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/12/23 12:40:32.357
  W0812 12:40:32.370507      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 12 12:40:32.372: INFO: observed ReplicaSet test-rs in namespace replicaset-770 with ReadyReplicas 1, AvailableReplicas 1
  Aug 12 12:40:32.388: INFO: observed ReplicaSet test-rs in namespace replicaset-770 with ReadyReplicas 1, AvailableReplicas 1
  Aug 12 12:40:32.408: INFO: observed ReplicaSet test-rs in namespace replicaset-770 with ReadyReplicas 1, AvailableReplicas 1
  Aug 12 12:40:32.420: INFO: observed ReplicaSet test-rs in namespace replicaset-770 with ReadyReplicas 1, AvailableReplicas 1
  E0812 12:40:33.143237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:33.454: INFO: observed ReplicaSet test-rs in namespace replicaset-770 with ReadyReplicas 2, AvailableReplicas 2
  Aug 12 12:40:33.886: INFO: observed Replicaset test-rs in namespace replicaset-770 with ReadyReplicas 3 found true
  Aug 12 12:40:33.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-770" for this suite. @ 08/12/23 12:40:33.892
• [6.623 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/12/23 12:40:33.906
  Aug 12 12:40:33.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:40:33.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:33.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:33.929
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/12/23 12:40:33.934
  E0812 12:40:34.144247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:35.144685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:36.144742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:37.144817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:40:37.966
  Aug 12 12:40:37.971: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-90642f48-fca3-47d2-af66-0dd81bf33082 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:40:37.991
  Aug 12 12:40:38.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-616" for this suite. @ 08/12/23 12:40:38.02
• [4.124 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/12/23 12:40:38.031
  Aug 12 12:40:38.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/12/23 12:40:38.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:38.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:38.056
  STEP: create the container to handle the HTTPGet hook request. @ 08/12/23 12:40:38.067
  E0812 12:40:38.145648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:39.146420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/12/23 12:40:40.097
  E0812 12:40:40.147331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:41.148036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/12/23 12:40:42.12
  STEP: delete the pod with lifecycle hook @ 08/12/23 12:40:42.144
  E0812 12:40:42.148208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:43.148746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:44.148888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:40:44.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3656" for this suite. @ 08/12/23 12:40:44.175
• [6.153 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/12/23 12:40:44.185
  Aug 12 12:40:44.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 12:40:44.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:40:44.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:40:44.21
  STEP: creating the pod with failed condition @ 08/12/23 12:40:44.215
  E0812 12:40:45.149013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:46.149777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:47.149864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:48.149994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:49.150181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:50.150283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:51.150398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:52.150659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:53.150788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:54.150913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:55.150972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:56.151712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:57.151774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:58.151978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:40:59.153048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:00.153157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:01.154092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:02.154380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:03.155460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:04.155566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:05.155707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:06.155873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:07.156442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:08.156681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:09.157778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:10.157943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:11.158785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:12.159295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:13.159386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:14.159527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:15.159643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:16.160313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:17.160760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:18.160931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:19.161843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:20.162153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:21.162788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:22.162898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:23.163834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:24.163956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:25.164113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:26.164228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:27.164363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:28.164742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:29.164862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:30.165003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:31.165091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:32.165779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:33.165880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:34.166191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:35.167120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:36.167923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:37.168010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:38.168146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:39.168817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:40.168922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:41.169528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:42.169650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:43.169792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:44.170076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:45.170211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:46.170890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:47.170970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:48.171304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:49.172156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:50.172531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:51.172910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:52.173197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:53.173280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:54.173411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:55.173928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:56.174434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:57.174600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:58.174882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:41:59.174967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:00.175916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:01.176318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:02.177290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:03.177736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:04.177863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:05.178423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:06.179052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:07.179627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:08.179910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:09.180468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:10.180662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:11.181741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:12.182075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:13.182780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:14.182896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:15.183038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:16.183255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:17.184249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:18.184339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:19.184469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:20.184736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:21.185364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:22.185438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:23.185614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:24.185825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:25.186693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:26.187261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:27.187833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:28.188079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:29.188882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:30.189740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:31.190564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:32.190855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:33.191697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:34.191783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:35.192761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:36.193349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:37.193705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:38.193948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:39.194074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:40.194762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:41.194860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:42.195916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:43.196627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:44.196678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 08/12/23 12:42:44.228
  Aug 12 12:42:44.746: INFO: Successfully updated pod "var-expansion-49ec3a81-015f-458e-97f0-22df596db187"
  STEP: waiting for pod running @ 08/12/23 12:42:44.746
  E0812 12:42:45.196816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:46.197386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/12/23 12:42:46.757
  Aug 12 12:42:46.757: INFO: Deleting pod "var-expansion-49ec3a81-015f-458e-97f0-22df596db187" in namespace "var-expansion-1189"
  Aug 12 12:42:46.768: INFO: Wait up to 5m0s for pod "var-expansion-49ec3a81-015f-458e-97f0-22df596db187" to be fully deleted
  E0812 12:42:47.197489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:48.197614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:49.198423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:50.198582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:51.198708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:52.199169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:53.199267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:54.199394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:55.199510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:56.200232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:57.200335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:58.200475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:42:59.201532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:00.201826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:01.202177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:02.202287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:03.203247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:04.203385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:05.204090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:06.204227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:07.204366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:08.204733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:09.205399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:10.205498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:11.205729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:12.206041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:13.207087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:14.207184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:15.207274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:16.207951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:17.208547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:18.208671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:43:18.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1189" for this suite. @ 08/12/23 12:43:18.876
• [154.700 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/12/23 12:43:18.886
  Aug 12 12:43:18.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:43:18.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:43:18.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:43:18.923
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/12/23 12:43:18.927
  E0812 12:43:19.209384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:20.209514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:21.210024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:22.210458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:43:22.957
  Aug 12 12:43:22.961: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-965e86ee-8c0c-4b20-ba49-e4cac1a5a1b7 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:43:22.981
  Aug 12 12:43:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-64" for this suite. @ 08/12/23 12:43:23.009
• [4.131 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/12/23 12:43:23.017
  Aug 12 12:43:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 12:43:23.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:43:23.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:43:23.041
  STEP: Create a ReplicaSet @ 08/12/23 12:43:23.045
  STEP: Verify that the required pods have come up @ 08/12/23 12:43:23.052
  Aug 12 12:43:23.058: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0812 12:43:23.211068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:24.211359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:25.211436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:26.212256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:27.213139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:43:28.065: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/12/23 12:43:28.065
  Aug 12 12:43:28.069: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/12/23 12:43:28.069
  STEP: DeleteCollection of the ReplicaSets @ 08/12/23 12:43:28.074
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/12/23 12:43:28.087
  Aug 12 12:43:28.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3412" for this suite. @ 08/12/23 12:43:28.103
• [5.105 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/12/23 12:43:28.123
  Aug 12 12:43:28.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 12:43:28.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:43:28.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:43:28.165
  STEP: Creating a pod to test service account token:  @ 08/12/23 12:43:28.169
  E0812 12:43:28.213974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:29.214074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:30.214290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:31.214940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:43:32.199
  Aug 12 12:43:32.205: INFO: Trying to get logs from node ip-172-31-32-142 pod test-pod-9d2e2d24-f3fa-4133-8f0a-85fe49a488ef container agnhost-container: <nil>
  E0812 12:43:32.215409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 08/12/23 12:43:32.216
  Aug 12 12:43:32.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2610" for this suite. @ 08/12/23 12:43:32.244
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/12/23 12:43:32.258
  Aug 12 12:43:32.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 12:43:32.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:43:32.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:43:32.298
  STEP: create the rc1 @ 08/12/23 12:43:32.311
  STEP: create the rc2 @ 08/12/23 12:43:32.324
  E0812 12:43:33.215626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:34.215972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:35.216225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:36.216534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:37.216702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:38.217478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/12/23 12:43:38.349
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/12/23 12:43:39.073
  STEP: wait for the rc to be deleted @ 08/12/23 12:43:39.085
  E0812 12:43:39.218091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:40.218219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:41.218741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:42.219004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:43.219197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:43:44.123: INFO: 71 pods remaining
  Aug 12 12:43:44.123: INFO: 71 pods has nil DeletionTimestamp
  Aug 12 12:43:44.123: INFO: 
  E0812 12:43:44.222017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:45.222300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:46.224296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:47.232243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:48.233925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/12/23 12:43:49.109
  W0812 12:43:49.116379      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 12:43:49.116: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 12:43:49.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bs8t" in namespace "gc-4257"
  Aug 12 12:43:49.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p4mw" in namespace "gc-4257"
  Aug 12 12:43:49.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pdfv" in namespace "gc-4257"
  Aug 12 12:43:49.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-567kk" in namespace "gc-4257"
  Aug 12 12:43:49.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-579q8" in namespace "gc-4257"
  E0812 12:43:49.234318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:43:49.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-59v25" in namespace "gc-4257"
  Aug 12 12:43:49.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-5f4ks" in namespace "gc-4257"
  Aug 12 12:43:49.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kwr9" in namespace "gc-4257"
  Aug 12 12:43:49.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n4gb" in namespace "gc-4257"
  Aug 12 12:43:49.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-5skf2" in namespace "gc-4257"
  Aug 12 12:43:49.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f5zj" in namespace "gc-4257"
  Aug 12 12:43:49.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jv8f" in namespace "gc-4257"
  Aug 12 12:43:49.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-78ccr" in namespace "gc-4257"
  Aug 12 12:43:49.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d8sg" in namespace "gc-4257"
  Aug 12 12:43:49.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n4r9" in namespace "gc-4257"
  Aug 12 12:43:49.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rvgh" in namespace "gc-4257"
  Aug 12 12:43:49.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tz4r" in namespace "gc-4257"
  Aug 12 12:43:49.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-896rj" in namespace "gc-4257"
  Aug 12 12:43:49.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kckz" in namespace "gc-4257"
  Aug 12 12:43:49.540: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lx5f" in namespace "gc-4257"
  Aug 12 12:43:49.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-94trd" in namespace "gc-4257"
  Aug 12 12:43:49.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-98r5r" in namespace "gc-4257"
  Aug 12 12:43:49.606: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kj9r" in namespace "gc-4257"
  Aug 12 12:43:49.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xcml" in namespace "gc-4257"
  Aug 12 12:43:49.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcdnw" in namespace "gc-4257"
  Aug 12 12:43:49.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-bggn5" in namespace "gc-4257"
  Aug 12 12:43:49.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxbst" in namespace "gc-4257"
  Aug 12 12:43:49.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4r82" in namespace "gc-4257"
  Aug 12 12:43:49.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr56f" in namespace "gc-4257"
  Aug 12 12:43:49.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6tlb" in namespace "gc-4257"
  Aug 12 12:43:49.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-d786z" in namespace "gc-4257"
  Aug 12 12:43:49.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4hr5" in namespace "gc-4257"
  Aug 12 12:43:49.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6n6w" in namespace "gc-4257"
  Aug 12 12:43:49.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc8rp" in namespace "gc-4257"
  Aug 12 12:43:49.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh9pm" in namespace "gc-4257"
  Aug 12 12:43:49.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpdh9" in namespace "gc-4257"
  Aug 12 12:43:49.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjst7" in namespace "gc-4257"
  Aug 12 12:43:49.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmvj5" in namespace "gc-4257"
  Aug 12 12:43:49.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5pwv" in namespace "gc-4257"
  Aug 12 12:43:49.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7q6x" in namespace "gc-4257"
  Aug 12 12:43:50.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkgs7" in namespace "gc-4257"
  Aug 12 12:43:50.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9v9m" in namespace "gc-4257"
  Aug 12 12:43:50.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj8wj" in namespace "gc-4257"
  Aug 12 12:43:50.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkzql" in namespace "gc-4257"
  Aug 12 12:43:50.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-js4nb" in namespace "gc-4257"
  Aug 12 12:43:50.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxbkm" in namespace "gc-4257"
  Aug 12 12:43:50.147: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzncn" in namespace "gc-4257"
  Aug 12 12:43:50.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-kb4ch" in namespace "gc-4257"
  Aug 12 12:43:50.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbzvw" in namespace "gc-4257"
  Aug 12 12:43:50.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfbpc" in namespace "gc-4257"
  E0812 12:43:50.234933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:43:50.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4257" for this suite. @ 08/12/23 12:43:50.259
• [18.016 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/12/23 12:43:50.275
  Aug 12 12:43:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 12:43:50.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:43:50.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:43:50.319
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/12/23 12:43:50.325
  Aug 12 12:43:50.346: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3165  406854b3-7312-4983-bdbf-618a534303ea 15469 0 2023-08-12 12:43:50 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-12 12:43:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mst5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mst5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0812 12:43:51.235545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:52.236115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:53.236854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:54.237243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:55.237946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:56.238401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:57.238633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:43:58.238793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/12/23 12:43:58.391
  Aug 12 12:43:58.391: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3165 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:43:58.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:43:58.393: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:43:58.393: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3165/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0812 12:43:59.239526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 08/12/23 12:43:59.525
  Aug 12 12:43:59.525: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3165 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:43:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:43:59.527: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:43:59.527: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-3165/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0812 12:44:00.240704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:00.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:44:00.575: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3165" for this suite. @ 08/12/23 12:44:00.589
• [10.324 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/12/23 12:44:00.611
  Aug 12 12:44:00.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 12:44:00.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:00.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:00.639
  Aug 12 12:44:00.644: INFO: Creating ReplicaSet my-hostname-basic-9aeaec03-b848-48e0-866a-1e4835a936eb
  Aug 12 12:44:00.659: INFO: Pod name my-hostname-basic-9aeaec03-b848-48e0-866a-1e4835a936eb: Found 0 pods out of 1
  E0812 12:44:01.240956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:02.241871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:03.242103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:04.242971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:05.243323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:05.665: INFO: Pod name my-hostname-basic-9aeaec03-b848-48e0-866a-1e4835a936eb: Found 1 pods out of 1
  Aug 12 12:44:05.665: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9aeaec03-b848-48e0-866a-1e4835a936eb" is running
  Aug 12 12:44:05.670: INFO: Pod "my-hostname-basic-9aeaec03-b848-48e0-866a-1e4835a936eb-pdx9d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:44:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:44:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:44:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-12 12:44:00 +0000 UTC Reason: Message:}])
  Aug 12 12:44:05.670: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/12/23 12:44:05.67
  Aug 12 12:44:05.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-421" for this suite. @ 08/12/23 12:44:05.695
• [5.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/12/23 12:44:05.706
  Aug 12 12:44:05.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:44:05.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:05.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:05.735
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:44:05.74
  E0812 12:44:06.244287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:07.244671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:08.245483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:09.245736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:44:09.778
  Aug 12 12:44:09.783: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-c7a57f98-23a9-4f62-85e2-7f505f8e1b2d container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:44:09.797
  Aug 12 12:44:09.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5073" for this suite. @ 08/12/23 12:44:09.825
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/12/23 12:44:09.837
  Aug 12 12:44:09.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:44:09.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:09.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:09.862
  STEP: Creating service test in namespace statefulset-212 @ 08/12/23 12:44:09.867
  STEP: Creating statefulset ss in namespace statefulset-212 @ 08/12/23 12:44:09.883
  Aug 12 12:44:09.911: INFO: Found 0 stateful pods, waiting for 1
  E0812 12:44:10.246002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:11.246495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:12.246656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:13.246753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:14.247175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:15.247632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:16.248472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:17.248694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:18.248862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:19.248985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:19.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/12/23 12:44:19.929
  STEP: Getting /status @ 08/12/23 12:44:19.94
  Aug 12 12:44:19.946: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/12/23 12:44:19.946
  Aug 12 12:44:19.959: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/12/23 12:44:19.96
  Aug 12 12:44:19.963: INFO: Observed &StatefulSet event: ADDED
  Aug 12 12:44:19.963: INFO: Found Statefulset ss in namespace statefulset-212 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 12:44:19.963: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/12/23 12:44:19.963
  Aug 12 12:44:19.963: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 12 12:44:19.976: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/12/23 12:44:19.976
  Aug 12 12:44:19.979: INFO: Observed &StatefulSet event: ADDED
  Aug 12 12:44:19.979: INFO: Deleting all statefulset in ns statefulset-212
  Aug 12 12:44:19.985: INFO: Scaling statefulset ss to 0
  E0812 12:44:20.249079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:21.249605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:22.250088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:23.250338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:24.250684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:25.251152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:26.251685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:27.251780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:28.252321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:29.252724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:30.014: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 12:44:30.019: INFO: Deleting statefulset ss
  Aug 12 12:44:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-212" for this suite. @ 08/12/23 12:44:30.047
• [20.219 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/12/23 12:44:30.056
  Aug 12 12:44:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:44:30.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:30.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:30.084
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:44:30.088
  E0812 12:44:30.252807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:31.253543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:32.254360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:33.254483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:44:34.118
  Aug 12 12:44:34.122: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-8177f906-2172-44df-94e7-9b1009b42230 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:44:34.132
  Aug 12 12:44:34.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-208" for this suite. @ 08/12/23 12:44:34.161
• [4.114 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/12/23 12:44:34.171
  Aug 12 12:44:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:44:34.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:34.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:34.195
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:44:34.204
  E0812 12:44:34.254964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:35.255379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:36.256464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:37.256748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:44:38.237
  Aug 12 12:44:38.241: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-07b4aeac-024b-471d-8876-3ec170760aa5 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:44:38.252
  E0812 12:44:38.257456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:38.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4621" for this suite. @ 08/12/23 12:44:38.276
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/12/23 12:44:38.293
  Aug 12 12:44:38.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/12/23 12:44:38.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:38.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:38.318
  Aug 12 12:44:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:44:38.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-383" for this suite. @ 08/12/23 12:44:38.883
• [0.602 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/12/23 12:44:38.896
  Aug 12 12:44:38.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:44:38.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:38.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:38.921
  STEP: Creating configMap with name configmap-test-volume-map-2dbaf8db-b5db-4669-a6a7-4cb652e727ee @ 08/12/23 12:44:38.925
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:44:38.934
  E0812 12:44:39.258079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:40.259160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:41.260027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:42.260130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:44:42.968
  Aug 12 12:44:42.973: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-c4486801-2188-45ee-a307-2cbf972aa387 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:44:42.983
  Aug 12 12:44:43.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3829" for this suite. @ 08/12/23 12:44:43.012
• [4.125 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/12/23 12:44:43.021
  Aug 12 12:44:43.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:44:43.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:43.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:43.046
  STEP: creating Agnhost RC @ 08/12/23 12:44:43.05
  Aug 12 12:44:43.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8377 create -f -'
  E0812 12:44:43.260690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:44.261801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:44.349: INFO: stderr: ""
  Aug 12 12:44:44.349: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/12/23 12:44:44.349
  E0812 12:44:45.262925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:45.356: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:44:45.356: INFO: Found 0 / 1
  E0812 12:44:46.264002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:46.355: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:44:46.355: INFO: Found 1 / 1
  Aug 12 12:44:46.355: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/12/23 12:44:46.355
  Aug 12 12:44:46.360: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:44:46.360: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 12 12:44:46.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8377 patch pod agnhost-primary-5bq5j -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug 12 12:44:46.456: INFO: stderr: ""
  Aug 12 12:44:46.456: INFO: stdout: "pod/agnhost-primary-5bq5j patched\n"
  STEP: checking annotations @ 08/12/23 12:44:46.456
  Aug 12 12:44:46.460: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:44:46.460: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 12 12:44:46.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8377" for this suite. @ 08/12/23 12:44:46.466
• [3.453 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/12/23 12:44:46.476
  Aug 12 12:44:46.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename hostport @ 08/12/23 12:44:46.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:46.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:46.505
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/12/23 12:44:46.515
  E0812 12:44:47.264178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:48.264737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.79.233 on the node which pod1 resides and expect scheduled @ 08/12/23 12:44:48.539
  E0812 12:44:49.265683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:50.266311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.79.233 but use UDP protocol on the node which pod2 resides @ 08/12/23 12:44:50.56
  E0812 12:44:51.267080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:52.267194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:53.268189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:54.268540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/12/23 12:44:54.603
  Aug 12 12:44:54.603: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.79.233 http://127.0.0.1:54323/hostname] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:44:54.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:44:54.604: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:44:54.604: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1932/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.79.233+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.79.233, port: 54323 @ 08/12/23 12:44:54.693
  Aug 12 12:44:54.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.79.233:54323/hostname] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:44:54.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:44:54.694: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:44:54.694: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1932/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.79.233%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.79.233, port: 54323 UDP @ 08/12/23 12:44:54.768
  Aug 12 12:44:54.768: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.79.233 54323] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:44:54.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:44:54.770: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:44:54.770: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1932/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.79.233+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0812 12:44:55.269359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:56.269751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:57.269873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:58.270007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:44:59.270207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:44:59.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1932" for this suite. @ 08/12/23 12:44:59.853
• [13.387 seconds]
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/12/23 12:44:59.863
  Aug 12 12:44:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:44:59.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:44:59.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:44:59.888
  STEP: Creating secret with name secret-test-5c0b6f8b-d0ee-4c78-8749-8a0c692cb520 @ 08/12/23 12:44:59.921
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:44:59.928
  E0812 12:45:00.270299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:01.270838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:02.270937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:03.271140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:45:03.956
  Aug 12 12:45:03.961: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-020919b1-dea1-4c25-9c7b-dc8d42d5aa30 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:45:03.971
  Aug 12 12:45:03.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3361" for this suite. @ 08/12/23 12:45:03.996
  STEP: Destroying namespace "secret-namespace-2546" for this suite. @ 08/12/23 12:45:04.006
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/12/23 12:45:04.026
  Aug 12 12:45:04.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename watch @ 08/12/23 12:45:04.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:04.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:04.05
  STEP: creating a new configmap @ 08/12/23 12:45:04.055
  STEP: modifying the configmap once @ 08/12/23 12:45:04.062
  STEP: modifying the configmap a second time @ 08/12/23 12:45:04.072
  STEP: deleting the configmap @ 08/12/23 12:45:04.082
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/12/23 12:45:04.091
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/12/23 12:45:04.093
  Aug 12 12:45:04.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8085  f181d71d-b015-42a1-8227-6667ed932e5f 16776 0 2023-08-12 12:45:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-12 12:45:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 12:45:04.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8085  f181d71d-b015-42a1-8227-6667ed932e5f 16777 0 2023-08-12 12:45:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-12 12:45:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 12:45:04.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8085" for this suite. @ 08/12/23 12:45:04.1
• [0.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/12/23 12:45:04.113
  Aug 12 12:45:04.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:45:04.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:04.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:04.137
  STEP: Creating a ResourceQuota with terminating scope @ 08/12/23 12:45:04.141
  STEP: Ensuring ResourceQuota status is calculated @ 08/12/23 12:45:04.148
  E0812 12:45:04.271830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:05.271962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 08/12/23 12:45:06.154
  STEP: Ensuring ResourceQuota status is calculated @ 08/12/23 12:45:06.16
  E0812 12:45:06.272055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:07.272185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 08/12/23 12:45:08.166
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/12/23 12:45:08.186
  E0812 12:45:08.272310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:09.272695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/12/23 12:45:10.191
  E0812 12:45:10.273808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:11.274314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/12/23 12:45:12.196
  STEP: Ensuring resource quota status released the pod usage @ 08/12/23 12:45:12.214
  E0812 12:45:12.274548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:13.274684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 08/12/23 12:45:14.219
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/12/23 12:45:14.233
  E0812 12:45:14.275354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:15.275484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/12/23 12:45:16.239
  E0812 12:45:16.276104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:17.276231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/12/23 12:45:18.244
  STEP: Ensuring resource quota status released the pod usage @ 08/12/23 12:45:18.258
  E0812 12:45:18.277119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:19.277753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:45:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4263" for this suite. @ 08/12/23 12:45:20.268
  E0812 12:45:20.278382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [16.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/12/23 12:45:20.28
  Aug 12 12:45:20.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:45:20.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:20.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:20.305
  STEP: Creating Pod @ 08/12/23 12:45:20.314
  E0812 12:45:21.278535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:22.278976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 08/12/23 12:45:22.346
  Aug 12 12:45:22.346: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2272 PodName:pod-sharedvolume-999a301d-b75c-47df-ba61-703a2ff9f406 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:45:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:45:22.347: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:45:22.347: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-2272/pods/pod-sharedvolume-999a301d-b75c-47df-ba61-703a2ff9f406/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Aug 12 12:45:22.434: INFO: Exec stderr: ""
  Aug 12 12:45:22.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2272" for this suite. @ 08/12/23 12:45:22.44
• [2.170 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/12/23 12:45:22.45
  Aug 12 12:45:22.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pod-network-test @ 08/12/23 12:45:22.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:22.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:22.477
  STEP: Performing setup for networking test in namespace pod-network-test-3151 @ 08/12/23 12:45:22.486
  STEP: creating a selector @ 08/12/23 12:45:22.486
  STEP: Creating the service pods in kubernetes @ 08/12/23 12:45:22.486
  Aug 12 12:45:22.486: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0812 12:45:23.279992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:24.280084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:25.280848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:26.281655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:27.281830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:28.281932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:29.282095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:30.282431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:31.283210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:32.283589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:33.283764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:34.283987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:35.284954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:36.285354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:37.286043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:38.286407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:39.287331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:40.288067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:41.289073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:42.289766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:43.289895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:44.290024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/12/23 12:45:44.618
  E0812 12:45:45.290163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:46.290934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:45:46.641: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 12 12:45:46.641: INFO: Breadth first check of 192.168.87.147 on host 172.31.32.142...
  Aug 12 12:45:46.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.149:9080/dial?request=hostname&protocol=udp&host=192.168.87.147&port=8081&tries=1'] Namespace:pod-network-test-3151 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:45:46.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:45:46.647: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:45:46.647: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3151/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.149%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.87.147%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:45:46.733: INFO: Waiting for responses: map[]
  Aug 12 12:45:46.734: INFO: reached 192.168.87.147 after 0/1 tries
  Aug 12 12:45:46.734: INFO: Breadth first check of 192.168.182.13 on host 172.31.79.233...
  Aug 12 12:45:46.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.149:9080/dial?request=hostname&protocol=udp&host=192.168.182.13&port=8081&tries=1'] Namespace:pod-network-test-3151 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:45:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:45:46.740: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:45:46.740: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3151/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.149%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.182.13%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:45:46.830: INFO: Waiting for responses: map[]
  Aug 12 12:45:46.830: INFO: reached 192.168.182.13 after 0/1 tries
  Aug 12 12:45:46.830: INFO: Breadth first check of 192.168.177.8 on host 172.31.84.203...
  Aug 12 12:45:46.835: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.149:9080/dial?request=hostname&protocol=udp&host=192.168.177.8&port=8081&tries=1'] Namespace:pod-network-test-3151 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:45:46.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:45:46.836: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:45:46.836: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3151/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.149%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.177.8%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:45:46.914: INFO: Waiting for responses: map[]
  Aug 12 12:45:46.914: INFO: reached 192.168.177.8 after 0/1 tries
  Aug 12 12:45:46.914: INFO: Going to retry 0 out of 3 pods....
  Aug 12 12:45:46.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3151" for this suite. @ 08/12/23 12:45:46.92
• [24.479 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/12/23 12:45:46.93
  Aug 12 12:45:46.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:45:46.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:46.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:46.956
  STEP: Creating a pod to test downward api env vars @ 08/12/23 12:45:46.96
  E0812 12:45:47.291070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:48.291166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:49.291649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:50.291758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:45:50.991
  Aug 12 12:45:50.996: INFO: Trying to get logs from node ip-172-31-32-142 pod downward-api-f7ef76ef-1287-4865-a68c-aa5217932829 container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:45:51.006
  Aug 12 12:45:51.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9333" for this suite. @ 08/12/23 12:45:51.036
• [4.117 seconds]
------------------------------
SS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/12/23 12:45:51.048
  Aug 12 12:45:51.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename ingress @ 08/12/23 12:45:51.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:51.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:51.073
  STEP: getting /apis @ 08/12/23 12:45:51.076
  STEP: getting /apis/networking.k8s.io @ 08/12/23 12:45:51.081
  STEP: getting /apis/networking.k8s.iov1 @ 08/12/23 12:45:51.083
  STEP: creating @ 08/12/23 12:45:51.086
  STEP: getting @ 08/12/23 12:45:51.116
  STEP: listing @ 08/12/23 12:45:51.122
  STEP: watching @ 08/12/23 12:45:51.127
  Aug 12 12:45:51.128: INFO: starting watch
  STEP: cluster-wide listing @ 08/12/23 12:45:51.13
  STEP: cluster-wide watching @ 08/12/23 12:45:51.135
  Aug 12 12:45:51.135: INFO: starting watch
  STEP: patching @ 08/12/23 12:45:51.137
  STEP: updating @ 08/12/23 12:45:51.145
  Aug 12 12:45:51.160: INFO: waiting for watch events with expected annotations
  Aug 12 12:45:51.161: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/12/23 12:45:51.162
  STEP: updating /status @ 08/12/23 12:45:51.172
  STEP: get /status @ 08/12/23 12:45:51.186
  STEP: deleting @ 08/12/23 12:45:51.193
  STEP: deleting a collection @ 08/12/23 12:45:51.212
  Aug 12 12:45:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-6600" for this suite. @ 08/12/23 12:45:51.253
• [0.216 seconds]
------------------------------
SS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/12/23 12:45:51.265
  Aug 12 12:45:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:45:51.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:51.289
  E0812 12:45:51.292282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:51.295
  STEP: Creating configMap that has name configmap-test-emptyKey-412d6187-4403-43b6-82a7-863166315c6c @ 08/12/23 12:45:51.3
  Aug 12 12:45:51.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-725" for this suite. @ 08/12/23 12:45:51.312
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/12/23 12:45:51.342
  Aug 12 12:45:51.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:45:51.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:51.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:51.374
  STEP: creating an Endpoint @ 08/12/23 12:45:51.382
  STEP: waiting for available Endpoint @ 08/12/23 12:45:51.389
  STEP: listing all Endpoints @ 08/12/23 12:45:51.391
  STEP: updating the Endpoint @ 08/12/23 12:45:51.397
  STEP: fetching the Endpoint @ 08/12/23 12:45:51.404
  STEP: patching the Endpoint @ 08/12/23 12:45:51.409
  STEP: fetching the Endpoint @ 08/12/23 12:45:51.42
  STEP: deleting the Endpoint by Collection @ 08/12/23 12:45:51.425
  STEP: waiting for Endpoint deletion @ 08/12/23 12:45:51.435
  STEP: fetching the Endpoint @ 08/12/23 12:45:51.438
  Aug 12 12:45:51.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9019" for this suite. @ 08/12/23 12:45:51.453
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/12/23 12:45:51.473
  Aug 12 12:45:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/12/23 12:45:51.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:51.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:51.504
  STEP: mirroring a new custom Endpoint @ 08/12/23 12:45:51.52
  Aug 12 12:45:51.536: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0812 12:45:52.292952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:53.293325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 08/12/23 12:45:53.543
  Aug 12 12:45:53.554: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0812 12:45:54.293444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:55.293623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 08/12/23 12:45:55.564
  Aug 12 12:45:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9009" for this suite. @ 08/12/23 12:45:55.594
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/12/23 12:45:55.605
  Aug 12 12:45:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-watch @ 08/12/23 12:45:55.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:45:55.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:45:55.627
  Aug 12 12:45:55.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 12:45:56.293991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:57.294547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 08/12/23 12:45:58.19
  Aug 12 12:45:58.197: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:45:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:45:58Z]] name:name1 resourceVersion:17281 uid:5cb82475-2505-462f-9cfe-f5ff98cc5b6c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:45:58.295507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:45:59.295643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:00.295783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:01.296011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:02.296158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:03.297202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:04.297365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:05.297811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:06.297899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:07.298656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 08/12/23 12:46:08.197
  Aug 12 12:46:08.206: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:46:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:46:08Z]] name:name2 resourceVersion:17309 uid:325dfce1-a001-4403-bb95-c02d82253fbe] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:46:08.299222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:09.299739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:10.299882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:11.300408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:12.300699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:13.300787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:14.301795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:15.301903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:16.302549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:17.303182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 08/12/23 12:46:18.207
  Aug 12 12:46:18.215: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:45:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:46:18Z]] name:name1 resourceVersion:17329 uid:5cb82475-2505-462f-9cfe-f5ff98cc5b6c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:46:18.303939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:19.304064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:20.304307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:21.304869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:22.304996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:23.305475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:24.305947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:25.306036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:26.306653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:27.306781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 08/12/23 12:46:28.216
  Aug 12 12:46:28.227: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:46:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:46:28Z]] name:name2 resourceVersion:17349 uid:325dfce1-a001-4403-bb95-c02d82253fbe] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:46:28.307519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:29.307651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:30.307955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:31.308290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:32.309373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:33.310334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:34.311062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:35.311178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:36.311391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:37.311662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 08/12/23 12:46:38.227
  Aug 12 12:46:38.238: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:45:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:46:18Z]] name:name1 resourceVersion:17368 uid:5cb82475-2505-462f-9cfe-f5ff98cc5b6c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:46:38.312164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:39.312271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:40.312752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:41.313760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:42.313888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:43.314208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:44.314423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:45.314547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:46.315167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:47.315302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 08/12/23 12:46:48.239
  Aug 12 12:46:48.251: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-12T12:46:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-12T12:46:28Z]] name:name2 resourceVersion:17388 uid:325dfce1-a001-4403-bb95-c02d82253fbe] num:map[num1:9223372036854775807 num2:1000000]]}
  E0812 12:46:48.315537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:49.316134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:50.316674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:51.317767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:52.317889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:53.318088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:54.318371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:55.318502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:56.319257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:57.319526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:46:58.320018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:46:58.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7928" for this suite. @ 08/12/23 12:46:58.783
• [63.187 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/12/23 12:46:58.794
  Aug 12 12:46:58.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 12:46:58.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:46:58.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:46:58.827
  STEP: Creating a test namespace @ 08/12/23 12:46:58.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:46:58.848
  STEP: Creating a service in the namespace @ 08/12/23 12:46:58.853
  STEP: Deleting the namespace @ 08/12/23 12:46:58.873
  STEP: Waiting for the namespace to be removed. @ 08/12/23 12:46:58.896
  E0812 12:46:59.320996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:00.321127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:01.321289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:02.322359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:03.323420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:04.324199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/12/23 12:47:04.901
  STEP: Verifying there is no service in the namespace @ 08/12/23 12:47:04.92
  Aug 12 12:47:04.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5251" for this suite. @ 08/12/23 12:47:04.931
  STEP: Destroying namespace "nsdeletetest-8860" for this suite. @ 08/12/23 12:47:04.939
  Aug 12 12:47:04.944: INFO: Namespace nsdeletetest-8860 was already deleted
  STEP: Destroying namespace "nsdeletetest-8587" for this suite. @ 08/12/23 12:47:04.944
• [6.159 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/12/23 12:47:04.954
  Aug 12 12:47:04.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename ingressclass @ 08/12/23 12:47:04.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:04.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:04.976
  STEP: getting /apis @ 08/12/23 12:47:04.981
  STEP: getting /apis/networking.k8s.io @ 08/12/23 12:47:04.986
  STEP: getting /apis/networking.k8s.iov1 @ 08/12/23 12:47:04.987
  STEP: creating @ 08/12/23 12:47:04.989
  STEP: getting @ 08/12/23 12:47:05.008
  STEP: listing @ 08/12/23 12:47:05.013
  STEP: watching @ 08/12/23 12:47:05.017
  Aug 12 12:47:05.017: INFO: starting watch
  STEP: patching @ 08/12/23 12:47:05.018
  STEP: updating @ 08/12/23 12:47:05.025
  Aug 12 12:47:05.031: INFO: waiting for watch events with expected annotations
  Aug 12 12:47:05.031: INFO: saw patched and updated annotations
  STEP: deleting @ 08/12/23 12:47:05.032
  STEP: deleting a collection @ 08/12/23 12:47:05.048
  Aug 12 12:47:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-6490" for this suite. @ 08/12/23 12:47:05.076
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/12/23 12:47:05.087
  Aug 12 12:47:05.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 12:47:05.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:05.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:05.113
  STEP: Creating a test headless service @ 08/12/23 12:47:05.117
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9733.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9733.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.33_tcp@PTR;sleep 1; done
   @ 08/12/23 12:47:05.146
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9733.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9733.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9733.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9733.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9733.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 33.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.33_tcp@PTR;sleep 1; done
   @ 08/12/23 12:47:05.146
  STEP: creating a pod to probe DNS @ 08/12/23 12:47:05.147
  STEP: submitting the pod to kubernetes @ 08/12/23 12:47:05.147
  E0812 12:47:05.325005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:06.325420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 12:47:07.18
  STEP: looking for the results for each expected name from probers @ 08/12/23 12:47:07.184
  Aug 12 12:47:07.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.198: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.203: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.210: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.240: INFO: Unable to read jessie_udp@dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.246: INFO: Unable to read jessie_tcp@dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.252: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local from pod dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218: the server could not find the requested resource (get pods dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218)
  Aug 12 12:47:07.283: INFO: Lookups using dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218 failed for: [wheezy_udp@dns-test-service.dns-9733.svc.cluster.local wheezy_tcp@dns-test-service.dns-9733.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local jessie_udp@dns-test-service.dns-9733.svc.cluster.local jessie_tcp@dns-test-service.dns-9733.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9733.svc.cluster.local]

  E0812 12:47:07.325709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:08.325852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:09.325976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:10.326813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:11.327150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:12.327184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:47:12.376: INFO: DNS probes using dns-9733/dns-test-5b31597f-0e9a-41c4-92fe-71a92c89a218 succeeded

  Aug 12 12:47:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 12:47:12.382
  STEP: deleting the test service @ 08/12/23 12:47:12.411
  STEP: deleting the test headless service @ 08/12/23 12:47:12.444
  STEP: Destroying namespace "dns-9733" for this suite. @ 08/12/23 12:47:12.461
• [7.384 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/12/23 12:47:12.473
  Aug 12 12:47:12.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:47:12.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:12.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:12.523
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:47:12.526
  E0812 12:47:13.327336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:14.327699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:15.327800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:16.328488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:47:16.557
  Aug 12 12:47:16.561: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-38522d2b-3ec2-4aa1-8aca-8c4640cacac9 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:47:16.572
  Aug 12 12:47:16.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9864" for this suite. @ 08/12/23 12:47:16.601
• [4.136 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/12/23 12:47:16.61
  Aug 12 12:47:16.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:47:16.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:16.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:16.655
  STEP: Creating a pod to test downward api env vars @ 08/12/23 12:47:16.659
  E0812 12:47:17.329540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:18.329675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:19.330637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:20.331099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:47:20.691
  Aug 12 12:47:20.696: INFO: Trying to get logs from node ip-172-31-32-142 pod downward-api-f56b4a69-43f0-4724-8cf2-63c256526300 container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:47:20.708
  Aug 12 12:47:20.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7445" for this suite. @ 08/12/23 12:47:20.735
• [4.134 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/12/23 12:47:20.745
  Aug 12 12:47:20.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:47:20.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:20.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:20.769
  STEP: Creating a ResourceQuota with best effort scope @ 08/12/23 12:47:20.774
  STEP: Ensuring ResourceQuota status is calculated @ 08/12/23 12:47:20.8
  E0812 12:47:21.331261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:22.331396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 08/12/23 12:47:22.807
  STEP: Ensuring ResourceQuota status is calculated @ 08/12/23 12:47:22.812
  E0812 12:47:23.332424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:24.332705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 08/12/23 12:47:24.819
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/12/23 12:47:24.836
  E0812 12:47:25.333767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:26.334187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/12/23 12:47:26.84
  E0812 12:47:27.335112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:28.335496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/12/23 12:47:28.846
  STEP: Ensuring resource quota status released the pod usage @ 08/12/23 12:47:28.861
  E0812 12:47:29.336644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:30.336732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 08/12/23 12:47:30.866
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/12/23 12:47:30.884
  E0812 12:47:31.337770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:32.337921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/12/23 12:47:32.89
  E0812 12:47:33.337977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:34.338525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/12/23 12:47:34.895
  STEP: Ensuring resource quota status released the pod usage @ 08/12/23 12:47:34.914
  E0812 12:47:35.339226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:36.339405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:47:36.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2747" for this suite. @ 08/12/23 12:47:36.925
• [16.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/12/23 12:47:36.939
  Aug 12 12:47:36.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:47:36.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:36.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:36.964
  STEP: Creating configMap with name projected-configmap-test-volume-map-d5112f17-9acd-40a7-82b0-154efb5f1d61 @ 08/12/23 12:47:36.968
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:47:36.975
  E0812 12:47:37.339883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:38.340299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:39.340416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:40.340684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:47:41.006
  Aug 12 12:47:41.012: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-25008d8a-84d9-4a3f-b429-73ed17cc83aa container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:47:41.022
  Aug 12 12:47:41.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6399" for this suite. @ 08/12/23 12:47:41.047
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/12/23 12:47:41.059
  Aug 12 12:47:41.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 12:47:41.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:47:41.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:47:41.083
  STEP: Creating service test in namespace statefulset-4703 @ 08/12/23 12:47:41.092
  Aug 12 12:47:41.220: INFO: Found 0 stateful pods, waiting for 1
  E0812 12:47:41.341219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:42.341797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:43.342872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:44.342962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:45.343946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:46.344604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:47.344674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:48.345751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:49.346158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:50.346302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:47:51.226: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/12/23 12:47:51.236
  W0812 12:47:51.245628      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 12 12:47:51.259: INFO: Found 1 stateful pods, waiting for 2
  E0812 12:47:51.347396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:52.347525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:53.347663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:54.347791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:55.347920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:56.348211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:57.348458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:58.348699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:47:59.348811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:00.349751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:01.265: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 12:48:01.265: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/12/23 12:48:01.275
  STEP: Delete all of the StatefulSets @ 08/12/23 12:48:01.279
  STEP: Verify that StatefulSets have been deleted @ 08/12/23 12:48:01.291
  Aug 12 12:48:01.298: INFO: Deleting all statefulset in ns statefulset-4703
  Aug 12 12:48:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4703" for this suite. @ 08/12/23 12:48:01.347
  E0812 12:48:01.350567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [20.302 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/12/23 12:48:01.361
  Aug 12 12:48:01.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:48:01.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:01.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:01.394
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/12/23 12:48:01.405
  E0812 12:48:02.350808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:03.351083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:04.351142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:05.351444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:48:05.437
  Aug 12 12:48:05.442: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-1b1eeb95-3d44-4186-9664-eb2e7053b528 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:48:05.452
  Aug 12 12:48:05.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-919" for this suite. @ 08/12/23 12:48:05.48
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/12/23 12:48:05.504
  Aug 12 12:48:05.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 12:48:05.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:05.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:05.532
  STEP: Creating a pod to test substitution in volume subpath @ 08/12/23 12:48:05.537
  E0812 12:48:06.352226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:07.353243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:08.353357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:09.354249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:48:09.569
  Aug 12 12:48:09.574: INFO: Trying to get logs from node ip-172-31-32-142 pod var-expansion-b7887650-be99-4f18-943c-ec4b0f3b6c04 container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:48:09.585
  Aug 12 12:48:09.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9659" for this suite. @ 08/12/23 12:48:09.609
• [4.114 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/12/23 12:48:09.621
  Aug 12 12:48:09.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 12:48:09.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:09.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:09.644
  STEP: Creating a pod to test substitution in container's command @ 08/12/23 12:48:09.649
  E0812 12:48:10.355286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:11.355715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:12.355857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:13.356343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:48:13.684
  Aug 12 12:48:13.689: INFO: Trying to get logs from node ip-172-31-32-142 pod var-expansion-02876e77-6f87-4c88-af34-13577ad3c1ca container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:48:13.699
  Aug 12 12:48:13.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4826" for this suite. @ 08/12/23 12:48:13.722
• [4.111 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/12/23 12:48:13.733
  Aug 12 12:48:13.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:48:13.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:13.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:13.759
  STEP: creating service in namespace services-2771 @ 08/12/23 12:48:13.764
  STEP: creating service affinity-clusterip in namespace services-2771 @ 08/12/23 12:48:13.764
  STEP: creating replication controller affinity-clusterip in namespace services-2771 @ 08/12/23 12:48:13.779
  I0812 12:48:13.792928      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-2771, replica count: 3
  E0812 12:48:14.357219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:15.357666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:16.358405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 12:48:16.844073      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 12:48:16.853: INFO: Creating new exec pod
  E0812 12:48:17.358520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:18.358843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:19.359610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:19.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2771 exec execpod-affinityfsrnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Aug 12 12:48:20.039: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug 12 12:48:20.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:48:20.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2771 exec execpod-affinityfsrnz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.175 80'
  Aug 12 12:48:20.201: INFO: stderr: "+ nc -v -t -w 2 10.152.183.175 80\n+ echo hostName\nConnection to 10.152.183.175 80 port [tcp/http] succeeded!\n"
  Aug 12 12:48:20.201: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:48:20.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2771 exec execpod-affinityfsrnz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.175:80/ ; done'
  E0812 12:48:20.360673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:20.459: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.175:80/\n"
  Aug 12 12:48:20.459: INFO: stdout: "\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22\naffinity-clusterip-fxg22"
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Received response from host: affinity-clusterip-fxg22
  Aug 12 12:48:20.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:48:20.466: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-2771, will wait for the garbage collector to delete the pods @ 08/12/23 12:48:20.479
  Aug 12 12:48:20.548: INFO: Deleting ReplicationController affinity-clusterip took: 13.605645ms
  Aug 12 12:48:20.649: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.615669ms
  E0812 12:48:21.361729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:22.361789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2771" for this suite. @ 08/12/23 12:48:22.981
• [9.258 seconds]
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/12/23 12:48:22.991
  Aug 12 12:48:22.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption @ 08/12/23 12:48:22.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:23.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:23.021
  STEP: Creating a kubernetes client @ 08/12/23 12:48:23.025
  Aug 12 12:48:23.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption-2 @ 08/12/23 12:48:23.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:23.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:23.05
  STEP: Waiting for the pdb to be processed @ 08/12/23 12:48:23.061
  E0812 12:48:23.362392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:24.362583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/12/23 12:48:25.085
  E0812 12:48:25.363281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:26.364165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/12/23 12:48:27.104
  E0812 12:48:27.364302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:28.364351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 08/12/23 12:48:29.118
  STEP: listing a collection of PDBs in namespace disruption-9061 @ 08/12/23 12:48:29.123
  STEP: deleting a collection of PDBs @ 08/12/23 12:48:29.128
  STEP: Waiting for the PDB collection to be deleted @ 08/12/23 12:48:29.145
  Aug 12 12:48:29.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:48:29.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-3687" for this suite. @ 08/12/23 12:48:29.161
  STEP: Destroying namespace "disruption-9061" for this suite. @ 08/12/23 12:48:29.172
• [6.190 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/12/23 12:48:29.183
  Aug 12 12:48:29.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:48:29.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:29.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:29.208
  STEP: creating a replication controller @ 08/12/23 12:48:29.212
  Aug 12 12:48:29.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 create -f -'
  E0812 12:48:29.365103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:29.911: INFO: stderr: ""
  Aug 12 12:48:29.911: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/12/23 12:48:29.911
  Aug 12 12:48:29.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 12:48:30.009: INFO: stderr: ""
  Aug 12 12:48:30.009: INFO: stdout: "update-demo-nautilus-jdc9h update-demo-nautilus-rj4cw "
  Aug 12 12:48:30.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods update-demo-nautilus-jdc9h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 12:48:30.094: INFO: stderr: ""
  Aug 12 12:48:30.094: INFO: stdout: ""
  Aug 12 12:48:30.094: INFO: update-demo-nautilus-jdc9h is created but not running
  E0812 12:48:30.365514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:31.366312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:32.366450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:33.366597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:34.367050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:35.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 12:48:35.186: INFO: stderr: ""
  Aug 12 12:48:35.186: INFO: stdout: "update-demo-nautilus-jdc9h update-demo-nautilus-rj4cw "
  Aug 12 12:48:35.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods update-demo-nautilus-jdc9h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 12:48:35.268: INFO: stderr: ""
  Aug 12 12:48:35.268: INFO: stdout: "true"
  Aug 12 12:48:35.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods update-demo-nautilus-jdc9h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 12:48:35.350: INFO: stderr: ""
  Aug 12 12:48:35.350: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 12:48:35.350: INFO: validating pod update-demo-nautilus-jdc9h
  Aug 12 12:48:35.357: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 12:48:35.357: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 12:48:35.358: INFO: update-demo-nautilus-jdc9h is verified up and running
  Aug 12 12:48:35.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods update-demo-nautilus-rj4cw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0812 12:48:35.367740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:35.442: INFO: stderr: ""
  Aug 12 12:48:35.442: INFO: stdout: "true"
  Aug 12 12:48:35.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods update-demo-nautilus-rj4cw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 12:48:35.526: INFO: stderr: ""
  Aug 12 12:48:35.526: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 12:48:35.526: INFO: validating pod update-demo-nautilus-rj4cw
  Aug 12 12:48:35.535: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 12:48:35.535: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 12:48:35.535: INFO: update-demo-nautilus-rj4cw is verified up and running
  STEP: using delete to clean up resources @ 08/12/23 12:48:35.535
  Aug 12 12:48:35.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 delete --grace-period=0 --force -f -'
  Aug 12 12:48:35.622: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:48:35.622: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 12 12:48:35.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get rc,svc -l name=update-demo --no-headers'
  Aug 12 12:48:35.714: INFO: stderr: "No resources found in kubectl-3474 namespace.\n"
  Aug 12 12:48:35.714: INFO: stdout: ""
  Aug 12 12:48:35.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3474 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 12 12:48:35.801: INFO: stderr: ""
  Aug 12 12:48:35.801: INFO: stdout: ""
  Aug 12 12:48:35.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3474" for this suite. @ 08/12/23 12:48:35.806
• [6.637 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/12/23 12:48:35.821
  Aug 12 12:48:35.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:48:35.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:35.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:35.844
  STEP: creating Agnhost RC @ 08/12/23 12:48:35.849
  Aug 12 12:48:35.849: INFO: namespace kubectl-9622
  Aug 12 12:48:35.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9622 create -f -'
  E0812 12:48:36.368660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:36.569: INFO: stderr: ""
  Aug 12 12:48:36.569: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/12/23 12:48:36.569
  E0812 12:48:37.368678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:37.574: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:48:37.574: INFO: Found 0 / 1
  E0812 12:48:38.369702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:38.574: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:48:38.575: INFO: Found 1 / 1
  Aug 12 12:48:38.575: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 12 12:48:38.579: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 12:48:38.579: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 12 12:48:38.579: INFO: wait on agnhost-primary startup in kubectl-9622 
  Aug 12 12:48:38.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9622 logs agnhost-primary-d8r5w agnhost-primary'
  Aug 12 12:48:38.695: INFO: stderr: ""
  Aug 12 12:48:38.695: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/12/23 12:48:38.695
  Aug 12 12:48:38.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9622 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Aug 12 12:48:38.799: INFO: stderr: ""
  Aug 12 12:48:38.799: INFO: stdout: "service/rm2 exposed\n"
  Aug 12 12:48:38.815: INFO: Service rm2 in namespace kubectl-9622 found.
  E0812 12:48:39.370529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:40.371510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 08/12/23 12:48:40.825
  Aug 12 12:48:40.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9622 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Aug 12 12:48:40.935: INFO: stderr: ""
  Aug 12 12:48:40.935: INFO: stdout: "service/rm3 exposed\n"
  Aug 12 12:48:40.944: INFO: Service rm3 in namespace kubectl-9622 found.
  E0812 12:48:41.372635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:42.372759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:48:42.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9622" for this suite. @ 08/12/23 12:48:42.96
• [7.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/12/23 12:48:42.975
  Aug 12 12:48:42.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 12:48:42.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:48:42.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:48:43.002
  STEP: Creating a job @ 08/12/23 12:48:43.009
  STEP: Ensuring active pods == parallelism @ 08/12/23 12:48:43.017
  E0812 12:48:43.372811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:44.374177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 08/12/23 12:48:45.025
  STEP: deleting Job.batch foo in namespace job-6982, will wait for the garbage collector to delete the pods @ 08/12/23 12:48:45.025
  Aug 12 12:48:45.091: INFO: Deleting Job.batch foo took: 9.982975ms
  Aug 12 12:48:45.193: INFO: Terminating Job.batch foo pods took: 101.089685ms
  E0812 12:48:45.374777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:46.374964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:47.375737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:48.375970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:49.376788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:50.377631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:51.377984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:52.378717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:53.379399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:54.380225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:55.381246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:56.381537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:57.382252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:58.382988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:48:59.384013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:00.385003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:01.385875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:02.386746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:03.387574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:04.388242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:05.388993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:06.389122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:07.390092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:08.390996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:09.391810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:10.392587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:11.392650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:12.393190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:13.393763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:14.394161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:15.394607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:16.395332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 08/12/23 12:49:17.193
  Aug 12 12:49:17.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6982" for this suite. @ 08/12/23 12:49:17.204
• [34.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/12/23 12:49:17.217
  Aug 12 12:49:17.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:49:17.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:49:17.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:49:17.24
  STEP: Creating projection with secret that has name projected-secret-test-417ed2a0-c08e-4aeb-9431-8c53f0e791f0 @ 08/12/23 12:49:17.243
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:49:17.251
  E0812 12:49:17.396199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:18.396761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:19.397365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:20.397853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:49:21.277
  Aug 12 12:49:21.282: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-secrets-2e6f9ae1-d999-447f-b746-c34e7f00f5ba container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:49:21.298
  Aug 12 12:49:21.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8731" for this suite. @ 08/12/23 12:49:21.324
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/12/23 12:49:21.336
  Aug 12 12:49:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename endpointslice @ 08/12/23 12:49:21.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:49:21.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:49:21.358
  Aug 12 12:49:21.375: INFO: Endpoints addresses: [172.31.16.37 172.31.77.174] , ports: [6443]
  Aug 12 12:49:21.376: INFO: EndpointSlices addresses: [172.31.16.37 172.31.77.174] , ports: [6443]
  Aug 12 12:49:21.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-570" for this suite. @ 08/12/23 12:49:21.382
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/12/23 12:49:21.396
  Aug 12 12:49:21.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename limitrange @ 08/12/23 12:49:21.397
  E0812 12:49:21.398599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:49:21.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:49:21.419
  STEP: Creating LimitRange "e2e-limitrange-xmjx7" in namespace "limitrange-2198" @ 08/12/23 12:49:21.424
  STEP: Creating another limitRange in another namespace @ 08/12/23 12:49:21.43
  Aug 12 12:49:21.444: INFO: Namespace "e2e-limitrange-xmjx7-4324" created
  Aug 12 12:49:21.445: INFO: Creating LimitRange "e2e-limitrange-xmjx7" in namespace "e2e-limitrange-xmjx7-4324"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-xmjx7" @ 08/12/23 12:49:21.454
  Aug 12 12:49:21.458: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-xmjx7" in "limitrange-2198" namespace @ 08/12/23 12:49:21.458
  Aug 12 12:49:21.467: INFO: LimitRange "e2e-limitrange-xmjx7" has been patched
  STEP: Delete LimitRange "e2e-limitrange-xmjx7" by Collection with labelSelector: "e2e-limitrange-xmjx7=patched" @ 08/12/23 12:49:21.467
  STEP: Confirm that the limitRange "e2e-limitrange-xmjx7" has been deleted @ 08/12/23 12:49:21.479
  Aug 12 12:49:21.479: INFO: Requesting list of LimitRange to confirm quantity
  Aug 12 12:49:21.484: INFO: Found 0 LimitRange with label "e2e-limitrange-xmjx7=patched"
  Aug 12 12:49:21.484: INFO: LimitRange "e2e-limitrange-xmjx7" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-xmjx7" @ 08/12/23 12:49:21.484
  Aug 12 12:49:21.488: INFO: Found 1 limitRange
  Aug 12 12:49:21.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2198" for this suite. @ 08/12/23 12:49:21.493
  STEP: Destroying namespace "e2e-limitrange-xmjx7-4324" for this suite. @ 08/12/23 12:49:21.502
• [0.115 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/12/23 12:49:21.513
  Aug 12 12:49:21.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 12:49:21.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:49:21.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:49:21.541
  STEP: creating the pod @ 08/12/23 12:49:21.547
  STEP: waiting for pod running @ 08/12/23 12:49:21.56
  E0812 12:49:22.398746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:23.399478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 08/12/23 12:49:23.575
  Aug 12 12:49:23.580: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4149 PodName:var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:49:23.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:49:23.581: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:49:23.581: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-4149/pods/var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/12/23 12:49:23.664
  Aug 12 12:49:23.671: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4149 PodName:var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:49:23.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:49:23.673: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:49:23.673: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-4149/pods/var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 08/12/23 12:49:23.747
  Aug 12 12:49:24.265: INFO: Successfully updated pod "var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed"
  STEP: waiting for annotated pod running @ 08/12/23 12:49:24.265
  STEP: deleting the pod gracefully @ 08/12/23 12:49:24.27
  Aug 12 12:49:24.270: INFO: Deleting pod "var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed" in namespace "var-expansion-4149"
  Aug 12 12:49:24.279: INFO: Wait up to 5m0s for pod "var-expansion-d39585c4-0d37-40a9-904b-afe00861c3ed" to be fully deleted
  E0812 12:49:24.399949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:25.400244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:26.401330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:27.401463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:28.402462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:29.402564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:30.403453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:31.404555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:32.404893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:33.405794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:34.406126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:35.406228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:36.406954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:37.407111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:38.407582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:39.407755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:40.407817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:41.408189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:42.408445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:43.408708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:44.409304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:45.409440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:46.409908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:47.410378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:48.411075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:49.411581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:50.412083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:51.412201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:52.412808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:53.413753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:54.414526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:55.414644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:49:56.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4149" for this suite. @ 08/12/23 12:49:56.389
• [34.890 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/12/23 12:49:56.412
  Aug 12 12:49:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subpath @ 08/12/23 12:49:56.413
  E0812 12:49:56.414733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:49:56.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:49:56.436
  STEP: Setting up data @ 08/12/23 12:49:56.441
  STEP: Creating pod pod-subpath-test-configmap-t5vm @ 08/12/23 12:49:56.452
  STEP: Creating a pod to test atomic-volume-subpath @ 08/12/23 12:49:56.452
  E0812 12:49:57.414895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:58.415033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:49:59.415516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:00.415917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:01.416772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:02.417099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:03.417185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:04.417311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:05.418314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:06.418502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:07.418563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:08.418728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:09.418842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:10.419139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:11.419458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:12.420497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:13.420761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:14.420849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:15.420978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:16.421572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:17.421658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:18.421974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:19.422061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:20.422137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:50:20.552
  Aug 12 12:50:20.558: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-subpath-test-configmap-t5vm container test-container-subpath-configmap-t5vm: <nil>
  STEP: delete the pod @ 08/12/23 12:50:20.568
  STEP: Deleting pod pod-subpath-test-configmap-t5vm @ 08/12/23 12:50:20.589
  Aug 12 12:50:20.589: INFO: Deleting pod "pod-subpath-test-configmap-t5vm" in namespace "subpath-7763"
  Aug 12 12:50:20.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7763" for this suite. @ 08/12/23 12:50:20.6
• [24.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/12/23 12:50:20.618
  Aug 12 12:50:20.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 12:50:20.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:20.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:20.643
  STEP: Creating a pod to test env composition @ 08/12/23 12:50:20.647
  E0812 12:50:21.423003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:22.423088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:23.423293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:24.423386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:50:24.677
  Aug 12 12:50:24.682: INFO: Trying to get logs from node ip-172-31-32-142 pod var-expansion-7d77521e-e57d-4488-9638-26bef712fb2f container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:50:24.692
  Aug 12 12:50:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-480" for this suite. @ 08/12/23 12:50:24.717
• [4.108 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/12/23 12:50:24.727
  Aug 12 12:50:24.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pod-network-test @ 08/12/23 12:50:24.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:24.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:24.753
  STEP: Performing setup for networking test in namespace pod-network-test-3056 @ 08/12/23 12:50:24.757
  STEP: creating a selector @ 08/12/23 12:50:24.757
  STEP: Creating the service pods in kubernetes @ 08/12/23 12:50:24.757
  Aug 12 12:50:24.757: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0812 12:50:25.423655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:26.424595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:27.424707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:28.424817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:29.424929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:30.425751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:31.426710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:32.427198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:33.427342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:34.427462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:35.427603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:36.428329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/12/23 12:50:36.871
  E0812 12:50:37.429183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:38.429296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:50:38.922: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 12 12:50:38.922: INFO: Going to poll 192.168.87.168 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:50:38.926: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.87.168:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:50:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:50:38.927: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:50:38.927: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.87.168%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 12 12:50:39.014: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 12 12:50:39.014: INFO: Going to poll 192.168.182.18 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:50:39.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.182.18:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:50:39.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:50:39.020: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:50:39.020: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.182.18%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 12 12:50:39.099: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 12 12:50:39.099: INFO: Going to poll 192.168.177.11 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:50:39.104: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.177.11:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3056 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:50:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:50:39.105: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:50:39.105: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3056/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.177.11%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 12 12:50:39.190: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 12 12:50:39.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3056" for this suite. @ 08/12/23 12:50:39.195
• [14.478 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/12/23 12:50:39.206
  Aug 12 12:50:39.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:50:39.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:39.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:39.234
  STEP: creating pod @ 08/12/23 12:50:39.24
  E0812 12:50:39.429401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:40.429841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:50:41.274: INFO: Pod pod-hostip-0844bc21-ab80-413b-925f-1806acfe35eb has hostIP: 172.31.84.203
  Aug 12 12:50:41.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7359" for this suite. @ 08/12/23 12:50:41.279
• [2.082 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/12/23 12:50:41.289
  Aug 12 12:50:41.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:50:41.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:41.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:41.314
  STEP: Creating configMap with name projected-configmap-test-volume-300a1d53-f8a1-46b4-a070-9373cb39dc3d @ 08/12/23 12:50:41.323
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:50:41.33
  E0812 12:50:41.430668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:42.430782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:43.430923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:44.431056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:50:45.362
  Aug 12 12:50:45.367: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-64299cbb-c548-4ea0-888c-948f023712c9 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:50:45.377
  Aug 12 12:50:45.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9624" for this suite. @ 08/12/23 12:50:45.402
• [4.123 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/12/23 12:50:45.413
  Aug 12 12:50:45.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 12:50:45.414
  E0812 12:50:45.431728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:45.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:45.444
  STEP: create the deployment @ 08/12/23 12:50:45.448
  W0812 12:50:45.456106      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/12/23 12:50:45.456
  STEP: delete the deployment @ 08/12/23 12:50:45.966
  STEP: wait for all rs to be garbage collected @ 08/12/23 12:50:45.976
  STEP: expected 0 rs, got 1 rs @ 08/12/23 12:50:45.981
  STEP: expected 0 pods, got 2 pods @ 08/12/23 12:50:45.987
  E0812 12:50:46.431863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/12/23 12:50:46.508
  W0812 12:50:46.516838      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 12:50:46.517: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 12:50:46.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3221" for this suite. @ 08/12/23 12:50:46.527
• [1.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/12/23 12:50:46.541
  Aug 12 12:50:46.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pod-network-test @ 08/12/23 12:50:46.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:50:46.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:50:46.576
  STEP: Performing setup for networking test in namespace pod-network-test-509 @ 08/12/23 12:50:46.581
  STEP: creating a selector @ 08/12/23 12:50:46.581
  STEP: Creating the service pods in kubernetes @ 08/12/23 12:50:46.581
  Aug 12 12:50:46.581: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0812 12:50:47.432714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:48.432869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:49.433779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:50.433901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:51.434400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:52.434526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:53.434661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:54.434786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:55.434911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:56.435597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:57.435694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:58.435998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:50:59.436070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:00.436205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:01.436373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:02.436690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:03.436812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:04.437752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:05.438832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:06.439118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:07.439252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:08.439885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/12/23 12:51:08.753
  E0812 12:51:09.440412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:10.440785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:51:10.779: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 12 12:51:10.779: INFO: Breadth first check of 192.168.87.176 on host 172.31.32.142...
  Aug 12 12:51:10.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.134:9080/dial?request=hostname&protocol=http&host=192.168.87.176&port=8083&tries=1'] Namespace:pod-network-test-509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:51:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:51:10.785: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:51:10.785: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.87.176%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:51:10.890: INFO: Waiting for responses: map[]
  Aug 12 12:51:10.890: INFO: reached 192.168.87.176 after 0/1 tries
  Aug 12 12:51:10.890: INFO: Breadth first check of 192.168.182.15 on host 172.31.79.233...
  Aug 12 12:51:10.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.134:9080/dial?request=hostname&protocol=http&host=192.168.182.15&port=8083&tries=1'] Namespace:pod-network-test-509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:51:10.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:51:10.897: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:51:10.897: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.182.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:51:10.982: INFO: Waiting for responses: map[]
  Aug 12 12:51:10.983: INFO: reached 192.168.182.15 after 0/1 tries
  Aug 12 12:51:10.983: INFO: Breadth first check of 192.168.177.16 on host 172.31.84.203...
  Aug 12 12:51:10.987: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.87.134:9080/dial?request=hostname&protocol=http&host=192.168.177.16&port=8083&tries=1'] Namespace:pod-network-test-509 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:51:10.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:51:10.989: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:51:10.989: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-509/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.87.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.177.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 12 12:51:11.075: INFO: Waiting for responses: map[]
  Aug 12 12:51:11.075: INFO: reached 192.168.177.16 after 0/1 tries
  Aug 12 12:51:11.075: INFO: Going to retry 0 out of 3 pods....
  Aug 12 12:51:11.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-509" for this suite. @ 08/12/23 12:51:11.081
• [24.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/12/23 12:51:11.096
  Aug 12 12:51:11.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename tables @ 08/12/23 12:51:11.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:11.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:11.121
  Aug 12 12:51:11.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-7750" for this suite. @ 08/12/23 12:51:11.135
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/12/23 12:51:11.146
  Aug 12 12:51:11.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename events @ 08/12/23 12:51:11.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:11.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:11.171
  STEP: Create set of events @ 08/12/23 12:51:11.176
  STEP: get a list of Events with a label in the current namespace @ 08/12/23 12:51:11.2
  STEP: delete a list of events @ 08/12/23 12:51:11.205
  Aug 12 12:51:11.205: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/12/23 12:51:11.236
  Aug 12 12:51:11.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5554" for this suite. @ 08/12/23 12:51:11.247
• [0.110 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/12/23 12:51:11.256
  Aug 12 12:51:11.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename containers @ 08/12/23 12:51:11.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:11.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:11.278
  STEP: Creating a pod to test override arguments @ 08/12/23 12:51:11.283
  E0812 12:51:11.441328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:12.442262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:13.442605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:14.442914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:15.308
  Aug 12 12:51:15.312: INFO: Trying to get logs from node ip-172-31-32-142 pod client-containers-c8c024c4-bf2e-4934-8657-063295d4729a container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:15.323
  Aug 12 12:51:15.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8484" for this suite. @ 08/12/23 12:51:15.349
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/12/23 12:51:15.359
  Aug 12 12:51:15.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:51:15.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:15.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:15.384
  STEP: fetching services @ 08/12/23 12:51:15.393
  Aug 12 12:51:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1454" for this suite. @ 08/12/23 12:51:15.404
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/12/23 12:51:15.419
  Aug 12 12:51:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:51:15.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:15.44
  E0812 12:51:15.442891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:15.444
  STEP: create deployment with httpd image @ 08/12/23 12:51:15.448
  Aug 12 12:51:15.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3471 create -f -'
  Aug 12 12:51:15.927: INFO: stderr: ""
  Aug 12 12:51:15.927: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/12/23 12:51:15.927
  Aug 12 12:51:15.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3471 diff -f -'
  Aug 12 12:51:16.346: INFO: rc: 1
  Aug 12 12:51:16.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-3471 delete -f -'
  E0812 12:51:16.443729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:51:16.472: INFO: stderr: ""
  Aug 12 12:51:16.472: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug 12 12:51:16.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3471" for this suite. @ 08/12/23 12:51:16.48
• [1.076 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/12/23 12:51:16.495
  Aug 12 12:51:16.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename events @ 08/12/23 12:51:16.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:16.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:16.531
  STEP: Create set of events @ 08/12/23 12:51:16.536
  Aug 12 12:51:16.544: INFO: created test-event-1
  Aug 12 12:51:16.553: INFO: created test-event-2
  Aug 12 12:51:16.559: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/12/23 12:51:16.559
  STEP: delete collection of events @ 08/12/23 12:51:16.565
  Aug 12 12:51:16.565: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/12/23 12:51:16.606
  Aug 12 12:51:16.606: INFO: requesting list of events to confirm quantity
  Aug 12 12:51:16.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1427" for this suite. @ 08/12/23 12:51:16.622
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/12/23 12:51:16.641
  Aug 12 12:51:16.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:51:16.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:16.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:16.675
  STEP: Creating configMap with name projected-configmap-test-volume-d720d871-ece1-4505-9285-1aa79b328354 @ 08/12/23 12:51:16.68
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:51:16.688
  E0812 12:51:17.443937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:18.443950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:19.444071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:20.444389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:20.715
  Aug 12 12:51:20.720: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-30a14666-439c-4f1a-8fec-f154fa4641b4 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:51:20.735
  Aug 12 12:51:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7746" for this suite. @ 08/12/23 12:51:20.765
• [4.135 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/12/23 12:51:20.777
  Aug 12 12:51:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:51:20.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:20.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:20.801
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/12/23 12:51:20.805
  E0812 12:51:21.444691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:22.444684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:23.444892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:24.445289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:24.835
  Aug 12 12:51:24.839: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-7c40117b-df23-4acd-ae96-385a54c39b64 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:24.85
  Aug 12 12:51:24.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9315" for this suite. @ 08/12/23 12:51:24.879
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/12/23 12:51:24.892
  Aug 12 12:51:24.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:51:24.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:24.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:24.917
  STEP: Creating configMap with name projected-configmap-test-volume-map-0794ac79-a551-4f95-96a5-1c92a9d29403 @ 08/12/23 12:51:24.922
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:51:24.928
  E0812 12:51:25.445443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:26.445750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:27.446602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:28.447177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:28.959
  Aug 12 12:51:28.964: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-39e4826a-d50a-4f93-aa17-beef315bffbc container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:28.973
  Aug 12 12:51:28.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2617" for this suite. @ 08/12/23 12:51:29.005
• [4.123 seconds]
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/12/23 12:51:29.016
  Aug 12 12:51:29.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:51:29.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:29.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:29.042
  STEP: Creating a pod to test downward api env vars @ 08/12/23 12:51:29.047
  E0812 12:51:29.447206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:30.447788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:31.448599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:32.448841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:33.081
  Aug 12 12:51:33.085: INFO: Trying to get logs from node ip-172-31-32-142 pod downward-api-ef1e48f2-524c-4022-aa98-5a0c61cd56bc container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:33.099
  Aug 12 12:51:33.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3366" for this suite. @ 08/12/23 12:51:33.126
• [4.122 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/12/23 12:51:33.139
  Aug 12 12:51:33.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename containers @ 08/12/23 12:51:33.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:33.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:33.163
  STEP: Creating a pod to test override all @ 08/12/23 12:51:33.167
  E0812 12:51:33.449364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:34.449605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:35.450451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:36.451114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:37.198
  Aug 12 12:51:37.203: INFO: Trying to get logs from node ip-172-31-32-142 pod client-containers-639e6bb5-4e43-4f72-9ffd-cd7f71000169 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:37.214
  Aug 12 12:51:37.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3" for this suite. @ 08/12/23 12:51:37.238
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/12/23 12:51:37.25
  Aug 12 12:51:37.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 12:51:37.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:37.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:37.275
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/12/23 12:51:37.279
  E0812 12:51:37.451696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:38.451837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:39.452636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:40.452688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:41.308
  Aug 12 12:51:41.313: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-adccd894-0822-47e6-949b-b5dd84125bbf container test-container: <nil>
  STEP: delete the pod @ 08/12/23 12:51:41.323
  Aug 12 12:51:41.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4218" for this suite. @ 08/12/23 12:51:41.353
• [4.112 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/12/23 12:51:41.362
  Aug 12 12:51:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 12:51:41.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:41.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:41.391
  STEP: Creating a job @ 08/12/23 12:51:41.396
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/12/23 12:51:41.402
  E0812 12:51:41.453186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:42.453355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/12/23 12:51:43.408
  STEP: updating /status @ 08/12/23 12:51:43.417
  E0812 12:51:43.454198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get /status @ 08/12/23 12:51:43.454
  Aug 12 12:51:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6770" for this suite. @ 08/12/23 12:51:43.466
• [2.113 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/12/23 12:51:43.476
  Aug 12 12:51:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 12:51:43.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:43.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:43.517
  STEP: Creating projection with secret that has name projected-secret-test-65c9970d-51e8-4e71-8c19-a61de5cca6a6 @ 08/12/23 12:51:43.52
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:51:43.529
  E0812 12:51:44.454356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:45.454728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:46.455073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:47.455380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:51:47.561
  Aug 12 12:51:47.565: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-secrets-e194d3d5-2870-4c7e-afc0-8f7c9186141c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:51:47.576
  Aug 12 12:51:47.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-509" for this suite. @ 08/12/23 12:51:47.605
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/12/23 12:51:47.621
  Aug 12 12:51:47.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:51:47.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:47.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:47.651
  STEP: Create set of pods @ 08/12/23 12:51:47.655
  Aug 12 12:51:47.666: INFO: created test-pod-1
  Aug 12 12:51:47.682: INFO: created test-pod-2
  Aug 12 12:51:47.695: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/12/23 12:51:47.695
  E0812 12:51:48.455617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:49.456393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 08/12/23 12:51:49.762
  Aug 12 12:51:49.769: INFO: Pod quantity 3 is different from expected quantity 0
  E0812 12:51:50.456737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:51:50.778: INFO: Pod quantity 3 is different from expected quantity 0
  E0812 12:51:51.457091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:51:51.779: INFO: Pod quantity 1 is different from expected quantity 0
  E0812 12:51:52.457779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:51:52.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2268" for this suite. @ 08/12/23 12:51:52.78
• [5.171 seconds]
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/12/23 12:51:52.792
  Aug 12 12:51:52.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 12:51:52.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:51:52.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:51:52.816
  E0812 12:51:53.458095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:54.458684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:55.458772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:56.459122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:57.459457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:58.459719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:51:59.460658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:00.460790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:01.460939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:02.461759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:03.461816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:04.462077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:05.462196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:06.462744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:07.462895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:08.463006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:09.463287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:10.463851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:11.463871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:12.464676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:13.464690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:14.465745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:14.910: INFO: Container started at 2023-08-12 12:51:53 +0000 UTC, pod became ready at 2023-08-12 12:52:13 +0000 UTC
  Aug 12 12:52:14.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-4331" for this suite. @ 08/12/23 12:52:14.916
• [22.133 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/12/23 12:52:14.926
  Aug 12 12:52:14.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:52:14.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:14.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:14.951
  STEP: Creating configMap with name configmap-test-volume-2797fd8e-e7ce-4533-9745-382566dcfe6e @ 08/12/23 12:52:14.956
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:52:14.962
  E0812 12:52:15.466133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:16.466578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:17.467374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:18.467696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:52:18.989
  Aug 12 12:52:18.995: INFO: Trying to get logs from node ip-172-31-84-203 pod pod-configmaps-4915c891-a713-4203-9617-a79a5a114e17 container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:52:19.018
  Aug 12 12:52:19.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4191" for this suite. @ 08/12/23 12:52:19.043
• [4.125 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/12/23 12:52:19.053
  Aug 12 12:52:19.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubelet-test @ 08/12/23 12:52:19.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:19.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:19.078
  E0812 12:52:19.468632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:20.468840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:21.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7856" for this suite. @ 08/12/23 12:52:21.129
• [2.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/12/23 12:52:21.149
  Aug 12 12:52:21.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 12:52:21.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:21.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:21.171
  STEP: Creating a ResourceQuota @ 08/12/23 12:52:21.177
  STEP: Getting a ResourceQuota @ 08/12/23 12:52:21.185
  STEP: Updating a ResourceQuota @ 08/12/23 12:52:21.192
  STEP: Verifying a ResourceQuota was modified @ 08/12/23 12:52:21.2
  STEP: Deleting a ResourceQuota @ 08/12/23 12:52:21.204
  STEP: Verifying the deleted ResourceQuota @ 08/12/23 12:52:21.215
  Aug 12 12:52:21.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3434" for this suite. @ 08/12/23 12:52:21.224
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/12/23 12:52:21.237
  Aug 12 12:52:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 12:52:21.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:21.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:21.268
  STEP: Setting up server cert @ 08/12/23 12:52:21.309
  E0812 12:52:21.469219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 12:52:22.015
  STEP: Deploying the webhook pod @ 08/12/23 12:52:22.026
  STEP: Wait for the deployment to be ready @ 08/12/23 12:52:22.042
  Aug 12 12:52:22.050: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0812 12:52:22.469387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:23.469492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:24.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 12, 52, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 52, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 12, 52, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 12, 52, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 12:52:24.469770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:25.470188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 12:52:26.071
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 12:52:26.09
  E0812 12:52:26.470985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:27.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/12/23 12:52:27.096
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/12/23 12:52:27.12
  Aug 12 12:52:27.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:52:27.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1531" for this suite. @ 08/12/23 12:52:27.225
  STEP: Destroying namespace "webhook-markers-3606" for this suite. @ 08/12/23 12:52:27.239
• [6.011 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/12/23 12:52:27.249
  Aug 12 12:52:27.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:52:27.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:27.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:27.284
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:52:27.288
  E0812 12:52:27.471360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:28.471491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:29.471858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:30.472207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:52:31.314
  Aug 12 12:52:31.320: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-c67dcc54-11cd-43a7-b398-b6d825252904 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:52:31.331
  Aug 12 12:52:31.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3479" for this suite. @ 08/12/23 12:52:31.358
• [4.118 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/12/23 12:52:31.368
  Aug 12 12:52:31.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:52:31.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:31.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:31.394
  STEP: validating cluster-info @ 08/12/23 12:52:31.399
  Aug 12 12:52:31.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-7757 cluster-info'
  E0812 12:52:31.472889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:31.504: INFO: stderr: ""
  Aug 12 12:52:31.504: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug 12 12:52:31.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7757" for this suite. @ 08/12/23 12:52:31.51
• [0.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/12/23 12:52:31.526
  Aug 12 12:52:31.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 12:52:31.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:31.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:31.553
  STEP: Creating configMap configmap-5875/configmap-test-867e4a22-9884-4562-92e2-09b53ff30450 @ 08/12/23 12:52:31.558
  STEP: Creating a pod to test consume configMaps @ 08/12/23 12:52:31.568
  E0812 12:52:32.472912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:33.473026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:34.473319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:35.473471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:52:35.6
  Aug 12 12:52:35.604: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-773fa9b8-d603-4b3d-bf3c-326043e0ba76 container env-test: <nil>
  STEP: delete the pod @ 08/12/23 12:52:35.616
  Aug 12 12:52:35.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5875" for this suite. @ 08/12/23 12:52:35.644
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/12/23 12:52:35.655
  Aug 12 12:52:35.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl-logs @ 08/12/23 12:52:35.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:35.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:35.677
  STEP: creating an pod @ 08/12/23 12:52:35.687
  Aug 12 12:52:35.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug 12 12:52:35.780: INFO: stderr: ""
  Aug 12 12:52:35.780: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/12/23 12:52:35.78
  Aug 12 12:52:35.780: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0812 12:52:36.474552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:37.475272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:37.791: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/12/23 12:52:37.791
  Aug 12 12:52:37.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator'
  Aug 12 12:52:37.882: INFO: stderr: ""
  Aug 12 12:52:37.882: INFO: stdout: "I0812 12:52:36.821922       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/8p77 377\nI0812 12:52:37.022260       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/g9p6 477\nI0812 12:52:37.222535       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/nn57 220\nI0812 12:52:37.422881       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mcg 594\nI0812 12:52:37.622049       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/h75 522\nI0812 12:52:37.822405       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wgq 430\n"
  STEP: limiting log lines @ 08/12/23 12:52:37.882
  Aug 12 12:52:37.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator --tail=1'
  Aug 12 12:52:37.974: INFO: stderr: ""
  Aug 12 12:52:37.974: INFO: stdout: "I0812 12:52:37.822405       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wgq 430\n"
  Aug 12 12:52:37.974: INFO: got output "I0812 12:52:37.822405       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wgq 430\n"
  STEP: limiting log bytes @ 08/12/23 12:52:37.974
  Aug 12 12:52:37.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator --limit-bytes=1'
  Aug 12 12:52:38.208: INFO: stderr: ""
  Aug 12 12:52:38.208: INFO: stdout: "I"
  Aug 12 12:52:38.208: INFO: got output "I"
  STEP: exposing timestamps @ 08/12/23 12:52:38.208
  Aug 12 12:52:38.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug 12 12:52:38.297: INFO: stderr: ""
  Aug 12 12:52:38.297: INFO: stdout: "2023-08-12T12:52:38.223010353Z I0812 12:52:38.222842       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/bwz 297\n"
  Aug 12 12:52:38.297: INFO: got output "2023-08-12T12:52:38.223010353Z I0812 12:52:38.222842       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/bwz 297\n"
  STEP: restricting to a time range @ 08/12/23 12:52:38.297
  E0812 12:52:38.475707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:39.476025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:40.476165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:40.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator --since=1s'
  Aug 12 12:52:40.897: INFO: stderr: ""
  Aug 12 12:52:40.898: INFO: stdout: "I0812 12:52:40.022386       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/d2p 469\nI0812 12:52:40.222561       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tkhr 277\nI0812 12:52:40.422949       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/lkcc 544\nI0812 12:52:40.622298       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/xqp 229\nI0812 12:52:40.822563       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/dcdc 320\n"
  Aug 12 12:52:40.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 logs logs-generator logs-generator --since=24h'
  Aug 12 12:52:40.999: INFO: stderr: ""
  Aug 12 12:52:40.999: INFO: stdout: "I0812 12:52:36.821922       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/8p77 377\nI0812 12:52:37.022260       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/g9p6 477\nI0812 12:52:37.222535       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/nn57 220\nI0812 12:52:37.422881       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mcg 594\nI0812 12:52:37.622049       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/h75 522\nI0812 12:52:37.822405       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wgq 430\nI0812 12:52:38.022522       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/dk2 493\nI0812 12:52:38.222842       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/bwz 297\nI0812 12:52:38.422078       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/gbbt 324\nI0812 12:52:38.622439       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/g5ff 298\nI0812 12:52:38.822775       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/r6pq 533\nI0812 12:52:39.022049       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/qj5h 201\nI0812 12:52:39.222389       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/tzfh 312\nI0812 12:52:39.422560       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/xrb 504\nI0812 12:52:39.622796       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/tkm 247\nI0812 12:52:39.822048       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/j7r 435\nI0812 12:52:40.022386       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/d2p 469\nI0812 12:52:40.222561       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tkhr 277\nI0812 12:52:40.422949       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/lkcc 544\nI0812 12:52:40.622298       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/xqp 229\nI0812 12:52:40.822563       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/dcdc 320\n"
  Aug 12 12:52:40.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-logs-2695 delete pod logs-generator'
  E0812 12:52:41.476294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:42.368: INFO: stderr: ""
  Aug 12 12:52:42.368: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug 12 12:52:42.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-2695" for this suite. @ 08/12/23 12:52:42.375
• [6.729 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/12/23 12:52:42.385
  Aug 12 12:52:42.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 12:52:42.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:42.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:42.409
  STEP: creating the pod @ 08/12/23 12:52:42.414
  Aug 12 12:52:42.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 create -f -'
  E0812 12:52:42.476407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:42.741: INFO: stderr: ""
  Aug 12 12:52:42.741: INFO: stdout: "pod/pause created\n"
  E0812 12:52:43.476693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:44.477765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/12/23 12:52:44.752
  Aug 12 12:52:44.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 label pods pause testing-label=testing-label-value'
  Aug 12 12:52:44.851: INFO: stderr: ""
  Aug 12 12:52:44.851: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/12/23 12:52:44.851
  Aug 12 12:52:44.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 get pod pause -L testing-label'
  Aug 12 12:52:44.941: INFO: stderr: ""
  Aug 12 12:52:44.941: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/12/23 12:52:44.941
  Aug 12 12:52:44.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 label pods pause testing-label-'
  Aug 12 12:52:45.036: INFO: stderr: ""
  Aug 12 12:52:45.036: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/12/23 12:52:45.036
  Aug 12 12:52:45.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 get pod pause -L testing-label'
  Aug 12 12:52:45.119: INFO: stderr: ""
  Aug 12 12:52:45.119: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 08/12/23 12:52:45.119
  Aug 12 12:52:45.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 delete --grace-period=0 --force -f -'
  Aug 12 12:52:45.210: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 12:52:45.210: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug 12 12:52:45.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 get rc,svc -l name=pause --no-headers'
  Aug 12 12:52:45.299: INFO: stderr: "No resources found in kubectl-9876 namespace.\n"
  Aug 12 12:52:45.299: INFO: stdout: ""
  Aug 12 12:52:45.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-9876 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 12 12:52:45.396: INFO: stderr: ""
  Aug 12 12:52:45.396: INFO: stdout: ""
  Aug 12 12:52:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9876" for this suite. @ 08/12/23 12:52:45.403
• [3.028 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 08/12/23 12:52:45.415
  Aug 12 12:52:45.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 12:52:45.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:45.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:45.441
  Aug 12 12:52:45.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 12:52:45.477807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:46.478133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:47.478439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0812 12:52:48.013014      19 warnings.go:70] unknown field "alpha"
  W0812 12:52:48.013047      19 warnings.go:70] unknown field "beta"
  W0812 12:52:48.013058      19 warnings.go:70] unknown field "delta"
  W0812 12:52:48.013074      19 warnings.go:70] unknown field "epsilon"
  W0812 12:52:48.013084      19 warnings.go:70] unknown field "gamma"
  E0812 12:52:48.479142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:48.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6671" for this suite. @ 08/12/23 12:52:48.593
• [3.186 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/12/23 12:52:48.602
  Aug 12 12:52:48.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename prestop @ 08/12/23 12:52:48.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:48.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:48.624
  STEP: Creating server pod server in namespace prestop-1732 @ 08/12/23 12:52:48.629
  STEP: Waiting for pods to come up. @ 08/12/23 12:52:48.639
  E0812 12:52:49.479238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:50.480035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-1732 @ 08/12/23 12:52:50.653
  E0812 12:52:51.481157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:52.481271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 08/12/23 12:52:52.674
  E0812 12:52:53.481786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:54.482072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:55.482180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:56.483288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:57.483971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:52:57.691: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug 12 12:52:57.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/12/23 12:52:57.699
  STEP: Destroying namespace "prestop-1732" for this suite. @ 08/12/23 12:52:57.723
• [9.132 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/12/23 12:52:57.734
  Aug 12 12:52:57.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:52:57.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:52:57.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:52:57.759
  STEP: Creating secret with name secret-test-3c0a3a8a-d481-4a41-ab39-1af9167ca308 @ 08/12/23 12:52:57.763
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:52:57.77
  E0812 12:52:58.484928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:52:59.485057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:00.485754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:01.486625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:53:01.796
  Aug 12 12:53:01.800: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-2ab24e77-dabf-46e4-a864-ef3e1aac9996 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 12:53:01.812
  Aug 12 12:53:01.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6863" for this suite. @ 08/12/23 12:53:01.841
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/12/23 12:53:01.858
  Aug 12 12:53:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:53:01.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:01.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:01.884
  STEP: creating the pod @ 08/12/23 12:53:01.888
  STEP: submitting the pod to kubernetes @ 08/12/23 12:53:01.888
  STEP: verifying QOS class is set on the pod @ 08/12/23 12:53:01.9
  Aug 12 12:53:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2779" for this suite. @ 08/12/23 12:53:01.914
• [0.066 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/12/23 12:53:01.924
  Aug 12 12:53:01.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:53:01.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:01.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:01.953
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7903 @ 08/12/23 12:53:01.958
  STEP: changing the ExternalName service to type=ClusterIP @ 08/12/23 12:53:01.966
  STEP: creating replication controller externalname-service in namespace services-7903 @ 08/12/23 12:53:01.991
  I0812 12:53:02.007904      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7903, replica count: 2
  E0812 12:53:02.486994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:03.487230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:04.487337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 12:53:05.058760      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 12:53:05.058: INFO: Creating new exec pod
  E0812 12:53:05.488354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:06.488464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:07.489139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:08.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-7903 exec execpodfzfgs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 12 12:53:08.241: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 12 12:53:08.241: INFO: stdout: "externalname-service-mcz4k"
  Aug 12 12:53:08.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-7903 exec execpodfzfgs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.106 80'
  Aug 12 12:53:08.399: INFO: stderr: "+ nc -v -t -w 2 10.152.183.106 80\n+ echo hostName\nConnection to 10.152.183.106 80 port [tcp/http] succeeded!\n"
  Aug 12 12:53:08.399: INFO: stdout: ""
  E0812 12:53:08.489712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:09.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-7903 exec execpodfzfgs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.106 80'
  E0812 12:53:09.490205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:09.561: INFO: stderr: "+ nc -v -t -w 2 10.152.183.106 80\n+ echo hostName\nConnection to 10.152.183.106 80 port [tcp/http] succeeded!\n"
  Aug 12 12:53:09.561: INFO: stdout: "externalname-service-mcz4k"
  Aug 12 12:53:09.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:53:09.567: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7903" for this suite. @ 08/12/23 12:53:09.604
• [7.690 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/12/23 12:53:09.615
  Aug 12 12:53:09.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 12:53:09.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:09.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:09.64
  STEP: creating service in namespace services-6093 @ 08/12/23 12:53:09.645
  STEP: creating service affinity-nodeport in namespace services-6093 @ 08/12/23 12:53:09.645
  STEP: creating replication controller affinity-nodeport in namespace services-6093 @ 08/12/23 12:53:09.666
  I0812 12:53:09.677028      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6093, replica count: 3
  E0812 12:53:10.490969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:11.491661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:12.491812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 12:53:12.728472      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 12:53:12.743: INFO: Creating new exec pod
  E0812 12:53:13.492307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:14.492420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:15.492707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:15.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-6093 exec execpod-affinity8r6d5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug 12 12:53:15.948: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug 12 12:53:15.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:53:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-6093 exec execpod-affinity8r6d5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.53 80'
  Aug 12 12:53:16.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.53 80\nConnection to 10.152.183.53 80 port [tcp/http] succeeded!\n"
  Aug 12 12:53:16.172: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:53:16.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-6093 exec execpod-affinity8r6d5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.84.203 31354'
  Aug 12 12:53:16.443: INFO: stderr: "+ nc -v -t -w 2 172.31.84.203 31354\n+ echo hostName\nConnection to 172.31.84.203 31354 port [tcp/*] succeeded!\n"
  Aug 12 12:53:16.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:53:16.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-6093 exec execpod-affinity8r6d5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.32.142 31354'
  E0812 12:53:16.493253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:16.663: INFO: stderr: "+ nc -v -t -w 2 172.31.32.142 31354\n+ echo hostName\nConnection to 172.31.32.142 31354 port [tcp/*] succeeded!\n"
  Aug 12 12:53:16.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 12 12:53:16.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-6093 exec execpod-affinity8r6d5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.32.142:31354/ ; done'
  Aug 12 12:53:16.927: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.32.142:31354/\n"
  Aug 12 12:53:16.927: INFO: stdout: "\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f\naffinity-nodeport-nvn8f"
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Received response from host: affinity-nodeport-nvn8f
  Aug 12 12:53:16.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 12:53:16.932: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6093, will wait for the garbage collector to delete the pods @ 08/12/23 12:53:16.954
  Aug 12 12:53:17.022: INFO: Deleting ReplicationController affinity-nodeport took: 10.549853ms
  Aug 12 12:53:17.123: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.068457ms
  E0812 12:53:17.493550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:18.494659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:19.495172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-6093" for this suite. @ 08/12/23 12:53:19.562
• [9.959 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/12/23 12:53:19.575
  Aug 12 12:53:19.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-runtime @ 08/12/23 12:53:19.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:19.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:19.598
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/12/23 12:53:19.612
  E0812 12:53:20.495321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:21.496296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:22.496697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:23.496851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:24.496946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:25.497818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:26.498669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:27.498763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:28.499697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:29.500804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:30.501751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:31.501944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:32.502066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:33.502180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:34.502637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/12/23 12:53:34.706
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/12/23 12:53:34.711
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/12/23 12:53:34.721
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/12/23 12:53:34.721
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/12/23 12:53:34.751
  E0812 12:53:35.502723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:36.503671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:37.503797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/12/23 12:53:37.779
  E0812 12:53:38.504644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:39.505664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/12/23 12:53:39.794
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/12/23 12:53:39.804
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/12/23 12:53:39.804
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/12/23 12:53:39.835
  E0812 12:53:40.506216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/12/23 12:53:40.85
  E0812 12:53:41.506986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:42.507838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:43.507969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/12/23 12:53:43.873
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/12/23 12:53:43.882
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/12/23 12:53:43.882
  Aug 12 12:53:43.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1637" for this suite. @ 08/12/23 12:53:43.931
• [24.370 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/12/23 12:53:43.946
  Aug 12 12:53:43.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 12:53:43.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:43.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:43.972
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 12:53:43.977
  E0812 12:53:44.508865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:45.509298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:46.509449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:47.509599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:53:48.009
  Aug 12 12:53:48.014: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-5867e57b-761c-4c65-82a0-97a01857b817 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 12:53:48.025
  Aug 12 12:53:48.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2242" for this suite. @ 08/12/23 12:53:48.049
• [4.111 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/12/23 12:53:48.06
  Aug 12 12:53:48.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:53:48.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:48.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:48.084
  STEP: creating a secret @ 08/12/23 12:53:48.094
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/12/23 12:53:48.101
  STEP: patching the secret @ 08/12/23 12:53:48.107
  STEP: deleting the secret using a LabelSelector @ 08/12/23 12:53:48.118
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/12/23 12:53:48.13
  Aug 12 12:53:48.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5780" for this suite. @ 08/12/23 12:53:48.142
• [0.092 seconds]
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/12/23 12:53:48.152
  Aug 12 12:53:48.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 12:53:48.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:53:48.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:53:48.181
  STEP: Creating pod test-grpc-6daa84f8-a07a-48a2-98a8-4e21c3fecfb9 in namespace container-probe-6924 @ 08/12/23 12:53:48.185
  E0812 12:53:48.510642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:49.511302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:53:50.215: INFO: Started pod test-grpc-6daa84f8-a07a-48a2-98a8-4e21c3fecfb9 in namespace container-probe-6924
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 12:53:50.215
  Aug 12 12:53:50.220: INFO: Initial restart count of pod test-grpc-6daa84f8-a07a-48a2-98a8-4e21c3fecfb9 is 0
  E0812 12:53:50.512391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:51.512699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:52.513777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:53.513894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:54.514976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:55.515263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:56.515894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:57.516012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:58.516480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:53:59.516710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:00.517630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:01.518053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:02.519120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:03.519596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:04.520094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:05.520219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:06.520294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:07.521059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:08.521209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:09.522153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:10.522278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:11.522562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:12.522952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:13.523230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:14.523865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:15.523954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:16.524436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:17.525487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:18.526313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:19.527438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:20.527849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:21.528215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:22.529317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:23.529431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:24.530117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:25.530254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:26.531227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:27.531424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:28.531622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:29.532020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:30.532598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:31.532739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:32.533245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:33.533411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:34.533789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:35.533903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:36.534292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:37.534793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:38.535732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:39.535864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:40.536759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:41.536984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:42.537389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:43.537544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:44.537736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:45.537981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:46.538031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:47.539040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:48.539467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:49.539807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:50.540104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:51.540359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:52.541422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:53.541560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:54.541633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:55.542095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:56.542323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:57.542456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:58.543195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:54:59.543403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:00.544391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:01.545116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:02.545862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:03.546002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:04.445: INFO: Restart count of pod container-probe-6924/test-grpc-6daa84f8-a07a-48a2-98a8-4e21c3fecfb9 is now 1 (1m14.224515528s elapsed)
  Aug 12 12:55:04.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 12:55:04.449
  STEP: Destroying namespace "container-probe-6924" for this suite. @ 08/12/23 12:55:04.467
• [76.324 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/12/23 12:55:04.48
  Aug 12 12:55:04.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename endpointslice @ 08/12/23 12:55:04.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:55:04.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:55:04.505
  E0812 12:55:04.546048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:05.546199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:06.546198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:06.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3971" for this suite. @ 08/12/23 12:55:06.595
• [2.126 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/12/23 12:55:06.606
  Aug 12 12:55:06.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pod-network-test @ 08/12/23 12:55:06.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:55:06.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:55:06.631
  STEP: Performing setup for networking test in namespace pod-network-test-7665 @ 08/12/23 12:55:06.635
  STEP: creating a selector @ 08/12/23 12:55:06.635
  STEP: Creating the service pods in kubernetes @ 08/12/23 12:55:06.635
  Aug 12 12:55:06.635: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0812 12:55:07.546357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:08.546464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:09.546782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:10.546913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:11.547843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:12.547936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:13.548054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:14.548367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:15.548717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:16.549371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:17.549482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:18.549762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:19.549900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:20.550462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:21.551020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:22.551508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:23.552181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:24.552324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:25.553044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:26.553213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:27.553776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:28.554258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/12/23 12:55:28.777
  E0812 12:55:29.554430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:30.555327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:30.815: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 12 12:55:30.815: INFO: Going to poll 192.168.87.141 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:55:30.819: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.87.141 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7665 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:55:30.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:55:30.820: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:55:30.820: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7665/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.87.141+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0812 12:55:31.555680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:31.910: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 12 12:55:31.910: INFO: Going to poll 192.168.182.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:55:31.915: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.182.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7665 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:55:31.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:55:31.916: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:55:31.916: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7665/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.182.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0812 12:55:32.556676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:32.991: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 12 12:55:32.991: INFO: Going to poll 192.168.177.24 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 12 12:55:32.997: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.177.24 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7665 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 12:55:32.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 12:55:32.998: INFO: ExecWithOptions: Clientset creation
  Aug 12 12:55:32.998: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7665/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.177.24+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0812 12:55:33.557756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:34.080: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 12 12:55:34.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7665" for this suite. @ 08/12/23 12:55:34.086
• [27.488 seconds]
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/12/23 12:55:34.095
  Aug 12 12:55:34.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 12:55:34.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:55:34.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:55:34.118
  STEP: Creating secret with name secret-test-4b848ee0-5367-4a4a-afc6-3704b32afda7 @ 08/12/23 12:55:34.123
  STEP: Creating a pod to test consume secrets @ 08/12/23 12:55:34.13
  E0812 12:55:34.558739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:35.559018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:36.559743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:37.560450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 12:55:38.16
  Aug 12 12:55:38.165: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-71247e91-9dcf-4f50-937f-2ed01a698772 container secret-env-test: <nil>
  STEP: delete the pod @ 08/12/23 12:55:38.192
  Aug 12 12:55:38.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-493" for this suite. @ 08/12/23 12:55:38.217
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/12/23 12:55:38.23
  Aug 12 12:55:38.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 12:55:38.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:55:38.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:55:38.281
  Aug 12 12:55:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: creating the pod @ 08/12/23 12:55:38.286
  STEP: submitting the pod to kubernetes @ 08/12/23 12:55:38.286
  E0812 12:55:38.560607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:39.561142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 12:55:40.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2105" for this suite. @ 08/12/23 12:55:40.438
• [2.218 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/12/23 12:55:40.449
  Aug 12 12:55:40.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename cronjob @ 08/12/23 12:55:40.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 12:55:40.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 12:55:40.48
  STEP: Creating a ForbidConcurrent cronjob @ 08/12/23 12:55:40.484
  STEP: Ensuring a job is scheduled @ 08/12/23 12:55:40.491
  E0812 12:55:40.561534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:41.561765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:42.562008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:43.563083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:44.563744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:45.563812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:46.564671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:47.565452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:48.565819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:49.566420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:50.567028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:51.567545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:52.567655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:53.567984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:54.569064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:55.569245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:56.569755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:57.569457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:58.569644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:55:59.569969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/12/23 12:56:00.498
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/12/23 12:56:00.503
  STEP: Ensuring no more jobs are scheduled @ 08/12/23 12:56:00.508
  E0812 12:56:00.570726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:01.571449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:02.572434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:03.573118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:04.573801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:05.574133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:06.574866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:07.574999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:08.575256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:09.576041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:10.576892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:11.577539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:12.578389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:13.578527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:14.579204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:15.579312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:16.580022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:17.580380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:18.581333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:19.581626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:20.581691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:21.582108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:22.583017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:23.583113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:24.583789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:25.584035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:26.584700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:27.584807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:28.585389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:29.585756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:30.586018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:31.586439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:32.586783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:33.586819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:34.587510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:35.587658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:36.588384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:37.588983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:38.589713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:39.589854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:40.590064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:41.590705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:42.591038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:43.591523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:44.592098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:45.592231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:46.592575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:47.592905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:48.593657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:49.593772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:50.594783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:51.595144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:52.595806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:53.595999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:54.596815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:55.596959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:56.597925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:57.598066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:58.598406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:56:59.598549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:00.598638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:01.599453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:02.599823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:03.600143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:04.600384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:05.600767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:06.601733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:07.601997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:08.602610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:09.602858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:10.603186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:11.603711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:12.604335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:13.604708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:14.605096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:15.605447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:16.605913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:17.606058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:18.606532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:19.607558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:20.608722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:21.608697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:22.609300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:23.609475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:24.610092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:25.610400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:26.610857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:27.611040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:28.611980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:29.612116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:30.615221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:31.616052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:32.616572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:33.616778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:34.617371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:35.617501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:36.617583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:37.617960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:38.618651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:39.618772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:40.619433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:41.620251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:42.620652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:43.620918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:44.621656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:45.622582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:46.623479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:47.623618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:48.624089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:49.624706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:50.625045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:51.625660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:52.626602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:53.626983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:54.627835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:55.627965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:56.628532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:57.628868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:58.629768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:57:59.629870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:00.630631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:01.630693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:02.631461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:03.631591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:04.631711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:05.631821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:06.632552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:07.632775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:08.633350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:09.634079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:10.634990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:11.635121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:12.635780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:13.636891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:14.637099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:15.637229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:16.638101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:17.638212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:18.638651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:19.639077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:20.640013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:21.640418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:22.641170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:23.641293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:24.642282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:25.642435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:26.642523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:27.642828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:28.643803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:29.643932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:30.644860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:31.644987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:32.645505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:33.645783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:34.646700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:35.646830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:36.646950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:37.647970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:38.648042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:39.648336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:40.648945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:41.649324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:42.650191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:43.651185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:44.651654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:45.651760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:46.652256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:47.652664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:48.652804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:49.653192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:50.653562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:51.653640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:52.653913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:53.654060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:54.654839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:55.654963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:56.655983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:57.656449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:58.657186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:58:59.657749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:00.658666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:01.659604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:02.660647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:03.660688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:04.661055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:05.661187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:06.662128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:07.662359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:08.663306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:09.663434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:10.663569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:11.664349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:12.664871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:13.665750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:14.666670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:15.666806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:16.667661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:17.667901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:18.668052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:19.668331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:20.669452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:21.669820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:22.670887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:23.671348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:24.671516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:25.671796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:26.671921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:27.672902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:28.673757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:29.674020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:30.674681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:31.674816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:32.675737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:33.675901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:34.676010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:35.676098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:36.676690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:37.676793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:38.676905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:39.677757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:40.678624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:41.678899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:42.679194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:43.679331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:44.680256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:45.680388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:46.680483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:47.680780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:48.681549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:49.681825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:50.681973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:51.682112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:52.682233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:53.683310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:54.683442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:55.683613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:56.683930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:57.684054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:58.684393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 12:59:59.684492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:00.684634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:01.684686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:02.684991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:03.685751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:04.686606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:05.686741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:06.687110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:07.687277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:08.687934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:09.688251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:10.689269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:11.689408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:12.689733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:13.690013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:14.691081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:15.691362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:16.691525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:17.692403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:18.692556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:19.692823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:20.693551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:21.693931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:22.694744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:23.695748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:24.696558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:25.696700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:26.696795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:27.697780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:28.698634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:29.699462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:30.699861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:31.699993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:32.700793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:33.701743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:34.702662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:35.703273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:36.703364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:37.704108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:38.704227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:39.704658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:40.705740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:41.705867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:42.706403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:43.706691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:44.707648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:45.707916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:46.708458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:47.708680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:48.709508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:49.709695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:50.710290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:51.710772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:52.711574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:53.712130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:54.713127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:55.713228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:56.714198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:57.714344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:58.714468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:00:59.714777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/12/23 13:01:00.52
  Aug 12 13:01:00.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1547" for this suite. @ 08/12/23 13:01:00.534
• [320.094 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/12/23 13:01:00.548
  Aug 12 13:01:00.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context-test @ 08/12/23 13:01:00.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:00.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:00.63
  E0812 13:01:00.715718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:01.715971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:02.716712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:03.717787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:04.693: INFO: Got logs for pod "busybox-privileged-false-eefe5b44-e844-4102-b344-0e61af906202": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug 12 13:01:04.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1543" for this suite. @ 08/12/23 13:01:04.7
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/12/23 13:01:04.717
  Aug 12 13:01:04.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:01:04.718511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:01:04.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:04.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:04.746
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:01:04.751
  E0812 13:01:05.719400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:06.719937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:07.720045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:08.720627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:01:08.786
  Aug 12 13:01:08.790: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-960462ac-0c04-4aaf-81fd-c6efb4d4a963 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:01:08.799
  Aug 12 13:01:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8249" for this suite. @ 08/12/23 13:01:08.827
• [4.122 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/12/23 13:01:08.841
  Aug 12 13:01:08.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:01:08.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:08.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:08.864
  STEP: creating a replication controller @ 08/12/23 13:01:08.869
  Aug 12 13:01:08.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 create -f -'
  Aug 12 13:01:09.273: INFO: stderr: ""
  Aug 12 13:01:09.273: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/12/23 13:01:09.273
  Aug 12 13:01:09.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 13:01:09.364: INFO: stderr: ""
  Aug 12 13:01:09.364: INFO: stdout: "update-demo-nautilus-6j8cx update-demo-nautilus-8dvcq "
  Aug 12 13:01:09.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:09.446: INFO: stderr: ""
  Aug 12 13:01:09.446: INFO: stdout: ""
  Aug 12 13:01:09.446: INFO: update-demo-nautilus-6j8cx is created but not running
  E0812 13:01:09.721354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:10.721469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:11.722013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:12.722105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:13.722236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:14.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 13:01:14.533: INFO: stderr: ""
  Aug 12 13:01:14.533: INFO: stdout: "update-demo-nautilus-6j8cx update-demo-nautilus-8dvcq "
  Aug 12 13:01:14.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:14.615: INFO: stderr: ""
  Aug 12 13:01:14.615: INFO: stdout: "true"
  Aug 12 13:01:14.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:14.697: INFO: stderr: ""
  Aug 12 13:01:14.697: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:14.697: INFO: validating pod update-demo-nautilus-6j8cx
  Aug 12 13:01:14.703: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:14.704: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:14.704: INFO: update-demo-nautilus-6j8cx is verified up and running
  Aug 12 13:01:14.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-8dvcq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0812 13:01:14.722733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:14.787: INFO: stderr: ""
  Aug 12 13:01:14.787: INFO: stdout: "true"
  Aug 12 13:01:14.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-8dvcq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:14.868: INFO: stderr: ""
  Aug 12 13:01:14.868: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:14.868: INFO: validating pod update-demo-nautilus-8dvcq
  Aug 12 13:01:14.877: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:14.877: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:14.877: INFO: update-demo-nautilus-8dvcq is verified up and running
  STEP: scaling down the replication controller @ 08/12/23 13:01:14.877
  Aug 12 13:01:14.878: INFO: scanned /root for discovery docs: <nil>
  Aug 12 13:01:14.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0812 13:01:15.722801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:15.982: INFO: stderr: ""
  Aug 12 13:01:15.982: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/12/23 13:01:15.982
  Aug 12 13:01:15.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 13:01:16.065: INFO: stderr: ""
  Aug 12 13:01:16.065: INFO: stdout: "update-demo-nautilus-6j8cx "
  Aug 12 13:01:16.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:16.144: INFO: stderr: ""
  Aug 12 13:01:16.144: INFO: stdout: "true"
  Aug 12 13:01:16.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:16.224: INFO: stderr: ""
  Aug 12 13:01:16.224: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:16.224: INFO: validating pod update-demo-nautilus-6j8cx
  Aug 12 13:01:16.230: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:16.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:16.230: INFO: update-demo-nautilus-6j8cx is verified up and running
  STEP: scaling up the replication controller @ 08/12/23 13:01:16.23
  Aug 12 13:01:16.232: INFO: scanned /root for discovery docs: <nil>
  Aug 12 13:01:16.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0812 13:01:16.723871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:17.339: INFO: stderr: ""
  Aug 12 13:01:17.340: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/12/23 13:01:17.34
  Aug 12 13:01:17.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 12 13:01:17.427: INFO: stderr: ""
  Aug 12 13:01:17.427: INFO: stdout: "update-demo-nautilus-6j8cx update-demo-nautilus-jzrxq "
  Aug 12 13:01:17.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:17.508: INFO: stderr: ""
  Aug 12 13:01:17.508: INFO: stdout: "true"
  Aug 12 13:01:17.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:17.590: INFO: stderr: ""
  Aug 12 13:01:17.590: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:17.590: INFO: validating pod update-demo-nautilus-6j8cx
  Aug 12 13:01:17.595: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:17.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:17.595: INFO: update-demo-nautilus-6j8cx is verified up and running
  Aug 12 13:01:17.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-jzrxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:17.677: INFO: stderr: ""
  Aug 12 13:01:17.677: INFO: stdout: ""
  Aug 12 13:01:17.677: INFO: update-demo-nautilus-jzrxq is created but not running
  E0812 13:01:17.723921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:18.724028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:19.724323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:20.724467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:21.724663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:22.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0812 13:01:22.725385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:22.763: INFO: stderr: ""
  Aug 12 13:01:22.763: INFO: stdout: "update-demo-nautilus-6j8cx update-demo-nautilus-jzrxq "
  Aug 12 13:01:22.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:22.845: INFO: stderr: ""
  Aug 12 13:01:22.845: INFO: stdout: "true"
  Aug 12 13:01:22.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-6j8cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:22.926: INFO: stderr: ""
  Aug 12 13:01:22.926: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:22.926: INFO: validating pod update-demo-nautilus-6j8cx
  Aug 12 13:01:22.931: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:22.931: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:22.931: INFO: update-demo-nautilus-6j8cx is verified up and running
  Aug 12 13:01:22.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-jzrxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 12 13:01:23.018: INFO: stderr: ""
  Aug 12 13:01:23.018: INFO: stdout: "true"
  Aug 12 13:01:23.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods update-demo-nautilus-jzrxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 12 13:01:23.098: INFO: stderr: ""
  Aug 12 13:01:23.098: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 12 13:01:23.098: INFO: validating pod update-demo-nautilus-jzrxq
  Aug 12 13:01:23.105: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 12 13:01:23.105: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 12 13:01:23.105: INFO: update-demo-nautilus-jzrxq is verified up and running
  STEP: using delete to clean up resources @ 08/12/23 13:01:23.105
  Aug 12 13:01:23.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 delete --grace-period=0 --force -f -'
  Aug 12 13:01:23.195: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 12 13:01:23.195: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 12 13:01:23.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get rc,svc -l name=update-demo --no-headers'
  Aug 12 13:01:23.341: INFO: stderr: "No resources found in kubectl-1644 namespace.\n"
  Aug 12 13:01:23.341: INFO: stdout: ""
  Aug 12 13:01:23.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-1644 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 12 13:01:23.472: INFO: stderr: ""
  Aug 12 13:01:23.472: INFO: stdout: ""
  Aug 12 13:01:23.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1644" for this suite. @ 08/12/23 13:01:23.48
• [14.648 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/12/23 13:01:23.491
  Aug 12 13:01:23.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename endpointslice @ 08/12/23 13:01:23.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:23.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:23.52
  E0812 13:01:23.726189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:24.726291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:25.727363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:26.727643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:27.727621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/12/23 13:01:28.629
  E0812 13:01:28.728737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:29.728840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:30.728996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:31.729971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:32.730086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/12/23 13:01:33.643
  E0812 13:01:33.730821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:34.731180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:35.731249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:36.731619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:37.731694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/12/23 13:01:38.653
  E0812 13:01:38.732244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:39.732581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:40.732774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:41.733223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:42.733309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/12/23 13:01:43.663
  Aug 12 13:01:43.691: INFO: EndpointSlice for Service endpointslice-4550/example-named-port not found
  E0812 13:01:43.733956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:44.734074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:45.734198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:46.734776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:47.734891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:48.735477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:49.735742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:50.735882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:51.736348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:52.736451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:53.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4550" for this suite. @ 08/12/23 13:01:53.71
• [30.228 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/12/23 13:01:53.72
  Aug 12 13:01:53.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 13:01:53.721
  E0812 13:01:53.737082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:53.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:53.743
  E0812 13:01:54.737049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:55.737182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:01:55.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 13:01:55.785: INFO: Deleting pod "var-expansion-fc7db6c1-98e8-411e-94f9-469a5bb77810" in namespace "var-expansion-5548"
  Aug 12 13:01:55.794: INFO: Wait up to 5m0s for pod "var-expansion-fc7db6c1-98e8-411e-94f9-469a5bb77810" to be fully deleted
  E0812 13:01:56.738232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:57.737482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5548" for this suite. @ 08/12/23 13:01:57.804
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/12/23 13:01:57.816
  Aug 12 13:01:57.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:01:57.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:01:57.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:01:57.839
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/12/23 13:01:57.844
  E0812 13:01:58.738056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:01:59.738649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:00.739521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:01.739638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:02:01.879
  Aug 12 13:02:01.884: INFO: Trying to get logs from node ip-172-31-84-203 pod pod-2ede272f-55b2-4123-8c5a-f9ddc775b4ea container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:02:01.906
  Aug 12 13:02:01.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4833" for this suite. @ 08/12/23 13:02:01.939
• [4.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/12/23 13:02:01.955
  Aug 12 13:02:01.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/12/23 13:02:01.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:02:01.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:02:01.98
  Aug 12 13:02:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:02:02.740737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:03.741772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:04.742307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:02:05.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4181" for this suite. @ 08/12/23 13:02:05.238
• [3.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/12/23 13:02:05.251
  Aug 12 13:02:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename cronjob @ 08/12/23 13:02:05.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:02:05.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:02:05.276
  STEP: Creating a cronjob @ 08/12/23 13:02:05.28
  STEP: Ensuring more than one job is running at a time @ 08/12/23 13:02:05.287
  E0812 13:02:05.742915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:06.743601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:07.743753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:08.744101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:09.744237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:10.744363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:11.745119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:12.745247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:13.745440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:14.745546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:15.746264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:16.746826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:17.746895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:18.747757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:19.748820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:20.748961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:21.749553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:22.749670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:23.749806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:24.749947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:25.750716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:26.751335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:27.752320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:28.752657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:29.752825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:30.752933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:31.753296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:32.753790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:33.754415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:34.754467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:35.754628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:36.755349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:37.755418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:38.755727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:39.756669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:40.756813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:41.757781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:42.757904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:43.758205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:44.758454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:45.758587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:46.759452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:47.759528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:48.759671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:49.759769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:50.759883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:51.760640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:52.760681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:53.761413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:54.761631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:55.761745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:56.761923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:57.762612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:58.762908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:02:59.762992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:00.763066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:01.763201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:02.763302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:03.763444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:04.763680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:05.764058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:06.764703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:07.765811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:08.766095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:09.767167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:10.767446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:11.768253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:12.768690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:13.768816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:14.768929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:15.769781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:16.770496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:17.771357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:18.772315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:19.772400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:20.772686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:21.772941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:22.773055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:23.773765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:24.773882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:25.773957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:26.774580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:27.774662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:28.774846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:29.774941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:30.775273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:31.775792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:32.775904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:33.776741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:34.776901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:35.777031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:36.777650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:37.778529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:38.778840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:39.778937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:40.779216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:41.779823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:42.779952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:43.781009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:44.781834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:45.782512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:46.783536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:47.784373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:48.784708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:49.785769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:50.786334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:51.786974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:52.787296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:53.787906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:54.788000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:55.788182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:56.788732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:57.789133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:58.789239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:03:59.789366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:00.790221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/12/23 13:04:01.294
  STEP: Removing cronjob @ 08/12/23 13:04:01.299
  Aug 12 13:04:01.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5422" for this suite. @ 08/12/23 13:04:01.317
• [116.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/12/23 13:04:01.33
  Aug 12 13:04:01.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 13:04:01.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:04:01.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:04:01.368
  STEP: Creating a job @ 08/12/23 13:04:01.372
  STEP: Ensuring job reaches completions @ 08/12/23 13:04:01.379
  E0812 13:04:01.790453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:02.790539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:03.790929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:04.791302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:05.792256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:06.792687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:07.792792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:08.792991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:09.793179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:10.793479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:11.793560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:12.793698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:04:13.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1026" for this suite. @ 08/12/23 13:04:13.391
• [12.070 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/12/23 13:04:13.402
  Aug 12 13:04:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename init-container @ 08/12/23 13:04:13.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:04:13.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:04:13.426
  STEP: creating the pod @ 08/12/23 13:04:13.436
  Aug 12 13:04:13.436: INFO: PodSpec: initContainers in spec.initContainers
  E0812 13:04:13.794768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:14.795375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:15.796496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:16.796813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:04:17.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4142" for this suite. @ 08/12/23 13:04:17.212
• [3.819 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/12/23 13:04:17.224
  Aug 12 13:04:17.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:04:17.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:04:17.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:04:17.244
  STEP: Creating configMap with name configmap-test-upd-27e94b7c-e8a4-44e8-a825-159fd2937a8b @ 08/12/23 13:04:17.258
  STEP: Creating the pod @ 08/12/23 13:04:17.265
  E0812 13:04:17.797311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:18.797502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-27e94b7c-e8a4-44e8-a825-159fd2937a8b @ 08/12/23 13:04:19.315
  STEP: waiting to observe update in volume @ 08/12/23 13:04:19.321
  E0812 13:04:19.797619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:20.797762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:21.797855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:22.798002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:23.798128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:24.798273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:25.798394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:26.798984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:27.799720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:28.800163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:29.800825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:30.800973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:31.801590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:32.801727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:33.802736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:34.803045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:35.803145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:36.803985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:37.804520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:38.804727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:39.804840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:40.805093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:41.805779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:42.806067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:43.806983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:44.807393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:45.807966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:46.808685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:47.809531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:48.809958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:49.810716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:50.811215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:51.812152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:52.812303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:53.812338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:54.812679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:55.812805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:56.812923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:57.813794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:58.814573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:04:59.815365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:00.815577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:01.816060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:02.816617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:03.817178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:04.817299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:05.817667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:06.818268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:07.819250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:08.819397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:09.820380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:10.820747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:11.821825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:12.822040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:13.822069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:14.822402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:15.823278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:16.823398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:17.823761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:18.824439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:19.825179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:20.826180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:21.826893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:22.827266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:23.828197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:24.828416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:25.828760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:26.829735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:27.830356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:28.830495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:29.830616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:30.830929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:31.831908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:32.832012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:33.832268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:34.833094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:35.834111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:36.834317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:37.834389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:38.834703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:39.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-565" for this suite. @ 08/12/23 13:05:39.827
  E0812 13:05:39.835513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [82.612 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/12/23 13:05:39.838
  Aug 12 13:05:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 13:05:39.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:05:39.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:05:39.885
  STEP: creating a Namespace @ 08/12/23 13:05:39.892
  STEP: patching the Namespace @ 08/12/23 13:05:39.921
  STEP: get the Namespace and ensuring it has the label @ 08/12/23 13:05:39.945
  Aug 12 13:05:39.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4259" for this suite. @ 08/12/23 13:05:39.957
  STEP: Destroying namespace "nspatchtest-508131ce-9106-468c-907d-1aaa87fe25a3-1510" for this suite. @ 08/12/23 13:05:39.967
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/12/23 13:05:39.984
  Aug 12 13:05:39.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:05:39.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:05:40.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:05:40.011
  STEP: creating service nodeport-test with type=NodePort in namespace services-9610 @ 08/12/23 13:05:40.016
  STEP: creating replication controller nodeport-test in namespace services-9610 @ 08/12/23 13:05:40.043
  I0812 13:05:40.062366      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-9610, replica count: 2
  E0812 13:05:40.835942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:41.841546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:42.841648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:05:43.114348      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 13:05:43.114: INFO: Creating new exec pod
  E0812 13:05:43.842329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:44.842475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:45.842555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:46.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug 12 13:05:46.315: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 12 13:05:46.315: INFO: stdout: "nodeport-test-qtqt7"
  Aug 12 13:05:46.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.93 80'
  Aug 12 13:05:46.474: INFO: stderr: "+ nc -v -t -w 2 10.152.183.93 80\n+ echo hostName\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
  Aug 12 13:05:46.474: INFO: stdout: ""
  E0812 13:05:46.843562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:47.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.93 80'
  Aug 12 13:05:47.643: INFO: stderr: "+ nc -v -t -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug 12 13:05:47.643: INFO: stdout: ""
  E0812 13:05:47.843943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:48.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.93 80'
  Aug 12 13:05:48.636: INFO: stderr: "+ nc -v -t -w 2 10.152.183.93 80\n+ echo hostName\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
  Aug 12 13:05:48.636: INFO: stdout: "nodeport-test-f9jvn"
  Aug 12 13:05:48.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.84.203 32199'
  Aug 12 13:05:48.798: INFO: stderr: "+ nc -v -t -w 2 172.31.84.203 32199\n+ echo hostName\nConnection to 172.31.84.203 32199 port [tcp/*] succeeded!\n"
  Aug 12 13:05:48.798: INFO: stdout: "nodeport-test-qtqt7"
  Aug 12 13:05:48.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-9610 exec execpod7gbl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.79.233 32199'
  E0812 13:05:48.844342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:48.962: INFO: stderr: "+ nc -v -t -w 2 172.31.79.233 32199\n+ echo hostName\nConnection to 172.31.79.233 32199 port [tcp/*] succeeded!\n"
  Aug 12 13:05:48.962: INFO: stdout: "nodeport-test-f9jvn"
  Aug 12 13:05:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9610" for this suite. @ 08/12/23 13:05:48.968
• [8.994 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 08/12/23 13:05:48.98
  Aug 12 13:05:48.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 13:05:48.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:05:48.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:05:49.004
  Aug 12 13:05:49.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  W0812 13:05:49.009832      19 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc001464c30 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0812 13:05:49.844332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:50.844709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0812 13:05:51.573996      19 warnings.go:70] unknown field "alpha"
  W0812 13:05:51.574028      19 warnings.go:70] unknown field "beta"
  W0812 13:05:51.574038      19 warnings.go:70] unknown field "delta"
  W0812 13:05:51.574046      19 warnings.go:70] unknown field "epsilon"
  W0812 13:05:51.574055      19 warnings.go:70] unknown field "gamma"
  E0812 13:05:51.845433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:05:52.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4793" for this suite. @ 08/12/23 13:05:52.151
• [3.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/12/23 13:05:52.164
  Aug 12 13:05:52.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:05:52.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:05:52.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:05:52.19
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:05:52.195
  E0812 13:05:52.845776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:53.845878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:54.847058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:55.847145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:05:56.225
  Aug 12 13:05:56.231: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-64908287-43fd-4e40-87be-99726c44b5f6 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:05:56.255
  Aug 12 13:05:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1186" for this suite. @ 08/12/23 13:05:56.287
• [4.135 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/12/23 13:05:56.3
  Aug 12 13:05:56.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/12/23 13:05:56.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:05:56.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:05:56.373
  Aug 12 13:05:56.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:05:56.847796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:57.848723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:58.849786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:05:59.849907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:00.850992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:01.851932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:02.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8529" for this suite. @ 08/12/23 13:06:02.671
• [6.381 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/12/23 13:06:02.685
  Aug 12 13:06:02.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 13:06:02.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:02.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:02.711
  STEP: create the rc @ 08/12/23 13:06:02.721
  W0812 13:06:02.730566      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0812 13:06:02.853997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:03.854570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:04.855350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:05.855785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:06.856707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:07.856831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/12/23 13:06:08.739
  STEP: wait for the rc to be deleted @ 08/12/23 13:06:08.752
  E0812 13:06:08.857314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:09.786: INFO: 80 pods remaining
  Aug 12 13:06:09.786: INFO: 80 pods has nil DeletionTimestamp
  Aug 12 13:06:09.786: INFO: 
  E0812 13:06:09.858187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:10.791: INFO: 71 pods remaining
  Aug 12 13:06:10.791: INFO: 70 pods has nil DeletionTimestamp
  Aug 12 13:06:10.791: INFO: 
  E0812 13:06:10.858779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:11.781: INFO: 60 pods remaining
  Aug 12 13:06:11.781: INFO: 60 pods has nil DeletionTimestamp
  Aug 12 13:06:11.782: INFO: 
  E0812 13:06:11.858894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:12.776: INFO: 40 pods remaining
  Aug 12 13:06:12.776: INFO: 40 pods has nil DeletionTimestamp
  Aug 12 13:06:12.776: INFO: 
  E0812 13:06:12.859391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:13.779: INFO: 30 pods remaining
  Aug 12 13:06:13.780: INFO: 30 pods has nil DeletionTimestamp
  Aug 12 13:06:13.780: INFO: 
  E0812 13:06:13.860931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:14.765: INFO: 20 pods remaining
  Aug 12 13:06:14.765: INFO: 20 pods has nil DeletionTimestamp
  Aug 12 13:06:14.765: INFO: 
  E0812 13:06:14.861250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/12/23 13:06:15.762
  W0812 13:06:15.769028      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 13:06:15.769: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 13:06:15.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8074" for this suite. @ 08/12/23 13:06:15.784
• [13.118 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/12/23 13:06:15.803
  Aug 12 13:06:15.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-runtime @ 08/12/23 13:06:15.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:15.856
  E0812 13:06:15.864191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:15.87
  STEP: create the container @ 08/12/23 13:06:15.882
  W0812 13:06:15.900671      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/12/23 13:06:15.901
  E0812 13:06:16.864866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:17.865784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:18.865820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:19.865966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:20.866111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:21.868779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:22.869092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:23.870070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:24.870163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:25.870398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:26.870445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:27.871658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:28.872435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:29.872810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:30.873905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/12/23 13:06:31.007
  STEP: the container should be terminated @ 08/12/23 13:06:31.011
  STEP: the termination message should be set @ 08/12/23 13:06:31.011
  Aug 12 13:06:31.012: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/12/23 13:06:31.012
  Aug 12 13:06:31.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-184" for this suite. @ 08/12/23 13:06:31.042
• [15.249 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/12/23 13:06:31.056
  Aug 12 13:06:31.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 13:06:31.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:31.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:31.092
  STEP: create the deployment @ 08/12/23 13:06:31.097
  W0812 13:06:31.105429      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/12/23 13:06:31.105
  STEP: delete the deployment @ 08/12/23 13:06:31.232
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/12/23 13:06:31.242
  STEP: Gathering metrics @ 08/12/23 13:06:31.78
  W0812 13:06:31.787195      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 13:06:31.787: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 13:06:31.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6275" for this suite. @ 08/12/23 13:06:31.797
• [0.753 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/12/23 13:06:31.826
  Aug 12 13:06:31.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sysctl @ 08/12/23 13:06:31.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:31.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:31.85
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/12/23 13:06:31.855
  Aug 12 13:06:31.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1296" for this suite. @ 08/12/23 13:06:31.871
  E0812 13:06:31.874098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/12/23 13:06:31.884
  Aug 12 13:06:31.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:06:31.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:31.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:31.912
  STEP: creating secret secrets-9323/secret-test-21cbfd1d-098a-4a36-aef0-4d736ed10dea @ 08/12/23 13:06:31.917
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:06:31.924
  E0812 13:06:32.874375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:33.874666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:34.874769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:35.874904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:06:35.958
  Aug 12 13:06:35.962: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-63e581b6-b01d-4231-ab60-5592a682350d container env-test: <nil>
  STEP: delete the pod @ 08/12/23 13:06:35.98
  Aug 12 13:06:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9323" for this suite. @ 08/12/23 13:06:36.008
• [4.133 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/12/23 13:06:36.018
  Aug 12 13:06:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 13:06:36.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:36.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:36.045
  STEP: Creating Indexed job @ 08/12/23 13:06:36.053
  STEP: Ensuring job reaches completions @ 08/12/23 13:06:36.065
  E0812 13:06:36.876840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:37.877116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:38.878076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:39.878205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:40.879541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:41.879653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:42.879780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:43.880191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:44.880287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:45.880576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 08/12/23 13:06:46.072
  Aug 12 13:06:46.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-171" for this suite. @ 08/12/23 13:06:46.085
• [10.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/12/23 13:06:46.102
  Aug 12 13:06:46.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 13:06:46.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:46.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:46.13
  STEP: Counting existing ResourceQuota @ 08/12/23 13:06:46.136
  E0812 13:06:46.881527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:47.881689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:48.881841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:49.881939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:50.882106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 13:06:51.141
  STEP: Ensuring resource quota status is calculated @ 08/12/23 13:06:51.15
  E0812 13:06:51.882976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:52.883099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 08/12/23 13:06:53.155
  STEP: Ensuring resource quota status captures replicaset creation @ 08/12/23 13:06:53.169
  E0812 13:06:53.883222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:54.883396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 08/12/23 13:06:55.177
  STEP: Ensuring resource quota status released usage @ 08/12/23 13:06:55.185
  E0812 13:06:55.883396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:56.883521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:06:57.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2173" for this suite. @ 08/12/23 13:06:57.198
• [11.105 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/12/23 13:06:57.207
  Aug 12 13:06:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:06:57.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:06:57.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:06:57.237
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/12/23 13:06:57.242
  E0812 13:06:57.884042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:58.884220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:06:59.884302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:00.884678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:07:01.272
  Aug 12 13:07:01.276: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-408a24aa-7326-495b-b9fb-3bd6b58f5da4 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:07:01.288
  Aug 12 13:07:01.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9816" for this suite. @ 08/12/23 13:07:01.311
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/12/23 13:07:01.328
  Aug 12 13:07:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svc-latency @ 08/12/23 13:07:01.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:01.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:01.352
  Aug 12 13:07:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7523 @ 08/12/23 13:07:01.358
  I0812 13:07:01.366142      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7523, replica count: 1
  E0812 13:07:01.885554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:07:02.417311      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 13:07:02.537: INFO: Created: latency-svc-z44dq
  Aug 12 13:07:02.547: INFO: Got endpoints: latency-svc-z44dq [29.770609ms]
  Aug 12 13:07:02.580: INFO: Created: latency-svc-pjm77
  Aug 12 13:07:02.596: INFO: Created: latency-svc-d8xg6
  Aug 12 13:07:02.634: INFO: Created: latency-svc-89hkt
  Aug 12 13:07:02.635: INFO: Got endpoints: latency-svc-pjm77 [87.56051ms]
  Aug 12 13:07:02.639: INFO: Got endpoints: latency-svc-d8xg6 [90.545734ms]
  Aug 12 13:07:02.665: INFO: Created: latency-svc-25ft4
  Aug 12 13:07:02.670: INFO: Got endpoints: latency-svc-89hkt [121.711953ms]
  Aug 12 13:07:02.682: INFO: Got endpoints: latency-svc-25ft4 [133.99085ms]
  Aug 12 13:07:02.693: INFO: Created: latency-svc-tw9jf
  Aug 12 13:07:02.701: INFO: Got endpoints: latency-svc-tw9jf [62.496458ms]
  Aug 12 13:07:02.707: INFO: Created: latency-svc-b46tv
  Aug 12 13:07:02.719: INFO: Got endpoints: latency-svc-b46tv [83.226463ms]
  Aug 12 13:07:02.720: INFO: Created: latency-svc-j467x
  Aug 12 13:07:02.743: INFO: Got endpoints: latency-svc-j467x [194.807696ms]
  Aug 12 13:07:02.744: INFO: Created: latency-svc-2w4wm
  Aug 12 13:07:02.755: INFO: Created: latency-svc-mj4bc
  Aug 12 13:07:02.774: INFO: Created: latency-svc-h4h4t
  Aug 12 13:07:02.779: INFO: Got endpoints: latency-svc-2w4wm [229.439869ms]
  Aug 12 13:07:02.790: INFO: Got endpoints: latency-svc-mj4bc [240.860344ms]
  Aug 12 13:07:02.804: INFO: Created: latency-svc-j6khs
  Aug 12 13:07:02.804: INFO: Got endpoints: latency-svc-h4h4t [255.692452ms]
  Aug 12 13:07:02.812: INFO: Created: latency-svc-x6mkt
  Aug 12 13:07:02.814: INFO: Got endpoints: latency-svc-j6khs [265.214453ms]
  Aug 12 13:07:02.824: INFO: Created: latency-svc-rpc8m
  Aug 12 13:07:02.825: INFO: Got endpoints: latency-svc-x6mkt [276.017934ms]
  Aug 12 13:07:02.830: INFO: Got endpoints: latency-svc-rpc8m [281.308318ms]
  Aug 12 13:07:02.836: INFO: Created: latency-svc-dpf62
  Aug 12 13:07:02.844: INFO: Got endpoints: latency-svc-dpf62 [295.177728ms]
  Aug 12 13:07:02.848: INFO: Created: latency-svc-qzb6h
  Aug 12 13:07:02.858: INFO: Got endpoints: latency-svc-qzb6h [308.781357ms]
  Aug 12 13:07:02.862: INFO: Created: latency-svc-pncnp
  Aug 12 13:07:02.874: INFO: Got endpoints: latency-svc-pncnp [324.173156ms]
  Aug 12 13:07:02.875: INFO: Created: latency-svc-m5n6g
  Aug 12 13:07:02.880: INFO: Got endpoints: latency-svc-m5n6g [332.270356ms]
  E0812 13:07:02.885587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:02.889: INFO: Created: latency-svc-wx2z8
  Aug 12 13:07:02.894: INFO: Created: latency-svc-f678h
  Aug 12 13:07:02.899: INFO: Got endpoints: latency-svc-wx2z8 [228.418685ms]
  Aug 12 13:07:02.906: INFO: Got endpoints: latency-svc-f678h [224.198718ms]
  Aug 12 13:07:02.912: INFO: Created: latency-svc-flb24
  Aug 12 13:07:02.917: INFO: Got endpoints: latency-svc-flb24 [215.037514ms]
  Aug 12 13:07:02.924: INFO: Created: latency-svc-csp7v
  Aug 12 13:07:02.929: INFO: Got endpoints: latency-svc-csp7v [209.470296ms]
  Aug 12 13:07:02.931: INFO: Created: latency-svc-gshq2
  Aug 12 13:07:02.945: INFO: Got endpoints: latency-svc-gshq2 [202.225464ms]
  Aug 12 13:07:02.952: INFO: Created: latency-svc-pnz5q
  Aug 12 13:07:02.967: INFO: Got endpoints: latency-svc-pnz5q [187.495002ms]
  Aug 12 13:07:02.967: INFO: Created: latency-svc-dzr4d
  Aug 12 13:07:02.972: INFO: Created: latency-svc-5xwd6
  Aug 12 13:07:02.975: INFO: Got endpoints: latency-svc-dzr4d [184.803186ms]
  Aug 12 13:07:02.986: INFO: Created: latency-svc-ldpgn
  Aug 12 13:07:02.989: INFO: Got endpoints: latency-svc-5xwd6 [184.436449ms]
  Aug 12 13:07:02.996: INFO: Created: latency-svc-hlzrh
  Aug 12 13:07:03.000: INFO: Got endpoints: latency-svc-ldpgn [185.410901ms]
  Aug 12 13:07:03.008: INFO: Got endpoints: latency-svc-hlzrh [183.0522ms]
  Aug 12 13:07:03.009: INFO: Created: latency-svc-fldrl
  Aug 12 13:07:03.019: INFO: Got endpoints: latency-svc-fldrl [188.50663ms]
  Aug 12 13:07:03.019: INFO: Created: latency-svc-nn2fk
  Aug 12 13:07:03.022: INFO: Got endpoints: latency-svc-nn2fk [177.345304ms]
  Aug 12 13:07:03.030: INFO: Created: latency-svc-fwmxh
  Aug 12 13:07:03.039: INFO: Created: latency-svc-pqx9t
  Aug 12 13:07:03.146: INFO: Got endpoints: latency-svc-fwmxh [288.287061ms]
  Aug 12 13:07:03.150: INFO: Got endpoints: latency-svc-pqx9t [276.677414ms]
  Aug 12 13:07:03.158: INFO: Created: latency-svc-9ssw7
  Aug 12 13:07:03.163: INFO: Got endpoints: latency-svc-9ssw7 [282.449188ms]
  Aug 12 13:07:03.171: INFO: Created: latency-svc-v9wzn
  Aug 12 13:07:03.177: INFO: Got endpoints: latency-svc-v9wzn [277.98796ms]
  Aug 12 13:07:03.183: INFO: Created: latency-svc-l7fnc
  Aug 12 13:07:03.192: INFO: Created: latency-svc-dccvg
  Aug 12 13:07:03.195: INFO: Got endpoints: latency-svc-l7fnc [288.734368ms]
  Aug 12 13:07:03.201: INFO: Got endpoints: latency-svc-dccvg [284.060598ms]
  Aug 12 13:07:03.206: INFO: Created: latency-svc-tlrwd
  Aug 12 13:07:03.215: INFO: Got endpoints: latency-svc-tlrwd [285.98187ms]
  Aug 12 13:07:03.223: INFO: Created: latency-svc-9d5vl
  Aug 12 13:07:03.235: INFO: Got endpoints: latency-svc-9d5vl [289.287363ms]
  Aug 12 13:07:03.246: INFO: Created: latency-svc-d7sn4
  Aug 12 13:07:03.248: INFO: Got endpoints: latency-svc-d7sn4 [281.425218ms]
  Aug 12 13:07:03.259: INFO: Created: latency-svc-f9fkg
  Aug 12 13:07:03.262: INFO: Created: latency-svc-cfhr6
  Aug 12 13:07:03.263: INFO: Got endpoints: latency-svc-f9fkg [287.795297ms]
  Aug 12 13:07:03.286: INFO: Got endpoints: latency-svc-cfhr6 [297.403153ms]
  Aug 12 13:07:03.286: INFO: Created: latency-svc-hqqbr
  Aug 12 13:07:03.298: INFO: Got endpoints: latency-svc-hqqbr [298.396906ms]
  Aug 12 13:07:03.306: INFO: Created: latency-svc-f8b5w
  Aug 12 13:07:03.307: INFO: Got endpoints: latency-svc-f8b5w [298.725981ms]
  Aug 12 13:07:03.309: INFO: Created: latency-svc-sw54d
  Aug 12 13:07:03.324: INFO: Created: latency-svc-5v8wq
  Aug 12 13:07:03.326: INFO: Got endpoints: latency-svc-sw54d [305.997958ms]
  Aug 12 13:07:03.336: INFO: Got endpoints: latency-svc-5v8wq [314.01149ms]
  Aug 12 13:07:03.346: INFO: Created: latency-svc-fvp65
  Aug 12 13:07:03.355: INFO: Got endpoints: latency-svc-fvp65 [208.452911ms]
  Aug 12 13:07:03.363: INFO: Created: latency-svc-scsnk
  Aug 12 13:07:03.372: INFO: Created: latency-svc-px2qh
  Aug 12 13:07:03.381: INFO: Created: latency-svc-z9f2h
  Aug 12 13:07:03.398: INFO: Created: latency-svc-52b2f
  Aug 12 13:07:03.399: INFO: Got endpoints: latency-svc-scsnk [247.903008ms]
  Aug 12 13:07:03.411: INFO: Created: latency-svc-6r9f6
  Aug 12 13:07:03.446: INFO: Got endpoints: latency-svc-px2qh [283.538447ms]
  Aug 12 13:07:03.493: INFO: Got endpoints: latency-svc-z9f2h [316.246397ms]
  Aug 12 13:07:03.510: INFO: Created: latency-svc-hqjc2
  Aug 12 13:07:03.511: INFO: Created: latency-svc-t9pdv
  Aug 12 13:07:03.511: INFO: Created: latency-svc-x7wbj
  Aug 12 13:07:03.512: INFO: Created: latency-svc-v65bb
  Aug 12 13:07:03.512: INFO: Created: latency-svc-r7l5n
  Aug 12 13:07:03.521: INFO: Created: latency-svc-bt59w
  Aug 12 13:07:03.521: INFO: Created: latency-svc-d56h8
  Aug 12 13:07:03.522: INFO: Created: latency-svc-x48n2
  Aug 12 13:07:03.525: INFO: Created: latency-svc-xgwqm
  Aug 12 13:07:03.528: INFO: Created: latency-svc-xgv9x
  Aug 12 13:07:03.533: INFO: Created: latency-svc-xx667
  Aug 12 13:07:03.533: INFO: Created: latency-svc-fnmbw
  Aug 12 13:07:03.538: INFO: Created: latency-svc-fxd4b
  Aug 12 13:07:03.549: INFO: Got endpoints: latency-svc-52b2f [353.698951ms]
  Aug 12 13:07:03.565: INFO: Created: latency-svc-d8fqd
  Aug 12 13:07:03.593: INFO: Got endpoints: latency-svc-6r9f6 [392.254857ms]
  Aug 12 13:07:03.609: INFO: Created: latency-svc-52qbf
  Aug 12 13:07:03.644: INFO: Got endpoints: latency-svc-hqjc2 [345.145857ms]
  Aug 12 13:07:03.658: INFO: Created: latency-svc-tb6rx
  Aug 12 13:07:03.694: INFO: Got endpoints: latency-svc-d56h8 [339.195903ms]
  Aug 12 13:07:03.709: INFO: Created: latency-svc-gs97b
  Aug 12 13:07:03.745: INFO: Got endpoints: latency-svc-fnmbw [509.664215ms]
  Aug 12 13:07:03.762: INFO: Created: latency-svc-w8dwh
  Aug 12 13:07:03.794: INFO: Got endpoints: latency-svc-xx667 [457.574475ms]
  Aug 12 13:07:03.814: INFO: Created: latency-svc-pbzgs
  Aug 12 13:07:03.843: INFO: Got endpoints: latency-svc-x7wbj [628.067653ms]
  Aug 12 13:07:03.857: INFO: Created: latency-svc-wfts9
  E0812 13:07:03.886292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:03.893: INFO: Got endpoints: latency-svc-r7l5n [566.56191ms]
  Aug 12 13:07:03.908: INFO: Created: latency-svc-dsff7
  Aug 12 13:07:03.944: INFO: Got endpoints: latency-svc-t9pdv [497.389701ms]
  Aug 12 13:07:03.958: INFO: Created: latency-svc-gj669
  Aug 12 13:07:03.994: INFO: Got endpoints: latency-svc-v65bb [745.495528ms]
  Aug 12 13:07:04.007: INFO: Created: latency-svc-n8bfd
  Aug 12 13:07:04.045: INFO: Got endpoints: latency-svc-x48n2 [758.005573ms]
  Aug 12 13:07:04.061: INFO: Created: latency-svc-rb8dg
  Aug 12 13:07:04.094: INFO: Got endpoints: latency-svc-xgv9x [695.283174ms]
  Aug 12 13:07:04.110: INFO: Created: latency-svc-7sr2w
  Aug 12 13:07:04.157: INFO: Got endpoints: latency-svc-bt59w [893.730157ms]
  Aug 12 13:07:04.170: INFO: Created: latency-svc-r7vmv
  Aug 12 13:07:04.194: INFO: Got endpoints: latency-svc-xgwqm [886.587422ms]
  Aug 12 13:07:04.209: INFO: Created: latency-svc-zqkln
  Aug 12 13:07:04.244: INFO: Got endpoints: latency-svc-fxd4b [750.221198ms]
  Aug 12 13:07:04.260: INFO: Created: latency-svc-4h4p8
  Aug 12 13:07:04.293: INFO: Got endpoints: latency-svc-d8fqd [744.47812ms]
  Aug 12 13:07:04.308: INFO: Created: latency-svc-qnr2z
  Aug 12 13:07:04.343: INFO: Got endpoints: latency-svc-52qbf [749.58393ms]
  Aug 12 13:07:04.359: INFO: Created: latency-svc-kv8wf
  Aug 12 13:07:04.394: INFO: Got endpoints: latency-svc-tb6rx [750.532577ms]
  Aug 12 13:07:04.408: INFO: Created: latency-svc-52mnj
  Aug 12 13:07:04.443: INFO: Got endpoints: latency-svc-gs97b [748.562318ms]
  Aug 12 13:07:04.457: INFO: Created: latency-svc-r5n45
  Aug 12 13:07:04.495: INFO: Got endpoints: latency-svc-w8dwh [749.619747ms]
  Aug 12 13:07:04.510: INFO: Created: latency-svc-d7pj6
  Aug 12 13:07:04.546: INFO: Got endpoints: latency-svc-pbzgs [752.123678ms]
  Aug 12 13:07:04.562: INFO: Created: latency-svc-jx4xb
  Aug 12 13:07:04.592: INFO: Got endpoints: latency-svc-wfts9 [748.749493ms]
  Aug 12 13:07:04.606: INFO: Created: latency-svc-cbdsp
  Aug 12 13:07:04.646: INFO: Got endpoints: latency-svc-dsff7 [752.991527ms]
  Aug 12 13:07:04.663: INFO: Created: latency-svc-khtqt
  Aug 12 13:07:04.694: INFO: Got endpoints: latency-svc-gj669 [749.875491ms]
  Aug 12 13:07:04.709: INFO: Created: latency-svc-g88rw
  Aug 12 13:07:04.743: INFO: Got endpoints: latency-svc-n8bfd [748.927584ms]
  Aug 12 13:07:04.757: INFO: Created: latency-svc-j8jw7
  Aug 12 13:07:04.795: INFO: Got endpoints: latency-svc-rb8dg [750.470973ms]
  Aug 12 13:07:04.810: INFO: Created: latency-svc-4n7lp
  Aug 12 13:07:04.844: INFO: Got endpoints: latency-svc-7sr2w [749.718159ms]
  Aug 12 13:07:04.859: INFO: Created: latency-svc-jm67b
  E0812 13:07:04.886782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:04.895: INFO: Got endpoints: latency-svc-r7vmv [737.60459ms]
  Aug 12 13:07:04.908: INFO: Created: latency-svc-4rgm2
  Aug 12 13:07:04.944: INFO: Got endpoints: latency-svc-zqkln [750.068883ms]
  Aug 12 13:07:04.960: INFO: Created: latency-svc-2bppf
  Aug 12 13:07:04.994: INFO: Got endpoints: latency-svc-4h4p8 [749.621176ms]
  Aug 12 13:07:05.008: INFO: Created: latency-svc-qm9cl
  Aug 12 13:07:05.046: INFO: Got endpoints: latency-svc-qnr2z [752.597799ms]
  Aug 12 13:07:05.060: INFO: Created: latency-svc-crprv
  Aug 12 13:07:05.094: INFO: Got endpoints: latency-svc-kv8wf [750.924184ms]
  Aug 12 13:07:05.112: INFO: Created: latency-svc-xgm2f
  Aug 12 13:07:05.144: INFO: Got endpoints: latency-svc-52mnj [749.961903ms]
  Aug 12 13:07:05.159: INFO: Created: latency-svc-x6gsb
  Aug 12 13:07:05.193: INFO: Got endpoints: latency-svc-r5n45 [749.399544ms]
  Aug 12 13:07:05.209: INFO: Created: latency-svc-8k2k8
  Aug 12 13:07:05.244: INFO: Got endpoints: latency-svc-d7pj6 [748.151732ms]
  Aug 12 13:07:05.259: INFO: Created: latency-svc-49f5v
  Aug 12 13:07:05.294: INFO: Got endpoints: latency-svc-jx4xb [748.085738ms]
  Aug 12 13:07:05.311: INFO: Created: latency-svc-fcp2r
  Aug 12 13:07:05.345: INFO: Got endpoints: latency-svc-cbdsp [753.029135ms]
  Aug 12 13:07:05.359: INFO: Created: latency-svc-qqcrg
  Aug 12 13:07:05.393: INFO: Got endpoints: latency-svc-khtqt [747.050718ms]
  Aug 12 13:07:05.408: INFO: Created: latency-svc-cpj8d
  Aug 12 13:07:05.445: INFO: Got endpoints: latency-svc-g88rw [750.563337ms]
  Aug 12 13:07:05.460: INFO: Created: latency-svc-7gkg6
  Aug 12 13:07:05.494: INFO: Got endpoints: latency-svc-j8jw7 [750.560136ms]
  Aug 12 13:07:05.507: INFO: Created: latency-svc-hjlpc
  Aug 12 13:07:05.545: INFO: Got endpoints: latency-svc-4n7lp [749.563526ms]
  Aug 12 13:07:05.566: INFO: Created: latency-svc-jkjk7
  Aug 12 13:07:05.594: INFO: Got endpoints: latency-svc-jm67b [749.098627ms]
  Aug 12 13:07:05.608: INFO: Created: latency-svc-tjspc
  Aug 12 13:07:05.644: INFO: Got endpoints: latency-svc-4rgm2 [748.67493ms]
  Aug 12 13:07:05.658: INFO: Created: latency-svc-fhqxq
  Aug 12 13:07:05.693: INFO: Got endpoints: latency-svc-2bppf [749.319553ms]
  Aug 12 13:07:05.712: INFO: Created: latency-svc-nrv2w
  Aug 12 13:07:05.745: INFO: Got endpoints: latency-svc-qm9cl [751.210215ms]
  Aug 12 13:07:05.761: INFO: Created: latency-svc-zw9rp
  Aug 12 13:07:05.792: INFO: Got endpoints: latency-svc-crprv [746.428102ms]
  Aug 12 13:07:05.805: INFO: Created: latency-svc-fdm7x
  Aug 12 13:07:05.843: INFO: Got endpoints: latency-svc-xgm2f [748.869046ms]
  Aug 12 13:07:05.860: INFO: Created: latency-svc-bt28f
  E0812 13:07:05.887361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:05.894: INFO: Got endpoints: latency-svc-x6gsb [749.19078ms]
  Aug 12 13:07:05.908: INFO: Created: latency-svc-fq8kd
  Aug 12 13:07:05.942: INFO: Got endpoints: latency-svc-8k2k8 [749.205979ms]
  Aug 12 13:07:05.956: INFO: Created: latency-svc-fm7m4
  Aug 12 13:07:05.994: INFO: Got endpoints: latency-svc-49f5v [750.127915ms]
  Aug 12 13:07:06.009: INFO: Created: latency-svc-qhws8
  Aug 12 13:07:06.045: INFO: Got endpoints: latency-svc-fcp2r [750.863872ms]
  Aug 12 13:07:06.062: INFO: Created: latency-svc-9zlss
  Aug 12 13:07:06.094: INFO: Got endpoints: latency-svc-qqcrg [748.127933ms]
  Aug 12 13:07:06.107: INFO: Created: latency-svc-5fl7q
  Aug 12 13:07:06.147: INFO: Got endpoints: latency-svc-cpj8d [753.672275ms]
  Aug 12 13:07:06.163: INFO: Created: latency-svc-xlphx
  Aug 12 13:07:06.193: INFO: Got endpoints: latency-svc-7gkg6 [748.609368ms]
  Aug 12 13:07:06.208: INFO: Created: latency-svc-fzghm
  Aug 12 13:07:06.243: INFO: Got endpoints: latency-svc-hjlpc [748.580031ms]
  Aug 12 13:07:06.257: INFO: Created: latency-svc-k97pl
  Aug 12 13:07:06.295: INFO: Got endpoints: latency-svc-jkjk7 [748.75903ms]
  Aug 12 13:07:06.309: INFO: Created: latency-svc-4tknb
  Aug 12 13:07:06.345: INFO: Got endpoints: latency-svc-tjspc [751.151076ms]
  Aug 12 13:07:06.363: INFO: Created: latency-svc-kh57x
  Aug 12 13:07:06.393: INFO: Got endpoints: latency-svc-fhqxq [749.213467ms]
  Aug 12 13:07:06.409: INFO: Created: latency-svc-hj8l4
  Aug 12 13:07:06.444: INFO: Got endpoints: latency-svc-nrv2w [750.907148ms]
  Aug 12 13:07:06.471: INFO: Created: latency-svc-l98qt
  Aug 12 13:07:06.495: INFO: Got endpoints: latency-svc-zw9rp [749.251654ms]
  Aug 12 13:07:06.509: INFO: Created: latency-svc-rgwbj
  Aug 12 13:07:06.545: INFO: Got endpoints: latency-svc-fdm7x [751.553511ms]
  Aug 12 13:07:06.566: INFO: Created: latency-svc-cltng
  Aug 12 13:07:06.597: INFO: Got endpoints: latency-svc-bt28f [753.469405ms]
  Aug 12 13:07:06.615: INFO: Created: latency-svc-7bkts
  Aug 12 13:07:06.644: INFO: Got endpoints: latency-svc-fq8kd [750.398782ms]
  Aug 12 13:07:06.658: INFO: Created: latency-svc-glzmp
  Aug 12 13:07:06.696: INFO: Got endpoints: latency-svc-fm7m4 [753.688883ms]
  Aug 12 13:07:06.712: INFO: Created: latency-svc-qt95v
  Aug 12 13:07:06.743: INFO: Got endpoints: latency-svc-qhws8 [748.649521ms]
  Aug 12 13:07:06.757: INFO: Created: latency-svc-qc8bm
  Aug 12 13:07:06.793: INFO: Got endpoints: latency-svc-9zlss [748.394925ms]
  Aug 12 13:07:06.807: INFO: Created: latency-svc-m94cs
  Aug 12 13:07:06.843: INFO: Got endpoints: latency-svc-5fl7q [749.892318ms]
  Aug 12 13:07:06.859: INFO: Created: latency-svc-qdrfq
  E0812 13:07:06.888058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:06.892: INFO: Got endpoints: latency-svc-xlphx [745.499517ms]
  Aug 12 13:07:06.907: INFO: Created: latency-svc-5r5wp
  Aug 12 13:07:06.944: INFO: Got endpoints: latency-svc-fzghm [750.551775ms]
  Aug 12 13:07:06.959: INFO: Created: latency-svc-tbmmp
  Aug 12 13:07:06.995: INFO: Got endpoints: latency-svc-k97pl [751.897589ms]
  Aug 12 13:07:07.010: INFO: Created: latency-svc-dfxcx
  Aug 12 13:07:07.043: INFO: Got endpoints: latency-svc-4tknb [747.850338ms]
  Aug 12 13:07:07.057: INFO: Created: latency-svc-rkkss
  Aug 12 13:07:07.093: INFO: Got endpoints: latency-svc-kh57x [748.302375ms]
  Aug 12 13:07:07.108: INFO: Created: latency-svc-hb2bv
  Aug 12 13:07:07.144: INFO: Got endpoints: latency-svc-hj8l4 [750.796149ms]
  Aug 12 13:07:07.159: INFO: Created: latency-svc-qzm4w
  Aug 12 13:07:07.193: INFO: Got endpoints: latency-svc-l98qt [748.247501ms]
  Aug 12 13:07:07.207: INFO: Created: latency-svc-j5llx
  Aug 12 13:07:07.244: INFO: Got endpoints: latency-svc-rgwbj [748.949925ms]
  Aug 12 13:07:07.260: INFO: Created: latency-svc-nd8nj
  Aug 12 13:07:07.388: INFO: Got endpoints: latency-svc-cltng [843.070841ms]
  Aug 12 13:07:07.394: INFO: Got endpoints: latency-svc-7bkts [796.334922ms]
  Aug 12 13:07:07.395: INFO: Got endpoints: latency-svc-glzmp [751.143809ms]
  Aug 12 13:07:07.407: INFO: Created: latency-svc-5qjgg
  Aug 12 13:07:07.418: INFO: Created: latency-svc-pdh54
  Aug 12 13:07:07.427: INFO: Created: latency-svc-4tzs9
  Aug 12 13:07:07.444: INFO: Got endpoints: latency-svc-qt95v [747.833731ms]
  Aug 12 13:07:07.468: INFO: Created: latency-svc-s5kdr
  Aug 12 13:07:07.496: INFO: Got endpoints: latency-svc-qc8bm [753.0501ms]
  Aug 12 13:07:07.511: INFO: Created: latency-svc-jmmbb
  Aug 12 13:07:07.546: INFO: Got endpoints: latency-svc-m94cs [752.83525ms]
  Aug 12 13:07:07.564: INFO: Created: latency-svc-9mn4m
  Aug 12 13:07:07.592: INFO: Got endpoints: latency-svc-qdrfq [748.950527ms]
  Aug 12 13:07:07.610: INFO: Created: latency-svc-zxfzz
  Aug 12 13:07:07.643: INFO: Got endpoints: latency-svc-5r5wp [750.080292ms]
  Aug 12 13:07:07.657: INFO: Created: latency-svc-xnr9n
  Aug 12 13:07:07.693: INFO: Got endpoints: latency-svc-tbmmp [748.966432ms]
  Aug 12 13:07:07.707: INFO: Created: latency-svc-259vj
  Aug 12 13:07:07.745: INFO: Got endpoints: latency-svc-dfxcx [750.188673ms]
  Aug 12 13:07:07.763: INFO: Created: latency-svc-c9lvs
  Aug 12 13:07:07.794: INFO: Got endpoints: latency-svc-rkkss [750.48664ms]
  Aug 12 13:07:07.811: INFO: Created: latency-svc-mzm9n
  Aug 12 13:07:07.843: INFO: Got endpoints: latency-svc-hb2bv [749.470371ms]
  Aug 12 13:07:07.857: INFO: Created: latency-svc-9p644
  E0812 13:07:07.888449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:07.893: INFO: Got endpoints: latency-svc-qzm4w [748.668508ms]
  Aug 12 13:07:07.907: INFO: Created: latency-svc-q4b9h
  Aug 12 13:07:07.943: INFO: Got endpoints: latency-svc-j5llx [749.891624ms]
  Aug 12 13:07:07.956: INFO: Created: latency-svc-r6cx4
  Aug 12 13:07:07.993: INFO: Got endpoints: latency-svc-nd8nj [749.191362ms]
  Aug 12 13:07:08.007: INFO: Created: latency-svc-p7q28
  Aug 12 13:07:08.046: INFO: Got endpoints: latency-svc-5qjgg [657.690089ms]
  Aug 12 13:07:08.060: INFO: Created: latency-svc-vpgbm
  Aug 12 13:07:08.094: INFO: Got endpoints: latency-svc-pdh54 [699.974947ms]
  Aug 12 13:07:08.108: INFO: Created: latency-svc-4lmqs
  Aug 12 13:07:08.143: INFO: Got endpoints: latency-svc-4tzs9 [747.271088ms]
  Aug 12 13:07:08.157: INFO: Created: latency-svc-6tn56
  Aug 12 13:07:08.194: INFO: Got endpoints: latency-svc-s5kdr [748.918808ms]
  Aug 12 13:07:08.210: INFO: Created: latency-svc-gt28v
  Aug 12 13:07:08.245: INFO: Got endpoints: latency-svc-jmmbb [749.042742ms]
  Aug 12 13:07:08.259: INFO: Created: latency-svc-tpmzt
  Aug 12 13:07:08.295: INFO: Got endpoints: latency-svc-9mn4m [748.172387ms]
  Aug 12 13:07:08.308: INFO: Created: latency-svc-hswgg
  Aug 12 13:07:08.343: INFO: Got endpoints: latency-svc-zxfzz [750.388876ms]
  Aug 12 13:07:08.360: INFO: Created: latency-svc-pzspj
  Aug 12 13:07:08.393: INFO: Got endpoints: latency-svc-xnr9n [749.803569ms]
  Aug 12 13:07:08.410: INFO: Created: latency-svc-l5hqz
  Aug 12 13:07:08.443: INFO: Got endpoints: latency-svc-259vj [748.976897ms]
  Aug 12 13:07:08.456: INFO: Created: latency-svc-pmq8n
  Aug 12 13:07:08.493: INFO: Got endpoints: latency-svc-c9lvs [748.234666ms]
  Aug 12 13:07:08.509: INFO: Created: latency-svc-266j9
  Aug 12 13:07:08.544: INFO: Got endpoints: latency-svc-mzm9n [750.346006ms]
  Aug 12 13:07:08.563: INFO: Created: latency-svc-scbg5
  Aug 12 13:07:08.594: INFO: Got endpoints: latency-svc-9p644 [750.747599ms]
  Aug 12 13:07:08.609: INFO: Created: latency-svc-2xgjl
  Aug 12 13:07:08.643: INFO: Got endpoints: latency-svc-q4b9h [750.250191ms]
  Aug 12 13:07:08.658: INFO: Created: latency-svc-z9zb2
  Aug 12 13:07:08.697: INFO: Got endpoints: latency-svc-r6cx4 [753.763561ms]
  Aug 12 13:07:08.712: INFO: Created: latency-svc-rp9xj
  Aug 12 13:07:08.747: INFO: Got endpoints: latency-svc-p7q28 [753.251492ms]
  Aug 12 13:07:08.764: INFO: Created: latency-svc-l5hvt
  Aug 12 13:07:08.796: INFO: Got endpoints: latency-svc-vpgbm [749.933692ms]
  Aug 12 13:07:08.816: INFO: Created: latency-svc-4xc4d
  Aug 12 13:07:08.844: INFO: Got endpoints: latency-svc-4lmqs [750.090501ms]
  Aug 12 13:07:08.865: INFO: Created: latency-svc-wlvmr
  E0812 13:07:08.888771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:08.894: INFO: Got endpoints: latency-svc-6tn56 [750.837867ms]
  Aug 12 13:07:08.911: INFO: Created: latency-svc-8vk4x
  Aug 12 13:07:08.943: INFO: Got endpoints: latency-svc-gt28v [749.769663ms]
  Aug 12 13:07:08.962: INFO: Created: latency-svc-8pxtn
  Aug 12 13:07:08.996: INFO: Got endpoints: latency-svc-tpmzt [750.698252ms]
  Aug 12 13:07:09.016: INFO: Created: latency-svc-w4rt9
  Aug 12 13:07:09.043: INFO: Got endpoints: latency-svc-hswgg [748.151433ms]
  Aug 12 13:07:09.061: INFO: Created: latency-svc-mzl6f
  Aug 12 13:07:09.093: INFO: Got endpoints: latency-svc-pzspj [748.846712ms]
  Aug 12 13:07:09.110: INFO: Created: latency-svc-9dhnn
  Aug 12 13:07:09.145: INFO: Got endpoints: latency-svc-l5hqz [752.298036ms]
  Aug 12 13:07:09.160: INFO: Created: latency-svc-mllxr
  Aug 12 13:07:09.193: INFO: Got endpoints: latency-svc-pmq8n [750.286079ms]
  Aug 12 13:07:09.206: INFO: Created: latency-svc-8vx4g
  Aug 12 13:07:09.244: INFO: Got endpoints: latency-svc-266j9 [750.036715ms]
  Aug 12 13:07:09.258: INFO: Created: latency-svc-p2vqq
  Aug 12 13:07:09.293: INFO: Got endpoints: latency-svc-scbg5 [748.674786ms]
  Aug 12 13:07:09.308: INFO: Created: latency-svc-pnrs9
  Aug 12 13:07:09.343: INFO: Got endpoints: latency-svc-2xgjl [748.618729ms]
  Aug 12 13:07:09.357: INFO: Created: latency-svc-f88jw
  Aug 12 13:07:09.393: INFO: Got endpoints: latency-svc-z9zb2 [750.00468ms]
  Aug 12 13:07:09.408: INFO: Created: latency-svc-tj4jk
  Aug 12 13:07:09.444: INFO: Got endpoints: latency-svc-rp9xj [747.271901ms]
  Aug 12 13:07:09.471: INFO: Created: latency-svc-x7w7c
  Aug 12 13:07:09.494: INFO: Got endpoints: latency-svc-l5hvt [747.045077ms]
  Aug 12 13:07:09.507: INFO: Created: latency-svc-fkhv6
  Aug 12 13:07:09.544: INFO: Got endpoints: latency-svc-4xc4d [748.753012ms]
  Aug 12 13:07:09.564: INFO: Created: latency-svc-wxjs7
  Aug 12 13:07:09.594: INFO: Got endpoints: latency-svc-wlvmr [749.254266ms]
  Aug 12 13:07:09.608: INFO: Created: latency-svc-5cmt2
  Aug 12 13:07:09.643: INFO: Got endpoints: latency-svc-8vk4x [749.319902ms]
  Aug 12 13:07:09.657: INFO: Created: latency-svc-l8zfd
  Aug 12 13:07:09.694: INFO: Got endpoints: latency-svc-8pxtn [750.185924ms]
  Aug 12 13:07:09.708: INFO: Created: latency-svc-ds4qr
  Aug 12 13:07:09.744: INFO: Got endpoints: latency-svc-w4rt9 [748.042563ms]
  Aug 12 13:07:09.759: INFO: Created: latency-svc-rdlkq
  Aug 12 13:07:09.793: INFO: Got endpoints: latency-svc-mzl6f [749.747798ms]
  Aug 12 13:07:09.807: INFO: Created: latency-svc-nqz86
  Aug 12 13:07:09.846: INFO: Got endpoints: latency-svc-9dhnn [752.800628ms]
  Aug 12 13:07:09.859: INFO: Created: latency-svc-g5wwm
  E0812 13:07:09.889274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:09.892: INFO: Got endpoints: latency-svc-mllxr [746.568185ms]
  Aug 12 13:07:09.907: INFO: Created: latency-svc-kzpz6
  Aug 12 13:07:09.944: INFO: Got endpoints: latency-svc-8vx4g [750.513557ms]
  Aug 12 13:07:09.959: INFO: Created: latency-svc-wvfft
  Aug 12 13:07:09.993: INFO: Got endpoints: latency-svc-p2vqq [749.108363ms]
  Aug 12 13:07:10.007: INFO: Created: latency-svc-vjzlw
  Aug 12 13:07:10.043: INFO: Got endpoints: latency-svc-pnrs9 [750.012193ms]
  Aug 12 13:07:10.059: INFO: Created: latency-svc-qrg5t
  Aug 12 13:07:10.093: INFO: Got endpoints: latency-svc-f88jw [749.512108ms]
  Aug 12 13:07:10.113: INFO: Created: latency-svc-fbm4f
  Aug 12 13:07:10.144: INFO: Got endpoints: latency-svc-tj4jk [750.386308ms]
  Aug 12 13:07:10.158: INFO: Created: latency-svc-64vnd
  Aug 12 13:07:10.195: INFO: Got endpoints: latency-svc-x7w7c [750.926887ms]
  Aug 12 13:07:10.210: INFO: Created: latency-svc-sgw7f
  Aug 12 13:07:10.418: INFO: Got endpoints: latency-svc-fkhv6 [924.120927ms]
  Aug 12 13:07:10.420: INFO: Got endpoints: latency-svc-l8zfd [777.407565ms]
  Aug 12 13:07:10.421: INFO: Got endpoints: latency-svc-wxjs7 [875.7832ms]
  Aug 12 13:07:10.424: INFO: Got endpoints: latency-svc-5cmt2 [829.581109ms]
  Aug 12 13:07:10.442: INFO: Created: latency-svc-rmqbv
  Aug 12 13:07:10.445: INFO: Got endpoints: latency-svc-ds4qr [750.075783ms]
  Aug 12 13:07:10.448: INFO: Created: latency-svc-2vnzj
  Aug 12 13:07:10.459: INFO: Created: latency-svc-9f27c
  Aug 12 13:07:10.495: INFO: Got endpoints: latency-svc-rdlkq [750.243312ms]
  Aug 12 13:07:10.550: INFO: Got endpoints: latency-svc-nqz86 [756.563901ms]
  Aug 12 13:07:10.597: INFO: Got endpoints: latency-svc-g5wwm [751.136834ms]
  Aug 12 13:07:10.644: INFO: Got endpoints: latency-svc-kzpz6 [751.004768ms]
  Aug 12 13:07:10.693: INFO: Got endpoints: latency-svc-wvfft [749.531153ms]
  Aug 12 13:07:10.744: INFO: Got endpoints: latency-svc-vjzlw [751.191819ms]
  Aug 12 13:07:10.794: INFO: Got endpoints: latency-svc-qrg5t [750.460518ms]
  Aug 12 13:07:10.843: INFO: Got endpoints: latency-svc-fbm4f [750.677054ms]
  E0812 13:07:10.889758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:10.894: INFO: Got endpoints: latency-svc-64vnd [750.011135ms]
  Aug 12 13:07:10.944: INFO: Got endpoints: latency-svc-sgw7f [748.510228ms]
  Aug 12 13:07:10.995: INFO: Got endpoints: latency-svc-rmqbv [577.093062ms]
  Aug 12 13:07:11.044: INFO: Got endpoints: latency-svc-2vnzj [623.026216ms]
  Aug 12 13:07:11.095: INFO: Got endpoints: latency-svc-9f27c [673.138238ms]
  Aug 12 13:07:11.095: INFO: Latencies: [62.496458ms 83.226463ms 87.56051ms 90.545734ms 121.711953ms 133.99085ms 177.345304ms 183.0522ms 184.436449ms 184.803186ms 185.410901ms 187.495002ms 188.50663ms 194.807696ms 202.225464ms 208.452911ms 209.470296ms 215.037514ms 224.198718ms 228.418685ms 229.439869ms 240.860344ms 247.903008ms 255.692452ms 265.214453ms 276.017934ms 276.677414ms 277.98796ms 281.308318ms 281.425218ms 282.449188ms 283.538447ms 284.060598ms 285.98187ms 287.795297ms 288.287061ms 288.734368ms 289.287363ms 295.177728ms 297.403153ms 298.396906ms 298.725981ms 305.997958ms 308.781357ms 314.01149ms 316.246397ms 324.173156ms 332.270356ms 339.195903ms 345.145857ms 353.698951ms 392.254857ms 457.574475ms 497.389701ms 509.664215ms 566.56191ms 577.093062ms 623.026216ms 628.067653ms 657.690089ms 673.138238ms 695.283174ms 699.974947ms 737.60459ms 744.47812ms 745.495528ms 745.499517ms 746.428102ms 746.568185ms 747.045077ms 747.050718ms 747.271088ms 747.271901ms 747.833731ms 747.850338ms 748.042563ms 748.085738ms 748.127933ms 748.151433ms 748.151732ms 748.172387ms 748.234666ms 748.247501ms 748.302375ms 748.394925ms 748.510228ms 748.562318ms 748.580031ms 748.609368ms 748.618729ms 748.649521ms 748.668508ms 748.674786ms 748.67493ms 748.749493ms 748.753012ms 748.75903ms 748.846712ms 748.869046ms 748.918808ms 748.927584ms 748.949925ms 748.950527ms 748.966432ms 748.976897ms 749.042742ms 749.098627ms 749.108363ms 749.19078ms 749.191362ms 749.205979ms 749.213467ms 749.251654ms 749.254266ms 749.319553ms 749.319902ms 749.399544ms 749.470371ms 749.512108ms 749.531153ms 749.563526ms 749.58393ms 749.619747ms 749.621176ms 749.718159ms 749.747798ms 749.769663ms 749.803569ms 749.875491ms 749.891624ms 749.892318ms 749.933692ms 749.961903ms 750.00468ms 750.011135ms 750.012193ms 750.036715ms 750.068883ms 750.075783ms 750.080292ms 750.090501ms 750.127915ms 750.185924ms 750.188673ms 750.221198ms 750.243312ms 750.250191ms 750.286079ms 750.346006ms 750.386308ms 750.388876ms 750.398782ms 750.460518ms 750.470973ms 750.48664ms 750.513557ms 750.532577ms 750.551775ms 750.560136ms 750.563337ms 750.677054ms 750.698252ms 750.747599ms 750.796149ms 750.837867ms 750.863872ms 750.907148ms 750.924184ms 750.926887ms 751.004768ms 751.136834ms 751.143809ms 751.151076ms 751.191819ms 751.210215ms 751.553511ms 751.897589ms 752.123678ms 752.298036ms 752.597799ms 752.800628ms 752.83525ms 752.991527ms 753.029135ms 753.0501ms 753.251492ms 753.469405ms 753.672275ms 753.688883ms 753.763561ms 756.563901ms 758.005573ms 777.407565ms 796.334922ms 829.581109ms 843.070841ms 875.7832ms 886.587422ms 893.730157ms 924.120927ms]
  Aug 12 13:07:11.095: INFO: 50 %ile: 748.927584ms
  Aug 12 13:07:11.095: INFO: 90 %ile: 752.800628ms
  Aug 12 13:07:11.095: INFO: 99 %ile: 893.730157ms
  Aug 12 13:07:11.095: INFO: Total sample count: 200
  Aug 12 13:07:11.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7523" for this suite. @ 08/12/23 13:07:11.103
• [9.785 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/12/23 13:07:11.114
  Aug 12 13:07:11.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:07:11.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:11.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:11.141
  STEP: Creating secret with name secret-test-7d10fb57-188b-47f0-9b49-2488eaabea33 @ 08/12/23 13:07:11.145
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:07:11.152
  E0812 13:07:11.890437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:12.890569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:13.890697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:14.890987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:07:15.198
  Aug 12 13:07:15.202: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-ab894c24-3506-49aa-9b0f-925c14b08564 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:07:15.213
  Aug 12 13:07:15.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1224" for this suite. @ 08/12/23 13:07:15.24
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/12/23 13:07:15.259
  Aug 12 13:07:15.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 13:07:15.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:15.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:15.285
  STEP: Creating a test headless service @ 08/12/23 13:07:15.289
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9057.svc.cluster.local;sleep 1; done
   @ 08/12/23 13:07:15.298
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9057.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9057.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9057.svc.cluster.local;sleep 1; done
   @ 08/12/23 13:07:15.298
  STEP: creating a pod to probe DNS @ 08/12/23 13:07:15.298
  STEP: submitting the pod to kubernetes @ 08/12/23 13:07:15.298
  E0812 13:07:15.891876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:16.892638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:17.893323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:18.893504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:07:19.335
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:07:19.34
  Aug 12 13:07:19.352: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.358: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.372: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.380: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.386: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.395: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.400: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9057.svc.cluster.local from pod dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15: the server could not find the requested resource (get pods dns-test-8b5402c7-2735-4d97-9a23-7af372decc15)
  Aug 12 13:07:19.400: INFO: Lookups using dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9057.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9057.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9057.svc.cluster.local jessie_udp@dns-test-service-2.dns-9057.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9057.svc.cluster.local]

  E0812 13:07:19.894494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:20.894784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:21.894948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:22.895064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:23.895244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:24.446: INFO: DNS probes using dns-9057/dns-test-8b5402c7-2735-4d97-9a23-7af372decc15 succeeded

  Aug 12 13:07:24.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:07:24.451
  STEP: deleting the test headless service @ 08/12/23 13:07:24.47
  STEP: Destroying namespace "dns-9057" for this suite. @ 08/12/23 13:07:24.494
• [9.244 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/12/23 13:07:24.505
  Aug 12 13:07:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 13:07:24.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:24.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:24.532
  STEP: create the rc @ 08/12/23 13:07:24.537
  W0812 13:07:24.544790      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0812 13:07:24.895288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:25.895626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:26.895765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:27.896095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:28.896344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/12/23 13:07:29.551
  STEP: wait for all pods to be garbage collected @ 08/12/23 13:07:29.56
  E0812 13:07:29.896825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:30.896984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:31.897798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:32.897909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:33.898111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/12/23 13:07:34.569
  W0812 13:07:34.575860      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 13:07:34.575: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 13:07:34.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2718" for this suite. @ 08/12/23 13:07:34.582
• [10.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/12/23 13:07:34.594
  Aug 12 13:07:34.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename runtimeclass @ 08/12/23 13:07:34.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:34.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:34.621
  E0812 13:07:34.898933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:35.898984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:36.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1162" for this suite. @ 08/12/23 13:07:36.667
• [2.082 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/12/23 13:07:36.677
  Aug 12 13:07:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename podtemplate @ 08/12/23 13:07:36.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:36.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:36.706
  STEP: Create a pod template @ 08/12/23 13:07:36.715
  STEP: Replace a pod template @ 08/12/23 13:07:36.722
  Aug 12 13:07:36.733: INFO: Found updated podtemplate annotation: "true"

  Aug 12 13:07:36.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7418" for this suite. @ 08/12/23 13:07:36.739
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/12/23 13:07:36.752
  Aug 12 13:07:36.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-runtime @ 08/12/23 13:07:36.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:36.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:36.779
  STEP: create the container @ 08/12/23 13:07:36.789
  W0812 13:07:36.801906      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/12/23 13:07:36.802
  E0812 13:07:36.899896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:37.900825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:38.901262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:39.902039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/12/23 13:07:40.829
  STEP: the container should be terminated @ 08/12/23 13:07:40.834
  STEP: the termination message should be set @ 08/12/23 13:07:40.834
  Aug 12 13:07:40.834: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/12/23 13:07:40.834
  Aug 12 13:07:40.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9540" for this suite. @ 08/12/23 13:07:40.869
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/12/23 13:07:40.88
  Aug 12 13:07:40.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-pred @ 08/12/23 13:07:40.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:40.899
  E0812 13:07:40.903430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:40.906
  Aug 12 13:07:40.910: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 12 13:07:40.921: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 13:07:40.928: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-32-142 before test
  Aug 12 13:07:40.934: INFO: nginx-ingress-controller-kubernetes-worker-5944p from ingress-nginx-kubernetes-worker started at 2023-08-12 12:31:40 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.935: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:07:40.935: INFO: test-runtimeclass-runtimeclass-1162-preconfigured-handler-rcgmn from runtimeclass-1162 started at 2023-08-12 13:07:34 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.935: INFO: 	Container test ready: false, restart count 0
  Aug 12 13:07:40.935: INFO: sonobuoy from sonobuoy started at 2023-08-12 12:09:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.935: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 12 13:07:40.936: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-5tdg5 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:07:40.936: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:07:40.936: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 13:07:40.936: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-79-233 before test
  Aug 12 13:07:40.944: INFO: default-http-backend-kubernetes-worker-65fc475d49-9qsc9 from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:29 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: nginx-ingress-controller-kubernetes-worker-9f2bf from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:28 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: coredns-5c7f76ccb8-qnk7d from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container coredns ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: kube-state-metrics-5b95b4459c-v9mbb from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: metrics-server-v0.5.2-6cf8c8b69c-zqxs2 from kube-system started at 2023-08-12 11:54:23 +0000 UTC (2 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: dashboard-metrics-scraper-6b8586b5c9-vtmth from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: kubernetes-dashboard-6869f4cd5f-ttr98 from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-bvmhq from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:07:40.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 13:07:40.944: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-203 before test
  Aug 12 13:07:40.952: INFO: nginx-ingress-controller-kubernetes-worker-rdq6z from ingress-nginx-kubernetes-worker started at 2023-08-12 11:59:55 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.952: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:07:40.952: INFO: calico-kube-controllers-5f769b769b-ttpqk from kube-system started at 2023-08-12 12:03:40 +0000 UTC (1 container statuses recorded)
  Aug 12 13:07:40.952: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 12 13:07:40.952: INFO: sonobuoy-e2e-job-d2bbfc40cf5e42c2 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:07:40.952: INFO: 	Container e2e ready: true, restart count 0
  Aug 12 13:07:40.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:07:40.952: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-r2r8l from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:07:40.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:07:40.952: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/12/23 13:07:40.952
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.177aa4aa359c1c61], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 08/12/23 13:07:40.994
  E0812 13:07:41.903621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:41.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6592" for this suite. @ 08/12/23 13:07:41.998
• [1.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/12/23 13:07:42.012
  Aug 12 13:07:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:07:42.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:42.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:42.042
  STEP: set up a multi version CRD @ 08/12/23 13:07:42.047
  Aug 12 13:07:42.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:07:42.904029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:43.904572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:44.904620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:45.904727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 08/12/23 13:07:46.144
  STEP: check the new version name is served @ 08/12/23 13:07:46.162
  E0812 13:07:46.905192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 08/12/23 13:07:47.059
  STEP: check the other version is not changed @ 08/12/23 13:07:47.838
  E0812 13:07:47.906034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:48.906639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:49.906760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:50.906862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:07:50.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6622" for this suite. @ 08/12/23 13:07:50.983
• [8.980 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/12/23 13:07:50.995
  Aug 12 13:07:50.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 13:07:50.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:51.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:51.023
  E0812 13:07:51.906975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:52.907185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 08/12/23 13:07:53.06
  Aug 12 13:07:53.060: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5380 pod-service-account-70fbec7c-f093-4549-8472-d914611d05ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 08/12/23 13:07:53.218
  Aug 12 13:07:53.218: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5380 pod-service-account-70fbec7c-f093-4549-8472-d914611d05ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 08/12/23 13:07:53.382
  Aug 12 13:07:53.382: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5380 pod-service-account-70fbec7c-f093-4549-8472-d914611d05ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug 12 13:07:53.565: INFO: Got root ca configmap in namespace "svcaccounts-5380"
  Aug 12 13:07:53.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5380" for this suite. @ 08/12/23 13:07:53.574
• [2.588 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/12/23 13:07:53.584
  Aug 12 13:07:53.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:07:53.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:53.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:53.613
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:07:53.618
  E0812 13:07:53.907882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:54.908025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:55.908980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:56.909082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:07:57.645
  Aug 12 13:07:57.650: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-168e557d-d7f4-4985-a070-dd6e5751638d container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:07:57.661
  Aug 12 13:07:57.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7086" for this suite. @ 08/12/23 13:07:57.69
• [4.115 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/12/23 13:07:57.7
  Aug 12 13:07:57.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context @ 08/12/23 13:07:57.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:07:57.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:07:57.727
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/12/23 13:07:57.731
  E0812 13:07:57.909451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:58.909571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:07:59.910298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:00.910610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:08:01.771
  Aug 12 13:08:01.776: INFO: Trying to get logs from node ip-172-31-32-142 pod security-context-4115a89a-6f2b-4d81-b6b6-a2f028c04dc6 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:08:01.788
  Aug 12 13:08:01.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9506" for this suite. @ 08/12/23 13:08:01.811
• [4.122 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/12/23 13:08:01.824
  Aug 12 13:08:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:08:01.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:08:01.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:08:01.851
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:08:01.856
  E0812 13:08:01.911220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:02.911318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:03.912266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:04.913383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:08:05.886
  Aug 12 13:08:05.890: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-4aed4290-efa6-407b-8963-062b941c7cb3 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:08:05.901
  E0812 13:08:05.914387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:08:05.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1138" for this suite. @ 08/12/23 13:08:05.926
• [4.111 seconds]
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/12/23 13:08:05.935
  Aug 12 13:08:05.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 13:08:05.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:08:05.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:08:05.961
  STEP: Given a ReplicationController is created @ 08/12/23 13:08:05.966
  STEP: When the matched label of one of its pods change @ 08/12/23 13:08:05.973
  Aug 12 13:08:05.977: INFO: Pod name pod-release: Found 0 pods out of 1
  E0812 13:08:06.914561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:07.914687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:08.914903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:09.915002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:10.915294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:08:10.984: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/12/23 13:08:10.997
  E0812 13:08:11.915378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:08:12.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6430" for this suite. @ 08/12/23 13:08:12.015
• [6.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/12/23 13:08:12.028
  Aug 12 13:08:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 13:08:12.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:08:12.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:08:12.054
  STEP: Creating a job @ 08/12/23 13:08:12.058
  STEP: Ensuring active pods == parallelism @ 08/12/23 13:08:12.07
  E0812 13:08:12.915515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:13.916665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 08/12/23 13:08:14.076
  Aug 12 13:08:14.596: INFO: Successfully updated pod "adopt-release-6mnjh"
  STEP: Checking that the Job readopts the Pod @ 08/12/23 13:08:14.596
  E0812 13:08:14.917585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:15.917925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 08/12/23 13:08:16.61
  E0812 13:08:16.918222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:08:17.127: INFO: Successfully updated pod "adopt-release-6mnjh"
  STEP: Checking that the Job releases the Pod @ 08/12/23 13:08:17.127
  E0812 13:08:17.918249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:18.918554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:08:19.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5772" for this suite. @ 08/12/23 13:08:19.147
• [7.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/12/23 13:08:19.16
  Aug 12 13:08:19.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename cronjob @ 08/12/23 13:08:19.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:08:19.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:08:19.183
  STEP: Creating a suspended cronjob @ 08/12/23 13:08:19.188
  STEP: Ensuring no jobs are scheduled @ 08/12/23 13:08:19.194
  E0812 13:08:19.919570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:20.919689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:21.919826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:22.920220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:23.920713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:24.921756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:25.922198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:26.922880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:27.923940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:28.924102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:29.924191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:30.924391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:31.924729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:32.925787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:33.925881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:34.926017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:35.926937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:36.926970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:37.927095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:38.927505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:39.928075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:40.928390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:41.928486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:42.928741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:43.928873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:44.929780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:45.929823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:46.930367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:47.930506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:48.930591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:49.930739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:50.930861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:51.931376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:52.931495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:53.931589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:54.932473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:55.932633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:56.933371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:57.933478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:58.933658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:08:59.933733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:00.933791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:01.933855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:02.934585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:03.934685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:04.935811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:05.936075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:06.936183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:07.936313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:08.936437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:09.937440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:10.937771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:11.937921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:12.938037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:13.938994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:14.939302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:15.940039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:16.940346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:17.941444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:18.941751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:19.942872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:20.943372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:21.943471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:22.943616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:23.943741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:24.944373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:25.945099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:26.945284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:27.945367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:28.945770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:29.946454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:30.946938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:31.947240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:32.947527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:33.948029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:34.948154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:35.949187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:36.949642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:37.950771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:38.950893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:39.951530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:40.952057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:41.952933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:42.953065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:43.953576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:44.953669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:45.954124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:46.954231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:47.954815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:48.954934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:49.955528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:50.956324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:51.956694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:52.956793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:53.957741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:54.958009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:55.959021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:56.959161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:57.959257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:58.959354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:09:59.959630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:00.960293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:01.960436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:02.960668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:03.960795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:04.961746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:05.962075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:06.962209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:07.962734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:08.962881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:09.962996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:10.963650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:11.964290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:12.964413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:13.964665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:14.965743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:15.966177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:16.966518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:17.966962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:18.967203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:19.967330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:20.968240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:21.969223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:22.969756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:23.969885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:24.970131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:25.970380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:26.970627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:27.971229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:28.971326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:29.971847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:30.972338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:31.972684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:32.972900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:33.973046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:34.973120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:35.973395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:36.973595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:37.973903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:38.974142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:39.974567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:40.975026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:41.975790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:42.976560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:43.976673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:44.977789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:45.978164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:46.978289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:47.978650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:48.978744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:49.979400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:50.979850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:51.979975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:52.980302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:53.980664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:54.980789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:55.981086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:56.981211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:57.981293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:58.981370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:10:59.981753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:00.982263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:01.982839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:02.983127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:03.983207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:04.983325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:05.983504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:06.983678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:07.984783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:08.984957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:09.985745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:10.986793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:11.986953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:12.987068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:13.987193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:14.987439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:15.987996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:16.988287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:17.988669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:18.988724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:19.989758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:20.990235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:21.990342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:22.990607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:23.990735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:24.990988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:25.991199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:26.991504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:27.992650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:28.992827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:29.992980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:30.993722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:31.994210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:32.994713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:33.995679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:34.995808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:35.996160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:36.996291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:37.996676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:38.997762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:39.998355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:40.999302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:41.999424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:42.999679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:43.999813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:45.000667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:46.001218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:47.002023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:48.002161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:49.002308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:50.002421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:51.002980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:52.002791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:53.002882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:54.003467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:55.003543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:56.004162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:57.004287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:58.004933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:11:59.005063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:00.005574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:01.005991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:02.006121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:03.006357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:04.006477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:05.006709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:06.007043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:07.007311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:08.007444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:09.007742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:10.007903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:11.008380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:12.008997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:13.009742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:14.009940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:15.010230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:16.010442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:17.010550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:18.011491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:19.011624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:20.011714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:21.012408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:22.013285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:23.013754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:24.013880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:25.014135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:26.015218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:27.016149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:28.016664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:29.017742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:30.018282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:31.019286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:32.019405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:33.020470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:34.021526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:35.021652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:36.022228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:37.022726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:38.022858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:39.022941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:40.023057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:41.023483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:42.023639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:43.023794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:44.024358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:45.024702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:46.025825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:47.025918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:48.026016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:49.026843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:50.027455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:51.027865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:52.028138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:53.028247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:54.028606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:55.028819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:56.029098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:57.029221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:58.029893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:12:59.029999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:00.030852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:01.031546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:02.031846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:03.031981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:04.032824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:05.032972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:06.033224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:07.033738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:08.034732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:09.034861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:10.035277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:11.036002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:12.036952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:13.037071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:14.037746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:15.038016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:16.038132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:17.038423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:18.038886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:19.039010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/12/23 13:13:19.204
  STEP: Removing cronjob @ 08/12/23 13:13:19.209
  Aug 12 13:13:19.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2889" for this suite. @ 08/12/23 13:13:19.224
• [300.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/12/23 13:13:19.248
  Aug 12 13:13:19.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-preemption @ 08/12/23 13:13:19.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:13:19.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:13:19.274
  Aug 12 13:13:19.302: INFO: Waiting up to 1m0s for all nodes to be ready
  E0812 13:13:20.039169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:21.039288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:22.040369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:23.040805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:24.040908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:25.041052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:26.041275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:27.041316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:28.041784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:29.041908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:30.042347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:31.042851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:32.043332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:33.043435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:34.043849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:35.043942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:36.044067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:37.044169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:38.044756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:39.044876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:40.045736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:41.046482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:42.046851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:43.047153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:44.048232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:45.048954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:46.049132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:47.049320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:48.049838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:49.050309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:50.050790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:51.051787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:52.051894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:53.051977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:54.052099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:55.052215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:56.053073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:57.053134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:58.053606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:13:59.053708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:00.053851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:01.054368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:02.054472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:03.054716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:04.054847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:05.055082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:06.056002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:07.056123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:08.056666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:09.057778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:10.057820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:11.058837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:12.059112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:13.059329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:14.059907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:15.060199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:16.061249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:17.061371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:18.061505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:19.062217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:14:19.325: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/12/23 13:14:19.33
  Aug 12 13:14:19.357: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 12 13:14:19.369: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 12 13:14:19.393: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 12 13:14:19.403: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 12 13:14:19.429: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 12 13:14:19.438: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/12/23 13:14:19.438
  E0812 13:14:20.062385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:21.063013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/12/23 13:14:21.478
  E0812 13:14:22.063436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:23.063529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:24.063658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:25.063790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:14:25.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7733" for this suite. @ 08/12/23 13:14:25.59
• [66.352 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/12/23 13:14:25.601
  Aug 12 13:14:25.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 13:14:25.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:14:25.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:14:25.627
  STEP: Creating service test in namespace statefulset-7278 @ 08/12/23 13:14:25.632
  STEP: Looking for a node to schedule stateful set and pod @ 08/12/23 13:14:25.641
  STEP: Creating pod with conflicting port in namespace statefulset-7278 @ 08/12/23 13:14:25.65
  STEP: Waiting until pod test-pod will start running in namespace statefulset-7278 @ 08/12/23 13:14:25.661
  E0812 13:14:26.064645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:27.064687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-7278 @ 08/12/23 13:14:27.673
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7278 @ 08/12/23 13:14:27.68
  Aug 12 13:14:27.708: INFO: Observed stateful pod in namespace: statefulset-7278, name: ss-0, uid: 98484591-2d6b-4520-a8ef-40bfc8fd4956, status phase: Pending. Waiting for statefulset controller to delete.
  Aug 12 13:14:27.732: INFO: Observed stateful pod in namespace: statefulset-7278, name: ss-0, uid: 98484591-2d6b-4520-a8ef-40bfc8fd4956, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 12 13:14:27.747: INFO: Observed stateful pod in namespace: statefulset-7278, name: ss-0, uid: 98484591-2d6b-4520-a8ef-40bfc8fd4956, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 12 13:14:27.755: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7278
  STEP: Removing pod with conflicting port in namespace statefulset-7278 @ 08/12/23 13:14:27.756
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7278 and will be in running state @ 08/12/23 13:14:27.775
  E0812 13:14:28.065744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:29.066129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:14:29.786: INFO: Deleting all statefulset in ns statefulset-7278
  Aug 12 13:14:29.791: INFO: Scaling statefulset ss to 0
  E0812 13:14:30.066244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:31.067167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:32.067315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:33.067451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:34.068619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:35.068685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:36.069844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:37.069874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:38.070117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:39.070439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:14:39.814: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 13:14:39.818: INFO: Deleting statefulset ss
  Aug 12 13:14:39.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7278" for this suite. @ 08/12/23 13:14:39.844
• [14.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/12/23 13:14:39.855
  Aug 12 13:14:39.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 13:14:39.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:14:39.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:14:39.881
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9890.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9890.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/12/23 13:14:39.885
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9890.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9890.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/12/23 13:14:39.885
  STEP: creating a pod to probe /etc/hosts @ 08/12/23 13:14:39.885
  STEP: submitting the pod to kubernetes @ 08/12/23 13:14:39.885
  E0812 13:14:40.071418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:41.072455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:14:41.91
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:14:41.914
  Aug 12 13:14:41.938: INFO: DNS probes using dns-9890/dns-test-988f884f-185b-4f9a-8e41-0a57eb791dcd succeeded

  Aug 12 13:14:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:14:41.944
  STEP: Destroying namespace "dns-9890" for this suite. @ 08/12/23 13:14:41.968
• [2.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/12/23 13:14:41.983
  Aug 12 13:14:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/12/23 13:14:41.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:14:42.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:14:42.015
  STEP: fetching the /apis discovery document @ 08/12/23 13:14:42.02
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/12/23 13:14:42.022
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/12/23 13:14:42.022
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/12/23 13:14:42.022
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/12/23 13:14:42.023
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/12/23 13:14:42.023
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/12/23 13:14:42.025
  Aug 12 13:14:42.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-242" for this suite. @ 08/12/23 13:14:42.032
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/12/23 13:14:42.042
  Aug 12 13:14:42.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename watch @ 08/12/23 13:14:42.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:14:42.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:14:42.066
  STEP: creating a watch on configmaps with label A @ 08/12/23 13:14:42.07
  STEP: creating a watch on configmaps with label B @ 08/12/23 13:14:42.072
  E0812 13:14:42.072459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a watch on configmaps with label A or B @ 08/12/23 13:14:42.073
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/12/23 13:14:42.075
  Aug 12 13:14:42.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29346 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:42.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29346 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/12/23 13:14:42.083
  Aug 12 13:14:42.094: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29347 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:42.094: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29347 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/12/23 13:14:42.094
  Aug 12 13:14:42.107: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29348 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:42.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29348 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/12/23 13:14:42.112
  Aug 12 13:14:42.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29349 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:42.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1428  29fc8002-9d78-4cd2-85a0-0e2c2439d479 29349 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/12/23 13:14:42.123
  Aug 12 13:14:42.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1428  68915482-4e06-4b7d-a115-a5ed8e13c56d 29350 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:42.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1428  68915482-4e06-4b7d-a115-a5ed8e13c56d 29350 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0812 13:14:43.072697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:44.072923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:45.073089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:46.073157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:47.073268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:48.073380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:49.073502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:50.073616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:51.074352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:52.074670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/12/23 13:14:52.131
  Aug 12 13:14:52.141: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1428  68915482-4e06-4b7d-a115-a5ed8e13c56d 29417 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:14:52.141: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1428  68915482-4e06-4b7d-a115-a5ed8e13c56d 29417 0 2023-08-12 13:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-12 13:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0812 13:14:53.075204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:54.075307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:55.079029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:56.079593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:57.080485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:58.080710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:14:59.080824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:00.081763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:01.082391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:02.082517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:15:02.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1428" for this suite. @ 08/12/23 13:15:02.149
• [20.116 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/12/23 13:15:02.159
  Aug 12 13:15:02.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/12/23 13:15:02.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:15:02.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:15:02.191
  Aug 12 13:15:02.195: INFO: Waiting up to 1m0s for all nodes to be ready
  E0812 13:15:03.082796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:04.082876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:05.083533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:06.083730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:07.083815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:08.083971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:09.084331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:10.084639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:11.085427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:12.085542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:13.085660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:14.086783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:15.086902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:16.087319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:17.087430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:18.087558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:19.087685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:20.087949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:21.088255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:22.088386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:23.088671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:24.089741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:25.089881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:26.090853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:27.090984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:28.091241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:29.092111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:30.092229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:31.092981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:32.093031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:33.093238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:34.093360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:35.093454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:36.093553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:37.093723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:38.093980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:39.094091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:40.094932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:41.095310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:42.095496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:43.095623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:44.096002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:45.096202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:46.096799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:47.097284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:48.097872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:49.097967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:50.098104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:51.098694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:52.099026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:53.099371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:54.099501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:55.099638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:56.100239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:57.100752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:58.101792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:15:59.101888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:00.102060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:01.102499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:02.102873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:02.215: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 13:16:02.220: INFO: Starting informer...
  STEP: Starting pods... @ 08/12/23 13:16:02.22
  Aug 12 13:16:02.449: INFO: Pod1 is running on ip-172-31-32-142. Tainting Node
  E0812 13:16:03.103299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:04.103372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:04.683: INFO: Pod2 is running on ip-172-31-32-142. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/12/23 13:16:04.683
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/12/23 13:16:04.696
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/12/23 13:16:04.701
  E0812 13:16:05.104017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:06.104234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:07.105079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:08.105166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:09.105313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:10.105447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:10.893: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0812 13:16:11.106540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:12.107453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:13.107792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:14.107924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:15.108046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:16.108671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:17.108835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:18.108931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:19.109745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:20.110286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:21.110413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:22.110786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:23.111069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:24.111367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:25.111601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:26.112200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:27.112432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:28.112720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:29.112830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:30.112967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:30.959: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug 12 13:16:30.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/12/23 13:16:30.979
  STEP: Destroying namespace "taint-multiple-pods-1208" for this suite. @ 08/12/23 13:16:30.985
• [88.843 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/12/23 13:16:31.003
  Aug 12 13:16:31.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/12/23 13:16:31.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:16:31.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:16:31.04
  STEP: creating a target pod @ 08/12/23 13:16:31.045
  E0812 13:16:31.113845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:32.113971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 08/12/23 13:16:33.072
  E0812 13:16:33.114577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:34.114696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 08/12/23 13:16:35.094
  Aug 12 13:16:35.094: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9759 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:16:35.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:16:35.095: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:16:35.095: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-9759/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  E0812 13:16:35.115427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:35.173: INFO: Exec stderr: ""
  Aug 12 13:16:35.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-9759" for this suite. @ 08/12/23 13:16:35.205
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/12/23 13:16:35.218
  Aug 12 13:16:35.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:16:35.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:16:35.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:16:35.244
  STEP: Creating pod liveness-d345632c-54de-4e0e-be98-7dd1f6022da1 in namespace container-probe-267 @ 08/12/23 13:16:35.253
  E0812 13:16:36.115859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:37.116007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:37.279: INFO: Started pod liveness-d345632c-54de-4e0e-be98-7dd1f6022da1 in namespace container-probe-267
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:16:37.279
  Aug 12 13:16:37.283: INFO: Initial restart count of pod liveness-d345632c-54de-4e0e-be98-7dd1f6022da1 is 0
  E0812 13:16:38.116069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:39.116170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:40.116290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:41.117418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:42.117555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:43.117930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:44.118026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:45.118447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:46.119194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:47.120037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:48.120217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:49.120295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:50.121334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:51.121757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:52.121887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:53.121951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:54.122062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:55.122472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:56.123251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:16:57.124231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:16:57.346: INFO: Restart count of pod container-probe-267/liveness-d345632c-54de-4e0e-be98-7dd1f6022da1 is now 1 (20.062589819s elapsed)
  Aug 12 13:16:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:16:57.351
  STEP: Destroying namespace "container-probe-267" for this suite. @ 08/12/23 13:16:57.367
• [22.159 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/12/23 13:16:57.378
  Aug 12 13:16:57.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename events @ 08/12/23 13:16:57.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:16:57.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:16:57.405
  STEP: creating a test event @ 08/12/23 13:16:57.415
  STEP: listing all events in all namespaces @ 08/12/23 13:16:57.423
  STEP: patching the test event @ 08/12/23 13:16:57.429
  STEP: fetching the test event @ 08/12/23 13:16:57.438
  STEP: updating the test event @ 08/12/23 13:16:57.443
  STEP: getting the test event @ 08/12/23 13:16:57.457
  STEP: deleting the test event @ 08/12/23 13:16:57.461
  STEP: listing all events in all namespaces @ 08/12/23 13:16:57.473
  Aug 12 13:16:57.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1523" for this suite. @ 08/12/23 13:16:57.484
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/12/23 13:16:57.497
  Aug 12 13:16:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:16:57.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:16:57.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:16:57.521
  STEP: Setting up server cert @ 08/12/23 13:16:57.55
  E0812 13:16:58.124776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:16:58.211
  STEP: Deploying the webhook pod @ 08/12/23 13:16:58.222
  STEP: Wait for the deployment to be ready @ 08/12/23 13:16:58.24
  Aug 12 13:16:58.250: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:16:59.125808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:00.126086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:17:00.265
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:17:00.279
  E0812 13:17:01.126291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:01.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 12 13:17:01.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8795-crds.webhook.example.com via the AdmissionRegistration API @ 08/12/23 13:17:01.805
  STEP: Creating a custom resource while v1 is storage version @ 08/12/23 13:17:01.829
  E0812 13:17:02.127268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:03.127392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/12/23 13:17:03.895
  STEP: Patching the custom resource while v2 is storage version @ 08/12/23 13:17:03.951
  Aug 12 13:17:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0812 13:17:04.128437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4552" for this suite. @ 08/12/23 13:17:04.698
  STEP: Destroying namespace "webhook-markers-2472" for this suite. @ 08/12/23 13:17:04.708
• [7.221 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/12/23 13:17:04.718
  Aug 12 13:17:04.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/12/23 13:17:04.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:04.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:04.741
  E0812 13:17:05.129628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:06.130281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:06.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/12/23 13:17:06.795
  STEP: Cleaning up the configmap @ 08/12/23 13:17:06.804
  STEP: Cleaning up the pod @ 08/12/23 13:17:06.813
  STEP: Destroying namespace "emptydir-wrapper-9286" for this suite. @ 08/12/23 13:17:06.828
• [2.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/12/23 13:17:06.841
  Aug 12 13:17:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename certificates @ 08/12/23 13:17:06.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:06.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:06.871
  E0812 13:17:07.130863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 08/12/23 13:17:07.66
  STEP: getting /apis/certificates.k8s.io @ 08/12/23 13:17:07.665
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/12/23 13:17:07.667
  STEP: creating @ 08/12/23 13:17:07.668
  STEP: getting @ 08/12/23 13:17:07.69
  STEP: listing @ 08/12/23 13:17:07.694
  STEP: watching @ 08/12/23 13:17:07.699
  Aug 12 13:17:07.699: INFO: starting watch
  STEP: patching @ 08/12/23 13:17:07.7
  STEP: updating @ 08/12/23 13:17:07.709
  Aug 12 13:17:07.716: INFO: waiting for watch events with expected annotations
  Aug 12 13:17:07.716: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/12/23 13:17:07.716
  STEP: patching /approval @ 08/12/23 13:17:07.72
  STEP: updating /approval @ 08/12/23 13:17:07.73
  STEP: getting /status @ 08/12/23 13:17:07.738
  STEP: patching /status @ 08/12/23 13:17:07.743
  STEP: updating /status @ 08/12/23 13:17:07.757
  STEP: deleting @ 08/12/23 13:17:07.766
  STEP: deleting a collection @ 08/12/23 13:17:07.787
  Aug 12 13:17:07.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3433" for this suite. @ 08/12/23 13:17:07.818
• [0.996 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/12/23 13:17:07.841
  Aug 12 13:17:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context-test @ 08/12/23 13:17:07.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:07.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:07.866
  E0812 13:17:08.131423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:09.131531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:10.132020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:11.132276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:11.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3973" for this suite. @ 08/12/23 13:17:11.906
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/12/23 13:17:11.925
  Aug 12 13:17:11.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:17:11.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:11.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:11.957
  STEP: Creating the pod @ 08/12/23 13:17:11.962
  E0812 13:17:12.132920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:13.133039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:14.134139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:14.518: INFO: Successfully updated pod "annotationupdatea8939e11-9949-4a4b-ab86-d7bf2b5ea4d8"
  E0812 13:17:15.135220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:16.135800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:16.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-522" for this suite. @ 08/12/23 13:17:16.545
• [4.629 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/12/23 13:17:16.554
  Aug 12 13:17:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 13:17:16.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:16.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:16.577
  STEP: Creating ServiceAccount "e2e-sa-r9mwg"  @ 08/12/23 13:17:16.582
  Aug 12 13:17:16.588: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-r9mwg"  @ 08/12/23 13:17:16.588
  Aug 12 13:17:16.601: INFO: AutomountServiceAccountToken: true
  Aug 12 13:17:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4512" for this suite. @ 08/12/23 13:17:16.607
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/12/23 13:17:16.619
  Aug 12 13:17:16.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/12/23 13:17:16.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:16.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:16.643
  STEP: create the container to handle the HTTPGet hook request. @ 08/12/23 13:17:16.653
  E0812 13:17:17.136845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:18.137790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/12/23 13:17:18.678
  E0812 13:17:19.138882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:20.138970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/12/23 13:17:20.702
  STEP: delete the pod with lifecycle hook @ 08/12/23 13:17:20.725
  E0812 13:17:21.139701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:22.139819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:17:22.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6827" for this suite. @ 08/12/23 13:17:22.753
• [6.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/12/23 13:17:22.764
  Aug 12 13:17:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:17:22.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:22.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:22.789
  STEP: starting the proxy server @ 08/12/23 13:17:22.794
  Aug 12 13:17:22.794: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-6519 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/12/23 13:17:22.853
  Aug 12 13:17:22.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6519" for this suite. @ 08/12/23 13:17:22.87
• [0.115 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/12/23 13:17:22.88
  Aug 12 13:17:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 13:17:22.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:17:22.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:17:22.905
  STEP: create the rc @ 08/12/23 13:17:22.914
  W0812 13:17:22.921400      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0812 13:17:23.140813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:24.141000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:25.141399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:26.142063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:27.142967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:28.143255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/12/23 13:17:28.928
  STEP: wait for the rc to be deleted @ 08/12/23 13:17:28.942
  E0812 13:17:29.143911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:30.151447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:31.151618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:32.152024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:33.152512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/12/23 13:17:33.954
  E0812 13:17:34.157476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:35.158325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:36.159000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:37.159402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:38.159463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:39.160331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:40.160724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:41.161131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:42.161233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:43.161792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:44.161960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:45.162249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:46.163330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:47.164092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:48.164331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:49.164655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:50.164696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:51.165156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:52.165279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:53.165753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:54.165980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:55.166096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:56.166223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:57.166360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:58.167146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:17:59.167281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:00.167540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:01.168002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:02.168280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:03.168405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/12/23 13:18:03.973
  W0812 13:18:03.981930      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug 12 13:18:03.981: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 12 13:18:03.982: INFO: Deleting pod "simpletest.rc-249dd" in namespace "gc-8938"
  Aug 12 13:18:03.999: INFO: Deleting pod "simpletest.rc-2mjhh" in namespace "gc-8938"
  Aug 12 13:18:04.023: INFO: Deleting pod "simpletest.rc-2r88v" in namespace "gc-8938"
  Aug 12 13:18:04.040: INFO: Deleting pod "simpletest.rc-2sfkr" in namespace "gc-8938"
  Aug 12 13:18:04.058: INFO: Deleting pod "simpletest.rc-44mk7" in namespace "gc-8938"
  Aug 12 13:18:04.081: INFO: Deleting pod "simpletest.rc-4lcl4" in namespace "gc-8938"
  Aug 12 13:18:04.101: INFO: Deleting pod "simpletest.rc-4tq9z" in namespace "gc-8938"
  Aug 12 13:18:04.118: INFO: Deleting pod "simpletest.rc-4wbdx" in namespace "gc-8938"
  Aug 12 13:18:04.133: INFO: Deleting pod "simpletest.rc-4z4fs" in namespace "gc-8938"
  Aug 12 13:18:04.150: INFO: Deleting pod "simpletest.rc-56b8x" in namespace "gc-8938"
  Aug 12 13:18:04.166: INFO: Deleting pod "simpletest.rc-5gsrk" in namespace "gc-8938"
  E0812 13:18:04.169049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:04.183: INFO: Deleting pod "simpletest.rc-5hcs7" in namespace "gc-8938"
  Aug 12 13:18:04.197: INFO: Deleting pod "simpletest.rc-5ljkt" in namespace "gc-8938"
  Aug 12 13:18:04.216: INFO: Deleting pod "simpletest.rc-5rjxm" in namespace "gc-8938"
  Aug 12 13:18:04.234: INFO: Deleting pod "simpletest.rc-5s8tk" in namespace "gc-8938"
  Aug 12 13:18:04.254: INFO: Deleting pod "simpletest.rc-67h9m" in namespace "gc-8938"
  Aug 12 13:18:04.272: INFO: Deleting pod "simpletest.rc-689kd" in namespace "gc-8938"
  Aug 12 13:18:04.290: INFO: Deleting pod "simpletest.rc-6rcsk" in namespace "gc-8938"
  Aug 12 13:18:04.305: INFO: Deleting pod "simpletest.rc-7xqsn" in namespace "gc-8938"
  Aug 12 13:18:04.325: INFO: Deleting pod "simpletest.rc-8pfrr" in namespace "gc-8938"
  Aug 12 13:18:04.343: INFO: Deleting pod "simpletest.rc-8qnxf" in namespace "gc-8938"
  Aug 12 13:18:04.356: INFO: Deleting pod "simpletest.rc-97fg4" in namespace "gc-8938"
  Aug 12 13:18:04.376: INFO: Deleting pod "simpletest.rc-9bdfb" in namespace "gc-8938"
  Aug 12 13:18:04.391: INFO: Deleting pod "simpletest.rc-9l6fn" in namespace "gc-8938"
  Aug 12 13:18:04.412: INFO: Deleting pod "simpletest.rc-9p4df" in namespace "gc-8938"
  Aug 12 13:18:04.430: INFO: Deleting pod "simpletest.rc-bh4d4" in namespace "gc-8938"
  Aug 12 13:18:04.448: INFO: Deleting pod "simpletest.rc-bn4pt" in namespace "gc-8938"
  Aug 12 13:18:04.462: INFO: Deleting pod "simpletest.rc-c48xg" in namespace "gc-8938"
  Aug 12 13:18:04.478: INFO: Deleting pod "simpletest.rc-cbhqd" in namespace "gc-8938"
  Aug 12 13:18:04.501: INFO: Deleting pod "simpletest.rc-cpmmq" in namespace "gc-8938"
  Aug 12 13:18:04.516: INFO: Deleting pod "simpletest.rc-cztph" in namespace "gc-8938"
  Aug 12 13:18:04.535: INFO: Deleting pod "simpletest.rc-d4ktx" in namespace "gc-8938"
  Aug 12 13:18:04.551: INFO: Deleting pod "simpletest.rc-d4s8c" in namespace "gc-8938"
  Aug 12 13:18:04.568: INFO: Deleting pod "simpletest.rc-dfs8d" in namespace "gc-8938"
  Aug 12 13:18:04.587: INFO: Deleting pod "simpletest.rc-dnr2c" in namespace "gc-8938"
  Aug 12 13:18:04.601: INFO: Deleting pod "simpletest.rc-dps82" in namespace "gc-8938"
  Aug 12 13:18:04.633: INFO: Deleting pod "simpletest.rc-dqkfc" in namespace "gc-8938"
  Aug 12 13:18:04.648: INFO: Deleting pod "simpletest.rc-f7c9s" in namespace "gc-8938"
  Aug 12 13:18:04.665: INFO: Deleting pod "simpletest.rc-fpkwl" in namespace "gc-8938"
  Aug 12 13:18:04.680: INFO: Deleting pod "simpletest.rc-fq7f6" in namespace "gc-8938"
  Aug 12 13:18:04.700: INFO: Deleting pod "simpletest.rc-gfhzl" in namespace "gc-8938"
  Aug 12 13:18:04.718: INFO: Deleting pod "simpletest.rc-gzbs9" in namespace "gc-8938"
  Aug 12 13:18:04.736: INFO: Deleting pod "simpletest.rc-hcf9q" in namespace "gc-8938"
  Aug 12 13:18:04.757: INFO: Deleting pod "simpletest.rc-hkc7w" in namespace "gc-8938"
  Aug 12 13:18:04.772: INFO: Deleting pod "simpletest.rc-hncxc" in namespace "gc-8938"
  Aug 12 13:18:04.791: INFO: Deleting pod "simpletest.rc-htntz" in namespace "gc-8938"
  Aug 12 13:18:04.811: INFO: Deleting pod "simpletest.rc-hxb8q" in namespace "gc-8938"
  Aug 12 13:18:04.838: INFO: Deleting pod "simpletest.rc-j5vwk" in namespace "gc-8938"
  Aug 12 13:18:04.856: INFO: Deleting pod "simpletest.rc-jhqdq" in namespace "gc-8938"
  Aug 12 13:18:04.873: INFO: Deleting pod "simpletest.rc-jk5x9" in namespace "gc-8938"
  Aug 12 13:18:04.890: INFO: Deleting pod "simpletest.rc-jrbfl" in namespace "gc-8938"
  Aug 12 13:18:04.909: INFO: Deleting pod "simpletest.rc-jx624" in namespace "gc-8938"
  Aug 12 13:18:04.923: INFO: Deleting pod "simpletest.rc-kbhx6" in namespace "gc-8938"
  Aug 12 13:18:04.936: INFO: Deleting pod "simpletest.rc-kq9xg" in namespace "gc-8938"
  Aug 12 13:18:04.950: INFO: Deleting pod "simpletest.rc-kvjct" in namespace "gc-8938"
  Aug 12 13:18:04.976: INFO: Deleting pod "simpletest.rc-l2phh" in namespace "gc-8938"
  Aug 12 13:18:04.995: INFO: Deleting pod "simpletest.rc-ln2tb" in namespace "gc-8938"
  Aug 12 13:18:05.015: INFO: Deleting pod "simpletest.rc-lzqzw" in namespace "gc-8938"
  Aug 12 13:18:05.034: INFO: Deleting pod "simpletest.rc-m54vt" in namespace "gc-8938"
  Aug 12 13:18:05.052: INFO: Deleting pod "simpletest.rc-m54z5" in namespace "gc-8938"
  Aug 12 13:18:05.067: INFO: Deleting pod "simpletest.rc-m95dj" in namespace "gc-8938"
  Aug 12 13:18:05.085: INFO: Deleting pod "simpletest.rc-m995b" in namespace "gc-8938"
  Aug 12 13:18:05.104: INFO: Deleting pod "simpletest.rc-mhjkz" in namespace "gc-8938"
  Aug 12 13:18:05.124: INFO: Deleting pod "simpletest.rc-mn9p7" in namespace "gc-8938"
  Aug 12 13:18:05.142: INFO: Deleting pod "simpletest.rc-mq7lw" in namespace "gc-8938"
  Aug 12 13:18:05.159: INFO: Deleting pod "simpletest.rc-mwq86" in namespace "gc-8938"
  E0812 13:18:05.169484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:05.180: INFO: Deleting pod "simpletest.rc-mx7nt" in namespace "gc-8938"
  Aug 12 13:18:05.201: INFO: Deleting pod "simpletest.rc-nq4b7" in namespace "gc-8938"
  Aug 12 13:18:05.222: INFO: Deleting pod "simpletest.rc-nt6pm" in namespace "gc-8938"
  Aug 12 13:18:05.245: INFO: Deleting pod "simpletest.rc-p54jd" in namespace "gc-8938"
  Aug 12 13:18:05.262: INFO: Deleting pod "simpletest.rc-pcsrm" in namespace "gc-8938"
  Aug 12 13:18:05.287: INFO: Deleting pod "simpletest.rc-pfn6v" in namespace "gc-8938"
  Aug 12 13:18:05.305: INFO: Deleting pod "simpletest.rc-pndt4" in namespace "gc-8938"
  Aug 12 13:18:05.332: INFO: Deleting pod "simpletest.rc-pvtbk" in namespace "gc-8938"
  Aug 12 13:18:05.349: INFO: Deleting pod "simpletest.rc-pzfh8" in namespace "gc-8938"
  Aug 12 13:18:05.367: INFO: Deleting pod "simpletest.rc-pzh7p" in namespace "gc-8938"
  Aug 12 13:18:05.387: INFO: Deleting pod "simpletest.rc-pzpxv" in namespace "gc-8938"
  Aug 12 13:18:05.427: INFO: Deleting pod "simpletest.rc-qflkh" in namespace "gc-8938"
  Aug 12 13:18:05.473: INFO: Deleting pod "simpletest.rc-qvlkn" in namespace "gc-8938"
  Aug 12 13:18:05.522: INFO: Deleting pod "simpletest.rc-r7p7s" in namespace "gc-8938"
  Aug 12 13:18:05.572: INFO: Deleting pod "simpletest.rc-rbkmn" in namespace "gc-8938"
  Aug 12 13:18:05.620: INFO: Deleting pod "simpletest.rc-rnwd9" in namespace "gc-8938"
  Aug 12 13:18:05.672: INFO: Deleting pod "simpletest.rc-rpsrj" in namespace "gc-8938"
  Aug 12 13:18:05.727: INFO: Deleting pod "simpletest.rc-s2npx" in namespace "gc-8938"
  Aug 12 13:18:05.774: INFO: Deleting pod "simpletest.rc-s7fc6" in namespace "gc-8938"
  Aug 12 13:18:05.821: INFO: Deleting pod "simpletest.rc-sp5nc" in namespace "gc-8938"
  Aug 12 13:18:05.875: INFO: Deleting pod "simpletest.rc-sw4bx" in namespace "gc-8938"
  Aug 12 13:18:05.925: INFO: Deleting pod "simpletest.rc-swknh" in namespace "gc-8938"
  Aug 12 13:18:05.974: INFO: Deleting pod "simpletest.rc-t8fpv" in namespace "gc-8938"
  Aug 12 13:18:06.023: INFO: Deleting pod "simpletest.rc-tkcn8" in namespace "gc-8938"
  Aug 12 13:18:06.070: INFO: Deleting pod "simpletest.rc-vflzs" in namespace "gc-8938"
  Aug 12 13:18:06.119: INFO: Deleting pod "simpletest.rc-vlrw9" in namespace "gc-8938"
  E0812 13:18:06.170119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:06.176: INFO: Deleting pod "simpletest.rc-vxfn8" in namespace "gc-8938"
  Aug 12 13:18:06.225: INFO: Deleting pod "simpletest.rc-w49nm" in namespace "gc-8938"
  Aug 12 13:18:06.277: INFO: Deleting pod "simpletest.rc-ww7tc" in namespace "gc-8938"
  Aug 12 13:18:06.321: INFO: Deleting pod "simpletest.rc-xmnrb" in namespace "gc-8938"
  Aug 12 13:18:06.371: INFO: Deleting pod "simpletest.rc-xmpfz" in namespace "gc-8938"
  Aug 12 13:18:06.421: INFO: Deleting pod "simpletest.rc-xwvdw" in namespace "gc-8938"
  Aug 12 13:18:06.478: INFO: Deleting pod "simpletest.rc-zcd5g" in namespace "gc-8938"
  Aug 12 13:18:06.520: INFO: Deleting pod "simpletest.rc-zk9rw" in namespace "gc-8938"
  Aug 12 13:18:06.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8938" for this suite. @ 08/12/23 13:18:06.612
• [43.786 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/12/23 13:18:06.667
  Aug 12 13:18:06.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename server-version @ 08/12/23 13:18:06.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:06.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:06.704
  STEP: Request ServerVersion @ 08/12/23 13:18:06.709
  STEP: Confirm major version @ 08/12/23 13:18:06.71
  Aug 12 13:18:06.710: INFO: Major version: 1
  STEP: Confirm minor version @ 08/12/23 13:18:06.71
  Aug 12 13:18:06.710: INFO: cleanMinorVersion: 27
  Aug 12 13:18:06.710: INFO: Minor version: 27
  Aug 12 13:18:06.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-1088" for this suite. @ 08/12/23 13:18:06.716
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/12/23 13:18:06.727
  Aug 12 13:18:06.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:18:06.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:06.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:06.751
  STEP: Setting up server cert @ 08/12/23 13:18:06.787
  E0812 13:18:07.176662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:18:07.918
  STEP: Deploying the webhook pod @ 08/12/23 13:18:07.938
  STEP: Wait for the deployment to be ready @ 08/12/23 13:18:07.954
  Aug 12 13:18:07.967: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:18:08.177514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:09.177784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:09.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:18:10.178055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:11.178371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:11.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:18:12.178826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:13.178989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:13.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:18:14.179204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:15.179465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:15.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:18:16.179843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:17.180103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:17.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 18, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:18:18.180647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:19.180701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:18:19.988
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:18:20
  E0812 13:18:20.181791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:21.002: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/12/23 13:18:21.007
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/12/23 13:18:21.032
  STEP: Creating a configMap that should not be mutated @ 08/12/23 13:18:21.042
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/12/23 13:18:21.056
  STEP: Creating a configMap that should be mutated @ 08/12/23 13:18:21.065
  Aug 12 13:18:21.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6837" for this suite. @ 08/12/23 13:18:21.169
  STEP: Destroying namespace "webhook-markers-4580" for this suite. @ 08/12/23 13:18:21.177
  E0812 13:18:21.182944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [14.459 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/12/23 13:18:21.187
  Aug 12 13:18:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 13:18:21.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:21.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:21.214
  STEP: Counting existing ResourceQuota @ 08/12/23 13:18:21.219
  E0812 13:18:22.183105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:23.183493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:24.183774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:25.183889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:26.184351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 13:18:26.226
  STEP: Ensuring resource quota status is calculated @ 08/12/23 13:18:26.234
  E0812 13:18:27.184738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:28.185145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:28.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-476" for this suite. @ 08/12/23 13:18:28.245
• [7.068 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/12/23 13:18:28.256
  Aug 12 13:18:28.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename job @ 08/12/23 13:18:28.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:28.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:28.281
  STEP: Creating a suspended job @ 08/12/23 13:18:28.294
  STEP: Patching the Job @ 08/12/23 13:18:28.303
  STEP: Watching for Job to be patched @ 08/12/23 13:18:28.327
  Aug 12 13:18:28.329: INFO: Event ADDED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 12 13:18:28.329: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 12 13:18:28.329: INFO: Event MODIFIED found for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/12/23 13:18:28.33
  STEP: Watching for Job to be updated @ 08/12/23 13:18:28.342
  Aug 12 13:18:28.344: INFO: Event MODIFIED found for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:28.344: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/12/23 13:18:28.344
  Aug 12 13:18:28.351: INFO: Job: e2e-km64r as labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched]
  STEP: Waiting for job to complete @ 08/12/23 13:18:28.351
  E0812 13:18:29.185778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:30.185934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:31.186142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:32.186418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:33.186516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:34.186651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:35.186748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:36.187430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:37.187516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:38.187843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 08/12/23 13:18:38.357
  STEP: Watching for Job to be deleted @ 08/12/23 13:18:38.368
  Aug 12 13:18:38.372: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.373: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.373: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.374: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.374: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.374: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.374: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.375: INFO: Event MODIFIED observed for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 12 13:18:38.375: INFO: Event DELETED found for Job e2e-km64r in namespace job-2757 with labels: map[e2e-job-label:e2e-km64r e2e-km64r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/12/23 13:18:38.375
  Aug 12 13:18:38.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2757" for this suite. @ 08/12/23 13:18:38.397
• [10.155 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/12/23 13:18:38.411
  Aug 12 13:18:38.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:18:38.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:38.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:38.436
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/12/23 13:18:38.441
  Aug 12 13:18:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:18:39.187888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:40.188455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:41.189153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:42.189927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:43.190938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:44.191151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:45.191723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/12/23 13:18:45.291
  Aug 12 13:18:45.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:18:46.192672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:46.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:18:47.193299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:48.194162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:49.194986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:50.195141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:51.196047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:52.196237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:52.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8238" for this suite. @ 08/12/23 13:18:52.746
• [14.345 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/12/23 13:18:52.757
  Aug 12 13:18:52.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:18:52.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:18:52.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:18:52.782
  STEP: Creating pod busybox-7849a920-d8f0-4550-ad2a-7a8f45664825 in namespace container-probe-6382 @ 08/12/23 13:18:52.786
  E0812 13:18:53.196384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:54.196458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:18:54.810: INFO: Started pod busybox-7849a920-d8f0-4550-ad2a-7a8f45664825 in namespace container-probe-6382
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:18:54.81
  Aug 12 13:18:54.814: INFO: Initial restart count of pod busybox-7849a920-d8f0-4550-ad2a-7a8f45664825 is 0
  E0812 13:18:55.196602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:56.197482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:57.198005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:58.198180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:18:59.198279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:00.198431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:01.198488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:02.199052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:03.199565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:04.199686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:05.200753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:06.201236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:07.201379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:08.201846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:09.202952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:10.203027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:11.203855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:12.204822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:13.205776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:14.205968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:15.206069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:16.206278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:17.206672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:18.206808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:19.206987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:20.208085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:21.208186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:22.209158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:23.209763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:24.213168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:25.213278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:26.214368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:27.215140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:28.215529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:29.216206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:30.216316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:31.216403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:32.216704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:33.217586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:34.217674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:35.218388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:36.219232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:37.219963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:38.220075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:39.220714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:40.220826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:41.221571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:42.221947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:43.222086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:44.222172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:19:44.965: INFO: Restart count of pod container-probe-6382/busybox-7849a920-d8f0-4550-ad2a-7a8f45664825 is now 1 (50.150994847s elapsed)
  Aug 12 13:19:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:19:44.97
  STEP: Destroying namespace "container-probe-6382" for this suite. @ 08/12/23 13:19:44.988
• [52.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/12/23 13:19:45.009
  Aug 12 13:19:45.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:19:45.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:19:45.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:19:45.035
  STEP: Creating projection with secret that has name projected-secret-test-map-88bc5bff-3844-4001-8c62-adeaa5dac795 @ 08/12/23 13:19:45.04
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:19:45.047
  E0812 13:19:45.222743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:46.222963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:47.223868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:48.224134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:19:49.074
  Aug 12 13:19:49.078: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-secrets-d41bc7af-cc98-4809-b7ed-4c1a014fc5d3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:19:49.101
  Aug 12 13:19:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7977" for this suite. @ 08/12/23 13:19:49.126
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/12/23 13:19:49.138
  Aug 12 13:19:49.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:19:49.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:19:49.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:19:49.161
  STEP: Creating a pod to test downward api env vars @ 08/12/23 13:19:49.165
  E0812 13:19:49.224232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:50.224385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:51.225346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:52.225478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:19:53.193
  Aug 12 13:19:53.199: INFO: Trying to get logs from node ip-172-31-32-142 pod downward-api-a09cd40f-12b0-4174-bd50-f0016685e8c1 container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 13:19:53.21
  E0812 13:19:53.226131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:19:53.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2941" for this suite. @ 08/12/23 13:19:53.235
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/12/23 13:19:53.248
  Aug 12 13:19:53.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:19:53.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:19:53.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:19:53.272
  STEP: Starting the proxy @ 08/12/23 13:19:53.277
  Aug 12 13:19:53.277: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2714 proxy --unix-socket=/tmp/kubectl-proxy-unix2214881998/test'
  STEP: retrieving proxy /api/ output @ 08/12/23 13:19:53.338
  Aug 12 13:19:53.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2714" for this suite. @ 08/12/23 13:19:53.345
• [0.106 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/12/23 13:19:53.355
  Aug 12 13:19:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:19:53.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:19:53.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:19:53.38
  STEP: Creating pod liveness-042b16dd-5dab-471f-87eb-1ea75458cbaf in namespace container-probe-4728 @ 08/12/23 13:19:53.384
  E0812 13:19:54.226846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:55.226936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:19:55.414: INFO: Started pod liveness-042b16dd-5dab-471f-87eb-1ea75458cbaf in namespace container-probe-4728
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:19:55.414
  Aug 12 13:19:55.419: INFO: Initial restart count of pod liveness-042b16dd-5dab-471f-87eb-1ea75458cbaf is 0
  E0812 13:19:56.227203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:57.227312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:58.227456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:19:59.227765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:00.227879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:01.228548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:02.228794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:03.228935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:04.229117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:05.229369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:06.230444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:07.230518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:08.231104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:09.231480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:10.231741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:11.232277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:12.233268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:13.233378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:14.233523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:15.233574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:16.234365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:17.234621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:18.235567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:19.235703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:20.235836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:21.236560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:22.237488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:23.237973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:24.237989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:25.238134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:26.239109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:27.239240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:28.239369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:29.239846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:30.239974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:31.241009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:32.241929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:33.242922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:34.243011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:35.243143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:36.244011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:37.244136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:38.244272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:39.244586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:40.244756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:41.245126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:42.245265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:43.245390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:44.245770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:45.245883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:46.246424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:47.246773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:48.247849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:49.248183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:50.248692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:51.249766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:52.249916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:53.250594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:54.250593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:55.250715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:56.251059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:57.251273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:58.251395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:20:59.251675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:00.251741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:01.252295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:02.252378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:03.252714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:04.253571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:05.253701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:06.254681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:07.254955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:08.255113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:09.255250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:10.256244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:11.256478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:12.256656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:13.256686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:14.257744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:15.257865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:16.257931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:17.258187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:18.258307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:19.258541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:20.259068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:21.259343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:22.260040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:23.260307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:24.260373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:25.260674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:26.261428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:27.261903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:28.262027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:29.262310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:30.263178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:31.264096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:32.264349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:33.264619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:34.264677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:35.265751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:36.265874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:37.266765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:38.266973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:39.267271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:40.268111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:41.268606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:42.268674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:43.269751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:44.270104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:45.270278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:46.271370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:47.272349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:48.272435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:49.272665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:50.273762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:51.274311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:52.274444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:53.274711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:54.274960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:55.275199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:56.275285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:57.275671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:58.275918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:21:59.276055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:00.276897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:01.277820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:02.278667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:03.278967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:04.279091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:05.279217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:06.280147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:07.280763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:08.281710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:09.281845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:10.281971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:11.282597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:12.283184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:13.283307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:14.284173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:15.284418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:16.284617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:17.284702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:18.284924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:19.285000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:20.285355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:21.285484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:22.286375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:23.287270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:24.287923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:25.288056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:26.288168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:27.288829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:28.289354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:29.289479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:30.290500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:31.291339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:32.292278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:33.292686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:34.292965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:35.293754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:36.294671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:37.294905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:38.295896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:39.296320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:40.296881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:41.297390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:42.297750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:43.298068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:44.298320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:45.298633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:46.299127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:47.299394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:48.299500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:49.299939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:50.300252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:51.301279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:52.301461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:53.301827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:54.302705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:55.302820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:56.303475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:57.303612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:58.304418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:22:59.304660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:00.305428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:01.306399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:02.307207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:03.307598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:04.308662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:05.308695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:06.309738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:07.310380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:08.310579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:09.310846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:10.311168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:11.312235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:12.312333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:13.312441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:14.313493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:15.314593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:16.315175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:17.315968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:18.316104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:19.316333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:20.317404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:21.317749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:22.317863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:23.318630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:24.318753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:25.319001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:26.319967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:27.320133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:28.320546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:29.320676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:30.320861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:31.321015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:32.321128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:33.321847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:34.322341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:35.323095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:36.323979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:37.324180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:38.324380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:39.324674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:40.325148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:41.325271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:42.325449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:43.325686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:44.326199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:45.326303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:46.326788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:47.326923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:48.327823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:49.327951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:50.328003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:51.328417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:52.329360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:53.329684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:54.329815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:55.330100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:23:56.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:23:56.154
  STEP: Destroying namespace "container-probe-4728" for this suite. @ 08/12/23 13:23:56.174
• [242.829 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/12/23 13:23:56.194
  Aug 12 13:23:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subpath @ 08/12/23 13:23:56.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:23:56.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:23:56.221
  STEP: Setting up data @ 08/12/23 13:23:56.225
  STEP: Creating pod pod-subpath-test-downwardapi-lhc8 @ 08/12/23 13:23:56.237
  STEP: Creating a pod to test atomic-volume-subpath @ 08/12/23 13:23:56.238
  E0812 13:23:56.331079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:57.331893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:58.332971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:23:59.333135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:00.334131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:01.334262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:02.335114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:03.335313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:04.335743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:05.335880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:06.335976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:07.336292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:08.337136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:09.337261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:10.338348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:11.338534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:12.339389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:13.339503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:14.340090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:15.340282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:16.341022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:17.341153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:18.342114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:19.342250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:24:20.329
  Aug 12 13:24:20.334: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-subpath-test-downwardapi-lhc8 container test-container-subpath-downwardapi-lhc8: <nil>
  E0812 13:24:20.343085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 08/12/23 13:24:20.362
  STEP: Deleting pod pod-subpath-test-downwardapi-lhc8 @ 08/12/23 13:24:20.382
  Aug 12 13:24:20.382: INFO: Deleting pod "pod-subpath-test-downwardapi-lhc8" in namespace "subpath-6427"
  Aug 12 13:24:20.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6427" for this suite. @ 08/12/23 13:24:20.394
• [24.221 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/12/23 13:24:20.415
  Aug 12 13:24:20.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:24:20.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:20.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:20.49
  STEP: Setting up server cert @ 08/12/23 13:24:20.519
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:24:21.21
  STEP: Deploying the webhook pod @ 08/12/23 13:24:21.222
  STEP: Wait for the deployment to be ready @ 08/12/23 13:24:21.24
  Aug 12 13:24:21.252: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:24:21.344089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:22.344702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:24:23.268
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:24:23.291
  E0812 13:24:23.345951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:24.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0812 13:24:24.346076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Listing all of the created validation webhooks @ 08/12/23 13:24:24.38
  STEP: Creating a configMap that should be mutated @ 08/12/23 13:24:24.398
  STEP: Deleting the collection of validation webhooks @ 08/12/23 13:24:24.447
  STEP: Creating a configMap that should not be mutated @ 08/12/23 13:24:24.519
  Aug 12 13:24:24.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9330" for this suite. @ 08/12/23 13:24:24.6
  STEP: Destroying namespace "webhook-markers-1051" for this suite. @ 08/12/23 13:24:24.611
• [4.205 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/12/23 13:24:24.62
  Aug 12 13:24:24.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:24:24.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:24.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:24.642
  STEP: Creating configMap with name configmap-test-volume-ef110578-8432-459a-9f00-0f54256daecb @ 08/12/23 13:24:24.648
  STEP: Creating a pod to test consume configMaps @ 08/12/23 13:24:24.654
  E0812 13:24:25.346254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:26.347338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:27.348023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:28.348106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:24:28.68
  Aug 12 13:24:28.685: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-79906447-7c7b-452c-b72d-28fdbf1a0f78 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 13:24:28.696
  Aug 12 13:24:28.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1181" for this suite. @ 08/12/23 13:24:28.725
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/12/23 13:24:28.74
  Aug 12 13:24:28.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 13:24:28.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:28.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:28.767
  Aug 12 13:24:28.789: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0812 13:24:29.348361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:30.348761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:31.349788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:32.349889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:33.350222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:33.795: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 13:24:33.795
  Aug 12 13:24:33.796: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0812 13:24:34.350533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:35.350892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:35.804: INFO: Creating deployment "test-rollover-deployment"
  Aug 12 13:24:35.820: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0812 13:24:36.351954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:37.352007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:37.832: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug 12 13:24:37.843: INFO: Ensure that both replica sets have 1 created replica
  Aug 12 13:24:37.852: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug 12 13:24:37.864: INFO: Updating deployment test-rollover-deployment
  Aug 12 13:24:37.864: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0812 13:24:38.352180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:39.352580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:39.875: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug 12 13:24:39.886: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug 12 13:24:39.895: INFO: all replica sets need to contain the pod-template-hash label
  Aug 12 13:24:39.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:24:40.353302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:41.353415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:41.906: INFO: all replica sets need to contain the pod-template-hash label
  Aug 12 13:24:41.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:24:42.354025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:43.354102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:43.906: INFO: all replica sets need to contain the pod-template-hash label
  Aug 12 13:24:43.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:24:44.354965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:45.355516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:45.905: INFO: all replica sets need to contain the pod-template-hash label
  Aug 12 13:24:45.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:24:46.356262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:47.356593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:47.906: INFO: all replica sets need to contain the pod-template-hash label
  Aug 12 13:24:47.906: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 24, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:24:48.356647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:49.357417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:49.909: INFO: 
  Aug 12 13:24:49.909: INFO: Ensure that both old replica sets have no replicas
  Aug 12 13:24:49.924: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7494  f135ec5f-6e82-4cac-9875-0fa3f8e31f51 34193 2 2023-08-12 13:24:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-12 13:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c61aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-12 13:24:35 +0000 UTC,LastTransitionTime:2023-08-12 13:24:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-12 13:24:49 +0000 UTC,LastTransitionTime:2023-08-12 13:24:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 12 13:24:49.929: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-7494  fd96d740-42a0-42bc-a1ff-148062b17384 34182 2 2023-08-12 13:24:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f135ec5f-6e82-4cac-9875-0fa3f8e31f51 0xc0044d40f7 0xc0044d40f8}] [] [{kube-controller-manager Update apps/v1 2023-08-12 13:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f135ec5f-6e82-4cac-9875-0fa3f8e31f51\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:24:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044d41a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:24:49.929: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug 12 13:24:49.929: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7494  5918fdd6-c161-4924-8704-f227b5904d98 34192 2 2023-08-12 13:24:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f135ec5f-6e82-4cac-9875-0fa3f8e31f51 0xc0041f3fb7 0xc0041f3fb8}] [] [{e2e.test Update apps/v1 2023-08-12 13:24:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:24:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f135ec5f-6e82-4cac-9875-0fa3f8e31f51\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:24:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044d4078 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:24:49.929: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-7494  a3869650-55b0-442d-a665-a8c48777272f 34142 2 2023-08-12 13:24:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f135ec5f-6e82-4cac-9875-0fa3f8e31f51 0xc0044d4217 0xc0044d4218}] [] [{kube-controller-manager Update apps/v1 2023-08-12 13:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f135ec5f-6e82-4cac-9875-0fa3f8e31f51\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:24:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044d42c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:24:49.934: INFO: Pod "test-rollover-deployment-57777854c9-78722" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-78722 test-rollover-deployment-57777854c9- deployment-7494  a8356534-84b4-4ef0-a7dd-07601e1b307c 34158 0 2023-08-12 13:24:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 fd96d740-42a0-42bc-a1ff-148062b17384 0xc0044d4817 0xc0044d4818}] [] [{kube-controller-manager Update v1 2023-08-12 13:24:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd96d740-42a0-42bc-a1ff-148062b17384\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 13:24:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4r5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4r5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:24:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:24:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:24:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:24:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.144,StartTime:2023-08-12 13:24:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 13:24:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://15cd61ee789152786a1ca2c8141024ae10e457d18597c5917283bab2e11f351b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.144,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 13:24:49.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7494" for this suite. @ 08/12/23 13:24:49.939
• [21.209 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 08/12/23 13:24:49.949
  Aug 12 13:24:49.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 13:24:49.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:49.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:49.978
  Aug 12 13:24:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:24:50.358310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:51.358594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:52.358928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:24:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6396" for this suite. @ 08/12/23 13:24:53.124
• [3.183 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/12/23 13:24:53.134
  Aug 12 13:24:53.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:24:53.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:53.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:53.159
  STEP: Creating configMap with name configmap-test-volume-0349d411-13ef-4c93-9bc3-45ee3f86650d @ 08/12/23 13:24:53.163
  STEP: Creating a pod to test consume configMaps @ 08/12/23 13:24:53.169
  E0812 13:24:53.359643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:54.360113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:55.360901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:56.361567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:24:57.218
  Aug 12 13:24:57.222: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-053df729-9e05-4c2a-9721-b51b57b11344 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 13:24:57.233
  Aug 12 13:24:57.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5373" for this suite. @ 08/12/23 13:24:57.277
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/12/23 13:24:57.293
  Aug 12 13:24:57.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:24:57.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:24:57.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:24:57.316
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7842 @ 08/12/23 13:24:57.32
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/12/23 13:24:57.341
  STEP: creating service externalsvc in namespace services-7842 @ 08/12/23 13:24:57.341
  STEP: creating replication controller externalsvc in namespace services-7842 @ 08/12/23 13:24:57.359
  E0812 13:24:57.361948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:24:57.368368      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7842, replica count: 2
  E0812 13:24:58.363037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:24:59.363685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:00.364400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:25:00.420007      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/12/23 13:25:00.425
  Aug 12 13:25:00.447: INFO: Creating new exec pod
  E0812 13:25:01.365099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:02.365221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:25:02.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-7842 exec execpodg9jks -- /bin/sh -x -c nslookup clusterip-service.services-7842.svc.cluster.local'
  Aug 12 13:25:02.704: INFO: stderr: "+ nslookup clusterip-service.services-7842.svc.cluster.local\n"
  Aug 12 13:25:02.704: INFO: stdout: "Server:\t\t10.152.183.221\nAddress:\t10.152.183.221#53\n\nclusterip-service.services-7842.svc.cluster.local\tcanonical name = externalsvc.services-7842.svc.cluster.local.\nName:\texternalsvc.services-7842.svc.cluster.local\nAddress: 10.152.183.121\n\n"
  Aug 12 13:25:02.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7842, will wait for the garbage collector to delete the pods @ 08/12/23 13:25:02.711
  Aug 12 13:25:02.776: INFO: Deleting ReplicationController externalsvc took: 9.743843ms
  Aug 12 13:25:02.877: INFO: Terminating ReplicationController externalsvc pods took: 100.696651ms
  E0812 13:25:03.365627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:04.366326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:25:05.011: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7842" for this suite. @ 08/12/23 13:25:05.038
• [7.760 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/12/23 13:25:05.06
  Aug 12 13:25:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:25:05.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:25:05.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:25:05.101
  STEP: Creating pod test-webserver-30486072-18e2-4d1a-b9de-447ff5c49b52 in namespace container-probe-1186 @ 08/12/23 13:25:05.106
  E0812 13:25:05.367101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:06.367539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:25:07.135: INFO: Started pod test-webserver-30486072-18e2-4d1a-b9de-447ff5c49b52 in namespace container-probe-1186
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:25:07.135
  Aug 12 13:25:07.139: INFO: Initial restart count of pod test-webserver-30486072-18e2-4d1a-b9de-447ff5c49b52 is 0
  E0812 13:25:07.368641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:08.368747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:09.368935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:10.369051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:11.370019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:12.370155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:13.370219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:14.370311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:15.370499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:16.371214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:17.371342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:18.371771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:19.372873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:20.373744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:21.374341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:22.374475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:23.374569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:24.375143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:25.375468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:26.376293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:27.377378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:28.377482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:29.378077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:30.378833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:31.378906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:32.379292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:33.379738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:34.379875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:35.379992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:36.380490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:37.380552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:38.380763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:39.381731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:40.382104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:41.382778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:42.382948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:43.383986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:44.384096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:45.384944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:46.385439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:47.385783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:48.386217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:49.386881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:50.387031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:51.387512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:52.387835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:53.388237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:54.388486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:55.389611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:56.390076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:57.390400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:58.390523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:25:59.390767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:00.391216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:01.391793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:02.392967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:03.393256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:04.393394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:05.393502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:06.393972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:07.395026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:08.395799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:09.396368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:10.396582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:11.397135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:12.397316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:13.397517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:14.397999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:15.398452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:16.398830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:17.399890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:18.400225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:19.401198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:20.401321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:21.401559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:22.401697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:23.402540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:24.402695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:25.403568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:26.404256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:27.404616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:28.404769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:29.405185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:30.405358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:31.405954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:32.406324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:33.406761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:34.407593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:35.408032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:36.408548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:37.408691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:38.408844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:39.408924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:40.409043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:41.409499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:42.409629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:43.409754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:44.409884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:45.410005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:46.410167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:47.410506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:48.410670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:49.411121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:50.411203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:51.411650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:52.411983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:53.412531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:54.412699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:55.412802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:56.412927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:57.413228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:58.413557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:26:59.413837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:00.413964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:01.414381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:02.414507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:03.414649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:04.415121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:05.415375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:06.415510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:07.415632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:08.415754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:09.415891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:10.416562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:11.416666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:12.416789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:13.417763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:14.417875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:15.418120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:16.418368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:17.418502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:18.419207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:19.419357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:20.419460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:21.419830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:22.420492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:23.420728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:24.420912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:25.421756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:26.422183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:27.423054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:28.423193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:29.423573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:30.424113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:31.424629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:32.424673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:33.424754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:34.425742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:35.426007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:36.426505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:37.426743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:38.426877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:39.427266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:40.427379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:41.427714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:42.428053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:43.428881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:44.429749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:45.429868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:46.430543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:47.430806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:48.430874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:49.430994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:50.431839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:51.432202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:52.432659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:53.432918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:54.434003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:55.434125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:56.434615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:57.434951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:58.435081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:27:59.435240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:00.435362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:01.435679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:02.436764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:03.437750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:04.438584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:05.439040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:06.439967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:07.440091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:08.440221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:09.440916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:10.441028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:11.441753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:12.442819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:13.442956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:14.443708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:15.443864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:16.444838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:17.445747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:18.445875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:19.445972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:20.446092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:21.446229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:22.446586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:23.447031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:24.447493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:25.447777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:26.447904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:27.448196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:28.448294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:29.448608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:30.449629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:31.450293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:32.450416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:33.450688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:34.450846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:35.450953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:36.451130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:37.451315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:38.451786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:39.451953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:40.452110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:41.452759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:42.453346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:43.453524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:44.453779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:45.454498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:46.454797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:47.455187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:48.456206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:49.456342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:50.456769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:51.457494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:52.457623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:53.458133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:54.457883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:55.457983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:56.458790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:57.459699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:58.460148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:28:59.460784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:00.460900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:01.461068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:02.461323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:03.461900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:04.462659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:05.462905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:06.463057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:07.463336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:29:07.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:29:07.879
  STEP: Destroying namespace "container-probe-1186" for this suite. @ 08/12/23 13:29:07.896
• [242.846 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/12/23 13:29:07.91
  Aug 12 13:29:07.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 13:29:07.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:29:07.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:29:07.986
  STEP: creating the pod @ 08/12/23 13:29:07.992
  STEP: submitting the pod to kubernetes @ 08/12/23 13:29:07.992
  W0812 13:29:08.004978      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0812 13:29:08.463532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:09.463785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/12/23 13:29:10.021
  STEP: updating the pod @ 08/12/23 13:29:10.026
  E0812 13:29:10.463842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:29:10.545: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d8a36b09-3bfb-4bc4-b38a-c35c423466b5"
  E0812 13:29:11.464797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:12.464911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:13.465246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:14.465913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:15.466052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:16.466841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:29:16.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5205" for this suite. @ 08/12/23 13:29:16.574
• [8.673 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/12/23 13:29:16.587
  Aug 12 13:29:16.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubelet-test @ 08/12/23 13:29:16.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:29:16.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:29:16.611
  E0812 13:29:17.466946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:18.467419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:29:18.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3095" for this suite. @ 08/12/23 13:29:18.67
• [2.093 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/12/23 13:29:18.679
  Aug 12 13:29:18.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 13:29:18.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:29:18.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:29:18.711
  Aug 12 13:29:18.715: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0812 13:29:19.468259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/12/23 13:29:19.734
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/12/23 13:29:19.742
  E0812 13:29:20.468889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/12/23 13:29:20.755
  Aug 12 13:29:20.767: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/12/23 13:29:20.767
  Aug 12 13:29:20.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-624" for this suite. @ 08/12/23 13:29:20.78
• [2.111 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/12/23 13:29:20.791
  Aug 12 13:29:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 13:29:20.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:29:20.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:29:20.816
  Aug 12 13:29:20.820: INFO: Creating simple deployment test-new-deployment
  Aug 12 13:29:20.842: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0812 13:29:21.469064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:22.471073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 08/12/23 13:29:22.861
  STEP: updating a scale subresource @ 08/12/23 13:29:22.867
  STEP: verifying the deployment Spec.Replicas was modified @ 08/12/23 13:29:22.874
  STEP: Patch a scale subresource @ 08/12/23 13:29:22.878
  Aug 12 13:29:22.913: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-5849  3ba51ed7-8682-4ee9-9040-ef44c6f6ae3f 35107 3 2023-08-12 13:29:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-12 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004af00a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-12 13:29:22 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-12 13:29:22 +0000 UTC,LastTransitionTime:2023-08-12 13:29:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 12 13:29:22.919: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-5849  52c04885-aa32-4637-b86a-298654cab833 35113 2 2023-08-12 13:29:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3ba51ed7-8682-4ee9-9040-ef44c6f6ae3f 0xc004af04a7 0xc004af04a8}] [] [{kube-controller-manager Update apps/v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ba51ed7-8682-4ee9-9040-ef44c6f6ae3f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004af0538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:29:22.925: INFO: Pod "test-new-deployment-67bd4bf6dc-j7kll" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-j7kll test-new-deployment-67bd4bf6dc- deployment-5849  46fb5e42-67f3-4b81-bf46-f6c247a7ec82 35115 0 2023-08-12 13:29:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 52c04885-aa32-4637-b86a-298654cab833 0xc004fca9c7 0xc004fca9c8}] [] [{kube-controller-manager Update v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52c04885-aa32-4637-b86a-298654cab833\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cstgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cstgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-84-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.84.203,PodIP:,StartTime:2023-08-12 13:29:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 13:29:22.925: INFO: Pod "test-new-deployment-67bd4bf6dc-mlr28" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-mlr28 test-new-deployment-67bd4bf6dc- deployment-5849  0f96224b-5390-4459-965d-3f3fec1c91a7 35101 0 2023-08-12 13:29:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 52c04885-aa32-4637-b86a-298654cab833 0xc004fcab97 0xc004fcab98}] [] [{kube-controller-manager Update v1 2023-08-12 13:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52c04885-aa32-4637-b86a-298654cab833\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 13:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z45c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z45c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:29:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.152,StartTime:2023-08-12 13:29:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 13:29:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7360faf41af04544cebda8676300030d7804625031f5e6646894b55587178f22,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.152,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 13:29:22.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5849" for this suite. @ 08/12/23 13:29:22.934
• [2.159 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/12/23 13:29:22.95
  Aug 12 13:29:22.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename init-container @ 08/12/23 13:29:22.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:29:22.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:29:22.988
  STEP: creating the pod @ 08/12/23 13:29:22.993
  Aug 12 13:29:22.993: INFO: PodSpec: initContainers in spec.initContainers
  E0812 13:29:23.471813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:24.471871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:25.471970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:26.472778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:27.472863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:28.473015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:29.473172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:30.474104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:31.474200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:32.474508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:33.474638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:34.475194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:35.475325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:36.475455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:37.475585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:38.475967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:39.476348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:40.476732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:41.476952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:42.477762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:43.477887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:44.478195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:45.478515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:46.478737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:47.479147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:48.479359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:49.479671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:50.479786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:51.480762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:52.480924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:53.481392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:54.481694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:55.481773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:56.482500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:57.482656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:58.483348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:29:59.483508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:00.483769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:01.484208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:02.484539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:03.484727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:04.485757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:30:05.368: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-678896f7-abc1-476c-9819-57de2008d295", GenerateName:"", Namespace:"init-container-5208", SelfLink:"", UID:"06e20509-462a-4d7f-8136-548470354bc6", ResourceVersion:"35329", Generation:0, CreationTimestamp:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"993858027"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 12, 13, 29, 22, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004900558), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 12, 13, 30, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0049005a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ddb2w", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0037872e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ddb2w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ddb2w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ddb2w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0029b68d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-32-142", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000bacf50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0029b6b20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0029b6b40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0029b6b48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0029b6b4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00137c1c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.32.142", PodIP:"192.168.87.157", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.87.157"}}, StartTime:time.Date(2023, time.August, 12, 13, 29, 23, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bad030)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000bad0a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a388ed1505f976c7e683cfb7e055bfa571a8a6c49a0f8ab1c5ac71eda846cc9c", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003787360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003787340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0029b6d64), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug 12 13:30:05.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5208" for this suite. @ 08/12/23 13:30:05.375
• [42.435 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/12/23 13:30:05.385
  Aug 12 13:30:05.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename cronjob @ 08/12/23 13:30:05.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:30:05.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:30:05.415
  STEP: Creating a ReplaceConcurrent cronjob @ 08/12/23 13:30:05.419
  STEP: Ensuring a job is scheduled @ 08/12/23 13:30:05.427
  E0812 13:30:05.486230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:06.486695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:07.486806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:08.486951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:09.487036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:10.487121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:11.487850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:12.487980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:13.488956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:14.489072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:15.489156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:16.489834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:17.490282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:18.490478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:19.490571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:20.491065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:21.491798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:22.491845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:23.492925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:24.493313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:25.494002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:26.494595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:27.495078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:28.495280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:29.495519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:30.495626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:31.496243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:32.496420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:33.497260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:34.497384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:35.497846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:36.498439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:37.498459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:38.498782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:39.498828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:40.499719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:41.499794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:42.499950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:43.500739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:44.500848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:45.501215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:46.501762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:47.502815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:48.503065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:49.503433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:50.503582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:51.504330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:52.504469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:53.505098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:54.505170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:55.505387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:56.505858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:57.506739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:58.506854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:30:59.507481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:00.507623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/12/23 13:31:01.434
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/12/23 13:31:01.438
  STEP: Ensuring the job is replaced with a new one @ 08/12/23 13:31:01.443
  E0812 13:31:01.508437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:02.508723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:03.509281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:04.509416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:05.510249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:06.510845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:07.510918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:08.511048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:09.511892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:10.512036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:11.512823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:12.513800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:13.514882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:14.515025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:15.515729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:16.516554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:17.516995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:18.517255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:19.518014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:20.518576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:21.518956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:22.519089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:23.520239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:24.520334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:25.520705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:26.521443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:27.521716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:28.522292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:29.522677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:30.522916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:31.523599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:32.523815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:33.524188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:34.524354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:35.525278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:36.525798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:37.526888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:38.527180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:39.527660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:40.527935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:41.528276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:42.528701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:43.528928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:44.529051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:45.529297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:46.529873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:47.530625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:48.530932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:49.531014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:50.531170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:51.531412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:52.531837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:53.532581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:54.532687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:55.532980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:56.533469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:57.533521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:58.533685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:31:59.534256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:00.534449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/12/23 13:32:01.448
  Aug 12 13:32:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5864" for this suite. @ 08/12/23 13:32:01.464
• [116.093 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 08/12/23 13:32:01.479
  Aug 12 13:32:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 13:32:01.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:01.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:01.523
  E0812 13:32:01.534792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/12/23 13:32:01.555
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 13:32:01.562
  Aug 12 13:32:01.567: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:01.567: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:01.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:32:01.572: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:32:02.535214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:02.580: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:02.580: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:02.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:32:02.585: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:32:03.535569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:03.580: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:03.581: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:03.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 13:32:03.585: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  E0812 13:32:04.536673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:04.578: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:04.578: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:04.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 13:32:04.583: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/12/23 13:32:04.587
  Aug 12 13:32:04.608: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:04.608: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:32:04.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 13:32:04.619: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/12/23 13:32:04.619
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 13:32:04.637
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5396, will wait for the garbage collector to delete the pods @ 08/12/23 13:32:04.637
  Aug 12 13:32:04.704: INFO: Deleting DaemonSet.extensions daemon-set took: 10.199789ms
  Aug 12 13:32:04.804: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.400996ms
  E0812 13:32:05.537261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:06.537369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:07.538083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:07.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:32:07.810: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 13:32:07.815: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35750"},"items":null}

  Aug 12 13:32:07.820: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35750"},"items":null}

  Aug 12 13:32:07.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5396" for this suite. @ 08/12/23 13:32:07.844
• [6.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/12/23 13:32:07.855
  Aug 12 13:32:07.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 13:32:07.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:07.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:07.879
  STEP: Counting existing ResourceQuota @ 08/12/23 13:32:07.884
  E0812 13:32:08.538908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:09.539009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:10.540083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:11.540383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:12.540745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 13:32:12.89
  STEP: Ensuring resource quota status is calculated @ 08/12/23 13:32:12.897
  E0812 13:32:13.540862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:14.540988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 08/12/23 13:32:14.904
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/12/23 13:32:14.935
  E0812 13:32:15.541534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:16.542126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/12/23 13:32:16.941
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/12/23 13:32:16.944
  STEP: Ensuring a pod cannot update its resource requirements @ 08/12/23 13:32:16.947
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/12/23 13:32:16.953
  E0812 13:32:17.542235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:18.542612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/12/23 13:32:18.959
  STEP: Ensuring resource quota status released the pod usage @ 08/12/23 13:32:18.977
  E0812 13:32:19.543600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:20.544224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:20.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3835" for this suite. @ 08/12/23 13:32:20.988
• [13.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/12/23 13:32:21
  Aug 12 13:32:21.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:32:21.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:21.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:21.025
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/12/23 13:32:21.029
  E0812 13:32:21.544698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:22.545747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:23.545899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:24.546955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:32:25.064
  Aug 12 13:32:25.068: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-e19176de-33e5-4cd1-8d04-fbebd601ee9e container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:32:25.096
  Aug 12 13:32:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-951" for this suite. @ 08/12/23 13:32:25.124
• [4.137 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/12/23 13:32:25.139
  Aug 12 13:32:25.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename events @ 08/12/23 13:32:25.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:25.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:25.162
  STEP: creating a test event @ 08/12/23 13:32:25.166
  STEP: listing events in all namespaces @ 08/12/23 13:32:25.176
  STEP: listing events in test namespace @ 08/12/23 13:32:25.181
  STEP: listing events with field selection filtering on source @ 08/12/23 13:32:25.185
  STEP: listing events with field selection filtering on reportingController @ 08/12/23 13:32:25.189
  STEP: getting the test event @ 08/12/23 13:32:25.194
  STEP: patching the test event @ 08/12/23 13:32:25.197
  STEP: getting the test event @ 08/12/23 13:32:25.21
  STEP: updating the test event @ 08/12/23 13:32:25.214
  STEP: getting the test event @ 08/12/23 13:32:25.224
  STEP: deleting the test event @ 08/12/23 13:32:25.228
  STEP: listing events in all namespaces @ 08/12/23 13:32:25.24
  STEP: listing events in test namespace @ 08/12/23 13:32:25.245
  Aug 12 13:32:25.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8090" for this suite. @ 08/12/23 13:32:25.255
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/12/23 13:32:25.268
  Aug 12 13:32:25.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 13:32:25.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:25.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:25.293
  Aug 12 13:32:25.297: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug 12 13:32:25.309: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0812 13:32:25.547587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:26.548222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:27.548497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:28.548678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:29.548795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:30.316: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 13:32:30.316
  Aug 12 13:32:30.316: INFO: Creating deployment "test-rolling-update-deployment"
  Aug 12 13:32:30.324: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug 12 13:32:30.339: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0812 13:32:30.549313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:31.549367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:32.350: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug 12 13:32:32.355: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug 12 13:32:32.369: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3143  09ebb640-efaa-47f5-9250-94e6ca29c16e 35967 1 2023-08-12 13:32:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-12 13:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004399878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-12 13:32:30 +0000 UTC,LastTransitionTime:2023-08-12 13:32:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-12 13:32:31 +0000 UTC,LastTransitionTime:2023-08-12 13:32:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 12 13:32:32.374: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3143  4cf91367-e3f2-46b5-b981-accb4f4a4d5d 35954 1 2023-08-12 13:32:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 09ebb640-efaa-47f5-9250-94e6ca29c16e 0xc004399d67 0xc004399d68}] [] [{kube-controller-manager Update apps/v1 2023-08-12 13:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09ebb640-efaa-47f5-9250-94e6ca29c16e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:32:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004399e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:32:32.374: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug 12 13:32:32.374: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3143  d9101b1f-96fd-448e-a639-69dc312bdf72 35966 2 2023-08-12 13:32:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 09ebb640-efaa-47f5-9250-94e6ca29c16e 0xc004399c37 0xc004399c38}] [] [{e2e.test Update apps/v1 2023-08-12 13:32:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09ebb640-efaa-47f5-9250-94e6ca29c16e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:32:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004399cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:32:32.380: INFO: Pod "test-rolling-update-deployment-656d657cd8-7gczq" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-7gczq test-rolling-update-deployment-656d657cd8- deployment-3143  9f50d451-3016-47ad-9d35-9654498f598f 35953 0 2023-08-12 13:32:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 4cf91367-e3f2-46b5-b981-accb4f4a4d5d 0xc004875467 0xc004875468}] [] [{kube-controller-manager Update v1 2023-08-12 13:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4cf91367-e3f2-46b5-b981-accb4f4a4d5d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 13:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2knb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2knb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:32:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.161,StartTime:2023-08-12 13:32:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 13:32:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://eb7c340fde36f653876dbbbe9a99261d4be50acd89d9a7e22da8deb6c5d06aed,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.161,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 13:32:32.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3143" for this suite. @ 08/12/23 13:32:32.388
• [7.130 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/12/23 13:32:32.399
  Aug 12 13:32:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:32:32.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:32.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:32.424
  STEP: Setting up server cert @ 08/12/23 13:32:32.453
  E0812 13:32:32.550175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:32:32.885
  STEP: Deploying the webhook pod @ 08/12/23 13:32:32.898
  STEP: Wait for the deployment to be ready @ 08/12/23 13:32:32.914
  Aug 12 13:32:32.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:32:33.550295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:34.550476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:32:34.942
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:32:34.96
  E0812 13:32:35.551477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:35.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/12/23 13:32:35.968
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/12/23 13:32:35.97
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/12/23 13:32:35.97
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/12/23 13:32:35.97
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/12/23 13:32:35.971
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/12/23 13:32:35.971
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/12/23 13:32:35.973
  Aug 12 13:32:35.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8954" for this suite. @ 08/12/23 13:32:36.039
  STEP: Destroying namespace "webhook-markers-4809" for this suite. @ 08/12/23 13:32:36.049
• [3.661 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/12/23 13:32:36.061
  Aug 12 13:32:36.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 13:32:36.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:36.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:36.087
  STEP: Updating Namespace "namespaces-5360" @ 08/12/23 13:32:36.091
  Aug 12 13:32:36.105: INFO: Namespace "namespaces-5360" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"6291cefb-2518-4696-9a5a-b192283a03a4", "kubernetes.io/metadata.name":"namespaces-5360", "namespaces-5360":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug 12 13:32:36.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5360" for this suite. @ 08/12/23 13:32:36.115
• [0.068 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/12/23 13:32:36.131
  Aug 12 13:32:36.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:32:36.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:36.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:36.156
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-2343 @ 08/12/23 13:32:36.166
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/12/23 13:32:36.188
  STEP: creating service externalsvc in namespace services-2343 @ 08/12/23 13:32:36.188
  STEP: creating replication controller externalsvc in namespace services-2343 @ 08/12/23 13:32:36.204
  I0812 13:32:36.213652      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2343, replica count: 2
  E0812 13:32:36.551686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:37.551826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:38.552666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:32:39.264182      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/12/23 13:32:39.27
  Aug 12 13:32:39.297: INFO: Creating new exec pod
  E0812 13:32:39.553062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:40.553234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:41.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-2343 exec execpod2mtvt -- /bin/sh -x -c nslookup nodeport-service.services-2343.svc.cluster.local'
  Aug 12 13:32:41.509: INFO: stderr: "+ nslookup nodeport-service.services-2343.svc.cluster.local\n"
  Aug 12 13:32:41.510: INFO: stdout: "Server:\t\t10.152.183.221\nAddress:\t10.152.183.221#53\n\nnodeport-service.services-2343.svc.cluster.local\tcanonical name = externalsvc.services-2343.svc.cluster.local.\nName:\texternalsvc.services-2343.svc.cluster.local\nAddress: 10.152.183.156\n\n"
  Aug 12 13:32:41.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2343, will wait for the garbage collector to delete the pods @ 08/12/23 13:32:41.515
  E0812 13:32:41.553722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:41.580: INFO: Deleting ReplicationController externalsvc took: 9.419243ms
  Aug 12 13:32:41.681: INFO: Terminating ReplicationController externalsvc pods took: 100.561935ms
  E0812 13:32:42.554235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:43.555309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:32:44.116: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-2343" for this suite. @ 08/12/23 13:32:44.131
• [8.014 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/12/23 13:32:44.149
  Aug 12 13:32:44.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subpath @ 08/12/23 13:32:44.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:32:44.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:32:44.175
  STEP: Setting up data @ 08/12/23 13:32:44.18
  STEP: Creating pod pod-subpath-test-projected-8jt4 @ 08/12/23 13:32:44.193
  STEP: Creating a pod to test atomic-volume-subpath @ 08/12/23 13:32:44.193
  E0812 13:32:44.556335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:45.556392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:46.556580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:47.556724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:48.557077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:49.557769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:50.558350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:51.558903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:52.559029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:53.559530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:54.560150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:55.560432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:56.561512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:57.562043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:58.562801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:32:59.562935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:00.563672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:01.564019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:02.565030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:03.565290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:04.565411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:05.565747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:06.566364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:07.566503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:33:08.291
  Aug 12 13:33:08.297: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-subpath-test-projected-8jt4 container test-container-subpath-projected-8jt4: <nil>
  STEP: delete the pod @ 08/12/23 13:33:08.308
  STEP: Deleting pod pod-subpath-test-projected-8jt4 @ 08/12/23 13:33:08.332
  Aug 12 13:33:08.332: INFO: Deleting pod "pod-subpath-test-projected-8jt4" in namespace "subpath-1287"
  Aug 12 13:33:08.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1287" for this suite. @ 08/12/23 13:33:08.341
• [24.201 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/12/23 13:33:08.352
  Aug 12 13:33:08.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename deployment @ 08/12/23 13:33:08.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:08.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:08.377
  STEP: creating a Deployment @ 08/12/23 13:33:08.39
  Aug 12 13:33:08.390: INFO: Creating simple deployment test-deployment-crpjr
  Aug 12 13:33:08.407: INFO: deployment "test-deployment-crpjr" doesn't have the required revision set
  E0812 13:33:08.567366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:09.567505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 08/12/23 13:33:10.426
  Aug 12 13:33:10.431: INFO: Deployment test-deployment-crpjr has Conditions: [{Available True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crpjr-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/12/23 13:33:10.432
  Aug 12 13:33:10.444: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 33, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 33, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 33, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 33, 8, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-crpjr-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/12/23 13:33:10.444
  Aug 12 13:33:10.446: INFO: Observed &Deployment event: ADDED
  Aug 12 13:33:10.446: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crpjr-5994cf9475"}
  Aug 12 13:33:10.447: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.447: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crpjr-5994cf9475"}
  Aug 12 13:33:10.447: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 12 13:33:10.448: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.448: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 12 13:33:10.448: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-crpjr-5994cf9475" is progressing.}
  Aug 12 13:33:10.448: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.448: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 12 13:33:10.448: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crpjr-5994cf9475" has successfully progressed.}
  Aug 12 13:33:10.449: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.449: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 12 13:33:10.449: INFO: Observed Deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crpjr-5994cf9475" has successfully progressed.}
  Aug 12 13:33:10.449: INFO: Found Deployment test-deployment-crpjr in namespace deployment-2490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 13:33:10.449: INFO: Deployment test-deployment-crpjr has an updated status
  STEP: patching the Statefulset Status @ 08/12/23 13:33:10.449
  Aug 12 13:33:10.449: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 12 13:33:10.458: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/12/23 13:33:10.458
  Aug 12 13:33:10.461: INFO: Observed &Deployment event: ADDED
  Aug 12 13:33:10.461: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crpjr-5994cf9475"}
  Aug 12 13:33:10.461: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.461: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crpjr-5994cf9475"}
  Aug 12 13:33:10.461: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 12 13:33:10.461: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.462: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 12 13:33:10.462: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:08 +0000 UTC 2023-08-12 13:33:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-crpjr-5994cf9475" is progressing.}
  Aug 12 13:33:10.463: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.463: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 12 13:33:10.463: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crpjr-5994cf9475" has successfully progressed.}
  Aug 12 13:33:10.463: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.463: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 12 13:33:10.464: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-12 13:33:09 +0000 UTC 2023-08-12 13:33:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crpjr-5994cf9475" has successfully progressed.}
  Aug 12 13:33:10.464: INFO: Observed deployment test-deployment-crpjr in namespace deployment-2490 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 13:33:10.464: INFO: Observed &Deployment event: MODIFIED
  Aug 12 13:33:10.464: INFO: Found deployment test-deployment-crpjr in namespace deployment-2490 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug 12 13:33:10.464: INFO: Deployment test-deployment-crpjr has a patched status
  Aug 12 13:33:10.471: INFO: Deployment "test-deployment-crpjr":
  &Deployment{ObjectMeta:{test-deployment-crpjr  deployment-2490  ce56b41b-709a-4e16-9aba-58b4bc52750a 36354 1 2023-08-12 13:33:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-12 13:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-12 13:33:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-12 13:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00433ef18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-crpjr-5994cf9475",LastUpdateTime:2023-08-12 13:33:10 +0000 UTC,LastTransitionTime:2023-08-12 13:33:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 12 13:33:10.476: INFO: New ReplicaSet "test-deployment-crpjr-5994cf9475" of Deployment "test-deployment-crpjr":
  &ReplicaSet{ObjectMeta:{test-deployment-crpjr-5994cf9475  deployment-2490  15e7ffb7-019e-4dd3-984b-a932bd04295f 36346 1 2023-08-12 13:33:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-crpjr ce56b41b-709a-4e16-9aba-58b4bc52750a 0xc001fb11b0 0xc001fb11b1}] [] [{kube-controller-manager Update apps/v1 2023-08-12 13:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce56b41b-709a-4e16-9aba-58b4bc52750a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-12 13:33:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001fb1258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 12 13:33:10.484: INFO: Pod "test-deployment-crpjr-5994cf9475-j87vz" is available:
  &Pod{ObjectMeta:{test-deployment-crpjr-5994cf9475-j87vz test-deployment-crpjr-5994cf9475- deployment-2490  8e10724c-011f-4950-b005-a6c40ffbe792 36345 0 2023-08-12 13:33:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-crpjr-5994cf9475 15e7ffb7-019e-4dd3-984b-a932bd04295f 0xc00433f300 0xc00433f301}] [] [{kube-controller-manager Update v1 2023-08-12 13:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15e7ffb7-019e-4dd3-984b-a932bd04295f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-12 13:33:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4p2j6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4p2j6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-32-142,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:33:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:33:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-12 13:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.32.142,PodIP:192.168.87.180,StartTime:2023-08-12 13:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-12 13:33:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://309bae2a899ded7590f97f990e965af379d72acc409144197c7f36b976d30abe,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.87.180,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 12 13:33:10.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2490" for this suite. @ 08/12/23 13:33:10.491
• [2.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/12/23 13:33:10.504
  Aug 12 13:33:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 13:33:10.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:10.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:10.528
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/12/23 13:33:10.532
  Aug 12 13:33:10.546: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0812 13:33:10.567694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:11.567801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:12.567977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:13.568230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:14.568766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:33:15.552: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/12/23 13:33:15.552
  STEP: getting scale subresource @ 08/12/23 13:33:15.553
  STEP: updating a scale subresource @ 08/12/23 13:33:15.556
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/12/23 13:33:15.565
  E0812 13:33:15.569771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patch a scale subresource @ 08/12/23 13:33:15.575
  Aug 12 13:33:15.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7288" for this suite. @ 08/12/23 13:33:15.611
• [5.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/12/23 13:33:15.632
  Aug 12 13:33:15.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename runtimeclass @ 08/12/23 13:33:15.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:15.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:15.667
  STEP: Deleting RuntimeClass runtimeclass-1228-delete-me @ 08/12/23 13:33:15.686
  STEP: Waiting for the RuntimeClass to disappear @ 08/12/23 13:33:15.694
  Aug 12 13:33:15.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1228" for this suite. @ 08/12/23 13:33:15.716
• [0.095 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/12/23 13:33:15.727
  Aug 12 13:33:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 13:33:15.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:15.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:15.752
  STEP: Creating a test externalName service @ 08/12/23 13:33:15.758
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:15.766
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:15.767
  STEP: creating a pod to probe DNS @ 08/12/23 13:33:15.767
  STEP: submitting the pod to kubernetes @ 08/12/23 13:33:15.767
  E0812 13:33:16.570134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:17.570171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:18.571169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:19.571283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:33:19.806
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:33:19.812
  Aug 12 13:33:19.826: INFO: DNS probes using dns-test-8b9601ef-914a-4c4b-bf57-55af43e2ac91 succeeded

  STEP: changing the externalName to bar.example.com @ 08/12/23 13:33:19.826
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:19.837
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:19.837
  STEP: creating a second pod to probe DNS @ 08/12/23 13:33:19.837
  STEP: submitting the pod to kubernetes @ 08/12/23 13:33:19.837
  E0812 13:33:20.573582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:21.574041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:22.574233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:23.574510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:24.574642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:25.574742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:26.575802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:27.575909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:28.576031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:29.576340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:33:29.886
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:33:29.891
  Aug 12 13:33:29.902: INFO: DNS probes using dns-test-21e3ecfc-d539-4cc4-bfc5-406ca67ca968 succeeded

  STEP: changing the service to type=ClusterIP @ 08/12/23 13:33:29.902
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:29.94
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3278.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3278.svc.cluster.local; sleep 1; done
   @ 08/12/23 13:33:29.94
  STEP: creating a third pod to probe DNS @ 08/12/23 13:33:29.94
  STEP: submitting the pod to kubernetes @ 08/12/23 13:33:29.945
  E0812 13:33:30.577090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:31.577673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:33:31.98
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:33:31.984
  Aug 12 13:33:31.997: INFO: DNS probes using dns-test-1950bc21-aa2e-4b57-bbd3-862453735beb succeeded

  Aug 12 13:33:31.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:33:32.004
  STEP: deleting the pod @ 08/12/23 13:33:32.018
  STEP: deleting the pod @ 08/12/23 13:33:32.053
  STEP: deleting the test externalName service @ 08/12/23 13:33:32.068
  STEP: Destroying namespace "dns-3278" for this suite. @ 08/12/23 13:33:32.099
• [16.381 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/12/23 13:33:32.112
  Aug 12 13:33:32.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 13:33:32.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:32.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:32.142
  STEP: Discovering how many secrets are in namespace by default @ 08/12/23 13:33:32.147
  E0812 13:33:32.578749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:33.579823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:34.580011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:35.580665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:36.580977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/12/23 13:33:37.152
  E0812 13:33:37.581481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:38.582550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:39.583295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:40.584188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:41.584284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/12/23 13:33:42.158
  STEP: Ensuring resource quota status is calculated @ 08/12/23 13:33:42.164
  E0812 13:33:42.584380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:43.584492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 08/12/23 13:33:44.171
  STEP: Ensuring resource quota status captures secret creation @ 08/12/23 13:33:44.185
  E0812 13:33:44.585555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:45.585663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 08/12/23 13:33:46.191
  STEP: Ensuring resource quota status released usage @ 08/12/23 13:33:46.2
  E0812 13:33:46.586427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:47.586558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:33:48.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5855" for this suite. @ 08/12/23 13:33:48.213
• [16.111 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/12/23 13:33:48.224
  Aug 12 13:33:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:33:48.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:48.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:48.25
  STEP: validating api versions @ 08/12/23 13:33:48.255
  Aug 12 13:33:48.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-96 api-versions'
  Aug 12 13:33:48.335: INFO: stderr: ""
  Aug 12 13:33:48.335: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug 12 13:33:48.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-96" for this suite. @ 08/12/23 13:33:48.343
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/12/23 13:33:48.354
  Aug 12 13:33:48.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context-test @ 08/12/23 13:33:48.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:48.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:48.388
  E0812 13:33:48.587172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:49.587476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:50.587796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:51.588829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:33:52.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2700" for this suite. @ 08/12/23 13:33:52.426
• [4.081 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/12/23 13:33:52.435
  Aug 12 13:33:52.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:33:52.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:52.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:52.458
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:33:52.462
  E0812 13:33:52.589087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:53.589209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:54.590076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:55.590564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:33:56.491
  Aug 12 13:33:56.495: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-54364321-0d1b-4bdf-a39e-b1a67a8bb941 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:33:56.505
  Aug 12 13:33:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8082" for this suite. @ 08/12/23 13:33:56.535
• [4.108 seconds]
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/12/23 13:33:56.544
  Aug 12 13:33:56.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:33:56.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:33:56.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:33:56.569
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:33:56.573
  E0812 13:33:56.590828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:57.590968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:58.591830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:33:59.591942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:00.592874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:34:00.607
  Aug 12 13:34:00.612: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-030654b8-5af7-4ff5-877f-5fa1e8957bdd container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:34:00.623
  Aug 12 13:34:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4997" for this suite. @ 08/12/23 13:34:00.651
• [4.117 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/12/23 13:34:00.661
  Aug 12 13:34:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:34:00.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:34:00.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:34:00.683
  Aug 12 13:34:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:34:01.593795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/12/23 13:34:02.303
  Aug 12 13:34:02.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 create -f -'
  E0812 13:34:02.594317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:03.287: INFO: stderr: ""
  Aug 12 13:34:03.287: INFO: stdout: "e2e-test-crd-publish-openapi-9935-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 12 13:34:03.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 delete e2e-test-crd-publish-openapi-9935-crds test-foo'
  Aug 12 13:34:03.377: INFO: stderr: ""
  Aug 12 13:34:03.377: INFO: stdout: "e2e-test-crd-publish-openapi-9935-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug 12 13:34:03.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 apply -f -'
  E0812 13:34:03.595242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:04.176: INFO: stderr: ""
  Aug 12 13:34:04.176: INFO: stdout: "e2e-test-crd-publish-openapi-9935-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 12 13:34:04.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 delete e2e-test-crd-publish-openapi-9935-crds test-foo'
  Aug 12 13:34:04.271: INFO: stderr: ""
  Aug 12 13:34:04.271: INFO: stdout: "e2e-test-crd-publish-openapi-9935-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/12/23 13:34:04.271
  Aug 12 13:34:04.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 create -f -'
  E0812 13:34:04.596297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:04.653: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/12/23 13:34:04.653
  Aug 12 13:34:04.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 create -f -'
  Aug 12 13:34:04.917: INFO: rc: 1
  Aug 12 13:34:04.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 apply -f -'
  Aug 12 13:34:05.189: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/12/23 13:34:05.19
  Aug 12 13:34:05.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 create -f -'
  Aug 12 13:34:05.559: INFO: rc: 1
  Aug 12 13:34:05.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 --namespace=crd-publish-openapi-2107 apply -f -'
  E0812 13:34:05.596802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:05.931: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/12/23 13:34:05.932
  Aug 12 13:34:05.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 explain e2e-test-crd-publish-openapi-9935-crds'
  Aug 12 13:34:06.400: INFO: stderr: ""
  Aug 12 13:34:06.400: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9935-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/12/23 13:34:06.401
  Aug 12 13:34:06.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 explain e2e-test-crd-publish-openapi-9935-crds.metadata'
  E0812 13:34:06.596972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:06.859: INFO: stderr: ""
  Aug 12 13:34:06.859: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9935-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug 12 13:34:06.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 explain e2e-test-crd-publish-openapi-9935-crds.spec'
  Aug 12 13:34:07.170: INFO: stderr: ""
  Aug 12 13:34:07.170: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9935-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug 12 13:34:07.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 explain e2e-test-crd-publish-openapi-9935-crds.spec.bars'
  Aug 12 13:34:07.541: INFO: stderr: ""
  Aug 12 13:34:07.541: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9935-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/12/23 13:34:07.541
  Aug 12 13:34:07.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-2107 explain e2e-test-crd-publish-openapi-9935-crds.spec.bars2'
  E0812 13:34:07.597345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:07.993: INFO: rc: 1
  E0812 13:34:08.597778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:09.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2107" for this suite. @ 08/12/23 13:34:09.391
• [8.740 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 08/12/23 13:34:09.402
  Aug 12 13:34:09.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 13:34:09.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:34:09.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:34:09.423
  STEP: Creating simple DaemonSet "daemon-set" @ 08/12/23 13:34:09.453
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 13:34:09.459
  Aug 12 13:34:09.465: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:09.465: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:09.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:34:09.469: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:34:09.598487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:10.476: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:10.476: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:10.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:34:10.482: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:34:10.598657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:11.475: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:11.475: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:34:11.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 13:34:11.479: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/12/23 13:34:11.484
  STEP: DeleteCollection of the DaemonSets @ 08/12/23 13:34:11.489
  STEP: Verify that ReplicaSets have been deleted @ 08/12/23 13:34:11.5
  Aug 12 13:34:11.517: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36914"},"items":null}

  Aug 12 13:34:11.524: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36914"},"items":[{"metadata":{"name":"daemon-set-86x95","generateName":"daemon-set-","namespace":"daemonsets-8343","uid":"20dd5f56-900b-4d57-9359-4dd7ad828513","resourceVersion":"36908","creationTimestamp":"2023-08-12T13:34:09Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.87.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-shkxf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-shkxf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-32-142","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-32-142"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"}],"hostIP":"172.31.32.142","podIP":"192.168.87.179","podIPs":[{"ip":"192.168.87.179"}],"startTime":"2023-08-12T13:34:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-12T13:34:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8544f65e8781d7d66bd6bd624a96bc5ee984749d4f2c35bf8caea7b7807ce6ee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dm5v4","generateName":"daemon-set-","namespace":"daemonsets-8343","uid":"de54ed1c-1dd1-49b2-baa5-1e19a55a0f58","resourceVersion":"36910","creationTimestamp":"2023-08-12T13:34:09Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.177.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bn9wf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bn9wf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-84-203","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-84-203"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"}],"hostIP":"172.31.84.203","podIP":"192.168.177.63","podIPs":[{"ip":"192.168.177.63"}],"startTime":"2023-08-12T13:34:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-12T13:34:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a9b41b5e15875e2b3f8abfb4cf4f111448539a7da0c8655a6d236ad5a3ee3506","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rglnn","generateName":"daemon-set-","namespace":"daemonsets-8343","uid":"3a7aac27-ea28-467b-b511-ba5674a1d573","resourceVersion":"36911","creationTimestamp":"2023-08-12T13:34:09Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e15e3de2-c1f0-4d26-b12b-ebe8932f5a5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-12T13:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.182.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-89rsk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-89rsk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-79-233","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-79-233"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-12T13:34:09Z"}],"hostIP":"172.31.79.233","podIP":"192.168.182.33","podIPs":[{"ip":"192.168.182.33"}],"startTime":"2023-08-12T13:34:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-12T13:34:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://31676afa6ad5f2499c88c0d03cea5eadfa93ceb5e6c6d7ff80208cf1aadc4240","started":true}],"qosClass":"BestEffort"}}]}

  Aug 12 13:34:11.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8343" for this suite. @ 08/12/23 13:34:11.556
• [2.163 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/12/23 13:34:11.567
  Aug 12 13:34:11.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename security-context @ 08/12/23 13:34:11.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:34:11.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:34:11.592
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/12/23 13:34:11.597
  E0812 13:34:11.599113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:12.600688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:13.600693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:14.600837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:15.601216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:34:15.627
  Aug 12 13:34:15.631: INFO: Trying to get logs from node ip-172-31-32-142 pod security-context-d4b98963-1a24-42ad-94d5-b822c40d5322 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:34:15.641
  Aug 12 13:34:15.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9765" for this suite. @ 08/12/23 13:34:15.668
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/12/23 13:34:15.683
  Aug 12 13:34:15.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:34:15.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:34:15.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:34:15.705
  STEP: Creating pod test-grpc-e6e3ddf0-68bb-42b3-808d-299fb5be9b06 in namespace container-probe-6737 @ 08/12/23 13:34:15.711
  E0812 13:34:16.601538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:17.601919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:34:17.739: INFO: Started pod test-grpc-e6e3ddf0-68bb-42b3-808d-299fb5be9b06 in namespace container-probe-6737
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:34:17.739
  Aug 12 13:34:17.743: INFO: Initial restart count of pod test-grpc-e6e3ddf0-68bb-42b3-808d-299fb5be9b06 is 0
  E0812 13:34:18.602028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:19.602146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:20.602439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:21.602850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:22.602925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:23.603992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:24.604708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:25.604834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:26.605771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:27.606014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:28.606131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:29.606817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:30.606949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:31.607157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:32.607222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:33.607688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:34.607842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:35.608126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:36.608590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:37.608776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:38.608920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:39.609796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:40.610847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:41.611246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:42.612280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:43.612564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:44.612807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:45.612842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:46.613476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:47.613969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:48.614200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:49.614323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:50.614482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:51.614827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:52.614945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:53.615186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:54.615367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:55.615515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:56.616203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:57.616582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:58.616700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:34:59.617769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:00.617885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:01.618386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:02.618613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:03.618942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:04.619094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:05.619197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:06.619558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:07.619905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:08.620810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:09.620927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:10.621070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:11.621722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:12.622879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:13.623059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:14.624004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:15.624302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:16.624457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:17.624731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:18.624858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:19.625899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:20.626969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:21.627442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:22.627617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:23.627922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:24.628047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:25.628273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:26.628703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:27.628871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:28.628924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:29.629759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:30.630658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:31.630883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:32.630996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:33.631122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:34.631329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:35.631622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:36.632465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:37.633011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:38.633708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:39.634379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:40.634724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:41.635285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:42.635382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:43.635545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:44.636661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:45.636713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:46.636877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:47.637009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:48.637792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:49.637918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:50.638108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:51.638588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:52.638739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:53.638814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:54.639001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:55.639304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:56.640241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:57.640373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:58.640696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:35:59.641815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:00.641967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:01.642597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:02.642848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:03.643471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:04.643614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:05.643904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:06.644039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:07.644272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:08.644762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:09.644894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:10.645029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:11.645883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:12.646820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:13.647106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:14.647261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:15.647498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:16.647630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:17.647806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:18.648772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:19.649787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:20.650018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:21.650507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:22.650686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:23.650796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:24.650963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:25.651080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:26.651967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:27.652094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:28.652903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:29.653224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:30.654047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:31.654171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:32.654301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:33.654538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:34.655389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:35.655533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:36.655661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:37.655789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:38.655840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:39.656107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:40.656802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:41.656927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:42.657285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:43.657778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:44.658226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:45.658336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:46.659367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:47.659530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:48.659657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:49.659810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:50.660243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:51.660679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:52.660788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:53.661920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:54.661993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:55.662176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:56.662294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:57.662599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:58.662925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:36:59.663550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:00.663893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:01.664376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:02.665219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:03.665756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:04.666018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:05.666294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:06.667068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:07.667185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:08.667439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:09.667574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:10.668469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:11.668657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:12.668767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:13.668879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:14.669011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:15.669893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:16.670738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:17.671032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:18.671144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:19.671419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:20.672484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:21.672729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:22.673749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:23.674612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:24.675194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:25.675321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:26.675527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:27.675654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:28.675807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:29.675938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:30.676059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:31.676568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:32.676654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:33.676699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:34.677284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:35.677747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:36.678430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:37.679038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:38.679164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:39.680242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:40.681130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:41.681246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:42.681332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:43.681599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:44.681750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:45.681823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:46.682411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:47.682557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:48.683363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:49.684043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:50.685001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:51.685123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:52.685177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:53.685297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:54.685854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:55.686946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:56.687028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:57.687309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:58.687823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:37:59.688137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:00.688222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:01.688668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:02.689140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:03.689181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:04.689295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:05.690182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:06.690246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:07.691355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:08.692471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:09.692775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:10.693570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:11.693927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:12.694072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:13.694311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:14.694511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:15.694617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:16.695677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:17.695811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:18.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:38:18.484
  STEP: Destroying namespace "container-probe-6737" for this suite. @ 08/12/23 13:38:18.503
• [242.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 08/12/23 13:38:18.516
  Aug 12 13:38:18.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename daemonsets @ 08/12/23 13:38:18.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:18.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:18.543
  STEP: Creating simple DaemonSet "daemon-set" @ 08/12/23 13:38:18.574
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 13:38:18.582
  Aug 12 13:38:18.588: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:18.588: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:18.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:38:18.593: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:38:18.696663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:19.599: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:19.599: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:19.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:38:19.604: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:38:19.697050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:20.603: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:20.604: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:20.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 13:38:20.611: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/12/23 13:38:20.615
  Aug 12 13:38:20.637: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:20.637: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:20.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 13:38:20.641: INFO: Node ip-172-31-84-203 is running 0 daemon pod, expected 1
  E0812 13:38:20.697903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:21.647: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:21.647: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:21.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 13:38:21.653: INFO: Node ip-172-31-84-203 is running 0 daemon pod, expected 1
  E0812 13:38:21.698230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:22.647: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:22.648: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:22.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 12 13:38:22.653: INFO: Node ip-172-31-84-203 is running 0 daemon pod, expected 1
  E0812 13:38:22.698786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:23.648: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:23.649: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:23.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 12 13:38:23.656: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/12/23 13:38:23.661
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7788, will wait for the garbage collector to delete the pods @ 08/12/23 13:38:23.661
  E0812 13:38:23.699918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:23.733: INFO: Deleting DaemonSet.extensions daemon-set took: 16.444383ms
  Aug 12 13:38:23.834: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.748156ms
  E0812 13:38:24.699902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:24.939: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 12 13:38:24.939: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 12 13:38:24.943: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37618"},"items":null}

  Aug 12 13:38:24.947: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37618"},"items":null}

  Aug 12 13:38:24.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7788" for this suite. @ 08/12/23 13:38:24.971
• [6.464 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/12/23 13:38:24.984
  Aug 12 13:38:24.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:38:24.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:25.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:25.007
  STEP: Creating configMap with name configmap-test-volume-map-5f3f12b3-8f26-4547-b20d-abf65cf3a256 @ 08/12/23 13:38:25.012
  STEP: Creating a pod to test consume configMaps @ 08/12/23 13:38:25.018
  E0812 13:38:25.700001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:26.700450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:27.701253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:28.701379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:38:29.081
  Aug 12 13:38:29.086: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-configmaps-9abb7a85-21fe-4873-abc4-0368e5c1f322 container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 13:38:29.114
  Aug 12 13:38:29.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-966" for this suite. @ 08/12/23 13:38:29.143
• [4.169 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/12/23 13:38:29.154
  Aug 12 13:38:29.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:38:29.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:29.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:29.18
  STEP: Setting up server cert @ 08/12/23 13:38:29.208
  E0812 13:38:29.701763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:38:29.728
  STEP: Deploying the webhook pod @ 08/12/23 13:38:29.74
  STEP: Wait for the deployment to be ready @ 08/12/23 13:38:29.758
  Aug 12 13:38:29.771: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:38:30.701888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:31.702996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:38:31.79
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:38:31.812
  E0812 13:38:32.704034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:32.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/12/23 13:38:32.818
  STEP: create a pod @ 08/12/23 13:38:32.84
  E0812 13:38:33.705023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:34.705779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/12/23 13:38:34.873
  Aug 12 13:38:34.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=webhook-8851 attach --namespace=webhook-8851 to-be-attached-pod -i -c=container1'
  Aug 12 13:38:34.974: INFO: rc: 1
  Aug 12 13:38:34.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8851" for this suite. @ 08/12/23 13:38:35.05
  STEP: Destroying namespace "webhook-markers-6208" for this suite. @ 08/12/23 13:38:35.061
• [5.915 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/12/23 13:38:35.072
  Aug 12 13:38:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename podtemplate @ 08/12/23 13:38:35.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:35.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:35.096
  STEP: Create set of pod templates @ 08/12/23 13:38:35.101
  Aug 12 13:38:35.108: INFO: created test-podtemplate-1
  Aug 12 13:38:35.114: INFO: created test-podtemplate-2
  Aug 12 13:38:35.121: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/12/23 13:38:35.121
  STEP: delete collection of pod templates @ 08/12/23 13:38:35.126
  Aug 12 13:38:35.126: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/12/23 13:38:35.15
  Aug 12 13:38:35.150: INFO: requesting list of pod templates to confirm quantity
  Aug 12 13:38:35.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4211" for this suite. @ 08/12/23 13:38:35.161
• [0.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/12/23 13:38:35.171
  Aug 12 13:38:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:38:35.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:35.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:35.195
  STEP: creating a ConfigMap @ 08/12/23 13:38:35.205
  STEP: fetching the ConfigMap @ 08/12/23 13:38:35.212
  STEP: patching the ConfigMap @ 08/12/23 13:38:35.217
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/12/23 13:38:35.223
  STEP: deleting the ConfigMap by collection with a label selector @ 08/12/23 13:38:35.228
  STEP: listing all ConfigMaps in test namespace @ 08/12/23 13:38:35.241
  Aug 12 13:38:35.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6575" for this suite. @ 08/12/23 13:38:35.251
• [0.088 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/12/23 13:38:35.261
  Aug 12 13:38:35.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename init-container @ 08/12/23 13:38:35.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:35.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:35.286
  STEP: creating the pod @ 08/12/23 13:38:35.291
  Aug 12 13:38:35.291: INFO: PodSpec: initContainers in spec.initContainers
  E0812 13:38:35.705833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:36.705919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:37.706103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:38.706349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2500" for this suite. @ 08/12/23 13:38:39.185
• [3.933 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/12/23 13:38:39.207
  Aug 12 13:38:39.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename containers @ 08/12/23 13:38:39.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:39.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:39.238
  STEP: Creating a pod to test override command @ 08/12/23 13:38:39.243
  E0812 13:38:39.706786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:40.706946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:41.707912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:42.708008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:38:43.273
  Aug 12 13:38:43.277: INFO: Trying to get logs from node ip-172-31-32-142 pod client-containers-534b2129-d605-4a1b-9989-b387c4bf7b3c container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 13:38:43.286
  Aug 12 13:38:43.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3707" for this suite. @ 08/12/23 13:38:43.315
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/12/23 13:38:43.327
  Aug 12 13:38:43.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption @ 08/12/23 13:38:43.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:43.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:43.353
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:38:43.364
  E0812 13:38:43.708659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:44.708705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 08/12/23 13:38:45.374
  STEP: Waiting for all pods to be running @ 08/12/23 13:38:45.386
  Aug 12 13:38:45.397: INFO: running pods: 0 < 1
  E0812 13:38:45.709840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:46.709920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/12/23 13:38:47.403
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:38:47.419
  STEP: Patching PodDisruptionBudget status @ 08/12/23 13:38:47.43
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:38:47.442
  Aug 12 13:38:47.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9059" for this suite. @ 08/12/23 13:38:47.455
• [4.137 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/12/23 13:38:47.466
  Aug 12 13:38:47.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename controllerrevisions @ 08/12/23 13:38:47.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:47.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:47.491
  STEP: Creating DaemonSet "e2e-5285n-daemon-set" @ 08/12/23 13:38:47.524
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/12/23 13:38:47.533
  Aug 12 13:38:47.540: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:47.540: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:47.547: INFO: Number of nodes with available pods controlled by daemonset e2e-5285n-daemon-set: 0
  Aug 12 13:38:47.547: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:38:47.710980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:48.554: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:48.554: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:48.559: INFO: Number of nodes with available pods controlled by daemonset e2e-5285n-daemon-set: 0
  Aug 12 13:38:48.559: INFO: Node ip-172-31-32-142 is running 0 daemon pod, expected 1
  E0812 13:38:48.711780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:49.553: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:49.553: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:49.559: INFO: Number of nodes with available pods controlled by daemonset e2e-5285n-daemon-set: 2
  Aug 12 13:38:49.559: INFO: Node ip-172-31-79-233 is running 0 daemon pod, expected 1
  E0812 13:38:49.712318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:50.555: INFO: DaemonSet pods can't tolerate node ip-172-31-16-37 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:50.555: INFO: DaemonSet pods can't tolerate node ip-172-31-77-174 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug 12 13:38:50.563: INFO: Number of nodes with available pods controlled by daemonset e2e-5285n-daemon-set: 3
  Aug 12 13:38:50.563: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5285n-daemon-set
  STEP: Confirm DaemonSet "e2e-5285n-daemon-set" successfully created with "daemonset-name=e2e-5285n-daemon-set" label @ 08/12/23 13:38:50.568
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5285n-daemon-set" @ 08/12/23 13:38:50.577
  Aug 12 13:38:50.581: INFO: Located ControllerRevision: "e2e-5285n-daemon-set-6b56ddc567"
  STEP: Patching ControllerRevision "e2e-5285n-daemon-set-6b56ddc567" @ 08/12/23 13:38:50.588
  Aug 12 13:38:50.596: INFO: e2e-5285n-daemon-set-6b56ddc567 has been patched
  STEP: Create a new ControllerRevision @ 08/12/23 13:38:50.596
  Aug 12 13:38:50.602: INFO: Created ControllerRevision: e2e-5285n-daemon-set-6998959c8c
  STEP: Confirm that there are two ControllerRevisions @ 08/12/23 13:38:50.602
  Aug 12 13:38:50.602: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 12 13:38:50.609: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-5285n-daemon-set-6b56ddc567" @ 08/12/23 13:38:50.609
  STEP: Confirm that there is only one ControllerRevision @ 08/12/23 13:38:50.617
  Aug 12 13:38:50.617: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 12 13:38:50.622: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-5285n-daemon-set-6998959c8c" @ 08/12/23 13:38:50.627
  Aug 12 13:38:50.637: INFO: e2e-5285n-daemon-set-6998959c8c has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/12/23 13:38:50.637
  W0812 13:38:50.649204      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/12/23 13:38:50.649
  Aug 12 13:38:50.649: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0812 13:38:50.713003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:51.657: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 12 13:38:51.662: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5285n-daemon-set-6998959c8c=updated" @ 08/12/23 13:38:51.663
  STEP: Confirm that there is only one ControllerRevision @ 08/12/23 13:38:51.674
  Aug 12 13:38:51.674: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 12 13:38:51.679: INFO: Found 1 ControllerRevisions
  Aug 12 13:38:51.683: INFO: ControllerRevision "e2e-5285n-daemon-set-bdd94974" has revision 3
  STEP: Deleting DaemonSet "e2e-5285n-daemon-set" @ 08/12/23 13:38:51.692
  STEP: deleting DaemonSet.extensions e2e-5285n-daemon-set in namespace controllerrevisions-3769, will wait for the garbage collector to delete the pods @ 08/12/23 13:38:51.692
  E0812 13:38:51.713423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:51.757: INFO: Deleting DaemonSet.extensions e2e-5285n-daemon-set took: 9.698399ms
  Aug 12 13:38:51.857: INFO: Terminating DaemonSet.extensions e2e-5285n-daemon-set pods took: 100.320949ms
  E0812 13:38:52.714216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:53.065: INFO: Number of nodes with available pods controlled by daemonset e2e-5285n-daemon-set: 0
  Aug 12 13:38:53.065: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5285n-daemon-set
  Aug 12 13:38:53.069: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38072"},"items":null}

  Aug 12 13:38:53.073: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38072"},"items":null}

  Aug 12 13:38:53.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-3769" for this suite. @ 08/12/23 13:38:53.1
• [5.645 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/12/23 13:38:53.12
  Aug 12 13:38:53.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename discovery @ 08/12/23 13:38:53.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:53.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:53.143
  STEP: Setting up server cert @ 08/12/23 13:38:53.149
  Aug 12 13:38:53.632: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug 12 13:38:53.634: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug 12 13:38:53.634: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug 12 13:38:53.634: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug 12 13:38:53.634: INFO: Checking APIGroup: apps
  Aug 12 13:38:53.635: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug 12 13:38:53.635: INFO: Versions found [{apps/v1 v1}]
  Aug 12 13:38:53.635: INFO: apps/v1 matches apps/v1
  Aug 12 13:38:53.636: INFO: Checking APIGroup: events.k8s.io
  Aug 12 13:38:53.637: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug 12 13:38:53.637: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug 12 13:38:53.637: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug 12 13:38:53.637: INFO: Checking APIGroup: authentication.k8s.io
  Aug 12 13:38:53.639: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug 12 13:38:53.639: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug 12 13:38:53.639: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug 12 13:38:53.639: INFO: Checking APIGroup: authorization.k8s.io
  Aug 12 13:38:53.640: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug 12 13:38:53.640: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug 12 13:38:53.640: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug 12 13:38:53.640: INFO: Checking APIGroup: autoscaling
  Aug 12 13:38:53.642: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug 12 13:38:53.642: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug 12 13:38:53.642: INFO: autoscaling/v2 matches autoscaling/v2
  Aug 12 13:38:53.642: INFO: Checking APIGroup: batch
  Aug 12 13:38:53.643: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug 12 13:38:53.643: INFO: Versions found [{batch/v1 v1}]
  Aug 12 13:38:53.643: INFO: batch/v1 matches batch/v1
  Aug 12 13:38:53.643: INFO: Checking APIGroup: certificates.k8s.io
  Aug 12 13:38:53.645: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug 12 13:38:53.645: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug 12 13:38:53.645: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug 12 13:38:53.645: INFO: Checking APIGroup: networking.k8s.io
  Aug 12 13:38:53.646: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug 12 13:38:53.646: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug 12 13:38:53.646: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug 12 13:38:53.647: INFO: Checking APIGroup: policy
  Aug 12 13:38:53.648: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug 12 13:38:53.648: INFO: Versions found [{policy/v1 v1}]
  Aug 12 13:38:53.648: INFO: policy/v1 matches policy/v1
  Aug 12 13:38:53.648: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug 12 13:38:53.650: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug 12 13:38:53.650: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug 12 13:38:53.650: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug 12 13:38:53.650: INFO: Checking APIGroup: storage.k8s.io
  Aug 12 13:38:53.651: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug 12 13:38:53.651: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug 12 13:38:53.651: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug 12 13:38:53.651: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug 12 13:38:53.653: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug 12 13:38:53.653: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug 12 13:38:53.653: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug 12 13:38:53.653: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug 12 13:38:53.654: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug 12 13:38:53.654: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug 12 13:38:53.654: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug 12 13:38:53.654: INFO: Checking APIGroup: scheduling.k8s.io
  Aug 12 13:38:53.656: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug 12 13:38:53.656: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug 12 13:38:53.656: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug 12 13:38:53.656: INFO: Checking APIGroup: coordination.k8s.io
  Aug 12 13:38:53.658: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug 12 13:38:53.658: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug 12 13:38:53.658: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug 12 13:38:53.658: INFO: Checking APIGroup: node.k8s.io
  Aug 12 13:38:53.660: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug 12 13:38:53.660: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug 12 13:38:53.660: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug 12 13:38:53.660: INFO: Checking APIGroup: discovery.k8s.io
  Aug 12 13:38:53.661: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug 12 13:38:53.661: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug 12 13:38:53.661: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug 12 13:38:53.661: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug 12 13:38:53.663: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug 12 13:38:53.663: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug 12 13:38:53.663: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug 12 13:38:53.663: INFO: Checking APIGroup: metrics.k8s.io
  Aug 12 13:38:53.665: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Aug 12 13:38:53.665: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Aug 12 13:38:53.665: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Aug 12 13:38:53.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5687" for this suite. @ 08/12/23 13:38:53.671
• [0.561 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/12/23 13:38:53.681
  Aug 12 13:38:53.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:38:53.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:38:53.698
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:38:53.704
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/12/23 13:38:53.709
  Aug 12 13:38:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:38:53.714508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:54.715247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:38:55.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:38:55.716072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:56.716982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:57.717971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:58.718144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:38:59.718611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:00.719623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:01.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1151" for this suite. @ 08/12/23 13:39:01.25
• [7.579 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/12/23 13:39:01.262
  Aug 12 13:39:01.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:39:01.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:01.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:01.285
  STEP: creating a Service @ 08/12/23 13:39:01.294
  STEP: watching for the Service to be added @ 08/12/23 13:39:01.31
  Aug 12 13:39:01.314: INFO: Found Service test-service-kmch2 in namespace services-8071 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug 12 13:39:01.315: INFO: Service test-service-kmch2 created
  STEP: Getting /status @ 08/12/23 13:39:01.316
  Aug 12 13:39:01.322: INFO: Service test-service-kmch2 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/12/23 13:39:01.322
  STEP: watching for the Service to be patched @ 08/12/23 13:39:01.331
  Aug 12 13:39:01.333: INFO: observed Service test-service-kmch2 in namespace services-8071 with annotations: map[] & LoadBalancer: {[]}
  Aug 12 13:39:01.333: INFO: Found Service test-service-kmch2 in namespace services-8071 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug 12 13:39:01.333: INFO: Service test-service-kmch2 has service status patched
  STEP: updating the ServiceStatus @ 08/12/23 13:39:01.333
  Aug 12 13:39:01.349: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/12/23 13:39:01.349
  Aug 12 13:39:01.351: INFO: Observed Service test-service-kmch2 in namespace services-8071 with annotations: map[] & Conditions: {[]}
  Aug 12 13:39:01.351: INFO: Observed event: &Service{ObjectMeta:{test-service-kmch2  services-8071  f154bdbf-8d3c-4b08-adc9-f8a453861bfe 38164 0 2023-08-12 13:39:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-12 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-12 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.143,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.143],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug 12 13:39:01.352: INFO: Found Service test-service-kmch2 in namespace services-8071 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 12 13:39:01.352: INFO: Service test-service-kmch2 has service status updated
  STEP: patching the service @ 08/12/23 13:39:01.352
  STEP: watching for the Service to be patched @ 08/12/23 13:39:01.427
  Aug 12 13:39:01.430: INFO: observed Service test-service-kmch2 in namespace services-8071 with labels: map[test-service-static:true]
  Aug 12 13:39:01.430: INFO: observed Service test-service-kmch2 in namespace services-8071 with labels: map[test-service-static:true]
  Aug 12 13:39:01.430: INFO: observed Service test-service-kmch2 in namespace services-8071 with labels: map[test-service-static:true]
  Aug 12 13:39:01.430: INFO: Found Service test-service-kmch2 in namespace services-8071 with labels: map[test-service:patched test-service-static:true]
  Aug 12 13:39:01.430: INFO: Service test-service-kmch2 patched
  STEP: deleting the service @ 08/12/23 13:39:01.43
  STEP: watching for the Service to be deleted @ 08/12/23 13:39:01.456
  Aug 12 13:39:01.459: INFO: Observed event: ADDED
  Aug 12 13:39:01.459: INFO: Observed event: MODIFIED
  Aug 12 13:39:01.459: INFO: Observed event: MODIFIED
  Aug 12 13:39:01.459: INFO: Observed event: MODIFIED
  Aug 12 13:39:01.459: INFO: Found Service test-service-kmch2 in namespace services-8071 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug 12 13:39:01.460: INFO: Service test-service-kmch2 deleted
  Aug 12 13:39:01.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8071" for this suite. @ 08/12/23 13:39:01.466
• [0.224 seconds]
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/12/23 13:39:01.487
  Aug 12 13:39:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 13:39:01.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:01.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:01.516
  Aug 12 13:39:01.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: creating the pod @ 08/12/23 13:39:01.526
  STEP: submitting the pod to kubernetes @ 08/12/23 13:39:01.526
  E0812 13:39:01.720197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:02.720810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:03.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5693" for this suite. @ 08/12/23 13:39:03.581
• [2.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/12/23 13:39:03.596
  Aug 12 13:39:03.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:39:03.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:03.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:03.62
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/12/23 13:39:03.623
  E0812 13:39:03.721517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:04.721710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:05.722252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:06.722749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:39:07.653
  Aug 12 13:39:07.658: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-2244e276-3e51-455d-9305-d7b09f3da24f container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:39:07.669
  Aug 12 13:39:07.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4183" for this suite. @ 08/12/23 13:39:07.698
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/12/23 13:39:07.711
  Aug 12 13:39:07.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:39:07.713
  E0812 13:39:07.723933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:07.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:07.733
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:39:07.737
  E0812 13:39:08.724688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:09.725762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:10.726369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:11.727082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:39:11.77
  Aug 12 13:39:11.781: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-6b306621-0b8a-4e32-9084-225bc43ee2a3 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:39:11.791
  Aug 12 13:39:11.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4641" for this suite. @ 08/12/23 13:39:11.817
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/12/23 13:39:11.832
  Aug 12 13:39:11.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:39:11.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:11.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:11.857
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/12/23 13:39:11.863
  Aug 12 13:39:11.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8653 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 12 13:39:12.031: INFO: stderr: ""
  Aug 12 13:39:12.031: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/12/23 13:39:12.031
  E0812 13:39:12.728075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:13.728686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:14.728768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:15.728985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:16.729645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/12/23 13:39:17.083
  Aug 12 13:39:17.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8653 get pod e2e-test-httpd-pod -o json'
  Aug 12 13:39:17.165: INFO: stderr: ""
  Aug 12 13:39:17.165: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-12T13:39:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8653\",\n        \"resourceVersion\": \"38298\",\n        \"uid\": \"16864bae-b13c-4121-b118-452a9edab964\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-4pprn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-32-142\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-4pprn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-12T13:39:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-12T13:39:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-12T13:39:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-12T13:39:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://07a2a1d0cff5cf8cabedf0574d42389569f61a9089cae53b87e5448ad86189cd\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-12T13:39:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.32.142\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.87.172\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.87.172\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-12T13:39:12Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/12/23 13:39:17.166
  Aug 12 13:39:17.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8653 replace -f -'
  E0812 13:39:17.730064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:17.860: INFO: stderr: ""
  Aug 12 13:39:17.860: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/12/23 13:39:17.86
  Aug 12 13:39:17.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-8653 delete pods e2e-test-httpd-pod'
  E0812 13:39:18.730260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:19.730349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:20.030: INFO: stderr: ""
  Aug 12 13:39:20.030: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 12 13:39:20.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8653" for this suite. @ 08/12/23 13:39:20.037
• [8.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/12/23 13:39:20.047
  Aug 12 13:39:20.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:39:20.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:20.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:20.074
  STEP: Creating projection with secret that has name secret-emptykey-test-d2b2cadb-f949-47d8-90e5-fe0bde5e8f77 @ 08/12/23 13:39:20.078
  Aug 12 13:39:20.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6368" for this suite. @ 08/12/23 13:39:20.085
• [0.049 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/12/23 13:39:20.098
  Aug 12 13:39:20.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-webhook @ 08/12/23 13:39:20.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:20.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:20.123
  STEP: Setting up server cert @ 08/12/23 13:39:20.127
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/12/23 13:39:20.436
  STEP: Deploying the custom resource conversion webhook pod @ 08/12/23 13:39:20.447
  STEP: Wait for the deployment to be ready @ 08/12/23 13:39:20.462
  Aug 12 13:39:20.473: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0812 13:39:20.731212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:21.731811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:39:22.488
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:39:22.508
  E0812 13:39:22.732577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:23.509: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 12 13:39:23.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:39:23.732687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:24.733133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:25.733789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/12/23 13:39:26.133
  STEP: v2 custom resource should be converted @ 08/12/23 13:39:26.141
  Aug 12 13:39:26.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0812 13:39:26.734789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-91" for this suite. @ 08/12/23 13:39:26.74
• [6.653 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/12/23 13:39:26.752
  Aug 12 13:39:26.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:39:26.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:26.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:26.775
  STEP: Setting up server cert @ 08/12/23 13:39:26.806
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:39:27.341
  STEP: Deploying the webhook pod @ 08/12/23 13:39:27.348
  STEP: Wait for the deployment to be ready @ 08/12/23 13:39:27.364
  Aug 12 13:39:27.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:39:27.735312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:28.735431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:39:29.388
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:39:29.401
  E0812 13:39:29.735834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:30.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/12/23 13:39:30.408
  STEP: create a namespace for the webhook @ 08/12/23 13:39:30.431
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/12/23 13:39:30.455
  Aug 12 13:39:30.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4314" for this suite. @ 08/12/23 13:39:30.592
  STEP: Destroying namespace "webhook-markers-984" for this suite. @ 08/12/23 13:39:30.604
  STEP: Destroying namespace "fail-closed-namespace-1751" for this suite. @ 08/12/23 13:39:30.616
• [3.874 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/12/23 13:39:30.627
  Aug 12 13:39:30.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubelet-test @ 08/12/23 13:39:30.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:30.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:30.655
  STEP: Waiting for pod completion @ 08/12/23 13:39:30.671
  E0812 13:39:30.736700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:31.736810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:32.737931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:33.738894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:34.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5814" for this suite. @ 08/12/23 13:39:34.703
• [4.086 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/12/23 13:39:34.715
  Aug 12 13:39:34.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:39:34.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:34.734
  E0812 13:39:34.738936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:34.739
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:39:34.743
  E0812 13:39:35.739106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:36.739823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:37.740765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:38.740906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:39:38.772
  Aug 12 13:39:38.777: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-112ef5e6-a5ec-4593-adb2-d88ccbb45b3f container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:39:38.788
  Aug 12 13:39:38.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1463" for this suite. @ 08/12/23 13:39:38.818
• [4.113 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/12/23 13:39:38.828
  Aug 12 13:39:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:39:38.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:38.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:38.851
  STEP: Creating configMap with name projected-configmap-test-volume-3be4fae6-2cd3-4f21-83d6-4d3369153600 @ 08/12/23 13:39:38.855
  STEP: Creating a pod to test consume configMaps @ 08/12/23 13:39:38.874
  E0812 13:39:39.741841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:40.741918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:41.742359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:42.742685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:39:42.908
  Aug 12 13:39:42.913: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-configmaps-22e1c29e-82ba-421f-81f7-388e40989b5a container agnhost-container: <nil>
  STEP: delete the pod @ 08/12/23 13:39:42.924
  Aug 12 13:39:42.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4454" for this suite. @ 08/12/23 13:39:42.954
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/12/23 13:39:42.969
  Aug 12 13:39:42.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename limitrange @ 08/12/23 13:39:42.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:42.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:42.992
  STEP: Creating a LimitRange @ 08/12/23 13:39:42.996
  STEP: Setting up watch @ 08/12/23 13:39:42.997
  STEP: Submitting a LimitRange @ 08/12/23 13:39:43.102
  STEP: Verifying LimitRange creation was observed @ 08/12/23 13:39:43.108
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/12/23 13:39:43.109
  Aug 12 13:39:43.113: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 12 13:39:43.113: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/12/23 13:39:43.113
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/12/23 13:39:43.12
  Aug 12 13:39:43.126: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 12 13:39:43.126: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/12/23 13:39:43.126
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/12/23 13:39:43.132
  Aug 12 13:39:43.137: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug 12 13:39:43.137: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/12/23 13:39:43.137
  STEP: Failing to create a Pod with more than max resources @ 08/12/23 13:39:43.14
  STEP: Updating a LimitRange @ 08/12/23 13:39:43.142
  STEP: Verifying LimitRange updating is effective @ 08/12/23 13:39:43.151
  E0812 13:39:43.743677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:44.744553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 08/12/23 13:39:45.158
  STEP: Failing to create a Pod with more than max resources @ 08/12/23 13:39:45.166
  STEP: Deleting a LimitRange @ 08/12/23 13:39:45.177
  STEP: Verifying the LimitRange was deleted @ 08/12/23 13:39:45.191
  E0812 13:39:45.745360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:46.745661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:47.745719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:48.746000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:49.746119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:50.196: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/12/23 13:39:50.196
  Aug 12 13:39:50.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9535" for this suite. @ 08/12/23 13:39:50.213
• [7.259 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/12/23 13:39:50.229
  Aug 12 13:39:50.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename dns @ 08/12/23 13:39:50.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:50.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:50.253
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/12/23 13:39:50.257
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/12/23 13:39:50.257
  STEP: creating a pod to probe DNS @ 08/12/23 13:39:50.258
  STEP: submitting the pod to kubernetes @ 08/12/23 13:39:50.258
  E0812 13:39:50.747094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:51.747540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/12/23 13:39:52.286
  STEP: looking for the results for each expected name from probers @ 08/12/23 13:39:52.292
  Aug 12 13:39:52.315: INFO: DNS probes using dns-9110/dns-test-4b129204-941d-46c1-ba6d-ca4c633de667 succeeded

  Aug 12 13:39:52.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:39:52.321
  STEP: Destroying namespace "dns-9110" for this suite. @ 08/12/23 13:39:52.335
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/12/23 13:39:52.348
  Aug 12 13:39:52.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:39:52.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:52.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:52.378
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/12/23 13:39:52.383
  E0812 13:39:52.748355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:53.748660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:54.749675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:55.749991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:39:56.415
  Aug 12 13:39:56.419: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-5e767ac1-d2d3-4004-abb9-e6b65fa40a28 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:39:56.428
  Aug 12 13:39:56.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5164" for this suite. @ 08/12/23 13:39:56.456
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/12/23 13:39:56.468
  Aug 12 13:39:56.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:39:56.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:56.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:56.495
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/12/23 13:39:56.5
  Aug 12 13:39:56.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2133 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 12 13:39:56.589: INFO: stderr: ""
  Aug 12 13:39:56.589: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/12/23 13:39:56.589
  Aug 12 13:39:56.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2133 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Aug 12 13:39:56.682: INFO: stderr: ""
  Aug 12 13:39:56.682: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/12/23 13:39:56.682
  Aug 12 13:39:56.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-2133 delete pods e2e-test-httpd-pod'
  E0812 13:39:56.751038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:57.751265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:39:58.751384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:39:59.191: INFO: stderr: ""
  Aug 12 13:39:59.191: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 12 13:39:59.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2133" for this suite. @ 08/12/23 13:39:59.198
• [2.739 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/12/23 13:39:59.209
  Aug 12 13:39:59.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 13:39:59.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:59.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:39:59.246
  STEP: Creating a test namespace @ 08/12/23 13:39:59.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:39:59.265
  STEP: Creating a pod in the namespace @ 08/12/23 13:39:59.271
  STEP: Waiting for the pod to have running status @ 08/12/23 13:39:59.281
  E0812 13:39:59.752087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:00.752429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 08/12/23 13:40:01.293
  STEP: Waiting for the namespace to be removed. @ 08/12/23 13:40:01.306
  E0812 13:40:01.753267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:02.753759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:03.754440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:04.754560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:05.754807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:06.754934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:07.755062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:08.756037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:09.756347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:10.756863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:11.756984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/12/23 13:40:12.313
  STEP: Verifying there are no pods in the namespace @ 08/12/23 13:40:12.33
  Aug 12 13:40:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5751" for this suite. @ 08/12/23 13:40:12.34
  STEP: Destroying namespace "nsdeletetest-3509" for this suite. @ 08/12/23 13:40:12.35
  Aug 12 13:40:12.354: INFO: Namespace nsdeletetest-3509 was already deleted
  STEP: Destroying namespace "nsdeletetest-9949" for this suite. @ 08/12/23 13:40:12.354
• [13.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/12/23 13:40:12.372
  Aug 12 13:40:12.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename proxy @ 08/12/23 13:40:12.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:12.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:12.397
  Aug 12 13:40:12.401: INFO: Creating pod...
  E0812 13:40:12.757723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:13.758090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:14.427: INFO: Creating service...
  Aug 12 13:40:14.459: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/DELETE
  Aug 12 13:40:14.471: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 12 13:40:14.471: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/GET
  Aug 12 13:40:14.477: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 12 13:40:14.477: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/HEAD
  Aug 12 13:40:14.483: INFO: http.Client request:HEAD | StatusCode:200
  Aug 12 13:40:14.483: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug 12 13:40:14.494: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 12 13:40:14.494: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/PATCH
  Aug 12 13:40:14.503: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 12 13:40:14.503: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/POST
  Aug 12 13:40:14.509: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 12 13:40:14.509: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/pods/agnhost/proxy/some/path/with/PUT
  Aug 12 13:40:14.516: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 12 13:40:14.516: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/DELETE
  Aug 12 13:40:14.524: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 12 13:40:14.524: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/GET
  Aug 12 13:40:14.533: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 12 13:40:14.533: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/HEAD
  Aug 12 13:40:14.543: INFO: http.Client request:HEAD | StatusCode:200
  Aug 12 13:40:14.543: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/OPTIONS
  Aug 12 13:40:14.552: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 12 13:40:14.552: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/PATCH
  Aug 12 13:40:14.561: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 12 13:40:14.561: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/POST
  Aug 12 13:40:14.572: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 12 13:40:14.572: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2279/services/test-service/proxy/some/path/with/PUT
  Aug 12 13:40:14.580: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 12 13:40:14.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2279" for this suite. @ 08/12/23 13:40:14.588
• [2.229 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/12/23 13:40:14.602
  Aug 12 13:40:14.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:40:14.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:14.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:14.639
  STEP: Creating projection with secret that has name projected-secret-test-map-0e19358a-c2c0-4438-9224-690cec35f1fe @ 08/12/23 13:40:14.645
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:40:14.653
  E0812 13:40:14.758559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:15.759173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:16.759409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:17.759535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:40:18.687
  Aug 12 13:40:18.693: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-projected-secrets-8156a424-de17-45c4-848a-1115585520d6 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:40:18.704
  Aug 12 13:40:18.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9429" for this suite. @ 08/12/23 13:40:18.74
• [4.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/12/23 13:40:18.753
  Aug 12 13:40:18.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename namespaces @ 08/12/23 13:40:18.754
  E0812 13:40:18.759777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:18.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:18.789
  STEP: Creating namespace "e2e-ns-sm5lq" @ 08/12/23 13:40:18.795
  Aug 12 13:40:18.857: INFO: Namespace "e2e-ns-sm5lq-2341" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-sm5lq-2341" @ 08/12/23 13:40:18.857
  Aug 12 13:40:18.872: INFO: Namespace "e2e-ns-sm5lq-2341" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-sm5lq-2341" @ 08/12/23 13:40:18.872
  Aug 12 13:40:18.885: INFO: Namespace "e2e-ns-sm5lq-2341" has []v1.FinalizerName{"kubernetes"}
  Aug 12 13:40:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7051" for this suite. @ 08/12/23 13:40:18.893
  STEP: Destroying namespace "e2e-ns-sm5lq-2341" for this suite. @ 08/12/23 13:40:18.903
• [0.160 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/12/23 13:40:18.914
  Aug 12 13:40:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 13:40:18.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:18.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:18.944
  E0812 13:40:19.759946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:20.760785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:20.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 13:40:20.986: INFO: Deleting pod "var-expansion-e809a5e2-29ac-4699-961a-e0f9361772fb" in namespace "var-expansion-5653"
  Aug 12 13:40:21.000: INFO: Wait up to 5m0s for pod "var-expansion-e809a5e2-29ac-4699-961a-e0f9361772fb" to be fully deleted
  E0812 13:40:21.760891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:22.761033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5653" for this suite. @ 08/12/23 13:40:23.01
• [4.108 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/12/23 13:40:23.023
  Aug 12 13:40:23.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/12/23 13:40:23.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:23.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:23.055
  STEP: Creating 50 configmaps @ 08/12/23 13:40:23.059
  STEP: Creating RC which spawns configmap-volume pods @ 08/12/23 13:40:23.417
  Aug 12 13:40:23.441: INFO: Pod name wrapped-volume-race-56b2d484-8863-4d2f-84ba-6a6395e43c02: Found 0 pods out of 5
  E0812 13:40:23.761160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:24.761303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:25.761423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:26.762156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:27.762319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:28.453: INFO: Pod name wrapped-volume-race-56b2d484-8863-4d2f-84ba-6a6395e43c02: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/12/23 13:40:28.453
  STEP: Creating RC which spawns configmap-volume pods @ 08/12/23 13:40:28.492
  Aug 12 13:40:28.520: INFO: Pod name wrapped-volume-race-ba424b82-eaef-486b-8640-0bcf3fb6d162: Found 0 pods out of 5
  E0812 13:40:28.763251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:29.763268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:30.764096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:31.764470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:32.764731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:33.532: INFO: Pod name wrapped-volume-race-ba424b82-eaef-486b-8640-0bcf3fb6d162: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/12/23 13:40:33.532
  STEP: Creating RC which spawns configmap-volume pods @ 08/12/23 13:40:33.569
  Aug 12 13:40:33.610: INFO: Pod name wrapped-volume-race-72e08c7f-6d27-4dc8-8887-f6164f11710a: Found 0 pods out of 5
  E0812 13:40:33.765143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:34.765306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:35.768763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:36.769178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:37.769300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:38.621: INFO: Pod name wrapped-volume-race-72e08c7f-6d27-4dc8-8887-f6164f11710a: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/12/23 13:40:38.621
  Aug 12 13:40:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-72e08c7f-6d27-4dc8-8887-f6164f11710a in namespace emptydir-wrapper-3425, will wait for the garbage collector to delete the pods @ 08/12/23 13:40:38.652
  Aug 12 13:40:38.717: INFO: Deleting ReplicationController wrapped-volume-race-72e08c7f-6d27-4dc8-8887-f6164f11710a took: 9.249175ms
  E0812 13:40:38.769948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:38.917: INFO: Terminating ReplicationController wrapped-volume-race-72e08c7f-6d27-4dc8-8887-f6164f11710a pods took: 200.338755ms
  E0812 13:40:39.770104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:40.770747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-ba424b82-eaef-486b-8640-0bcf3fb6d162 in namespace emptydir-wrapper-3425, will wait for the garbage collector to delete the pods @ 08/12/23 13:40:41.718
  E0812 13:40:41.771699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:40:41.785: INFO: Deleting ReplicationController wrapped-volume-race-ba424b82-eaef-486b-8640-0bcf3fb6d162 took: 10.462795ms
  Aug 12 13:40:41.886: INFO: Terminating ReplicationController wrapped-volume-race-ba424b82-eaef-486b-8640-0bcf3fb6d162 pods took: 100.885656ms
  E0812 13:40:42.771903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:43.772270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-56b2d484-8863-4d2f-84ba-6a6395e43c02 in namespace emptydir-wrapper-3425, will wait for the garbage collector to delete the pods @ 08/12/23 13:40:44.587
  Aug 12 13:40:44.654: INFO: Deleting ReplicationController wrapped-volume-race-56b2d484-8863-4d2f-84ba-6a6395e43c02 took: 10.142629ms
  Aug 12 13:40:44.754: INFO: Terminating ReplicationController wrapped-volume-race-56b2d484-8863-4d2f-84ba-6a6395e43c02 pods took: 100.450696ms
  E0812 13:40:44.772945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:45.773647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:46.774266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 08/12/23 13:40:47.655
  E0812 13:40:47.775345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-3425" for this suite. @ 08/12/23 13:40:48.08
• [25.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/12/23 13:40:48.096
  Aug 12 13:40:48.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:40:48.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:48.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:48.124
  STEP: creating a collection of services @ 08/12/23 13:40:48.13
  Aug 12 13:40:48.130: INFO: Creating e2e-svc-a-95nhc
  Aug 12 13:40:48.145: INFO: Creating e2e-svc-b-56kbt
  Aug 12 13:40:48.159: INFO: Creating e2e-svc-c-fz6wv
  STEP: deleting service collection @ 08/12/23 13:40:48.181
  Aug 12 13:40:48.223: INFO: Collection of services has been deleted
  Aug 12 13:40:48.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6157" for this suite. @ 08/12/23 13:40:48.231
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 08/12/23 13:40:48.243
  Aug 12 13:40:48.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename field-validation @ 08/12/23 13:40:48.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:48.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:48.275
  Aug 12 13:40:48.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:40:48.776114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:49.776246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:50.776416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0812 13:40:50.850385      19 warnings.go:70] unknown field "alpha"
  W0812 13:40:50.850419      19 warnings.go:70] unknown field "beta"
  W0812 13:40:50.850431      19 warnings.go:70] unknown field "delta"
  W0812 13:40:50.850447      19 warnings.go:70] unknown field "epsilon"
  W0812 13:40:50.850456      19 warnings.go:70] unknown field "gamma"
  Aug 12 13:40:51.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3424" for this suite. @ 08/12/23 13:40:51.412
• [3.177 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/12/23 13:40:51.422
  Aug 12 13:40:51.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 13:40:51.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:40:51.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:40:51.447
  Aug 12 13:40:51.467: INFO: created pod
  E0812 13:40:51.776613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:52.776708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:53.776816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:54.776952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:40:55.49
  E0812 13:40:55.777177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:56.777877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:57.778090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:58.778169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:40:59.778309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:00.778612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:01.779032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:02.779608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:03.779882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:04.780067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:05.780220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:06.780682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:07.780860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:08.780988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:09.781788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:10.781912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:11.782157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:12.782241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:13.782360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:14.782514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:15.782990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:16.783235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:17.783451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:18.783593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:19.783710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:20.784330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:21.784668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:22.784786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:23.784969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:24.785782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:25.491: INFO: polling logs
  Aug 12 13:41:25.504: INFO: Pod logs: 
  I0812 13:40:52.290181       1 log.go:198] OK: Got token
  I0812 13:40:52.290234       1 log.go:198] validating with in-cluster discovery
  I0812 13:40:52.290646       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0812 13:40:52.290701       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1547:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691848251, NotBefore:1691847651, IssuedAt:1691847651, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1547", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"790eb974-3a11-467d-9647-834809d41c92"}}}
  I0812 13:40:52.304644       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0812 13:40:52.313877       1 log.go:198] OK: Validated signature on JWT
  I0812 13:40:52.314080       1 log.go:198] OK: Got valid claims from token!
  I0812 13:40:52.314138       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1547:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691848251, NotBefore:1691847651, IssuedAt:1691847651, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1547", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"790eb974-3a11-467d-9647-834809d41c92"}}}

  Aug 12 13:41:25.506: INFO: completed pod
  Aug 12 13:41:25.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1547" for this suite. @ 08/12/23 13:41:25.52
• [34.109 seconds]
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/12/23 13:41:25.531
  Aug 12 13:41:25.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption @ 08/12/23 13:41:25.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:25.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:25.556
  STEP: creating the pdb @ 08/12/23 13:41:25.56
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:41:25.567
  E0812 13:41:25.786810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:26.787036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 08/12/23 13:41:27.578
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:41:27.589
  E0812 13:41:27.787641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:28.787770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 08/12/23 13:41:29.601
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:41:29.614
  E0812 13:41:29.788128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:30.788273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 08/12/23 13:41:31.635
  Aug 12 13:41:31.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8441" for this suite. @ 08/12/23 13:41:31.647
• [6.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/12/23 13:41:31.659
  Aug 12 13:41:31.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename endpointslice @ 08/12/23 13:41:31.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:31.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:31.685
  STEP: getting /apis @ 08/12/23 13:41:31.69
  STEP: getting /apis/discovery.k8s.io @ 08/12/23 13:41:31.695
  STEP: getting /apis/discovery.k8s.iov1 @ 08/12/23 13:41:31.697
  STEP: creating @ 08/12/23 13:41:31.699
  STEP: getting @ 08/12/23 13:41:31.722
  STEP: listing @ 08/12/23 13:41:31.728
  STEP: watching @ 08/12/23 13:41:31.732
  Aug 12 13:41:31.732: INFO: starting watch
  STEP: cluster-wide listing @ 08/12/23 13:41:31.733
  STEP: cluster-wide watching @ 08/12/23 13:41:31.738
  Aug 12 13:41:31.738: INFO: starting watch
  STEP: patching @ 08/12/23 13:41:31.74
  STEP: updating @ 08/12/23 13:41:31.748
  Aug 12 13:41:31.761: INFO: waiting for watch events with expected annotations
  Aug 12 13:41:31.761: INFO: saw patched and updated annotations
  STEP: deleting @ 08/12/23 13:41:31.761
  STEP: deleting a collection @ 08/12/23 13:41:31.779
  E0812 13:41:31.788833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:31.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1387" for this suite. @ 08/12/23 13:41:31.816
• [0.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/12/23 13:41:31.833
  Aug 12 13:41:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename sched-pred @ 08/12/23 13:41:31.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:31.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:31.858
  Aug 12 13:41:31.862: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 12 13:41:31.872: INFO: Waiting for terminating namespaces to be deleted...
  Aug 12 13:41:31.878: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-32-142 before test
  Aug 12 13:41:31.884: INFO: nginx-ingress-controller-kubernetes-worker-6fqzn from ingress-nginx-kubernetes-worker started at 2023-08-12 13:16:31 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.884: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:41:31.884: INFO: sonobuoy from sonobuoy started at 2023-08-12 12:09:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.884: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 12 13:41:31.884: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-5tdg5 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:41:31.884: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:41:31.884: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 13:41:31.885: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-79-233 before test
  Aug 12 13:41:31.892: INFO: default-http-backend-kubernetes-worker-65fc475d49-9qsc9 from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:29 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: nginx-ingress-controller-kubernetes-worker-9f2bf from ingress-nginx-kubernetes-worker started at 2023-08-12 11:54:28 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: coredns-5c7f76ccb8-qnk7d from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container coredns ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: kube-state-metrics-5b95b4459c-v9mbb from kube-system started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: metrics-server-v0.5.2-6cf8c8b69c-zqxs2 from kube-system started at 2023-08-12 11:54:23 +0000 UTC (2 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container metrics-server ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: dashboard-metrics-scraper-6b8586b5c9-vtmth from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: kubernetes-dashboard-6869f4cd5f-ttr98 from kubernetes-dashboard started at 2023-08-12 11:54:23 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-bvmhq from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:41:31.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 12 13:41:31.892: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-84-203 before test
  Aug 12 13:41:31.899: INFO: nginx-ingress-controller-kubernetes-worker-rdq6z from ingress-nginx-kubernetes-worker started at 2023-08-12 11:59:55 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.899: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug 12 13:41:31.899: INFO: calico-kube-controllers-5f769b769b-ttpqk from kube-system started at 2023-08-12 12:03:40 +0000 UTC (1 container statuses recorded)
  Aug 12 13:41:31.899: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug 12 13:41:31.899: INFO: sonobuoy-e2e-job-d2bbfc40cf5e42c2 from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:41:31.899: INFO: 	Container e2e ready: true, restart count 0
  Aug 12 13:41:31.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:41:31.899: INFO: sonobuoy-systemd-logs-daemon-set-cdb230855f564179-r2r8l from sonobuoy started at 2023-08-12 12:09:26 +0000 UTC (2 container statuses recorded)
  Aug 12 13:41:31.899: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 12 13:41:31.899: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/12/23 13:41:31.899
  E0812 13:41:32.788989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:33.789783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/12/23 13:41:33.929
  STEP: Trying to apply a random label on the found node. @ 08/12/23 13:41:33.95
  STEP: verifying the node has the label kubernetes.io/e2e-0e50607b-d1ca-4521-b6de-a1dc651b0bfc 42 @ 08/12/23 13:41:33.962
  STEP: Trying to relaunch the pod, now with labels. @ 08/12/23 13:41:33.966
  E0812 13:41:34.789943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:35.790238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-0e50607b-d1ca-4521-b6de-a1dc651b0bfc off the node ip-172-31-32-142 @ 08/12/23 13:41:35.988
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-0e50607b-d1ca-4521-b6de-a1dc651b0bfc @ 08/12/23 13:41:36.005
  Aug 12 13:41:36.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7411" for this suite. @ 08/12/23 13:41:36.015
• [4.191 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/12/23 13:41:36.025
  Aug 12 13:41:36.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubectl @ 08/12/23 13:41:36.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:36.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:36.05
  Aug 12 13:41:36.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 create -f -'
  Aug 12 13:41:36.542: INFO: stderr: ""
  Aug 12 13:41:36.542: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug 12 13:41:36.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 create -f -'
  E0812 13:41:36.791142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:36.883: INFO: stderr: ""
  Aug 12 13:41:36.884: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/12/23 13:41:36.884
  E0812 13:41:37.792059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:37.890: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 13:41:37.890: INFO: Found 1 / 1
  Aug 12 13:41:37.890: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 12 13:41:37.896: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 12 13:41:37.896: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 12 13:41:37.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 describe pod agnhost-primary-2xwzs'
  Aug 12 13:41:38.002: INFO: stderr: ""
  Aug 12 13:41:38.002: INFO: stdout: "Name:             agnhost-primary-2xwzs\nNamespace:        kubectl-5707\nPriority:         0\nService Account:  default\nNode:             ip-172-31-32-142/172.31.32.142\nStart Time:       Sat, 12 Aug 2023 13:41:36 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.87.144\nIPs:\n  IP:           192.168.87.144\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://0b28e7f2cec9f574e0935b96304718a3200265537dd8e500d80089bac68ddffe\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 12 Aug 2023 13:41:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6f9p (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-p6f9p:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5707/agnhost-primary-2xwzs to ip-172-31-32-142\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Aug 12 13:41:38.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 describe rc agnhost-primary'
  Aug 12 13:41:38.105: INFO: stderr: ""
  Aug 12 13:41:38.105: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5707\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2xwzs\n"
  Aug 12 13:41:38.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 describe service agnhost-primary'
  Aug 12 13:41:38.200: INFO: stderr: ""
  Aug 12 13:41:38.200: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5707\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.41\nIPs:               10.152.183.41\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.87.144:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug 12 13:41:38.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 describe node ip-172-31-16-37'
  Aug 12 13:41:38.325: INFO: stderr: ""
  Aug 12 13:41:38.325: INFO: stdout: "Name:               ip-172-31-16-37\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-control-plane\n                    juju-charm=kubernetes-control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-16-37\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 12 Aug 2023 11:52:51 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-16-37\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 12 Aug 2023 13:41:37 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 12 Aug 2023 13:36:42 +0000   Sat, 12 Aug 2023 11:52:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 12 Aug 2023 13:36:42 +0000   Sat, 12 Aug 2023 11:52:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 12 Aug 2023 13:36:42 +0000   Sat, 12 Aug 2023 11:52:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 12 Aug 2023 13:36:42 +0000   Sat, 12 Aug 2023 11:52:51 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.16.37\n  Hostname:    ip-172-31-16-37\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15599232Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15496832Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 5e86a742f6fb4a6880ec1520d2e81213\n  System UUID:                ec281866-988d-6524-2e17-ea4a48dda8c3\n  Boot ID:                    fa97e015-c5d2-44f9-b5e7-ecd648f381ac\n  Kernel Version:             5.19.0-1029-aws\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.4\n  Kube-Proxy Version:         v1.27.4\nNon-terminated Pods:          (1 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cdb230855f564179-vhc4w    0 (0%)        0 (0%)      0 (0%)           0 (0%)         92m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:              <none>\n"
  Aug 12 13:41:38.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=kubectl-5707 describe namespace kubectl-5707'
  Aug 12 13:41:38.421: INFO: stderr: ""
  Aug 12 13:41:38.421: INFO: stdout: "Name:         kubectl-5707\nLabels:       e2e-framework=kubectl\n              e2e-run=6291cefb-2518-4696-9a5a-b192283a03a4\n              kubernetes.io/metadata.name=kubectl-5707\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug 12 13:41:38.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5707" for this suite. @ 08/12/23 13:41:38.428
• [2.411 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/12/23 13:41:38.436
  Aug 12 13:41:38.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:41:38.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:38.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:38.46
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/12/23 13:41:38.464
  Aug 12 13:41:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:41:38.792158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:39.792978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:39.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:41:40.793120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:41.793948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:42.794784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:43.795736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:44.796403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:45.797141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:45.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7668" for this suite. @ 08/12/23 13:41:45.882
• [7.457 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/12/23 13:41:45.893
  Aug 12 13:41:45.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:41:45.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:45.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:45.917
  Aug 12 13:41:45.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9149" for this suite. @ 08/12/23 13:41:45.937
• [0.052 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/12/23 13:41:45.947
  Aug 12 13:41:45.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename runtimeclass @ 08/12/23 13:41:45.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:45.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:45.971
  Aug 12 13:41:46.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1190" for this suite. @ 08/12/23 13:41:46.018
• [0.080 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/12/23 13:41:46.027
  Aug 12 13:41:46.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename init-container @ 08/12/23 13:41:46.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:46.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:46.053
  STEP: creating the pod @ 08/12/23 13:41:46.058
  Aug 12 13:41:46.058: INFO: PodSpec: initContainers in spec.initContainers
  E0812 13:41:46.797831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:47.798635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:48.799491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:49.799895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:50.799994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:51.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-862" for this suite. @ 08/12/23 13:41:51.104
• [5.098 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/12/23 13:41:51.126
  Aug 12 13:41:51.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename gc @ 08/12/23 13:41:51.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:51.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:51.172
  Aug 12 13:41:51.276: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a31019d8-5422-4699-a809-12904c322766", Controller:(*bool)(0xc002d3d0ce), BlockOwnerDeletion:(*bool)(0xc002d3d0cf)}}
  Aug 12 13:41:51.291: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b0e5f679-e8b9-478c-ac39-9bfa2d7311e8", Controller:(*bool)(0xc002d3d36e), BlockOwnerDeletion:(*bool)(0xc002d3d36f)}}
  Aug 12 13:41:51.309: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0a2fc8f1-46bd-4b04-9f37-80ca9324a0a2", Controller:(*bool)(0xc0030fd756), BlockOwnerDeletion:(*bool)(0xc0030fd757)}}
  E0812 13:41:51.800120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:52.800269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:53.800592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:54.800693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:55.800990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:41:56.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6305" for this suite. @ 08/12/23 13:41:56.345
• [5.231 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/12/23 13:41:56.368
  Aug 12 13:41:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename disruption @ 08/12/23 13:41:56.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:41:56.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:41:56.402
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/12/23 13:41:56.408
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:41:56.416
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/12/23 13:41:56.432
  STEP: Waiting for all pods to be running @ 08/12/23 13:41:56.432
  Aug 12 13:41:56.436: INFO: pods: 0 < 3
  E0812 13:41:56.801265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:57.801259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/12/23 13:41:58.442
  STEP: Updating the pdb to allow a pod to be evicted @ 08/12/23 13:41:58.456
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:41:58.468
  E0812 13:41:58.801823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:41:59.801915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/12/23 13:42:00.48
  STEP: Waiting for all pods to be running @ 08/12/23 13:42:00.48
  STEP: Waiting for the pdb to observed all healthy pods @ 08/12/23 13:42:00.485
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/12/23 13:42:00.519
  STEP: Waiting for the pdb to be processed @ 08/12/23 13:42:00.546
  E0812 13:42:00.803021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:01.803838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 08/12/23 13:42:02.562
  STEP: locating a running pod @ 08/12/23 13:42:02.567
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/12/23 13:42:02.582
  STEP: Waiting for the pdb to be deleted @ 08/12/23 13:42:02.59
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/12/23 13:42:02.596
  STEP: Waiting for all pods to be running @ 08/12/23 13:42:02.596
  Aug 12 13:42:02.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6926" for this suite. @ 08/12/23 13:42:02.636
• [6.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 08/12/23 13:42:02.654
  Aug 12 13:42:02.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename aggregator @ 08/12/23 13:42:02.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:42:02.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:42:02.683
  Aug 12 13:42:02.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Registering the sample API server. @ 08/12/23 13:42:02.69
  E0812 13:42:02.804775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:03.019: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug 12 13:42:03.068: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0812 13:42:03.805802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:04.806103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:05.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:05.807082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:06.807791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:07.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:07.808211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:08.808358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:09.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:09.809319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:10.809424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:11.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:11.809995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:12.810295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:13.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:13.810443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:14.810890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:15.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:15.811143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:16.811887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:17.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:17.812705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:18.812942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:19.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:19.813167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:20.813804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:21.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:21.814761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:22.814890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:23.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:23.815946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:24.816054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:25.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0812 13:42:25.816191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:26.816712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:27.264: INFO: Waited 116.22164ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/12/23 13:42:27.316
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/12/23 13:42:27.321
  STEP: List APIServices @ 08/12/23 13:42:27.331
  Aug 12 13:42:27.339: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/12/23 13:42:27.339
  Aug 12 13:42:27.357: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/12/23 13:42:27.357
  Aug 12 13:42:27.369: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 12, 13, 42, 27, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/12/23 13:42:27.369
  Aug 12 13:42:27.375: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-12 13:42:27 +0000 UTC Passed all checks passed}
  Aug 12 13:42:27.375: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 13:42:27.375: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/12/23 13:42:27.375
  Aug 12 13:42:27.389: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1085624458" @ 08/12/23 13:42:27.389
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/12/23 13:42:27.405
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/12/23 13:42:27.418
  STEP: Patch APIService Status @ 08/12/23 13:42:27.423
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/12/23 13:42:27.431
  Aug 12 13:42:27.436: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-12 13:42:27 +0000 UTC Passed all checks passed}
  Aug 12 13:42:27.436: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 12 13:42:27.436: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug 12 13:42:27.436: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/12/23 13:42:27.437
  STEP: Confirm that the generated APIService has been deleted @ 08/12/23 13:42:27.444
  Aug 12 13:42:27.444: INFO: Requesting list of APIServices to confirm quantity
  Aug 12 13:42:27.449: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug 12 13:42:27.449: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug 12 13:42:27.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-6382" for this suite. @ 08/12/23 13:42:27.616
• [24.971 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/12/23 13:42:27.627
  Aug 12 13:42:27.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replication-controller @ 08/12/23 13:42:27.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:42:27.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:42:27.656
  STEP: Creating ReplicationController "e2e-rc-hs72v" @ 08/12/23 13:42:27.66
  Aug 12 13:42:27.666: INFO: Get Replication Controller "e2e-rc-hs72v" to confirm replicas
  E0812 13:42:27.817556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:28.671: INFO: Get Replication Controller "e2e-rc-hs72v" to confirm replicas
  Aug 12 13:42:28.677: INFO: Found 1 replicas for "e2e-rc-hs72v" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-hs72v" @ 08/12/23 13:42:28.677
  STEP: Updating a scale subresource @ 08/12/23 13:42:28.682
  STEP: Verifying replicas where modified for replication controller "e2e-rc-hs72v" @ 08/12/23 13:42:28.69
  Aug 12 13:42:28.690: INFO: Get Replication Controller "e2e-rc-hs72v" to confirm replicas
  E0812 13:42:28.817895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:29.696: INFO: Get Replication Controller "e2e-rc-hs72v" to confirm replicas
  Aug 12 13:42:29.702: INFO: Found 2 replicas for "e2e-rc-hs72v" replication controller
  Aug 12 13:42:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2428" for this suite. @ 08/12/23 13:42:29.708
• [2.091 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/12/23 13:42:29.72
  Aug 12 13:42:29.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:42:29.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:42:29.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:42:29.742
  STEP: Creating the pod @ 08/12/23 13:42:29.747
  E0812 13:42:29.818268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:30.818695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:31.819493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:32.819851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:33.820297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:34.820836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:35.821894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:36.314: INFO: Successfully updated pod "annotationupdate32cfe9d0-f73e-4f35-ad93-d31ab991fb3f"
  E0812 13:42:36.822806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:37.822906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:42:38.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9659" for this suite. @ 08/12/23 13:42:38.343
• [8.633 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/12/23 13:42:38.356
  Aug 12 13:42:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:42:38.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:42:38.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:42:38.38
  STEP: Creating secret with name secret-test-map-dee3e85d-a99d-4e59-9811-2be6bea39784 @ 08/12/23 13:42:38.385
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:42:38.394
  E0812 13:42:38.823314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:39.823984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:40.825095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:41.825659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:42:42.43
  Aug 12 13:42:42.434: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-5e62a720-9f46-4fe3-9a1c-48d47a962142 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:42:42.444
  Aug 12 13:42:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6832" for this suite. @ 08/12/23 13:42:42.472
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/12/23 13:42:42.484
  Aug 12 13:42:42.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:42:42.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:42:42.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:42:42.507
  STEP: Creating secret with name s-test-opt-del-65548353-b051-4f32-8478-f700ddde3f26 @ 08/12/23 13:42:42.518
  STEP: Creating secret with name s-test-opt-upd-4e3b45ce-6a91-4f66-9024-5c6c6412ce6a @ 08/12/23 13:42:42.524
  STEP: Creating the pod @ 08/12/23 13:42:42.53
  E0812 13:42:42.826731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:43.827178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-65548353-b051-4f32-8478-f700ddde3f26 @ 08/12/23 13:42:44.591
  STEP: Updating secret s-test-opt-upd-4e3b45ce-6a91-4f66-9024-5c6c6412ce6a @ 08/12/23 13:42:44.601
  STEP: Creating secret with name s-test-opt-create-59ac9fc4-bffa-43a8-9331-857fd5e7f864 @ 08/12/23 13:42:44.608
  STEP: waiting to observe update in volume @ 08/12/23 13:42:44.614
  E0812 13:42:44.827849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:45.827971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:46.828140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:47.828420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:48.829410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:49.830368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:50.830585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:51.831447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:52.831576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:53.831701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:54.832747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:55.833789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:56.834099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:57.834246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:58.834966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:42:59.835113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:00.835585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:01.836046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:02.836142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:03.836480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:04.837081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:05.837791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:06.838614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:07.838914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:08.839923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:09.840256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:10.840937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:11.841749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:12.842873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:13.843193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:14.843960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:15.844082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:16.844290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:17.844716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:18.844737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:19.844914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:20.845835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:21.845965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:22.846261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:23.846373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:24.846668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:25.847465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:26.848010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:27.848114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:28.848677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:29.849331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:30.849428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:31.850322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:32.850751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:33.850846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:34.851244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:35.851395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:36.852416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:37.852748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:38.852966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:39.853782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:40.854525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:41.854690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:42.854796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:43.854864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:44.855291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:45.855827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:46.856067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:47.857138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:48.857275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:49.857777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:50.858102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:51.858258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:52.858362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:53.859345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:54.859571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:55.860367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:56.860686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:57.861051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:43:58.861196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:43:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1682" for this suite. @ 08/12/23 13:43:59.125
• [76.651 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/12/23 13:43:59.139
  Aug 12 13:43:59.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:43:59.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:43:59.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:43:59.163
  STEP: Creating secret with name projected-secret-test-feaff2ed-1abb-41f4-bed6-7f35f41ca44b @ 08/12/23 13:43:59.167
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:43:59.174
  E0812 13:43:59.861370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:00.861681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:01.862528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:02.863410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:44:03.205
  Aug 12 13:44:03.209: INFO: Trying to get logs from node ip-172-31-84-203 pod pod-projected-secrets-baf46abc-7360-424c-bb46-11939a4a776f container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:44:03.237
  Aug 12 13:44:03.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5670" for this suite. @ 08/12/23 13:44:03.279
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/12/23 13:44:03.294
  Aug 12 13:44:03.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename kubelet-test @ 08/12/23 13:44:03.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:44:03.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:44:03.318
  E0812 13:44:03.864290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:04.864672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:05.864862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:06.865778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:44:07.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9901" for this suite. @ 08/12/23 13:44:07.351
• [4.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/12/23 13:44:07.363
  Aug 12 13:44:07.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/12/23 13:44:07.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:44:07.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:44:07.388
  STEP: creating @ 08/12/23 13:44:07.392
  STEP: getting @ 08/12/23 13:44:07.414
  STEP: listing in namespace @ 08/12/23 13:44:07.419
  STEP: patching @ 08/12/23 13:44:07.424
  STEP: deleting @ 08/12/23 13:44:07.437
  Aug 12 13:44:07.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-4864" for this suite. @ 08/12/23 13:44:07.46
• [0.106 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/12/23 13:44:07.471
  Aug 12 13:44:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:44:07.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:44:07.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:44:07.498
  STEP: Creating secret with name secret-test-0bff1985-ccda-4bb4-97ee-f055fe7d292f @ 08/12/23 13:44:07.507
  STEP: Creating a pod to test consume secrets @ 08/12/23 13:44:07.515
  E0812 13:44:07.866509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:08.866518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:09.867390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:10.867768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:44:11.552
  Aug 12 13:44:11.557: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-secrets-98175a3e-3170-41b5-8e34-6784a933b830 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:44:11.566
  Aug 12 13:44:11.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1479" for this suite. @ 08/12/23 13:44:11.59
• [4.128 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/12/23 13:44:11.6
  Aug 12 13:44:11.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename downward-api @ 08/12/23 13:44:11.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:44:11.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:44:11.624
  STEP: Creating a pod to test downward API volume plugin @ 08/12/23 13:44:11.634
  E0812 13:44:11.867983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:12.868697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:13.869203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:14.869567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:44:15.671
  Aug 12 13:44:15.676: INFO: Trying to get logs from node ip-172-31-32-142 pod downwardapi-volume-1b3a9e39-d2db-43df-960b-e37ec64ec446 container client-container: <nil>
  STEP: delete the pod @ 08/12/23 13:44:15.687
  Aug 12 13:44:15.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2145" for this suite. @ 08/12/23 13:44:15.713
• [4.123 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/12/23 13:44:15.725
  Aug 12 13:44:15.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:44:15.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:44:15.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:44:15.754
  STEP: Creating configMap with name cm-test-opt-del-f796191f-18f8-482f-b5eb-ef1218ecdc67 @ 08/12/23 13:44:15.767
  STEP: Creating configMap with name cm-test-opt-upd-fc7a48b4-7123-4420-a49e-49f21fd8a83f @ 08/12/23 13:44:15.778
  STEP: Creating the pod @ 08/12/23 13:44:15.785
  E0812 13:44:15.870416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:16.870978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-f796191f-18f8-482f-b5eb-ef1218ecdc67 @ 08/12/23 13:44:17.844
  STEP: Updating configmap cm-test-opt-upd-fc7a48b4-7123-4420-a49e-49f21fd8a83f @ 08/12/23 13:44:17.853
  STEP: Creating configMap with name cm-test-opt-create-7ed61b7a-61b0-49a9-82e7-f1c6b274aca8 @ 08/12/23 13:44:17.861
  STEP: waiting to observe update in volume @ 08/12/23 13:44:17.868
  E0812 13:44:17.872026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:18.872802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:19.872929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:20.873070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:21.873517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:22.873578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:23.873676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:24.874242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:25.874368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:26.875473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:27.875796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:28.875886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:29.876211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:30.876689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:31.876814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:32.876938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:33.877774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:34.878308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:35.878421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:36.878545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:37.878860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:38.878942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:39.879442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:40.879560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:41.880305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:42.880704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:43.880841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:44.880948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:45.881070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:46.881216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:47.881800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:48.882612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:49.882886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:50.882900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:51.883060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:52.883068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:53.884133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:54.884279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:55.884397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:56.884786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:57.885799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:58.885894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:44:59.886364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:00.886483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:01.887547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:02.887698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:03.887846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:04.888831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:05.889758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:06.889885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:07.890149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:08.890284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:09.890410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:10.890564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:11.891091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:12.891142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:13.891272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:14.891522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:15.892018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:16.892953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:17.893284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:18.894163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:19.895256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:20.895523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:21.895612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:22.895829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:23.895889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:24.896007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:25.896727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:26.897602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:27.897779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:28.898459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:29.898808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:30.898884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:31.899015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:32.899873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:33.900831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:34.901117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:35.901516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:45:36.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5393" for this suite. @ 08/12/23 13:45:36.407
• [80.691 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/12/23 13:45:36.419
  Aug 12 13:45:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:45:36.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:45:36.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:45:36.446
  STEP: Creating secret with name s-test-opt-del-3898e383-6c0d-40eb-98af-4e7125aee730 @ 08/12/23 13:45:36.454
  STEP: Creating secret with name s-test-opt-upd-bd74a7c1-b7d2-44ad-8416-45e00cc4d97a @ 08/12/23 13:45:36.461
  STEP: Creating the pod @ 08/12/23 13:45:36.467
  E0812 13:45:36.901681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:37.902787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-3898e383-6c0d-40eb-98af-4e7125aee730 @ 08/12/23 13:45:38.534
  STEP: Updating secret s-test-opt-upd-bd74a7c1-b7d2-44ad-8416-45e00cc4d97a @ 08/12/23 13:45:38.543
  STEP: Creating secret with name s-test-opt-create-facc62cf-a356-4606-8433-5caa51f6fb5a @ 08/12/23 13:45:38.549
  STEP: waiting to observe update in volume @ 08/12/23 13:45:38.554
  E0812 13:45:38.903313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:39.903557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:40.904177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:41.904748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:42.905680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:43.905833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:44.906759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:45.907158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:46.907224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:47.907543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:48.908244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:49.908396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:50.909089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:51.909774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:52.909833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:53.909957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:54.910017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:55.910307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:56.910397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:57.910575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:58.911161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:45:59.911553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:00.912559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:01.912677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:02.913784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:03.914027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:04.914435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:05.914967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:06.915063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:07.915339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:08.915443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:09.915767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:10.916106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:11.916717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:12.917334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:13.917471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:14.918164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:15.918979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:16.919097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:17.919185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:18.919247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:19.919558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:20.920005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:21.920548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:22.920843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:23.921780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:24.922863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:25.923974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:26.924648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:27.924696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:28.924791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:29.925766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:30.926627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:31.927101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:32.927954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:33.928264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:34.928882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:35.929410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:36.929653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:37.929991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:38.930246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:39.930528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:40.930774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:41.930905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:42.931492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:43.931663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:44.932334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:45.933426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:46.933828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:47.934211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:48.934474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:49.934596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:50.935601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:51.935802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:52.936098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:53.936851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:54.936940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:55.937359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:56.937756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:57.938308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:58.938405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:46:59.939064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:00.939320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:01.939442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:02.939762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:03.940686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:04.940838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:05.941390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:06.941713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:07.942406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:08.942697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:09.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2992" for this suite. @ 08/12/23 13:47:09.143
• [92.733 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/12/23 13:47:09.153
  Aug 12 13:47:09.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename watch @ 08/12/23 13:47:09.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:09.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:09.177
  STEP: getting a starting resourceVersion @ 08/12/23 13:47:09.182
  STEP: starting a background goroutine to produce watch events @ 08/12/23 13:47:09.186
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/12/23 13:47:09.186
  E0812 13:47:09.943623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:10.943792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:11.944638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:11.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7209" for this suite. @ 08/12/23 13:47:12.011
• [2.912 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/12/23 13:47:12.07
  Aug 12 13:47:12.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:47:12.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:12.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:12.098
  STEP: Setting up server cert @ 08/12/23 13:47:12.131
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:47:12.507
  STEP: Deploying the webhook pod @ 08/12/23 13:47:12.517
  STEP: Wait for the deployment to be ready @ 08/12/23 13:47:12.531
  Aug 12 13:47:12.545: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:47:12.945620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:13.945949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:47:14.56
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:47:14.58
  E0812 13:47:14.946028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:15.580: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 12 13:47:15.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:47:15.946943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6663-crds.webhook.example.com via the AdmissionRegistration API @ 08/12/23 13:47:16.099
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/12/23 13:47:16.121
  E0812 13:47:16.948015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:17.948337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:18.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8051" for this suite. @ 08/12/23 13:47:18.755
  STEP: Destroying namespace "webhook-markers-8734" for this suite. @ 08/12/23 13:47:18.765
• [6.703 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/12/23 13:47:18.776
  Aug 12 13:47:18.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename secrets @ 08/12/23 13:47:18.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:18.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:18.809
  Aug 12 13:47:18.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2165" for this suite. @ 08/12/23 13:47:18.872
• [0.105 seconds]
------------------------------
SS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/12/23 13:47:18.882
  Aug 12 13:47:18.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/12/23 13:47:18.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:18.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:18.906
  STEP: getting /apis @ 08/12/23 13:47:18.912
  STEP: getting /apis/storage.k8s.io @ 08/12/23 13:47:18.922
  STEP: getting /apis/storage.k8s.io/v1 @ 08/12/23 13:47:18.927
  STEP: creating @ 08/12/23 13:47:18.929
  E0812 13:47:18.948898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: watching @ 08/12/23 13:47:18.959
  Aug 12 13:47:18.959: INFO: starting watch
  STEP: getting @ 08/12/23 13:47:18.974
  STEP: listing in namespace @ 08/12/23 13:47:18.98
  STEP: listing across namespaces @ 08/12/23 13:47:18.986
  STEP: patching @ 08/12/23 13:47:18.99
  STEP: updating @ 08/12/23 13:47:18.998
  Aug 12 13:47:19.008: INFO: waiting for watch events with expected annotations in namespace
  Aug 12 13:47:19.008: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/12/23 13:47:19.008
  STEP: deleting a collection @ 08/12/23 13:47:19.032
  Aug 12 13:47:19.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1573" for this suite. @ 08/12/23 13:47:19.071
• [0.199 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/12/23 13:47:19.082
  Aug 12 13:47:19.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename subpath @ 08/12/23 13:47:19.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:19.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:19.114
  STEP: Setting up data @ 08/12/23 13:47:19.122
  STEP: Creating pod pod-subpath-test-secret-vk46 @ 08/12/23 13:47:19.135
  STEP: Creating a pod to test atomic-volume-subpath @ 08/12/23 13:47:19.135
  E0812 13:47:19.949772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:20.950385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:21.951267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:22.951406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:23.951560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:24.951686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:25.952224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:26.952360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:27.952654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:28.953783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:29.953881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:30.954287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:31.954431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:32.955154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:33.956061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:34.956173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:35.957210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:36.957747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:37.957877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:38.958011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:39.958530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:40.959281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:41.960037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:42.960258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:47:43.255
  Aug 12 13:47:43.259: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-subpath-test-secret-vk46 container test-container-subpath-secret-vk46: <nil>
  STEP: delete the pod @ 08/12/23 13:47:43.283
  STEP: Deleting pod pod-subpath-test-secret-vk46 @ 08/12/23 13:47:43.306
  Aug 12 13:47:43.306: INFO: Deleting pod "pod-subpath-test-secret-vk46" in namespace "subpath-3205"
  Aug 12 13:47:43.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3205" for this suite. @ 08/12/23 13:47:43.316
• [24.242 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/12/23 13:47:43.325
  Aug 12 13:47:43.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:47:43.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:43.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:43.35
  STEP: Setting up server cert @ 08/12/23 13:47:43.379
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:47:43.934
  STEP: Deploying the webhook pod @ 08/12/23 13:47:43.947
  E0812 13:47:43.960582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 08/12/23 13:47:43.962
  Aug 12 13:47:43.972: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:47:44.960789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:45.961429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:47:45.996
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:47:46.011
  E0812 13:47:46.961672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:47.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/12/23 13:47:47.018
  STEP: create a configmap that should be updated by the webhook @ 08/12/23 13:47:47.039
  Aug 12 13:47:47.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2824" for this suite. @ 08/12/23 13:47:47.135
  STEP: Destroying namespace "webhook-markers-3127" for this suite. @ 08/12/23 13:47:47.145
• [3.828 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/12/23 13:47:47.154
  Aug 12 13:47:47.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:47:47.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:47.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:47.178
  Aug 12 13:47:47.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:47:47.962182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/12/23 13:47:48.569
  Aug 12 13:47:48.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-6293 --namespace=crd-publish-openapi-6293 create -f -'
  E0812 13:47:48.962872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:49.453: INFO: stderr: ""
  Aug 12 13:47:49.453: INFO: stdout: "e2e-test-crd-publish-openapi-2090-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 12 13:47:49.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-6293 --namespace=crd-publish-openapi-6293 delete e2e-test-crd-publish-openapi-2090-crds test-cr'
  Aug 12 13:47:49.545: INFO: stderr: ""
  Aug 12 13:47:49.545: INFO: stdout: "e2e-test-crd-publish-openapi-2090-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug 12 13:47:49.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-6293 --namespace=crd-publish-openapi-6293 apply -f -'
  E0812 13:47:49.963101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:50.323: INFO: stderr: ""
  Aug 12 13:47:50.323: INFO: stdout: "e2e-test-crd-publish-openapi-2090-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 12 13:47:50.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-6293 --namespace=crd-publish-openapi-6293 delete e2e-test-crd-publish-openapi-2090-crds test-cr'
  Aug 12 13:47:50.416: INFO: stderr: ""
  Aug 12 13:47:50.416: INFO: stdout: "e2e-test-crd-publish-openapi-2090-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/12/23 13:47:50.416
  Aug 12 13:47:50.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=crd-publish-openapi-6293 explain e2e-test-crd-publish-openapi-2090-crds'
  Aug 12 13:47:50.713: INFO: stderr: ""
  Aug 12 13:47:50.713: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-2090-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0812 13:47:50.964049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:51.965111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:47:52.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6293" for this suite. @ 08/12/23 13:47:52.109
• [4.964 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/12/23 13:47:52.12
  Aug 12 13:47:52.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename watch @ 08/12/23 13:47:52.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:47:52.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:47:52.146
  STEP: creating a watch on configmaps with a certain label @ 08/12/23 13:47:52.151
  STEP: creating a new configmap @ 08/12/23 13:47:52.152
  STEP: modifying the configmap once @ 08/12/23 13:47:52.159
  STEP: changing the label value of the configmap @ 08/12/23 13:47:52.17
  STEP: Expecting to observe a delete notification for the watched object @ 08/12/23 13:47:52.181
  Aug 12 13:47:52.181: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42042 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:47:52.181: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42043 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:47:52.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42044 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:47:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/12/23 13:47:52.181
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/12/23 13:47:52.192
  E0812 13:47:52.965819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:53.965918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:54.966469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:55.967242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:56.967736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:57.967883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:58.968006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:47:59.968455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:00.968992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:01.969114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 08/12/23 13:48:02.192
  STEP: modifying the configmap a third time @ 08/12/23 13:48:02.204
  STEP: deleting the configmap @ 08/12/23 13:48:02.215
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/12/23 13:48:02.223
  Aug 12 13:48:02.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42090 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:48:02.224: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42091 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:48:02.224: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6300  ffb5c927-939f-4ffa-928c-d14a3caffec6 42092 0 2023-08-12 13:47:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-12 13:48:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 12 13:48:02.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6300" for this suite. @ 08/12/23 13:48:02.23
• [10.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/12/23 13:48:02.25
  Aug 12 13:48:02.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/12/23 13:48:02.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:02.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:02.278
  STEP: set up a multi version CRD @ 08/12/23 13:48:02.283
  Aug 12 13:48:02.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:48:02.970088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:03.970474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:04.971561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:05.971983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 08/12/23 13:48:05.98
  STEP: check the unserved version gets removed @ 08/12/23 13:48:06.005
  STEP: check the other version is not changed @ 08/12/23 13:48:06.969
  E0812 13:48:06.972001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:07.972489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:08.973160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:09.973980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:10.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7070" for this suite. @ 08/12/23 13:48:10.101
• [7.862 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/12/23 13:48:10.116
  Aug 12 13:48:10.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename proxy @ 08/12/23 13:48:10.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:10.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:10.157
  Aug 12 13:48:10.165: INFO: Creating pod...
  E0812 13:48:10.974482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:11.974565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:12.187: INFO: Creating service...
  Aug 12 13:48:12.206: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=DELETE
  Aug 12 13:48:12.221: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 12 13:48:12.221: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=OPTIONS
  Aug 12 13:48:12.227: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 12 13:48:12.227: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=PATCH
  Aug 12 13:48:12.232: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 12 13:48:12.232: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=POST
  Aug 12 13:48:12.239: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 12 13:48:12.239: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=PUT
  Aug 12 13:48:12.244: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 12 13:48:12.244: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug 12 13:48:12.252: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 12 13:48:12.252: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug 12 13:48:12.261: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 12 13:48:12.261: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug 12 13:48:12.269: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 12 13:48:12.269: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=POST
  Aug 12 13:48:12.276: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 12 13:48:12.276: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=PUT
  Aug 12 13:48:12.284: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 12 13:48:12.284: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=GET
  Aug 12 13:48:12.288: INFO: http.Client request:GET StatusCode:301
  Aug 12 13:48:12.288: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=GET
  Aug 12 13:48:12.295: INFO: http.Client request:GET StatusCode:301
  Aug 12 13:48:12.296: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/pods/agnhost/proxy?method=HEAD
  Aug 12 13:48:12.300: INFO: http.Client request:HEAD StatusCode:301
  Aug 12 13:48:12.300: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7403/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug 12 13:48:12.306: INFO: http.Client request:HEAD StatusCode:301
  Aug 12 13:48:12.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7403" for this suite. @ 08/12/23 13:48:12.311
• [2.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/12/23 13:48:12.328
  Aug 12 13:48:12.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/12/23 13:48:12.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:12.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:12.36
  STEP: Setting up the test @ 08/12/23 13:48:12.371
  STEP: Creating hostNetwork=false pod @ 08/12/23 13:48:12.372
  E0812 13:48:12.974773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:13.974851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 08/12/23 13:48:14.405
  E0812 13:48:14.975527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:15.976414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 08/12/23 13:48:16.427
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/12/23 13:48:16.427
  Aug 12 13:48:16.427: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.428: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.428: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 12 13:48:16.509: INFO: Exec stderr: ""
  Aug 12 13:48:16.509: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.510: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.510: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 12 13:48:16.584: INFO: Exec stderr: ""
  Aug 12 13:48:16.585: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.585: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.585: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 12 13:48:16.660: INFO: Exec stderr: ""
  Aug 12 13:48:16.661: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.662: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.662: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 12 13:48:16.740: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/12/23 13:48:16.74
  Aug 12 13:48:16.740: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.742: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.742: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 12 13:48:16.820: INFO: Exec stderr: ""
  Aug 12 13:48:16.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.821: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.821: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 12 13:48:16.896: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/12/23 13:48:16.897
  Aug 12 13:48:16.897: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:16.898: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.898: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 12 13:48:16.976: INFO: Exec stderr: ""
  Aug 12 13:48:16.977: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:16.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:48:16.976984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:16.978: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:16.978: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 12 13:48:17.052: INFO: Exec stderr: ""
  Aug 12 13:48:17.052: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:17.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:17.053: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:17.053: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 12 13:48:17.124: INFO: Exec stderr: ""
  Aug 12 13:48:17.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1353 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 12 13:48:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  Aug 12 13:48:17.126: INFO: ExecWithOptions: Clientset creation
  Aug 12 13:48:17.126: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1353/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 12 13:48:17.201: INFO: Exec stderr: ""
  Aug 12 13:48:17.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-1353" for this suite. @ 08/12/23 13:48:17.207
• [4.887 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/12/23 13:48:17.215
  Aug 12 13:48:17.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:48:17.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:17.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:17.238
  STEP: Setting up server cert @ 08/12/23 13:48:17.265
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:48:17.564
  STEP: Deploying the webhook pod @ 08/12/23 13:48:17.575
  STEP: Wait for the deployment to be ready @ 08/12/23 13:48:17.591
  Aug 12 13:48:17.606: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:48:17.977597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:18.978661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:48:19.623
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:48:19.637
  E0812 13:48:19.978724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:20.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 12 13:48:20.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:48:20.979275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/12/23 13:48:21.16
  STEP: Creating a custom resource that should be denied by the webhook @ 08/12/23 13:48:21.179
  E0812 13:48:21.979360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:22.979639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/12/23 13:48:23.214
  STEP: Updating the custom resource with disallowed data should be denied @ 08/12/23 13:48:23.222
  STEP: Deleting the custom resource should be denied @ 08/12/23 13:48:23.236
  STEP: Remove the offending key and value from the custom resource data @ 08/12/23 13:48:23.245
  STEP: Deleting the updated custom resource should be successful @ 08/12/23 13:48:23.257
  Aug 12 13:48:23.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3143" for this suite. @ 08/12/23 13:48:23.884
  STEP: Destroying namespace "webhook-markers-5017" for this suite. @ 08/12/23 13:48:23.894
• [6.687 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/12/23 13:48:23.906
  Aug 12 13:48:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename resourcequota @ 08/12/23 13:48:23.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:23.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:23.935
  STEP: Creating a ResourceQuota @ 08/12/23 13:48:23.94
  STEP: Getting a ResourceQuota @ 08/12/23 13:48:23.946
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/12/23 13:48:23.951
  STEP: Patching the ResourceQuota @ 08/12/23 13:48:23.954
  STEP: Deleting a Collection of ResourceQuotas @ 08/12/23 13:48:23.963
  STEP: Verifying the deleted ResourceQuota @ 08/12/23 13:48:23.977
  E0812 13:48:23.980005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:23.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1001" for this suite. @ 08/12/23 13:48:23.988
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/12/23 13:48:24.001
  Aug 12 13:48:24.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-runtime @ 08/12/23 13:48:24.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:24.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:24.028
  STEP: create the container @ 08/12/23 13:48:24.031
  W0812 13:48:24.044647      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/12/23 13:48:24.044
  E0812 13:48:24.980796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:25.981146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:26.981276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:27.981808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/12/23 13:48:28.074
  STEP: the container should be terminated @ 08/12/23 13:48:28.079
  STEP: the termination message should be set @ 08/12/23 13:48:28.08
  Aug 12 13:48:28.080: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/12/23 13:48:28.08
  Aug 12 13:48:28.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1882" for this suite. @ 08/12/23 13:48:28.107
• [4.115 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/12/23 13:48:28.117
  Aug 12 13:48:28.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename conformance-tests @ 08/12/23 13:48:28.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:28.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:28.145
  STEP: Getting node addresses @ 08/12/23 13:48:28.148
  Aug 12 13:48:28.148: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug 12 13:48:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-6955" for this suite. @ 08/12/23 13:48:28.163
• [0.057 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/12/23 13:48:28.175
  Aug 12 13:48:28.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename proxy @ 08/12/23 13:48:28.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:28.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:28.198
  STEP: starting an echo server on multiple ports @ 08/12/23 13:48:28.223
  STEP: creating replication controller proxy-service-chvhl in namespace proxy-9068 @ 08/12/23 13:48:28.224
  I0812 13:48:28.238729      19 runners.go:194] Created replication controller with name: proxy-service-chvhl, namespace: proxy-9068, replica count: 1
  E0812 13:48:28.982635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:48:29.290298      19 runners.go:194] proxy-service-chvhl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0812 13:48:29.983065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:48:30.290566      19 runners.go:194] proxy-service-chvhl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0812 13:48:30.983941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:48:31.291605      19 runners.go:194] proxy-service-chvhl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 13:48:31.297: INFO: setup took 3.095061149s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/12/23 13:48:31.298
  Aug 12 13:48:31.307: INFO: (0) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.69887ms)
  Aug 12 13:48:31.308: INFO: (0) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 9.977838ms)
  Aug 12 13:48:31.309: INFO: (0) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 10.863415ms)
  Aug 12 13:48:31.311: INFO: (0) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 12.172708ms)
  Aug 12 13:48:31.313: INFO: (0) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 14.084783ms)
  Aug 12 13:48:31.313: INFO: (0) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 14.907977ms)
  Aug 12 13:48:31.316: INFO: (0) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 17.950319ms)
  Aug 12 13:48:31.321: INFO: (0) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 22.043943ms)
  Aug 12 13:48:31.322: INFO: (0) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 23.249092ms)
  Aug 12 13:48:31.322: INFO: (0) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 23.17242ms)
  Aug 12 13:48:31.322: INFO: (0) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 23.73688ms)
  Aug 12 13:48:31.323: INFO: (0) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 24.3463ms)
  Aug 12 13:48:31.324: INFO: (0) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 24.88422ms)
  Aug 12 13:48:31.324: INFO: (0) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 25.208554ms)
  Aug 12 13:48:31.327: INFO: (0) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 28.329508ms)
  Aug 12 13:48:31.327: INFO: (0) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 28.947953ms)
  Aug 12 13:48:31.334: INFO: (1) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 6.982638ms)
  Aug 12 13:48:31.335: INFO: (1) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 7.278889ms)
  Aug 12 13:48:31.337: INFO: (1) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 9.931654ms)
  Aug 12 13:48:31.339: INFO: (1) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 11.378217ms)
  Aug 12 13:48:31.340: INFO: (1) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 12.483545ms)
  Aug 12 13:48:31.340: INFO: (1) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.610428ms)
  Aug 12 13:48:31.342: INFO: (1) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 14.171142ms)
  Aug 12 13:48:31.342: INFO: (1) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 14.189882ms)
  Aug 12 13:48:31.343: INFO: (1) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 15.62633ms)
  Aug 12 13:48:31.343: INFO: (1) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 15.547049ms)
  Aug 12 13:48:31.344: INFO: (1) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.682195ms)
  Aug 12 13:48:31.344: INFO: (1) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 15.975023ms)
  Aug 12 13:48:31.344: INFO: (1) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 16.426328ms)
  Aug 12 13:48:31.344: INFO: (1) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 16.75172ms)
  Aug 12 13:48:31.345: INFO: (1) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 17.285297ms)
  Aug 12 13:48:31.345: INFO: (1) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 17.510034ms)
  Aug 12 13:48:31.353: INFO: (2) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 6.722262ms)
  Aug 12 13:48:31.354: INFO: (2) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 7.667561ms)
  Aug 12 13:48:31.355: INFO: (2) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 8.837691ms)
  Aug 12 13:48:31.357: INFO: (2) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.757628ms)
  Aug 12 13:48:31.357: INFO: (2) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 11.516345ms)
  Aug 12 13:48:31.358: INFO: (2) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.539048ms)
  Aug 12 13:48:31.359: INFO: (2) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.925506ms)
  Aug 12 13:48:31.359: INFO: (2) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 13.298935ms)
  Aug 12 13:48:31.359: INFO: (2) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 13.4013ms)
  Aug 12 13:48:31.360: INFO: (2) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 13.230486ms)
  Aug 12 13:48:31.360: INFO: (2) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 14.185635ms)
  Aug 12 13:48:31.361: INFO: (2) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 14.623828ms)
  Aug 12 13:48:31.361: INFO: (2) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 15.218694ms)
  Aug 12 13:48:31.362: INFO: (2) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.605018ms)
  Aug 12 13:48:31.362: INFO: (2) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 16.175818ms)
  Aug 12 13:48:31.363: INFO: (2) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 17.11941ms)
  Aug 12 13:48:31.371: INFO: (3) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 6.927246ms)
  Aug 12 13:48:31.371: INFO: (3) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 7.755079ms)
  Aug 12 13:48:31.375: INFO: (3) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.283886ms)
  Aug 12 13:48:31.375: INFO: (3) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.026208ms)
  Aug 12 13:48:31.376: INFO: (3) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 11.857007ms)
  Aug 12 13:48:31.377: INFO: (3) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 12.974511ms)
  Aug 12 13:48:31.378: INFO: (3) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 13.80078ms)
  Aug 12 13:48:31.378: INFO: (3) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 13.917934ms)
  Aug 12 13:48:31.379: INFO: (3) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 15.038663ms)
  Aug 12 13:48:31.379: INFO: (3) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 14.970442ms)
  Aug 12 13:48:31.379: INFO: (3) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 14.900467ms)
  Aug 12 13:48:31.380: INFO: (3) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.061448ms)
  Aug 12 13:48:31.380: INFO: (3) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.509284ms)
  Aug 12 13:48:31.380: INFO: (3) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 16.153619ms)
  Aug 12 13:48:31.381: INFO: (3) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 16.869398ms)
  Aug 12 13:48:31.381: INFO: (3) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.215743ms)
  Aug 12 13:48:31.390: INFO: (4) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 9.132043ms)
  Aug 12 13:48:31.391: INFO: (4) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 9.906697ms)
  Aug 12 13:48:31.394: INFO: (4) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 11.818136ms)
  Aug 12 13:48:31.394: INFO: (4) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 11.769752ms)
  Aug 12 13:48:31.396: INFO: (4) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 14.168574ms)
  Aug 12 13:48:31.397: INFO: (4) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 15.190391ms)
  Aug 12 13:48:31.398: INFO: (4) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.624439ms)
  Aug 12 13:48:31.398: INFO: (4) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 16.245189ms)
  Aug 12 13:48:31.398: INFO: (4) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.378814ms)
  Aug 12 13:48:31.399: INFO: (4) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 17.917214ms)
  Aug 12 13:48:31.399: INFO: (4) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 17.688121ms)
  Aug 12 13:48:31.400: INFO: (4) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 18.476663ms)
  Aug 12 13:48:31.400: INFO: (4) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 18.220335ms)
  Aug 12 13:48:31.400: INFO: (4) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 18.832909ms)
  Aug 12 13:48:31.401: INFO: (4) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 18.476224ms)
  Aug 12 13:48:31.401: INFO: (4) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 19.097525ms)
  Aug 12 13:48:31.409: INFO: (5) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 7.367426ms)
  Aug 12 13:48:31.410: INFO: (5) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 8.317144ms)
  Aug 12 13:48:31.412: INFO: (5) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 10.44068ms)
  Aug 12 13:48:31.413: INFO: (5) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.924814ms)
  Aug 12 13:48:31.413: INFO: (5) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 11.29475ms)
  Aug 12 13:48:31.413: INFO: (5) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 11.177882ms)
  Aug 12 13:48:31.414: INFO: (5) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 12.232723ms)
  Aug 12 13:48:31.414: INFO: (5) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 12.248615ms)
  Aug 12 13:48:31.415: INFO: (5) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 13.116829ms)
  Aug 12 13:48:31.415: INFO: (5) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 13.612669ms)
  Aug 12 13:48:31.416: INFO: (5) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 14.299204ms)
  Aug 12 13:48:31.417: INFO: (5) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 14.982865ms)
  Aug 12 13:48:31.418: INFO: (5) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 15.653726ms)
  Aug 12 13:48:31.418: INFO: (5) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 15.868313ms)
  Aug 12 13:48:31.419: INFO: (5) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 17.015998ms)
  Aug 12 13:48:31.420: INFO: (5) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 18.244593ms)
  Aug 12 13:48:31.427: INFO: (6) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 7.023778ms)
  Aug 12 13:48:31.429: INFO: (6) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.407056ms)
  Aug 12 13:48:31.429: INFO: (6) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 8.715444ms)
  Aug 12 13:48:31.429: INFO: (6) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 8.36697ms)
  Aug 12 13:48:31.431: INFO: (6) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 9.677917ms)
  Aug 12 13:48:31.433: INFO: (6) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.460688ms)
  Aug 12 13:48:31.433: INFO: (6) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 12.592358ms)
  Aug 12 13:48:31.434: INFO: (6) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 12.526707ms)
  Aug 12 13:48:31.434: INFO: (6) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 13.355863ms)
  Aug 12 13:48:31.435: INFO: (6) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 14.583224ms)
  Aug 12 13:48:31.437: INFO: (6) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.217417ms)
  Aug 12 13:48:31.437: INFO: (6) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.737819ms)
  Aug 12 13:48:31.437: INFO: (6) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.325235ms)
  Aug 12 13:48:31.437: INFO: (6) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 16.872814ms)
  Aug 12 13:48:31.437: INFO: (6) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 16.830651ms)
  Aug 12 13:48:31.438: INFO: (6) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 16.994507ms)
  Aug 12 13:48:31.446: INFO: (7) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 7.459936ms)
  Aug 12 13:48:31.447: INFO: (7) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 8.181295ms)
  Aug 12 13:48:31.447: INFO: (7) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 8.376389ms)
  Aug 12 13:48:31.448: INFO: (7) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 9.557742ms)
  Aug 12 13:48:31.449: INFO: (7) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.23505ms)
  Aug 12 13:48:31.450: INFO: (7) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.858377ms)
  Aug 12 13:48:31.451: INFO: (7) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 12.40706ms)
  Aug 12 13:48:31.451: INFO: (7) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 12.47753ms)
  Aug 12 13:48:31.452: INFO: (7) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 13.467709ms)
  Aug 12 13:48:31.452: INFO: (7) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 13.988122ms)
  Aug 12 13:48:31.452: INFO: (7) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 13.789237ms)
  Aug 12 13:48:31.453: INFO: (7) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 14.316824ms)
  Aug 12 13:48:31.454: INFO: (7) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 14.956292ms)
  Aug 12 13:48:31.454: INFO: (7) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.142688ms)
  Aug 12 13:48:31.455: INFO: (7) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.159301ms)
  Aug 12 13:48:31.455: INFO: (7) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 16.56604ms)
  Aug 12 13:48:31.463: INFO: (8) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.565729ms)
  Aug 12 13:48:31.464: INFO: (8) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 8.345554ms)
  Aug 12 13:48:31.465: INFO: (8) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 9.377905ms)
  Aug 12 13:48:31.466: INFO: (8) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 10.536537ms)
  Aug 12 13:48:31.466: INFO: (8) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.182025ms)
  Aug 12 13:48:31.468: INFO: (8) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 12.136283ms)
  Aug 12 13:48:31.469: INFO: (8) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 12.362521ms)
  Aug 12 13:48:31.469: INFO: (8) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 13.590656ms)
  Aug 12 13:48:31.470: INFO: (8) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 14.214457ms)
  Aug 12 13:48:31.471: INFO: (8) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 14.432656ms)
  Aug 12 13:48:31.471: INFO: (8) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 15.501551ms)
  Aug 12 13:48:31.471: INFO: (8) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.386648ms)
  Aug 12 13:48:31.471: INFO: (8) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 14.550868ms)
  Aug 12 13:48:31.472: INFO: (8) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 15.314325ms)
  Aug 12 13:48:31.472: INFO: (8) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.648216ms)
  Aug 12 13:48:31.475: INFO: (8) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 19.171742ms)
  Aug 12 13:48:31.482: INFO: (9) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 7.444125ms)
  Aug 12 13:48:31.484: INFO: (9) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 8.457336ms)
  Aug 12 13:48:31.484: INFO: (9) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.50663ms)
  Aug 12 13:48:31.485: INFO: (9) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 9.343673ms)
  Aug 12 13:48:31.486: INFO: (9) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.141322ms)
  Aug 12 13:48:31.487: INFO: (9) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 10.927253ms)
  Aug 12 13:48:31.488: INFO: (9) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.772592ms)
  Aug 12 13:48:31.489: INFO: (9) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 14.150785ms)
  Aug 12 13:48:31.490: INFO: (9) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 14.068932ms)
  Aug 12 13:48:31.491: INFO: (9) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 14.938148ms)
  Aug 12 13:48:31.491: INFO: (9) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 15.660548ms)
  Aug 12 13:48:31.491: INFO: (9) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.798837ms)
  Aug 12 13:48:31.491: INFO: (9) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 15.285829ms)
  Aug 12 13:48:31.491: INFO: (9) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 15.921846ms)
  Aug 12 13:48:31.492: INFO: (9) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 15.896063ms)
  Aug 12 13:48:31.492: INFO: (9) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 16.397996ms)
  Aug 12 13:48:31.499: INFO: (10) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 6.991333ms)
  Aug 12 13:48:31.500: INFO: (10) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 7.495136ms)
  Aug 12 13:48:31.501: INFO: (10) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 8.335287ms)
  Aug 12 13:48:31.502: INFO: (10) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 8.985338ms)
  Aug 12 13:48:31.504: INFO: (10) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.680341ms)
  Aug 12 13:48:31.504: INFO: (10) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 11.247566ms)
  Aug 12 13:48:31.505: INFO: (10) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 11.575913ms)
  Aug 12 13:48:31.506: INFO: (10) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 12.867765ms)
  Aug 12 13:48:31.506: INFO: (10) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 12.997731ms)
  Aug 12 13:48:31.507: INFO: (10) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 13.569863ms)
  Aug 12 13:48:31.507: INFO: (10) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 14.796999ms)
  Aug 12 13:48:31.508: INFO: (10) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.516449ms)
  Aug 12 13:48:31.509: INFO: (10) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.678613ms)
  Aug 12 13:48:31.509: INFO: (10) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.476026ms)
  Aug 12 13:48:31.512: INFO: (10) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 18.730191ms)
  Aug 12 13:48:31.512: INFO: (10) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 18.938665ms)
  Aug 12 13:48:31.520: INFO: (11) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 7.726093ms)
  Aug 12 13:48:31.521: INFO: (11) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 9.062811ms)
  Aug 12 13:48:31.522: INFO: (11) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 9.448216ms)
  Aug 12 13:48:31.523: INFO: (11) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 10.000717ms)
  Aug 12 13:48:31.524: INFO: (11) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 10.829779ms)
  Aug 12 13:48:31.524: INFO: (11) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 11.628824ms)
  Aug 12 13:48:31.525: INFO: (11) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 12.220586ms)
  Aug 12 13:48:31.525: INFO: (11) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 12.507691ms)
  Aug 12 13:48:31.527: INFO: (11) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.203172ms)
  Aug 12 13:48:31.528: INFO: (11) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 14.970444ms)
  Aug 12 13:48:31.528: INFO: (11) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 15.132118ms)
  Aug 12 13:48:31.528: INFO: (11) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 15.53251ms)
  Aug 12 13:48:31.529: INFO: (11) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.405123ms)
  Aug 12 13:48:31.529: INFO: (11) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 15.944688ms)
  Aug 12 13:48:31.529: INFO: (11) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 16.555303ms)
  Aug 12 13:48:31.530: INFO: (11) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 17.70759ms)
  Aug 12 13:48:31.538: INFO: (12) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.489975ms)
  Aug 12 13:48:31.538: INFO: (12) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 7.747581ms)
  Aug 12 13:48:31.540: INFO: (12) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.973408ms)
  Aug 12 13:48:31.541: INFO: (12) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 9.986293ms)
  Aug 12 13:48:31.541: INFO: (12) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 10.066849ms)
  Aug 12 13:48:31.541: INFO: (12) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 10.358889ms)
  Aug 12 13:48:31.545: INFO: (12) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 14.486143ms)
  Aug 12 13:48:31.545: INFO: (12) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 14.425026ms)
  Aug 12 13:48:31.547: INFO: (12) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 16.161052ms)
  Aug 12 13:48:31.548: INFO: (12) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 17.279008ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 17.787998ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 18.076876ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 18.320361ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.896398ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 18.048302ms)
  Aug 12 13:48:31.549: INFO: (12) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 18.026375ms)
  Aug 12 13:48:31.558: INFO: (13) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 7.628065ms)
  Aug 12 13:48:31.558: INFO: (13) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.018237ms)
  Aug 12 13:48:31.559: INFO: (13) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 8.991635ms)
  Aug 12 13:48:31.560: INFO: (13) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 9.604321ms)
  Aug 12 13:48:31.560: INFO: (13) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 10.040838ms)
  Aug 12 13:48:31.561: INFO: (13) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 10.516648ms)
  Aug 12 13:48:31.562: INFO: (13) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 11.170132ms)
  Aug 12 13:48:31.563: INFO: (13) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 12.44435ms)
  Aug 12 13:48:31.563: INFO: (13) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 13.261237ms)
  Aug 12 13:48:31.564: INFO: (13) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 13.836285ms)
  Aug 12 13:48:31.564: INFO: (13) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 13.795253ms)
  Aug 12 13:48:31.565: INFO: (13) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 15.096485ms)
  Aug 12 13:48:31.565: INFO: (13) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.321714ms)
  Aug 12 13:48:31.568: INFO: (13) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 17.415244ms)
  Aug 12 13:48:31.568: INFO: (13) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.780068ms)
  Aug 12 13:48:31.568: INFO: (13) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 18.030524ms)
  Aug 12 13:48:31.576: INFO: (14) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.197141ms)
  Aug 12 13:48:31.576: INFO: (14) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 7.968311ms)
  Aug 12 13:48:31.579: INFO: (14) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 9.765577ms)
  Aug 12 13:48:31.581: INFO: (14) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 11.351814ms)
  Aug 12 13:48:31.581: INFO: (14) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 11.82002ms)
  Aug 12 13:48:31.581: INFO: (14) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 11.952881ms)
  Aug 12 13:48:31.583: INFO: (14) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 13.821481ms)
  Aug 12 13:48:31.583: INFO: (14) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 14.341721ms)
  Aug 12 13:48:31.584: INFO: (14) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 14.814558ms)
  Aug 12 13:48:31.584: INFO: (14) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 15.061441ms)
  Aug 12 13:48:31.584: INFO: (14) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 15.339048ms)
  Aug 12 13:48:31.585: INFO: (14) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 16.281606ms)
  Aug 12 13:48:31.585: INFO: (14) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.990138ms)
  Aug 12 13:48:31.586: INFO: (14) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 17.376926ms)
  Aug 12 13:48:31.587: INFO: (14) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.866577ms)
  Aug 12 13:48:31.587: INFO: (14) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 18.03168ms)
  Aug 12 13:48:31.595: INFO: (15) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 6.915999ms)
  Aug 12 13:48:31.596: INFO: (15) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.851448ms)
  Aug 12 13:48:31.598: INFO: (15) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 10.043215ms)
  Aug 12 13:48:31.599: INFO: (15) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 11.698285ms)
  Aug 12 13:48:31.602: INFO: (15) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 13.99585ms)
  Aug 12 13:48:31.602: INFO: (15) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 13.593634ms)
  Aug 12 13:48:31.602: INFO: (15) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 13.727619ms)
  Aug 12 13:48:31.602: INFO: (15) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 13.571071ms)
  Aug 12 13:48:31.604: INFO: (15) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 15.805255ms)
  Aug 12 13:48:31.604: INFO: (15) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 15.870416ms)
  Aug 12 13:48:31.604: INFO: (15) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 16.185053ms)
  Aug 12 13:48:31.605: INFO: (15) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.946681ms)
  Aug 12 13:48:31.605: INFO: (15) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 16.927776ms)
  Aug 12 13:48:31.606: INFO: (15) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.165619ms)
  Aug 12 13:48:31.606: INFO: (15) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 17.355295ms)
  Aug 12 13:48:31.606: INFO: (15) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 17.785088ms)
  Aug 12 13:48:31.615: INFO: (16) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 8.893792ms)
  Aug 12 13:48:31.616: INFO: (16) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 9.91856ms)
  Aug 12 13:48:31.619: INFO: (16) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.201484ms)
  Aug 12 13:48:31.619: INFO: (16) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 12.631077ms)
  Aug 12 13:48:31.620: INFO: (16) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 12.342414ms)
  Aug 12 13:48:31.620: INFO: (16) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 13.073253ms)
  Aug 12 13:48:31.620: INFO: (16) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 13.044134ms)
  Aug 12 13:48:31.620: INFO: (16) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 13.185187ms)
  Aug 12 13:48:31.622: INFO: (16) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.463958ms)
  Aug 12 13:48:31.622: INFO: (16) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.063321ms)
  Aug 12 13:48:31.622: INFO: (16) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 15.920099ms)
  Aug 12 13:48:31.623: INFO: (16) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 15.487941ms)
  Aug 12 13:48:31.623: INFO: (16) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 15.680403ms)
  Aug 12 13:48:31.624: INFO: (16) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.783882ms)
  Aug 12 13:48:31.625: INFO: (16) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 17.725052ms)
  Aug 12 13:48:31.625: INFO: (16) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 17.851585ms)
  Aug 12 13:48:31.633: INFO: (17) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 7.624329ms)
  Aug 12 13:48:31.633: INFO: (17) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 8.07245ms)
  Aug 12 13:48:31.637: INFO: (17) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 11.706858ms)
  Aug 12 13:48:31.638: INFO: (17) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 12.626823ms)
  Aug 12 13:48:31.638: INFO: (17) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 12.499787ms)
  Aug 12 13:48:31.639: INFO: (17) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.743656ms)
  Aug 12 13:48:31.639: INFO: (17) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 13.657379ms)
  Aug 12 13:48:31.641: INFO: (17) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 15.509911ms)
  Aug 12 13:48:31.641: INFO: (17) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 15.351641ms)
  Aug 12 13:48:31.641: INFO: (17) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 15.902496ms)
  Aug 12 13:48:31.642: INFO: (17) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.805355ms)
  Aug 12 13:48:31.642: INFO: (17) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 16.556827ms)
  Aug 12 13:48:31.643: INFO: (17) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 17.114015ms)
  Aug 12 13:48:31.644: INFO: (17) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 18.05641ms)
  Aug 12 13:48:31.644: INFO: (17) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 18.411152ms)
  Aug 12 13:48:31.644: INFO: (17) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 18.377998ms)
  Aug 12 13:48:31.653: INFO: (18) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 8.490236ms)
  Aug 12 13:48:31.653: INFO: (18) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 8.288644ms)
  Aug 12 13:48:31.655: INFO: (18) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 9.619934ms)
  Aug 12 13:48:31.658: INFO: (18) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 12.16369ms)
  Aug 12 13:48:31.658: INFO: (18) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 12.051623ms)
  Aug 12 13:48:31.658: INFO: (18) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 12.667521ms)
  Aug 12 13:48:31.659: INFO: (18) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 13.796922ms)
  Aug 12 13:48:31.660: INFO: (18) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 14.361922ms)
  Aug 12 13:48:31.660: INFO: (18) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 14.417595ms)
  Aug 12 13:48:31.661: INFO: (18) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 15.460136ms)
  Aug 12 13:48:31.661: INFO: (18) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 15.415842ms)
  Aug 12 13:48:31.661: INFO: (18) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 15.704024ms)
  Aug 12 13:48:31.661: INFO: (18) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 16.458828ms)
  Aug 12 13:48:31.662: INFO: (18) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.709581ms)
  Aug 12 13:48:31.662: INFO: (18) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 16.781805ms)
  Aug 12 13:48:31.664: INFO: (18) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 18.744358ms)
  Aug 12 13:48:31.672: INFO: (19) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.332771ms)
  Aug 12 13:48:31.673: INFO: (19) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:160/proxy/: foo (200; 7.979531ms)
  Aug 12 13:48:31.674: INFO: (19) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:462/proxy/: tls qux (200; 9.120419ms)
  Aug 12 13:48:31.675: INFO: (19) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g/proxy/rewriteme">test</a> (200; 10.20273ms)
  Aug 12 13:48:31.676: INFO: (19) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 11.091811ms)
  Aug 12 13:48:31.676: INFO: (19) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:460/proxy/: tls baz (200; 10.966849ms)
  Aug 12 13:48:31.678: INFO: (19) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">... (200; 13.102211ms)
  Aug 12 13:48:31.678: INFO: (19) /api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/https:proxy-service-chvhl-pbn2g:443/proxy/tlsrewritem... (200; 13.662297ms)
  Aug 12 13:48:31.679: INFO: (19) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname2/proxy/: tls qux (200; 14.366325ms)
  Aug 12 13:48:31.679: INFO: (19) /api/v1/namespaces/proxy-9068/pods/http:proxy-service-chvhl-pbn2g:162/proxy/: bar (200; 14.225202ms)
  Aug 12 13:48:31.680: INFO: (19) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname2/proxy/: bar (200; 15.157263ms)
  Aug 12 13:48:31.680: INFO: (19) /api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9068/pods/proxy-service-chvhl-pbn2g:1080/proxy/rewriteme">test<... (200; 15.580538ms)
  Aug 12 13:48:31.681: INFO: (19) /api/v1/namespaces/proxy-9068/services/http:proxy-service-chvhl:portname1/proxy/: foo (200; 15.879854ms)
  Aug 12 13:48:31.681: INFO: (19) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname1/proxy/: foo (200; 15.915646ms)
  Aug 12 13:48:31.681: INFO: (19) /api/v1/namespaces/proxy-9068/services/proxy-service-chvhl:portname2/proxy/: bar (200; 16.508045ms)
  Aug 12 13:48:31.681: INFO: (19) /api/v1/namespaces/proxy-9068/services/https:proxy-service-chvhl:tlsportname1/proxy/: tls baz (200; 16.470259ms)
  Aug 12 13:48:31.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-chvhl in namespace proxy-9068, will wait for the garbage collector to delete the pods @ 08/12/23 13:48:31.687
  Aug 12 13:48:31.753: INFO: Deleting ReplicationController proxy-service-chvhl took: 9.449059ms
  Aug 12 13:48:31.853: INFO: Terminating ReplicationController proxy-service-chvhl pods took: 100.32826ms
  E0812 13:48:31.984943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:32.985009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-9068" for this suite. @ 08/12/23 13:48:33.854
• [5.688 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/12/23 13:48:33.863
  Aug 12 13:48:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename emptydir @ 08/12/23 13:48:33.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:33.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:33.887
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/12/23 13:48:33.892
  E0812 13:48:33.985093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:34.985780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:35.986121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:36.986421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:48:37.926
  Aug 12 13:48:37.930: INFO: Trying to get logs from node ip-172-31-32-142 pod pod-af69107b-2d12-4c4e-a234-0f38bf6dc3d2 container test-container: <nil>
  STEP: delete the pod @ 08/12/23 13:48:37.946
  Aug 12 13:48:37.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-57" for this suite. @ 08/12/23 13:48:37.973
• [4.118 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/12/23 13:48:37.981
  Aug 12 13:48:37.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename var-expansion @ 08/12/23 13:48:37.983
  E0812 13:48:37.986578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:38.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:38.008
  STEP: Creating a pod to test substitution in container's args @ 08/12/23 13:48:38.012
  E0812 13:48:38.986763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:39.987268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:40.988339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:41.988621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:48:42.042
  Aug 12 13:48:42.046: INFO: Trying to get logs from node ip-172-31-32-142 pod var-expansion-5ccc3b91-18d3-4576-b597-79a6eb6f743c container dapi-container: <nil>
  STEP: delete the pod @ 08/12/23 13:48:42.056
  Aug 12 13:48:42.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9017" for this suite. @ 08/12/23 13:48:42.083
• [4.111 seconds]
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/12/23 13:48:42.093
  Aug 12 13:48:42.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename projected @ 08/12/23 13:48:42.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:42.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:42.116
  STEP: Creating configMap with name configmap-projected-all-test-volume-7e1d89bc-7e5e-4852-98b6-4eb98477d941 @ 08/12/23 13:48:42.12
  STEP: Creating secret with name secret-projected-all-test-volume-4bf20a0f-89d2-4581-9a1c-3d25349778ae @ 08/12/23 13:48:42.133
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/12/23 13:48:42.14
  E0812 13:48:42.989086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:43.989223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:44.989345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:45.990227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/12/23 13:48:46.172
  Aug 12 13:48:46.176: INFO: Trying to get logs from node ip-172-31-32-142 pod projected-volume-c178f3a5-eec6-4e75-95c0-bee2f59a222b container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/12/23 13:48:46.19
  Aug 12 13:48:46.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9927" for this suite. @ 08/12/23 13:48:46.218
• [4.134 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/12/23 13:48:46.227
  Aug 12 13:48:46.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename configmap @ 08/12/23 13:48:46.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:46.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:46.252
  Aug 12 13:48:46.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3348" for this suite. @ 08/12/23 13:48:46.317
• [0.099 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/12/23 13:48:46.327
  Aug 12 13:48:46.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 13:48:46.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:46.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:46.355
  Aug 12 13:48:46.383: INFO: created pod pod-service-account-defaultsa
  Aug 12 13:48:46.383: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug 12 13:48:46.390: INFO: created pod pod-service-account-mountsa
  Aug 12 13:48:46.390: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug 12 13:48:46.398: INFO: created pod pod-service-account-nomountsa
  Aug 12 13:48:46.398: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Aug 12 13:48:46.410: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug 12 13:48:46.410: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug 12 13:48:46.425: INFO: created pod pod-service-account-mountsa-mountspec
  Aug 12 13:48:46.425: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug 12 13:48:46.438: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug 12 13:48:46.439: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug 12 13:48:46.449: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug 12 13:48:46.449: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug 12 13:48:46.460: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug 12 13:48:46.460: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug 12 13:48:46.472: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug 12 13:48:46.472: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug 12 13:48:46.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4598" for this suite. @ 08/12/23 13:48:46.483
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/12/23 13:48:46.505
  Aug 12 13:48:46.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename svcaccounts @ 08/12/23 13:48:46.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:46.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:46.534
  STEP: creating a ServiceAccount @ 08/12/23 13:48:46.538
  STEP: watching for the ServiceAccount to be added @ 08/12/23 13:48:46.55
  STEP: patching the ServiceAccount @ 08/12/23 13:48:46.559
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/12/23 13:48:46.569
  STEP: deleting the ServiceAccount @ 08/12/23 13:48:46.575
  Aug 12 13:48:46.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1081" for this suite. @ 08/12/23 13:48:46.616
• [0.124 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/12/23 13:48:46.631
  Aug 12 13:48:46.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-probe @ 08/12/23 13:48:46.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:48:46.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:48:46.658
  STEP: Creating pod busybox-3ed74f61-95e9-42f5-a104-c3bf855f9714 in namespace container-probe-4342 @ 08/12/23 13:48:46.663
  E0812 13:48:46.991305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:47.992367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:48.993079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:49.994021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:48:50.689: INFO: Started pod busybox-3ed74f61-95e9-42f5-a104-c3bf855f9714 in namespace container-probe-4342
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/12/23 13:48:50.689
  Aug 12 13:48:50.694: INFO: Initial restart count of pod busybox-3ed74f61-95e9-42f5-a104-c3bf855f9714 is 0
  E0812 13:48:50.994766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:51.994952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:52.995856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:53.995958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:54.996792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:55.997205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:56.997289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:57.997409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:58.998228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:48:59.999154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:01.000265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:02.001150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:03.001759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:04.001833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:05.002231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:06.002857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:07.003318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:08.003653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:09.003753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:10.003899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:11.004322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:12.005243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:13.005334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:14.005463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:15.005753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:16.006302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:17.006437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:18.006760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:19.006829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:20.007424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:21.007643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:22.008144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:23.008454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:24.008692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:25.009551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:26.010121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:27.010660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:28.010939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:29.011533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:30.011664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:31.012773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:32.012930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:33.014047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:34.014395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:35.014945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:36.015177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:37.015405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:38.015824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:39.015953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:40.016231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:41.016557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:42.016665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:43.017075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:44.017748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:45.017871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:46.018401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:47.018460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:48.018598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:49.018635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:50.018764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:51.019450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:52.019587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:53.020438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:54.020669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:55.021549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:56.021644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:57.022614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:58.022772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:49:59.023600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:00.023738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:01.024117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:02.024252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:03.024748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:04.024884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:05.025897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:06.026190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:07.026262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:08.026496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:09.026959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:10.028035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:11.028492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:12.028658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:13.028974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:14.029115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:15.029542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:16.029870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:17.030346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:18.030835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:19.031147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:20.031275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:21.032294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:22.032672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:23.032790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:24.033757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:25.034226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:26.035209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:27.035329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:28.035470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:29.036320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:30.036630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:31.037355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:32.037485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:33.037554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:34.037832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:35.038722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:36.039332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:37.039915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:38.040194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:39.041001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:40.041265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:41.041478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:42.041629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:43.042035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:44.042685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:45.043265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:46.044200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:47.044622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:48.044743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:49.045692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:50.045840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:51.046316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:52.046537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:53.046684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:54.046990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:55.047103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:56.048181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:57.048349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:58.048408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:50:59.048702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:00.048937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:01.049411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:02.049558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:03.049691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:04.049823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:05.050203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:06.051246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:07.051640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:08.051848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:09.052295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:10.052713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:11.053812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:12.053929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:13.054047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:14.054180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:15.054602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:16.055192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:17.055302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:18.055440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:19.055778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:20.056043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:21.056449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:22.056674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:23.056826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:24.056925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:25.057871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:26.058235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:27.058544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:28.059585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:29.059705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:30.059849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:31.060349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:32.060662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:33.061747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:34.062197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:35.062318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:36.063193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:37.063972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:38.064749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:39.065744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:40.066796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:41.067626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:42.067748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:43.068193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:44.068938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:45.069756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:46.070441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:47.070810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:48.071166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:49.071296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:50.071791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:51.072344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:52.073238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:53.073781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:54.074053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:55.074181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:56.074530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:57.074659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:58.075341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:51:59.075656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:00.075803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:01.076343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:02.076463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:03.076664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:04.077675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:05.078095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:06.078971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:07.079096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:08.079228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:09.079949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:10.080371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:11.080477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:12.080703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:13.080858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:14.080950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:15.081752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:16.082442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:17.083072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:18.083635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:19.083755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:20.083889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:21.084471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:22.084693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:23.084824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:24.085735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:25.085870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:26.086681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:27.086811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:28.087318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:29.087439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:30.087989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:31.088452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:32.088622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:33.088686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:34.089759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:35.089884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:36.090285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:37.090543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:38.091570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:39.091924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:40.092940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:41.093422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:42.094044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:43.094162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:44.094214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:45.094715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:46.094872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:47.095812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:48.095950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:49.096256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:50.096464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:51.097392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:52:51.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/12/23 13:52:51.427
  STEP: Destroying namespace "container-probe-4342" for this suite. @ 08/12/23 13:52:51.442
• [244.821 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/12/23 13:52:51.456
  Aug 12 13:52:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename services @ 08/12/23 13:52:51.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:52:51.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:52:51.48
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3207 @ 08/12/23 13:52:51.484
  STEP: changing the ExternalName service to type=NodePort @ 08/12/23 13:52:51.492
  STEP: creating replication controller externalname-service in namespace services-3207 @ 08/12/23 13:52:51.515
  I0812 13:52:51.528240      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3207, replica count: 2
  E0812 13:52:52.097939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:53.098589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:54.098701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0812 13:52:54.579466      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 12 13:52:54.579: INFO: Creating new exec pod
  E0812 13:52:55.099570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:56.100254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:52:57.100424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:52:57.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-3207 exec execpodkrwlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 12 13:52:57.778: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 12 13:52:57.778: INFO: stdout: ""
  E0812 13:52:58.101030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:52:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-3207 exec execpodkrwlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 12 13:52:58.945: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 12 13:52:58.945: INFO: stdout: "externalname-service-hzr4b"
  Aug 12 13:52:58.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-3207 exec execpodkrwlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.141 80'
  E0812 13:52:59.101457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:52:59.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.141 80\nConnection to 10.152.183.141 80 port [tcp/http] succeeded!\n"
  Aug 12 13:52:59.105: INFO: stdout: "externalname-service-hzr4b"
  Aug 12 13:52:59.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-3207 exec execpodkrwlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.79.233 30515'
  Aug 12 13:52:59.291: INFO: stderr: "+ nc -v -t -w 2 172.31.79.233 30515\n+ echo hostName\nConnection to 172.31.79.233 30515 port [tcp/*] succeeded!\n"
  Aug 12 13:52:59.291: INFO: stdout: "externalname-service-k2xj7"
  Aug 12 13:52:59.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=services-3207 exec execpodkrwlz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.32.142 30515'
  Aug 12 13:52:59.452: INFO: stderr: "+ nc -v -t -w 2 172.31.32.142 30515\nConnection to 172.31.32.142 30515 port [tcp/*] succeeded!\n+ echo hostName\n"
  Aug 12 13:52:59.452: INFO: stdout: "externalname-service-k2xj7"
  Aug 12 13:52:59.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 12 13:52:59.459: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3207" for this suite. @ 08/12/23 13:52:59.495
• [8.055 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/12/23 13:52:59.512
  Aug 12 13:52:59.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:52:59.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:52:59.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:52:59.535
  STEP: Setting up server cert @ 08/12/23 13:52:59.562
  E0812 13:53:00.102015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:53:00.114
  STEP: Deploying the webhook pod @ 08/12/23 13:53:00.125
  STEP: Wait for the deployment to be ready @ 08/12/23 13:53:00.139
  Aug 12 13:53:00.152: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:53:01.102502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:02.102831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:53:02.168
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:53:02.18
  E0812 13:53:03.103653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:03.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/12/23 13:53:03.186
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/12/23 13:53:03.207
  STEP: Creating a dummy validating-webhook-configuration object @ 08/12/23 13:53:03.228
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/12/23 13:53:03.241
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/12/23 13:53:03.249
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/12/23 13:53:03.259
  Aug 12 13:53:03.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1858" for this suite. @ 08/12/23 13:53:03.347
  STEP: Destroying namespace "webhook-markers-6411" for this suite. @ 08/12/23 13:53:03.356
• [3.854 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/12/23 13:53:03.37
  Aug 12 13:53:03.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:53:03.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:53:03.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:53:03.397
  STEP: Setting up server cert @ 08/12/23 13:53:03.428
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:53:03.628
  STEP: Deploying the webhook pod @ 08/12/23 13:53:03.655
  STEP: Wait for the deployment to be ready @ 08/12/23 13:53:03.674
  Aug 12 13:53:03.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:53:04.104290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:05.104394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:53:05.698
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:53:05.717
  E0812 13:53:06.104480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:06.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 12 13:53:06.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  E0812 13:53:07.105097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8949-crds.webhook.example.com via the AdmissionRegistration API @ 08/12/23 13:53:07.265
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/12/23 13:53:07.287
  E0812 13:53:08.105231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:09.105311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:09.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4052" for this suite. @ 08/12/23 13:53:09.947
  STEP: Destroying namespace "webhook-markers-9255" for this suite. @ 08/12/23 13:53:09.956
• [6.594 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/12/23 13:53:09.978
  Aug 12 13:53:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename statefulset @ 08/12/23 13:53:09.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:53:10.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:53:10.008
  STEP: Creating service test in namespace statefulset-9000 @ 08/12/23 13:53:10.02
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/12/23 13:53:10.029
  STEP: Creating stateful set ss in namespace statefulset-9000 @ 08/12/23 13:53:10.034
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9000 @ 08/12/23 13:53:10.043
  Aug 12 13:53:10.049: INFO: Found 0 stateful pods, waiting for 1
  E0812 13:53:10.105899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:11.106609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:12.107666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:13.107932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:14.108450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:15.109233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:16.109760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:17.110529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:18.110953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:19.111404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:20.056: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/12/23 13:53:20.056
  Aug 12 13:53:20.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0812 13:53:20.112172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:20.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 13:53:20.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 13:53:20.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 13:53:20.235: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0812 13:53:21.113045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:22.113149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:23.113373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:24.114361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:25.114505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:26.115104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:27.115231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:28.115368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:29.116229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:30.116361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:30.242: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 13:53:30.242: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 13:53:30.265: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998585s
  E0812 13:53:31.116973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:31.271: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994898486s
  E0812 13:53:32.117753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:32.277: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988088268s
  E0812 13:53:33.118201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:33.282: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982725729s
  E0812 13:53:34.118390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:34.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977567745s
  E0812 13:53:35.118499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:35.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971235425s
  E0812 13:53:36.119358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:36.304: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962603453s
  E0812 13:53:37.120172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:37.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.955842685s
  E0812 13:53:38.120953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:38.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.950769606s
  E0812 13:53:39.121776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:39.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 944.027113ms
  E0812 13:53:40.122455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9000 @ 08/12/23 13:53:40.321
  Aug 12 13:53:40.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 13:53:40.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 13:53:40.495: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 13:53:40.495: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 13:53:40.500: INFO: Found 1 stateful pods, waiting for 3
  E0812 13:53:41.123419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:42.124018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:43.124127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:44.124665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:45.124694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:46.125296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:47.125361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:48.125473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:49.125619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:50.125723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:53:50.507: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 13:53:50.507: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 12 13:53:50.507: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/12/23 13:53:50.507
  STEP: Scale down will halt with unhealthy stateful pod @ 08/12/23 13:53:50.507
  Aug 12 13:53:50.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 13:53:50.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 13:53:50.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 13:53:50.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 13:53:50.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 13:53:50.933: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 13:53:50.934: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 13:53:50.934: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 13:53:50.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 12 13:53:51.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 12 13:53:51.096: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 12 13:53:51.096: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 12 13:53:51.096: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 13:53:51.102: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0812 13:53:51.126425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:52.127052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:53.127384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:54.127804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:55.127952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:56.128170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:57.128326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:58.128651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:53:59.128718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:00.129766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:01.113: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 13:54:01.113: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 12 13:54:01.113: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  E0812 13:54:01.129956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:01.132: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998688s
  E0812 13:54:02.130138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:02.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993176595s
  E0812 13:54:03.130822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:03.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987564218s
  E0812 13:54:04.130864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:04.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981473608s
  E0812 13:54:05.131877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:05.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974521475s
  E0812 13:54:06.132213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:06.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968848555s
  E0812 13:54:07.132591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:07.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962662671s
  E0812 13:54:08.132760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:08.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955419535s
  E0812 13:54:09.133720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:09.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948480777s
  E0812 13:54:10.134825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:10.191: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.511752ms
  E0812 13:54:11.135582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9000 @ 08/12/23 13:54:11.191
  Aug 12 13:54:11.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 13:54:11.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 13:54:11.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 13:54:11.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 13:54:11.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 13:54:11.522: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 13:54:11.522: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 13:54:11.522: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 13:54:11.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2808119746 --namespace=statefulset-9000 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 12 13:54:11.678: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 12 13:54:11.678: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 12 13:54:11.678: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 12 13:54:11.678: INFO: Scaling statefulset ss to 0
  E0812 13:54:12.136676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:13.137066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:14.137794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:15.138887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:16.139295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:17.139951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:18.140076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:19.140219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:20.140596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:21.140756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/12/23 13:54:21.699
  Aug 12 13:54:21.700: INFO: Deleting all statefulset in ns statefulset-9000
  Aug 12 13:54:21.705: INFO: Scaling statefulset ss to 0
  Aug 12 13:54:21.720: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 12 13:54:21.725: INFO: Deleting statefulset ss
  Aug 12 13:54:21.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9000" for this suite. @ 08/12/23 13:54:21.748
• [71.781 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/12/23 13:54:21.76
  Aug 12 13:54:21.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename webhook @ 08/12/23 13:54:21.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:54:21.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:54:21.786
  STEP: Setting up server cert @ 08/12/23 13:54:21.816
  E0812 13:54:22.141338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/12/23 13:54:22.328
  STEP: Deploying the webhook pod @ 08/12/23 13:54:22.34
  STEP: Wait for the deployment to be ready @ 08/12/23 13:54:22.353
  Aug 12 13:54:22.365: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0812 13:54:23.141782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:24.141898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/12/23 13:54:24.38
  STEP: Verifying the service has paired with the endpoint @ 08/12/23 13:54:24.4
  E0812 13:54:25.142931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 12 13:54:25.401: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/12/23 13:54:25.406
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/12/23 13:54:25.425
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/12/23 13:54:25.436
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/12/23 13:54:25.449
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/12/23 13:54:25.464
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/12/23 13:54:25.474
  Aug 12 13:54:25.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-930" for this suite. @ 08/12/23 13:54:25.55
  STEP: Destroying namespace "webhook-markers-1052" for this suite. @ 08/12/23 13:54:25.562
• [3.810 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/12/23 13:54:25.572
  Aug 12 13:54:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/12/23 13:54:25.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:54:25.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:54:25.594
  STEP: create the container to handle the HTTPGet hook request. @ 08/12/23 13:54:25.604
  E0812 13:54:26.143474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:27.143517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/12/23 13:54:27.631
  E0812 13:54:28.144532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:29.144803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/12/23 13:54:29.653
  E0812 13:54:30.145751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:31.146340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/12/23 13:54:31.673
  Aug 12 13:54:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5898" for this suite. @ 08/12/23 13:54:31.704
• [6.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/12/23 13:54:31.72
  Aug 12 13:54:31.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename pods @ 08/12/23 13:54:31.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:54:31.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:54:31.744
  STEP: Create a pod @ 08/12/23 13:54:31.748
  E0812 13:54:32.147064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:33.147241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/12/23 13:54:33.773
  Aug 12 13:54:33.784: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug 12 13:54:33.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4557" for this suite. @ 08/12/23 13:54:33.791
• [2.080 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/12/23 13:54:33.801
  Aug 12 13:54:33.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2808119746
  STEP: Building a namespace api object, basename replicaset @ 08/12/23 13:54:33.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/12/23 13:54:33.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/12/23 13:54:33.83
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/12/23 13:54:33.834
  E0812 13:54:34.147829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0812 13:54:35.147969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 08/12/23 13:54:35.858
  STEP: Then the orphan pod is adopted @ 08/12/23 13:54:35.867
  E0812 13:54:36.148105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 08/12/23 13:54:36.878
  Aug 12 13:54:36.886: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/12/23 13:54:36.915
  Aug 12 13:54:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1763" for this suite. @ 08/12/23 13:54:36.931
• [3.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug 12 13:54:36.955: INFO: Running AfterSuite actions on node 1
  Aug 12 13:54:36.955: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.099 seconds]
------------------------------

Ran 378 of 7207 Specs in 6290.709 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h44m51.250803834s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

