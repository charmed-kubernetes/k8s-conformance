  I0805 12:08:10.653206      19 e2e.go:117] Starting e2e run "8b103839-1338-4d28-9fa2-89009d2c96d1" on Ginkgo node 1
  Aug  5 12:08:10.677: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1691237290 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug  5 12:08:10.873: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:08:10.874: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug  5 12:08:10.904: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug  5 12:08:10.909: INFO: e2e test version: v1.27.4
  Aug  5 12:08:10.911: INFO: kube-apiserver version: v1.27.4
  Aug  5 12:08:10.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:08:10.916: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/05/23 12:08:11.208
  Aug  5 12:08:11.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:08:11.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:08:11.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:08:11.231
  STEP: reading a file in the container @ 08/05/23 12:08:13.26
  Aug  5 12:08:13.261: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2714 pod-service-account-9a874c6d-fe31-4abb-ade0-2b3738f31a81 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 08/05/23 12:08:13.401
  Aug  5 12:08:13.401: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2714 pod-service-account-9a874c6d-fe31-4abb-ade0-2b3738f31a81 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 08/05/23 12:08:13.531
  Aug  5 12:08:13.531: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2714 pod-service-account-9a874c6d-fe31-4abb-ade0-2b3738f31a81 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug  5 12:08:13.652: INFO: Got root ca configmap in namespace "svcaccounts-2714"
  Aug  5 12:08:13.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2714" for this suite. @ 08/05/23 12:08:13.659
• [2.459 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/05/23 12:08:13.668
  Aug  5 12:08:13.668: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 12:08:13.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:08:13.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:08:13.688
  STEP: Creating pod test-grpc-789172bd-cc60-4391-a9c8-aaa198a3d992 in namespace container-probe-6704 @ 08/05/23 12:08:13.69
  Aug  5 12:08:17.716: INFO: Started pod test-grpc-789172bd-cc60-4391-a9c8-aaa198a3d992 in namespace container-probe-6704
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 12:08:17.716
  Aug  5 12:08:17.720: INFO: Initial restart count of pod test-grpc-789172bd-cc60-4391-a9c8-aaa198a3d992 is 0
  Aug  5 12:12:18.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:12:18.287
  STEP: Destroying namespace "container-probe-6704" for this suite. @ 08/05/23 12:12:18.299
• [244.641 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/05/23 12:12:18.313
  Aug  5 12:12:18.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context-test @ 08/05/23 12:12:18.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:12:18.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:12:18.334
  Aug  5 12:12:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1170" for this suite. @ 08/05/23 12:12:22.37
• [4.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/05/23 12:12:22.378
  Aug  5 12:12:22.378: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename runtimeclass @ 08/05/23 12:12:22.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:12:22.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:12:22.397
  Aug  5 12:12:22.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7159" for this suite. @ 08/05/23 12:12:22.416
• [0.044 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/05/23 12:12:22.423
  Aug  5 12:12:22.423: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 12:12:22.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:12:22.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:12:22.443
  STEP: create the rc @ 08/05/23 12:12:22.449
  W0805 12:12:22.454068      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 08/05/23 12:12:28.459
  STEP: wait for the rc to be deleted @ 08/05/23 12:12:28.468
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/05/23 12:12:33.473
  STEP: Gathering metrics @ 08/05/23 12:13:03.486
  W0805 12:13:03.490932      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 12:13:03.491: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 12:13:03.492: INFO: Deleting pod "simpletest.rc-24vx5" in namespace "gc-5059"
  Aug  5 12:13:03.504: INFO: Deleting pod "simpletest.rc-2fxm6" in namespace "gc-5059"
  Aug  5 12:13:03.519: INFO: Deleting pod "simpletest.rc-4kqx8" in namespace "gc-5059"
  Aug  5 12:13:03.535: INFO: Deleting pod "simpletest.rc-4s6x5" in namespace "gc-5059"
  Aug  5 12:13:03.546: INFO: Deleting pod "simpletest.rc-527jx" in namespace "gc-5059"
  Aug  5 12:13:03.557: INFO: Deleting pod "simpletest.rc-56ww4" in namespace "gc-5059"
  Aug  5 12:13:03.571: INFO: Deleting pod "simpletest.rc-59sxt" in namespace "gc-5059"
  Aug  5 12:13:03.583: INFO: Deleting pod "simpletest.rc-5gcnl" in namespace "gc-5059"
  Aug  5 12:13:03.598: INFO: Deleting pod "simpletest.rc-5mrmv" in namespace "gc-5059"
  Aug  5 12:13:03.609: INFO: Deleting pod "simpletest.rc-5r66q" in namespace "gc-5059"
  Aug  5 12:13:03.624: INFO: Deleting pod "simpletest.rc-5rn9s" in namespace "gc-5059"
  Aug  5 12:13:03.638: INFO: Deleting pod "simpletest.rc-5tz72" in namespace "gc-5059"
  Aug  5 12:13:03.650: INFO: Deleting pod "simpletest.rc-629l4" in namespace "gc-5059"
  Aug  5 12:13:03.668: INFO: Deleting pod "simpletest.rc-6pk4v" in namespace "gc-5059"
  Aug  5 12:13:03.682: INFO: Deleting pod "simpletest.rc-75zpj" in namespace "gc-5059"
  Aug  5 12:13:03.694: INFO: Deleting pod "simpletest.rc-7dzzq" in namespace "gc-5059"
  Aug  5 12:13:03.710: INFO: Deleting pod "simpletest.rc-7fhx6" in namespace "gc-5059"
  Aug  5 12:13:03.722: INFO: Deleting pod "simpletest.rc-7grzn" in namespace "gc-5059"
  Aug  5 12:13:03.745: INFO: Deleting pod "simpletest.rc-7rlmh" in namespace "gc-5059"
  Aug  5 12:13:03.760: INFO: Deleting pod "simpletest.rc-94hmm" in namespace "gc-5059"
  Aug  5 12:13:03.774: INFO: Deleting pod "simpletest.rc-9nmws" in namespace "gc-5059"
  Aug  5 12:13:03.790: INFO: Deleting pod "simpletest.rc-9rm87" in namespace "gc-5059"
  Aug  5 12:13:03.802: INFO: Deleting pod "simpletest.rc-9rxpt" in namespace "gc-5059"
  Aug  5 12:13:03.815: INFO: Deleting pod "simpletest.rc-b2kxd" in namespace "gc-5059"
  Aug  5 12:13:03.831: INFO: Deleting pod "simpletest.rc-bdffk" in namespace "gc-5059"
  Aug  5 12:13:03.846: INFO: Deleting pod "simpletest.rc-c2dgn" in namespace "gc-5059"
  Aug  5 12:13:03.857: INFO: Deleting pod "simpletest.rc-c77nn" in namespace "gc-5059"
  Aug  5 12:13:03.870: INFO: Deleting pod "simpletest.rc-crwgn" in namespace "gc-5059"
  Aug  5 12:13:03.881: INFO: Deleting pod "simpletest.rc-cwlqs" in namespace "gc-5059"
  Aug  5 12:13:03.899: INFO: Deleting pod "simpletest.rc-d6jwb" in namespace "gc-5059"
  Aug  5 12:13:03.912: INFO: Deleting pod "simpletest.rc-d6mtd" in namespace "gc-5059"
  Aug  5 12:13:03.924: INFO: Deleting pod "simpletest.rc-d9dv6" in namespace "gc-5059"
  Aug  5 12:13:03.940: INFO: Deleting pod "simpletest.rc-dt9mg" in namespace "gc-5059"
  Aug  5 12:13:03.954: INFO: Deleting pod "simpletest.rc-frj6q" in namespace "gc-5059"
  Aug  5 12:13:03.967: INFO: Deleting pod "simpletest.rc-fsvhm" in namespace "gc-5059"
  Aug  5 12:13:03.978: INFO: Deleting pod "simpletest.rc-g72t7" in namespace "gc-5059"
  Aug  5 12:13:03.991: INFO: Deleting pod "simpletest.rc-g7ddx" in namespace "gc-5059"
  Aug  5 12:13:04.002: INFO: Deleting pod "simpletest.rc-g9vdx" in namespace "gc-5059"
  Aug  5 12:13:04.018: INFO: Deleting pod "simpletest.rc-gtnvw" in namespace "gc-5059"
  Aug  5 12:13:04.109: INFO: Deleting pod "simpletest.rc-hjc6b" in namespace "gc-5059"
  Aug  5 12:13:04.121: INFO: Deleting pod "simpletest.rc-hp967" in namespace "gc-5059"
  Aug  5 12:13:04.136: INFO: Deleting pod "simpletest.rc-hqhtx" in namespace "gc-5059"
  Aug  5 12:13:04.150: INFO: Deleting pod "simpletest.rc-hxzss" in namespace "gc-5059"
  Aug  5 12:13:04.164: INFO: Deleting pod "simpletest.rc-j4rwb" in namespace "gc-5059"
  Aug  5 12:13:04.178: INFO: Deleting pod "simpletest.rc-jfwn4" in namespace "gc-5059"
  Aug  5 12:13:04.189: INFO: Deleting pod "simpletest.rc-jltgq" in namespace "gc-5059"
  Aug  5 12:13:04.204: INFO: Deleting pod "simpletest.rc-jltz6" in namespace "gc-5059"
  Aug  5 12:13:04.225: INFO: Deleting pod "simpletest.rc-jq2km" in namespace "gc-5059"
  Aug  5 12:13:04.240: INFO: Deleting pod "simpletest.rc-jqqdf" in namespace "gc-5059"
  Aug  5 12:13:04.252: INFO: Deleting pod "simpletest.rc-jtq64" in namespace "gc-5059"
  Aug  5 12:13:04.267: INFO: Deleting pod "simpletest.rc-kjznt" in namespace "gc-5059"
  Aug  5 12:13:04.278: INFO: Deleting pod "simpletest.rc-kpxn9" in namespace "gc-5059"
  Aug  5 12:13:04.292: INFO: Deleting pod "simpletest.rc-ktzgs" in namespace "gc-5059"
  Aug  5 12:13:04.305: INFO: Deleting pod "simpletest.rc-l8j5g" in namespace "gc-5059"
  Aug  5 12:13:04.318: INFO: Deleting pod "simpletest.rc-lbxql" in namespace "gc-5059"
  Aug  5 12:13:04.333: INFO: Deleting pod "simpletest.rc-lh8dr" in namespace "gc-5059"
  Aug  5 12:13:04.348: INFO: Deleting pod "simpletest.rc-lvszq" in namespace "gc-5059"
  Aug  5 12:13:04.364: INFO: Deleting pod "simpletest.rc-m8n8w" in namespace "gc-5059"
  Aug  5 12:13:04.383: INFO: Deleting pod "simpletest.rc-m95rt" in namespace "gc-5059"
  Aug  5 12:13:04.395: INFO: Deleting pod "simpletest.rc-mfhzr" in namespace "gc-5059"
  Aug  5 12:13:04.410: INFO: Deleting pod "simpletest.rc-mjpk4" in namespace "gc-5059"
  Aug  5 12:13:04.430: INFO: Deleting pod "simpletest.rc-mp849" in namespace "gc-5059"
  Aug  5 12:13:04.443: INFO: Deleting pod "simpletest.rc-mwls4" in namespace "gc-5059"
  Aug  5 12:13:04.458: INFO: Deleting pod "simpletest.rc-nbtm5" in namespace "gc-5059"
  Aug  5 12:13:04.473: INFO: Deleting pod "simpletest.rc-nwmdf" in namespace "gc-5059"
  Aug  5 12:13:04.487: INFO: Deleting pod "simpletest.rc-nzwtr" in namespace "gc-5059"
  Aug  5 12:13:04.499: INFO: Deleting pod "simpletest.rc-p4jrx" in namespace "gc-5059"
  Aug  5 12:13:04.512: INFO: Deleting pod "simpletest.rc-p887m" in namespace "gc-5059"
  Aug  5 12:13:04.528: INFO: Deleting pod "simpletest.rc-pw2p6" in namespace "gc-5059"
  Aug  5 12:13:04.542: INFO: Deleting pod "simpletest.rc-pww7x" in namespace "gc-5059"
  Aug  5 12:13:04.587: INFO: Deleting pod "simpletest.rc-q85pr" in namespace "gc-5059"
  Aug  5 12:13:04.636: INFO: Deleting pod "simpletest.rc-qgfwj" in namespace "gc-5059"
  Aug  5 12:13:04.688: INFO: Deleting pod "simpletest.rc-qt24f" in namespace "gc-5059"
  Aug  5 12:13:04.740: INFO: Deleting pod "simpletest.rc-r68lg" in namespace "gc-5059"
  Aug  5 12:13:04.792: INFO: Deleting pod "simpletest.rc-rptwh" in namespace "gc-5059"
  Aug  5 12:13:04.839: INFO: Deleting pod "simpletest.rc-s9dkj" in namespace "gc-5059"
  Aug  5 12:13:04.892: INFO: Deleting pod "simpletest.rc-sfs8t" in namespace "gc-5059"
  Aug  5 12:13:04.939: INFO: Deleting pod "simpletest.rc-snfnr" in namespace "gc-5059"
  Aug  5 12:13:04.987: INFO: Deleting pod "simpletest.rc-svrnr" in namespace "gc-5059"
  Aug  5 12:13:05.036: INFO: Deleting pod "simpletest.rc-svsls" in namespace "gc-5059"
  Aug  5 12:13:05.087: INFO: Deleting pod "simpletest.rc-th6nq" in namespace "gc-5059"
  Aug  5 12:13:05.140: INFO: Deleting pod "simpletest.rc-tnm4l" in namespace "gc-5059"
  Aug  5 12:13:05.189: INFO: Deleting pod "simpletest.rc-v766p" in namespace "gc-5059"
  Aug  5 12:13:05.237: INFO: Deleting pod "simpletest.rc-vdx2n" in namespace "gc-5059"
  Aug  5 12:13:05.285: INFO: Deleting pod "simpletest.rc-vwd8q" in namespace "gc-5059"
  Aug  5 12:13:05.339: INFO: Deleting pod "simpletest.rc-vwsrc" in namespace "gc-5059"
  Aug  5 12:13:05.390: INFO: Deleting pod "simpletest.rc-w6nh4" in namespace "gc-5059"
  Aug  5 12:13:05.439: INFO: Deleting pod "simpletest.rc-w74xv" in namespace "gc-5059"
  Aug  5 12:13:05.490: INFO: Deleting pod "simpletest.rc-whqhg" in namespace "gc-5059"
  Aug  5 12:13:05.537: INFO: Deleting pod "simpletest.rc-wls77" in namespace "gc-5059"
  Aug  5 12:13:05.589: INFO: Deleting pod "simpletest.rc-wps8f" in namespace "gc-5059"
  Aug  5 12:13:05.638: INFO: Deleting pod "simpletest.rc-x254x" in namespace "gc-5059"
  Aug  5 12:13:05.692: INFO: Deleting pod "simpletest.rc-xg59l" in namespace "gc-5059"
  Aug  5 12:13:05.739: INFO: Deleting pod "simpletest.rc-xvgmk" in namespace "gc-5059"
  Aug  5 12:13:05.790: INFO: Deleting pod "simpletest.rc-xzcpj" in namespace "gc-5059"
  Aug  5 12:13:05.837: INFO: Deleting pod "simpletest.rc-z64gc" in namespace "gc-5059"
  Aug  5 12:13:05.894: INFO: Deleting pod "simpletest.rc-z7p7k" in namespace "gc-5059"
  Aug  5 12:13:05.936: INFO: Deleting pod "simpletest.rc-zdqq7" in namespace "gc-5059"
  Aug  5 12:13:05.987: INFO: Deleting pod "simpletest.rc-zdrqc" in namespace "gc-5059"
  Aug  5 12:13:06.036: INFO: Deleting pod "simpletest.rc-zqb7r" in namespace "gc-5059"
  Aug  5 12:13:06.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5059" for this suite. @ 08/05/23 12:13:06.13
• [43.759 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/05/23 12:13:06.183
  Aug  5 12:13:06.183: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:13:06.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:13:06.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:13:06.21
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/05/23 12:13:06.213
  STEP: Saw pod success @ 08/05/23 12:13:16.257
  Aug  5 12:13:16.260: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-97b7af15-5892-4450-8b05-edc9b26eb851 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:13:16.282
  Aug  5 12:13:16.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6388" for this suite. @ 08/05/23 12:13:16.303
• [10.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/05/23 12:13:16.311
  Aug  5 12:13:16.311: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename cronjob @ 08/05/23 12:13:16.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:13:16.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:13:16.332
  STEP: Creating a ForbidConcurrent cronjob @ 08/05/23 12:13:16.334
  STEP: Ensuring a job is scheduled @ 08/05/23 12:13:16.34
  STEP: Ensuring exactly one is scheduled @ 08/05/23 12:14:00.346
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/05/23 12:14:00.349
  STEP: Ensuring no more jobs are scheduled @ 08/05/23 12:14:00.353
  STEP: Removing cronjob @ 08/05/23 12:19:00.363
  Aug  5 12:19:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8023" for this suite. @ 08/05/23 12:19:00.373
• [344.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/05/23 12:19:00.382
  Aug  5 12:19:00.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 12:19:00.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:00.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:00.406
  STEP: Creating projection with secret that has name secret-emptykey-test-1ef8f8ee-6db6-40ee-b28e-4ddbf78fcb4e @ 08/05/23 12:19:00.41
  Aug  5 12:19:00.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5656" for this suite. @ 08/05/23 12:19:00.415
• [0.040 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/05/23 12:19:00.422
  Aug  5 12:19:00.422: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:19:00.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:00.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:00.447
  STEP: Creating configMap with name configmap-test-volume-bcf987ea-9767-437e-bb4b-8007068619b2 @ 08/05/23 12:19:00.449
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:19:00.454
  STEP: Saw pod success @ 08/05/23 12:19:04.483
  Aug  5 12:19:04.487: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-4e8f3f35-ed62-4e3c-b83f-1c70142d1b5f container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:19:04.506
  Aug  5 12:19:04.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3976" for this suite. @ 08/05/23 12:19:04.525
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/05/23 12:19:04.532
  Aug  5 12:19:04.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 12:19:04.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:04.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:04.553
  STEP: creating the pod @ 08/05/23 12:19:04.555
  STEP: submitting the pod to kubernetes @ 08/05/23 12:19:04.555
  STEP: verifying QOS class is set on the pod @ 08/05/23 12:19:04.565
  Aug  5 12:19:04.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4291" for this suite. @ 08/05/23 12:19:04.573
• [0.048 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/05/23 12:19:04.58
  Aug  5 12:19:04.580: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:19:04.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:04.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:04.606
  STEP: creating the pod @ 08/05/23 12:19:04.608
  Aug  5 12:19:04.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 create -f -'
  Aug  5 12:19:04.855: INFO: stderr: ""
  Aug  5 12:19:04.856: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/05/23 12:19:06.865
  Aug  5 12:19:06.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 label pods pause testing-label=testing-label-value'
  Aug  5 12:19:06.934: INFO: stderr: ""
  Aug  5 12:19:06.934: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/05/23 12:19:06.934
  Aug  5 12:19:06.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 get pod pause -L testing-label'
  Aug  5 12:19:07.010: INFO: stderr: ""
  Aug  5 12:19:07.010: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/05/23 12:19:07.01
  Aug  5 12:19:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 label pods pause testing-label-'
  Aug  5 12:19:07.080: INFO: stderr: ""
  Aug  5 12:19:07.080: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/05/23 12:19:07.08
  Aug  5 12:19:07.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 get pod pause -L testing-label'
  Aug  5 12:19:07.166: INFO: stderr: ""
  Aug  5 12:19:07.166: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 08/05/23 12:19:07.166
  Aug  5 12:19:07.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 delete --grace-period=0 --force -f -'
  Aug  5 12:19:07.246: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 12:19:07.246: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug  5 12:19:07.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 get rc,svc -l name=pause --no-headers'
  Aug  5 12:19:07.314: INFO: stderr: "No resources found in kubectl-5017 namespace.\n"
  Aug  5 12:19:07.314: INFO: stdout: ""
  Aug  5 12:19:07.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5017 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  5 12:19:07.372: INFO: stderr: ""
  Aug  5 12:19:07.372: INFO: stdout: ""
  Aug  5 12:19:07.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5017" for this suite. @ 08/05/23 12:19:07.376
• [2.804 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/05/23 12:19:07.384
  Aug  5 12:19:07.384: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 12:19:07.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:07.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:07.417
  STEP: Given a ReplicationController is created @ 08/05/23 12:19:07.419
  STEP: When the matched label of one of its pods change @ 08/05/23 12:19:07.429
  Aug  5 12:19:07.434: INFO: Pod name pod-release: Found 0 pods out of 1
  Aug  5 12:19:12.438: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/05/23 12:19:12.449
  Aug  5 12:19:13.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8580" for this suite. @ 08/05/23 12:19:13.461
• [6.083 seconds]
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/05/23 12:19:13.467
  Aug  5 12:19:13.467: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:19:13.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:13.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:13.489
  STEP: Creating a pod to test service account token:  @ 08/05/23 12:19:13.491
  STEP: Saw pod success @ 08/05/23 12:19:17.512
  Aug  5 12:19:17.517: INFO: Trying to get logs from node ip-172-31-95-133 pod test-pod-46f71e69-6f54-4362-ba61-96822a7a25e7 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:19:17.526
  Aug  5 12:19:17.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9669" for this suite. @ 08/05/23 12:19:17.548
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/05/23 12:19:17.557
  Aug  5 12:19:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename watch @ 08/05/23 12:19:17.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:17.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:17.58
  STEP: creating a watch on configmaps @ 08/05/23 12:19:17.584
  STEP: creating a new configmap @ 08/05/23 12:19:17.586
  STEP: modifying the configmap once @ 08/05/23 12:19:17.591
  STEP: closing the watch once it receives two notifications @ 08/05/23 12:19:17.6
  Aug  5 12:19:17.600: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4843  d69f1cd2-b528-47d8-881c-7d283cc07744 6857 0 2023-08-05 12:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-05 12:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:19:17.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4843  d69f1cd2-b528-47d8-881c-7d283cc07744 6858 0 2023-08-05 12:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-05 12:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/05/23 12:19:17.601
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/05/23 12:19:17.61
  STEP: deleting the configmap @ 08/05/23 12:19:17.611
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/05/23 12:19:17.619
  Aug  5 12:19:17.619: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4843  d69f1cd2-b528-47d8-881c-7d283cc07744 6859 0 2023-08-05 12:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-05 12:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:19:17.619: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4843  d69f1cd2-b528-47d8-881c-7d283cc07744 6860 0 2023-08-05 12:19:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-05 12:19:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:19:17.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4843" for this suite. @ 08/05/23 12:19:17.624
• [0.076 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/05/23 12:19:17.633
  Aug  5 12:19:17.633: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:19:17.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:17.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:17.655
  STEP: Creating ServiceAccount "e2e-sa-5428v"  @ 08/05/23 12:19:17.658
  Aug  5 12:19:17.664: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-5428v"  @ 08/05/23 12:19:17.664
  Aug  5 12:19:17.674: INFO: AutomountServiceAccountToken: true
  Aug  5 12:19:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6350" for this suite. @ 08/05/23 12:19:17.678
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/05/23 12:19:17.687
  Aug  5 12:19:17.687: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename hostport @ 08/05/23 12:19:17.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:17.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:17.708
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/05/23 12:19:17.72
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.35.140 on the node which pod1 resides and expect scheduled @ 08/05/23 12:19:21.745
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.35.140 but use UDP protocol on the node which pod2 resides @ 08/05/23 12:19:33.787
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/05/23 12:19:37.821
  Aug  5 12:19:37.821: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.35.140 http://127.0.0.1:54323/hostname] Namespace:hostport-1487 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:19:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:19:37.822: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:19:37.822: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1487/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.35.140+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.35.140, port: 54323 @ 08/05/23 12:19:37.89
  Aug  5 12:19:37.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.35.140:54323/hostname] Namespace:hostport-1487 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:19:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:19:37.890: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:19:37.890: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1487/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.35.140%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.35.140, port: 54323 UDP @ 08/05/23 12:19:37.953
  Aug  5 12:19:37.953: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.35.140 54323] Namespace:hostport-1487 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:19:37.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:19:37.954: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:19:37.954: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-1487/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.35.140+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Aug  5 12:19:43.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1487" for this suite. @ 08/05/23 12:19:43.025
• [25.345 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/05/23 12:19:43.033
  Aug  5 12:19:43.033: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 12:19:43.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:43.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:43.054
  STEP: creating service in namespace services-1659 @ 08/05/23 12:19:43.057
  STEP: creating service affinity-clusterip in namespace services-1659 @ 08/05/23 12:19:43.057
  STEP: creating replication controller affinity-clusterip in namespace services-1659 @ 08/05/23 12:19:43.067
  I0805 12:19:43.077278      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-1659, replica count: 3
  I0805 12:19:46.128394      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0805 12:19:49.129664      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 12:19:49.136: INFO: Creating new exec pod
  Aug  5 12:19:52.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-1659 exec execpod-affinity2bvzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Aug  5 12:19:52.277: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug  5 12:19:52.277: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 12:19:52.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-1659 exec execpod-affinity2bvzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.99 80'
  Aug  5 12:19:52.395: INFO: stderr: "+ nc -v -t -w 2 10.152.183.99 80\n+ echo hostName\nConnection to 10.152.183.99 80 port [tcp/http] succeeded!\n"
  Aug  5 12:19:52.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 12:19:52.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-1659 exec execpod-affinity2bvzw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.99:80/ ; done'
  Aug  5 12:19:52.589: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.99:80/\n"
  Aug  5 12:19:52.589: INFO: stdout: "\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q\naffinity-clusterip-j2n8q"
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.589: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.590: INFO: Received response from host: affinity-clusterip-j2n8q
  Aug  5 12:19:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 12:19:52.594: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-1659, will wait for the garbage collector to delete the pods @ 08/05/23 12:19:52.607
  Aug  5 12:19:52.667: INFO: Deleting ReplicationController affinity-clusterip took: 6.002047ms
  Aug  5 12:19:52.768: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.841868ms
  STEP: Destroying namespace "services-1659" for this suite. @ 08/05/23 12:19:55.188
• [12.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/05/23 12:19:55.2
  Aug  5 12:19:55.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption @ 08/05/23 12:19:55.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:19:55.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:19:55.221
  Aug  5 12:19:55.237: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug  5 12:20:55.254: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/05/23 12:20:55.257
  Aug  5 12:20:55.278: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug  5 12:20:55.285: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug  5 12:20:55.299: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug  5 12:20:55.308: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug  5 12:20:55.324: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug  5 12:20:55.330: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/05/23 12:20:55.33
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/05/23 12:20:57.355
  Aug  5 12:21:01.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-945" for this suite. @ 08/05/23 12:21:01.438
• [66.245 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/05/23 12:21:01.447
  Aug  5 12:21:01.447: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 12:21:01.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:01.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:01.467
  Aug  5 12:21:01.481: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug  5 12:21:06.486: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 12:21:06.486
  STEP: Scaling up "test-rs" replicaset  @ 08/05/23 12:21:06.486
  Aug  5 12:21:06.495: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/05/23 12:21:06.495
  W0805 12:21:06.503285      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  5 12:21:06.513: INFO: observed ReplicaSet test-rs in namespace replicaset-1231 with ReadyReplicas 1, AvailableReplicas 1
  Aug  5 12:21:06.526: INFO: observed ReplicaSet test-rs in namespace replicaset-1231 with ReadyReplicas 1, AvailableReplicas 1
  Aug  5 12:21:06.551: INFO: observed ReplicaSet test-rs in namespace replicaset-1231 with ReadyReplicas 1, AvailableReplicas 1
  Aug  5 12:21:06.556: INFO: observed ReplicaSet test-rs in namespace replicaset-1231 with ReadyReplicas 1, AvailableReplicas 1
  Aug  5 12:21:07.675: INFO: observed ReplicaSet test-rs in namespace replicaset-1231 with ReadyReplicas 2, AvailableReplicas 2
  Aug  5 12:21:11.122: INFO: observed Replicaset test-rs in namespace replicaset-1231 with ReadyReplicas 3 found true
  Aug  5 12:21:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1231" for this suite. @ 08/05/23 12:21:11.128
• [9.688 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/05/23 12:21:11.135
  Aug  5 12:21:11.135: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:21:11.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:11.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:11.158
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:21:11.16
  STEP: Saw pod success @ 08/05/23 12:21:15.188
  Aug  5 12:21:15.191: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-94202705-13da-4d01-9df2-e5f9b663c908 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:21:15.208
  Aug  5 12:21:15.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6080" for this suite. @ 08/05/23 12:21:15.226
• [4.098 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/05/23 12:21:15.233
  Aug  5 12:21:15.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:21:15.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:15.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:15.257
  STEP: Creating projection with secret that has name projected-secret-test-84902621-816d-432e-8fc1-a7159788c178 @ 08/05/23 12:21:15.259
  STEP: Creating a pod to test consume secrets @ 08/05/23 12:21:15.263
  STEP: Saw pod success @ 08/05/23 12:21:19.285
  Aug  5 12:21:19.288: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-secrets-400c1703-f6f2-4dfc-a8e4-3bea527cdf4a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 12:21:19.296
  Aug  5 12:21:19.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-277" for this suite. @ 08/05/23 12:21:19.318
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/05/23 12:21:19.329
  Aug  5 12:21:19.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 12:21:19.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:19.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:19.354
  Aug  5 12:21:19.365: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Aug  5 12:21:24.372: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 12:21:24.372
  Aug  5 12:21:24.373: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/05/23 12:21:24.382
  Aug  5 12:21:24.392: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8076  18056bc8-80a7-4def-9ea8-b3d127bdfe4d 7794 1 2023-08-05 12:21:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-05 12:21:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c4d5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug  5 12:21:24.396: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Aug  5 12:21:24.396: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug  5 12:21:24.396: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8076  ebc226f4-5ab0-4dc1-b92a-a46c6b60cee8 7795 1 2023-08-05 12:21:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 18056bc8-80a7-4def-9ea8-b3d127bdfe4d 0xc004bea737 0xc004bea738}] [] [{e2e.test Update apps/v1 2023-08-05 12:21:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:21:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-05 12:21:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"18056bc8-80a7-4def-9ea8-b3d127bdfe4d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004bea7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:21:24.399: INFO: Pod "test-cleanup-controller-tmhdx" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-tmhdx test-cleanup-controller- deployment-8076  4c865384-a3e8-4963-a220-0af921ab7ab7 7771 0 2023-08-05 12:21:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller ebc226f4-5ab0-4dc1-b92a-a46c6b60cee8 0xc004c4d92f 0xc004c4d940}] [] [{kube-controller-manager Update v1 2023-08-05 12:21:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc226f4-5ab0-4dc1-b92a-a46c6b60cee8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 12:21:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wqj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wqj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:21:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:21:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:21:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.180,StartTime:2023-08-05 12:21:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 12:21:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://07000db8cc10ea6bbbf966d1e5eda82a3a5f81dcbf7680fe02a716953baf8d97,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.180,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 12:21:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8076" for this suite. @ 08/05/23 12:21:24.405
• [5.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/05/23 12:21:24.417
  Aug  5 12:21:24.417: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:21:24.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:24.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:24.445
  STEP: Creating the pod @ 08/05/23 12:21:24.447
  Aug  5 12:21:27.003: INFO: Successfully updated pod "annotationupdateb4601c38-8d2c-4946-8890-1c50ff0f2cd5"
  Aug  5 12:21:29.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7547" for this suite. @ 08/05/23 12:21:29.024
• [4.613 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/05/23 12:21:29.032
  Aug  5 12:21:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 12:21:29.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:29.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:29.055
  STEP: set up a multi version CRD @ 08/05/23 12:21:29.058
  Aug  5 12:21:29.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: rename a version @ 08/05/23 12:21:32.434
  STEP: check the new version name is served @ 08/05/23 12:21:32.449
  STEP: check the old version name is removed @ 08/05/23 12:21:33.75
  STEP: check the other version is not changed @ 08/05/23 12:21:34.509
  Aug  5 12:21:37.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2938" for this suite. @ 08/05/23 12:21:37.298
• [8.275 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/05/23 12:21:37.307
  Aug  5 12:21:37.308: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:21:37.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:37.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:37.328
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/05/23 12:21:37.331
  STEP: Saw pod success @ 08/05/23 12:21:41.353
  Aug  5 12:21:41.357: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-3eef7bc2-612c-4a99-919a-328e81c5b43b container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:21:41.372
  Aug  5 12:21:41.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9746" for this suite. @ 08/05/23 12:21:41.39
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/05/23 12:21:41.398
  Aug  5 12:21:41.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:21:41.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:41.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:41.417
  STEP: Creating configMap with name configmap-test-volume-map-9cd42792-c47a-47d4-819b-c4e820ad9980 @ 08/05/23 12:21:41.421
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:21:41.426
  STEP: Saw pod success @ 08/05/23 12:21:45.449
  Aug  5 12:21:45.452: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-6f566966-7dcd-4aca-a0a2-ffeaeeaa4c2c container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:21:45.46
  Aug  5 12:21:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2399" for this suite. @ 08/05/23 12:21:45.481
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/05/23 12:21:45.491
  Aug  5 12:21:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename endpointslice @ 08/05/23 12:21:45.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:45.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:45.514
  STEP: getting /apis @ 08/05/23 12:21:45.518
  STEP: getting /apis/discovery.k8s.io @ 08/05/23 12:21:45.524
  STEP: getting /apis/discovery.k8s.iov1 @ 08/05/23 12:21:45.525
  STEP: creating @ 08/05/23 12:21:45.527
  STEP: getting @ 08/05/23 12:21:45.545
  STEP: listing @ 08/05/23 12:21:45.549
  STEP: watching @ 08/05/23 12:21:45.552
  Aug  5 12:21:45.552: INFO: starting watch
  STEP: cluster-wide listing @ 08/05/23 12:21:45.554
  STEP: cluster-wide watching @ 08/05/23 12:21:45.558
  Aug  5 12:21:45.558: INFO: starting watch
  STEP: patching @ 08/05/23 12:21:45.56
  STEP: updating @ 08/05/23 12:21:45.566
  Aug  5 12:21:45.579: INFO: waiting for watch events with expected annotations
  Aug  5 12:21:45.579: INFO: saw patched and updated annotations
  STEP: deleting @ 08/05/23 12:21:45.579
  STEP: deleting a collection @ 08/05/23 12:21:45.593
  Aug  5 12:21:45.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2181" for this suite. @ 08/05/23 12:21:45.616
• [0.133 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/05/23 12:21:45.624
  Aug  5 12:21:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:21:45.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:45.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:45.643
  STEP: Creating a ResourceQuota @ 08/05/23 12:21:45.649
  STEP: Getting a ResourceQuota @ 08/05/23 12:21:45.655
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/05/23 12:21:45.664
  STEP: Patching the ResourceQuota @ 08/05/23 12:21:45.668
  STEP: Deleting a Collection of ResourceQuotas @ 08/05/23 12:21:45.674
  STEP: Verifying the deleted ResourceQuota @ 08/05/23 12:21:45.684
  Aug  5 12:21:45.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9290" for this suite. @ 08/05/23 12:21:45.691
• [0.077 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/05/23 12:21:45.702
  Aug  5 12:21:45.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption @ 08/05/23 12:21:45.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:45.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:45.724
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:21:45.739
  STEP: Updating PodDisruptionBudget status @ 08/05/23 12:21:47.747
  STEP: Waiting for all pods to be running @ 08/05/23 12:21:47.757
  Aug  5 12:21:47.764: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 08/05/23 12:21:49.769
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:21:49.782
  STEP: Patching PodDisruptionBudget status @ 08/05/23 12:21:49.793
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:21:49.802
  Aug  5 12:21:49.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4238" for this suite. @ 08/05/23 12:21:49.809
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/05/23 12:21:49.816
  Aug  5 12:21:49.816: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename limitrange @ 08/05/23 12:21:49.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:49.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:49.833
  STEP: Creating LimitRange "e2e-limitrange-hl84g" in namespace "limitrange-9097" @ 08/05/23 12:21:49.836
  STEP: Creating another limitRange in another namespace @ 08/05/23 12:21:49.842
  Aug  5 12:21:49.853: INFO: Namespace "e2e-limitrange-hl84g-3656" created
  Aug  5 12:21:49.853: INFO: Creating LimitRange "e2e-limitrange-hl84g" in namespace "e2e-limitrange-hl84g-3656"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hl84g" @ 08/05/23 12:21:49.861
  Aug  5 12:21:49.864: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-hl84g" in "limitrange-9097" namespace @ 08/05/23 12:21:49.864
  Aug  5 12:21:49.870: INFO: LimitRange "e2e-limitrange-hl84g" has been patched
  STEP: Delete LimitRange "e2e-limitrange-hl84g" by Collection with labelSelector: "e2e-limitrange-hl84g=patched" @ 08/05/23 12:21:49.87
  STEP: Confirm that the limitRange "e2e-limitrange-hl84g" has been deleted @ 08/05/23 12:21:49.879
  Aug  5 12:21:49.879: INFO: Requesting list of LimitRange to confirm quantity
  Aug  5 12:21:49.882: INFO: Found 0 LimitRange with label "e2e-limitrange-hl84g=patched"
  Aug  5 12:21:49.882: INFO: LimitRange "e2e-limitrange-hl84g" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hl84g" @ 08/05/23 12:21:49.882
  Aug  5 12:21:49.885: INFO: Found 1 limitRange
  Aug  5 12:21:49.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9097" for this suite. @ 08/05/23 12:21:49.889
  STEP: Destroying namespace "e2e-limitrange-hl84g-3656" for this suite. @ 08/05/23 12:21:49.896
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/05/23 12:21:49.903
  Aug  5 12:21:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:21:49.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:21:49.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:21:49.921
  Aug  5 12:21:49.938: INFO: created pod
  STEP: Saw pod success @ 08/05/23 12:21:53.953
  Aug  5 12:22:23.954: INFO: polling logs
  Aug  5 12:22:23.963: INFO: Pod logs: 
  I0805 12:21:50.650393       1 log.go:198] OK: Got token
  I0805 12:21:50.650434       1 log.go:198] validating with in-cluster discovery
  I0805 12:21:50.651709       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0805 12:21:50.651930       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691238710, NotBefore:1691238110, IssuedAt:1691238110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d70e0ed3-a29f-4831-a489-73dfd6b92d56"}}}
  I0805 12:21:50.661104       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0805 12:21:50.667289       1 log.go:198] OK: Validated signature on JWT
  I0805 12:21:50.667394       1 log.go:198] OK: Got valid claims from token!
  I0805 12:21:50.667425       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691238710, NotBefore:1691238110, IssuedAt:1691238110, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d70e0ed3-a29f-4831-a489-73dfd6b92d56"}}}

  Aug  5 12:22:23.963: INFO: completed pod
  Aug  5 12:22:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-635" for this suite. @ 08/05/23 12:22:23.974
• [34.078 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/05/23 12:22:23.982
  Aug  5 12:22:23.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 12:22:23.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:22:23.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:22:24.001
  STEP: Creating a pod to test downward api env vars @ 08/05/23 12:22:24.008
  STEP: Saw pod success @ 08/05/23 12:22:28.03
  Aug  5 12:22:28.034: INFO: Trying to get logs from node ip-172-31-95-133 pod downward-api-a051f8a2-f484-4889-b79b-f886931a2c72 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 12:22:28.041
  Aug  5 12:22:28.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5679" for this suite. @ 08/05/23 12:22:28.061
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 08/05/23 12:22:28.069
  Aug  5 12:22:28.069: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 12:22:28.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:22:28.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:22:28.088
  Aug  5 12:22:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  W0805 12:22:30.611310      19 warnings.go:70] unknown field "alpha"
  W0805 12:22:30.611332      19 warnings.go:70] unknown field "beta"
  W0805 12:22:30.611338      19 warnings.go:70] unknown field "delta"
  W0805 12:22:30.611372      19 warnings.go:70] unknown field "epsilon"
  W0805 12:22:30.611380      19 warnings.go:70] unknown field "gamma"
  Aug  5 12:22:31.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2011" for this suite. @ 08/05/23 12:22:31.167
• [3.104 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 08/05/23 12:22:31.173
  Aug  5 12:22:31.173: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 12:22:31.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:22:31.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:22:31.194
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/05/23 12:22:31.219
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 12:22:31.225
  Aug  5 12:22:31.232: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:31.232: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:31.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:22:31.236: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:22:32.240: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:32.240: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:32.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:22:32.244: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:22:33.241: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:33.241: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:33.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:22:33.245: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/05/23 12:22:33.251
  Aug  5 12:22:33.270: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:33.271: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:33.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 12:22:33.280: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:22:34.285: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:34.285: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:22:34.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:22:34.289: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/05/23 12:22:34.289
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 12:22:34.295
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4560, will wait for the garbage collector to delete the pods @ 08/05/23 12:22:34.295
  Aug  5 12:22:34.355: INFO: Deleting DaemonSet.extensions daemon-set took: 6.299173ms
  Aug  5 12:22:34.455: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.120863ms
  Aug  5 12:22:36.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:22:36.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 12:22:36.365: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8417"},"items":null}

  Aug  5 12:22:36.369: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8417"},"items":null}

  Aug  5 12:22:36.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4560" for this suite. @ 08/05/23 12:22:36.388
• [5.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/05/23 12:22:36.396
  Aug  5 12:22:36.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-runtime @ 08/05/23 12:22:36.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:22:36.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:22:36.416
  STEP: create the container @ 08/05/23 12:22:36.42
  W0805 12:22:36.432330      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/05/23 12:22:36.432
  STEP: get the container status @ 08/05/23 12:22:39.449
  STEP: the container should be terminated @ 08/05/23 12:22:39.452
  STEP: the termination message should be set @ 08/05/23 12:22:39.452
  Aug  5 12:22:39.452: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/05/23 12:22:39.452
  Aug  5 12:22:39.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4941" for this suite. @ 08/05/23 12:22:39.474
• [3.085 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/05/23 12:22:39.483
  Aug  5 12:22:39.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:22:39.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:22:39.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:22:39.509
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-2814801a-5dcc-49f5-837c-4c080007ddb2 @ 08/05/23 12:22:39.516
  STEP: Creating the pod @ 08/05/23 12:22:39.521
  STEP: Updating configmap projected-configmap-test-upd-2814801a-5dcc-49f5-837c-4c080007ddb2 @ 08/05/23 12:22:41.552
  STEP: waiting to observe update in volume @ 08/05/23 12:22:41.559
  Aug  5 12:23:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2743" for this suite. @ 08/05/23 12:23:57.9
• [78.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/05/23 12:23:57.908
  Aug  5 12:23:57.908: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 12:23:57.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:23:57.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:23:57.928
  STEP: Creating service test in namespace statefulset-8228 @ 08/05/23 12:23:57.932
  STEP: Creating a new StatefulSet @ 08/05/23 12:23:57.938
  Aug  5 12:23:57.951: INFO: Found 0 stateful pods, waiting for 3
  Aug  5 12:24:07.957: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 12:24:07.957: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 12:24:07.957: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 12:24:07.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8228 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 12:24:08.081: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 12:24:08.081: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 12:24:08.081: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/05/23 12:24:18.098
  Aug  5 12:24:18.119: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/05/23 12:24:18.119
  STEP: Updating Pods in reverse ordinal order @ 08/05/23 12:24:28.135
  Aug  5 12:24:28.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8228 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 12:24:28.275: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 12:24:28.275: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 12:24:28.275: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 12:24:38.303: INFO: Waiting for StatefulSet statefulset-8228/ss2 to complete update
  Aug  5 12:24:38.303: INFO: Waiting for Pod statefulset-8228/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Rolling back to a previous revision @ 08/05/23 12:24:48.311
  Aug  5 12:24:48.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8228 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 12:24:48.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 12:24:48.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 12:24:48.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 12:24:58.490: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 08/05/23 12:25:08.505
  Aug  5 12:25:08.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8228 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 12:25:08.627: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 12:25:08.627: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 12:25:08.627: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 12:25:18.653: INFO: Deleting all statefulset in ns statefulset-8228
  Aug  5 12:25:18.656: INFO: Scaling statefulset ss2 to 0
  Aug  5 12:25:28.675: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 12:25:28.678: INFO: Deleting statefulset ss2
  Aug  5 12:25:28.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8228" for this suite. @ 08/05/23 12:25:28.705
• [90.805 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/05/23 12:25:28.716
  Aug  5 12:25:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename runtimeclass @ 08/05/23 12:25:28.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:28.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:28.737
  STEP: getting /apis @ 08/05/23 12:25:28.741
  STEP: getting /apis/node.k8s.io @ 08/05/23 12:25:28.744
  STEP: getting /apis/node.k8s.io/v1 @ 08/05/23 12:25:28.746
  STEP: creating @ 08/05/23 12:25:28.747
  STEP: watching @ 08/05/23 12:25:28.763
  Aug  5 12:25:28.763: INFO: starting watch
  STEP: getting @ 08/05/23 12:25:28.77
  STEP: listing @ 08/05/23 12:25:28.773
  STEP: patching @ 08/05/23 12:25:28.776
  STEP: updating @ 08/05/23 12:25:28.781
  Aug  5 12:25:28.785: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/05/23 12:25:28.786
  STEP: deleting a collection @ 08/05/23 12:25:28.802
  Aug  5 12:25:28.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2062" for this suite. @ 08/05/23 12:25:28.82
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/05/23 12:25:28.828
  Aug  5 12:25:28.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:25:28.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:28.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:28.847
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/05/23 12:25:28.851
  Aug  5 12:25:28.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-9508 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug  5 12:25:28.922: INFO: stderr: ""
  Aug  5 12:25:28.922: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/05/23 12:25:28.922
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/05/23 12:25:33.974
  Aug  5 12:25:33.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-9508 get pod e2e-test-httpd-pod -o json'
  Aug  5 12:25:34.035: INFO: stderr: ""
  Aug  5 12:25:34.035: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-05T12:25:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9508\",\n        \"resourceVersion\": \"9286\",\n        \"uid\": \"6cb0a518-e7fd-4d77-bec8-0012048f2d73\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-phds6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-95-133\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-phds6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-05T12:25:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-05T12:25:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-05T12:25:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-05T12:25:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e55f79817cbfe6adfd3f27440c4d33c0a8be232abe81d4e88a281635bff4089e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-05T12:25:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.95.133\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.52.128\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.52.128\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-05T12:25:28Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/05/23 12:25:34.035
  Aug  5 12:25:34.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-9508 replace -f -'
  Aug  5 12:25:34.291: INFO: stderr: ""
  Aug  5 12:25:34.291: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/05/23 12:25:34.291
  Aug  5 12:25:34.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-9508 delete pods e2e-test-httpd-pod'
  Aug  5 12:25:35.809: INFO: stderr: ""
  Aug  5 12:25:35.809: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  5 12:25:35.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9508" for this suite. @ 08/05/23 12:25:35.813
• [6.993 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/05/23 12:25:35.821
  Aug  5 12:25:35.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:25:35.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:35.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:35.839
  STEP: Creating projection with secret that has name projected-secret-test-map-b233769a-479d-485d-9980-74b083b200df @ 08/05/23 12:25:35.842
  STEP: Creating a pod to test consume secrets @ 08/05/23 12:25:35.846
  STEP: Saw pod success @ 08/05/23 12:25:39.871
  Aug  5 12:25:39.875: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-secrets-64b75579-9dd5-490e-a7cd-214c80fa3587 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 12:25:39.896
  Aug  5 12:25:39.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1807" for this suite. @ 08/05/23 12:25:39.917
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/05/23 12:25:39.927
  Aug  5 12:25:39.927: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption @ 08/05/23 12:25:39.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:39.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:39.945
  STEP: creating the pdb @ 08/05/23 12:25:39.949
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:25:39.953
  STEP: updating the pdb @ 08/05/23 12:25:41.963
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:25:41.972
  STEP: patching the pdb @ 08/05/23 12:25:41.977
  STEP: Waiting for the pdb to be processed @ 08/05/23 12:25:41.986
  STEP: Waiting for the pdb to be deleted @ 08/05/23 12:25:44.003
  Aug  5 12:25:44.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1233" for this suite. @ 08/05/23 12:25:44.01
• [4.090 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/05/23 12:25:44.017
  Aug  5 12:25:44.017: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:25:44.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:44.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:44.039
  STEP: Setting up server cert @ 08/05/23 12:25:44.059
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:25:44.34
  STEP: Deploying the webhook pod @ 08/05/23 12:25:44.348
  STEP: Wait for the deployment to be ready @ 08/05/23 12:25:44.36
  Aug  5 12:25:44.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:25:46.38
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:25:46.39
  Aug  5 12:25:47.391: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  5 12:25:47.395: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/05/23 12:25:47.906
  STEP: Creating a custom resource that should be denied by the webhook @ 08/05/23 12:25:47.922
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/05/23 12:25:49.947
  STEP: Updating the custom resource with disallowed data should be denied @ 08/05/23 12:25:49.954
  STEP: Deleting the custom resource should be denied @ 08/05/23 12:25:49.963
  STEP: Remove the offending key and value from the custom resource data @ 08/05/23 12:25:49.971
  STEP: Deleting the updated custom resource should be successful @ 08/05/23 12:25:49.981
  Aug  5 12:25:49.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7767" for this suite. @ 08/05/23 12:25:50.559
  STEP: Destroying namespace "webhook-markers-6842" for this suite. @ 08/05/23 12:25:50.565
• [6.556 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/05/23 12:25:50.574
  Aug  5 12:25:50.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 12:25:50.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:50.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:50.597
  STEP: Creating a test namespace @ 08/05/23 12:25:50.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:50.613
  STEP: Creating a service in the namespace @ 08/05/23 12:25:50.618
  STEP: Deleting the namespace @ 08/05/23 12:25:50.629
  STEP: Waiting for the namespace to be removed. @ 08/05/23 12:25:50.636
  STEP: Recreating the namespace @ 08/05/23 12:25:56.641
  STEP: Verifying there is no service in the namespace @ 08/05/23 12:25:56.654
  Aug  5 12:25:56.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1970" for this suite. @ 08/05/23 12:25:56.661
  STEP: Destroying namespace "nsdeletetest-4151" for this suite. @ 08/05/23 12:25:56.667
  Aug  5 12:25:56.671: INFO: Namespace nsdeletetest-4151 was already deleted
  STEP: Destroying namespace "nsdeletetest-9645" for this suite. @ 08/05/23 12:25:56.671
• [6.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/05/23 12:25:56.677
  Aug  5 12:25:56.677: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 12:25:56.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:25:56.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:25:56.7
  STEP: creating secret secrets-630/secret-test-e8c6590d-73fe-4ed9-a2f6-8149c24d75f7 @ 08/05/23 12:25:56.703
  STEP: Creating a pod to test consume secrets @ 08/05/23 12:25:56.709
  STEP: Saw pod success @ 08/05/23 12:26:00.735
  Aug  5 12:26:00.738: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-fc3a7d93-a269-49ef-87cf-ff3b6e4967e6 container env-test: <nil>
  STEP: delete the pod @ 08/05/23 12:26:00.746
  Aug  5 12:26:00.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-630" for this suite. @ 08/05/23 12:26:00.767
• [4.096 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/05/23 12:26:00.774
  Aug  5 12:26:00.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 12:26:00.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:26:00.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:26:00.793
  STEP: Creating pod liveness-31be026c-a98b-408b-87ec-e81f17963ebe in namespace container-probe-844 @ 08/05/23 12:26:00.796
  Aug  5 12:26:02.815: INFO: Started pod liveness-31be026c-a98b-408b-87ec-e81f17963ebe in namespace container-probe-844
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 12:26:02.815
  Aug  5 12:26:02.818: INFO: Initial restart count of pod liveness-31be026c-a98b-408b-87ec-e81f17963ebe is 0
  Aug  5 12:30:03.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:30:03.389
  STEP: Destroying namespace "container-probe-844" for this suite. @ 08/05/23 12:30:03.403
• [242.636 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/05/23 12:30:03.411
  Aug  5 12:30:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename watch @ 08/05/23 12:30:03.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:30:03.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:30:03.433
  STEP: creating a watch on configmaps with label A @ 08/05/23 12:30:03.436
  STEP: creating a watch on configmaps with label B @ 08/05/23 12:30:03.438
  STEP: creating a watch on configmaps with label A or B @ 08/05/23 12:30:03.44
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/05/23 12:30:03.441
  Aug  5 12:30:03.446: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10167 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:03.447: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10167 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/05/23 12:30:03.447
  Aug  5 12:30:03.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10168 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:03.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10168 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/05/23 12:30:03.456
  Aug  5 12:30:03.464: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10169 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:03.464: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10169 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/05/23 12:30:03.464
  Aug  5 12:30:03.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10170 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:03.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5124  b66451a9-fa9b-465c-a444-132c04399040 10170 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/05/23 12:30:03.47
  Aug  5 12:30:03.475: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5124  61f74872-c6ec-48c9-b91a-a2ddef384931 10171 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:03.475: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5124  61f74872-c6ec-48c9-b91a-a2ddef384931 10171 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/05/23 12:30:13.475
  Aug  5 12:30:13.484: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5124  61f74872-c6ec-48c9-b91a-a2ddef384931 10205 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:13.484: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5124  61f74872-c6ec-48c9-b91a-a2ddef384931 10205 0 2023-08-05 12:30:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-05 12:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:30:23.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5124" for this suite. @ 08/05/23 12:30:23.49
• [20.087 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/05/23 12:30:23.498
  Aug  5 12:30:23.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/05/23 12:30:23.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:30:23.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:30:23.522
  STEP: create the container to handle the HTTPGet hook request. @ 08/05/23 12:30:23.529
  STEP: create the pod with lifecycle hook @ 08/05/23 12:30:25.552
  STEP: delete the pod with lifecycle hook @ 08/05/23 12:30:27.573
  STEP: check prestop hook @ 08/05/23 12:30:29.59
  Aug  5 12:30:29.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9514" for this suite. @ 08/05/23 12:30:29.615
• [6.123 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/05/23 12:30:29.622
  Aug  5 12:30:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 12:30:29.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:30:29.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:30:29.639
  STEP: create the rc1 @ 08/05/23 12:30:29.648
  STEP: create the rc2 @ 08/05/23 12:30:29.654
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/05/23 12:30:35.665
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/05/23 12:30:36.176
  STEP: wait for the rc to be deleted @ 08/05/23 12:30:36.184
  Aug  5 12:30:41.205: INFO: 72 pods remaining
  Aug  5 12:30:41.205: INFO: 72 pods has nil DeletionTimestamp
  Aug  5 12:30:41.205: INFO: 
  STEP: Gathering metrics @ 08/05/23 12:30:46.199
  W0805 12:30:46.203716      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 12:30:46.203: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 12:30:46.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q5cf" in namespace "gc-2912"
  Aug  5 12:30:46.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tk6r" in namespace "gc-2912"
  Aug  5 12:30:46.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tmjs" in namespace "gc-2912"
  Aug  5 12:30:46.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wm2m" in namespace "gc-2912"
  Aug  5 12:30:46.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-4468f" in namespace "gc-2912"
  Aug  5 12:30:46.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-48qsb" in namespace "gc-2912"
  Aug  5 12:30:46.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhl7" in namespace "gc-2912"
  Aug  5 12:30:46.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kcth" in namespace "gc-2912"
  Aug  5 12:30:46.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mjzp" in namespace "gc-2912"
  Aug  5 12:30:46.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mrqj" in namespace "gc-2912"
  Aug  5 12:30:46.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tk4q" in namespace "gc-2912"
  Aug  5 12:30:46.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z2d6" in namespace "gc-2912"
  Aug  5 12:30:46.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-66zzg" in namespace "gc-2912"
  Aug  5 12:30:46.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dp2x" in namespace "gc-2912"
  Aug  5 12:30:46.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f2gw" in namespace "gc-2912"
  Aug  5 12:30:46.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nw8w" in namespace "gc-2912"
  Aug  5 12:30:46.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wwsq" in namespace "gc-2912"
  Aug  5 12:30:46.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xsn4" in namespace "gc-2912"
  Aug  5 12:30:46.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dhkc" in namespace "gc-2912"
  Aug  5 12:30:46.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t4nj" in namespace "gc-2912"
  Aug  5 12:30:46.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2thn" in namespace "gc-2912"
  Aug  5 12:30:46.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5nd4" in namespace "gc-2912"
  Aug  5 12:30:46.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpxvs" in namespace "gc-2912"
  Aug  5 12:30:46.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqn7v" in namespace "gc-2912"
  Aug  5 12:30:46.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-bthz8" in namespace "gc-2912"
  Aug  5 12:30:46.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvm28" in namespace "gc-2912"
  Aug  5 12:30:46.563: INFO: Deleting pod "simpletest-rc-to-be-deleted-chbkv" in namespace "gc-2912"
  Aug  5 12:30:46.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-clf4w" in namespace "gc-2912"
  Aug  5 12:30:46.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2v67" in namespace "gc-2912"
  Aug  5 12:30:46.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5zq2" in namespace "gc-2912"
  Aug  5 12:30:46.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7pbw" in namespace "gc-2912"
  Aug  5 12:30:46.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjdq5" in namespace "gc-2912"
  Aug  5 12:30:46.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkpkw" in namespace "gc-2912"
  Aug  5 12:30:46.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwsbd" in namespace "gc-2912"
  Aug  5 12:30:46.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzvwb" in namespace "gc-2912"
  Aug  5 12:30:46.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7h99" in namespace "gc-2912"
  Aug  5 12:30:46.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7w2g" in namespace "gc-2912"
  Aug  5 12:30:46.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp5j9" in namespace "gc-2912"
  Aug  5 12:30:46.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvntm" in namespace "gc-2912"
  Aug  5 12:30:46.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxdvl" in namespace "gc-2912"
  Aug  5 12:30:46.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-h25qq" in namespace "gc-2912"
  Aug  5 12:30:46.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht8b2" in namespace "gc-2912"
  Aug  5 12:30:46.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwmp2" in namespace "gc-2912"
  Aug  5 12:30:46.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxhkz" in namespace "gc-2912"
  Aug  5 12:30:46.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb7hr" in namespace "gc-2912"
  Aug  5 12:30:46.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhvcw" in namespace "gc-2912"
  Aug  5 12:30:46.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkbcq" in namespace "gc-2912"
  Aug  5 12:30:46.887: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqt6q" in namespace "gc-2912"
  Aug  5 12:30:46.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwdfg" in namespace "gc-2912"
  Aug  5 12:30:46.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwzt2" in namespace "gc-2912"
  Aug  5 12:30:46.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2912" for this suite. @ 08/05/23 12:30:46.939
• [17.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/05/23 12:30:46.951
  Aug  5 12:30:46.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 12:30:46.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:30:46.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:30:46.971
  STEP: Creating a test externalName service @ 08/05/23 12:30:46.975
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:30:46.98
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:30:46.98
  STEP: creating a pod to probe DNS @ 08/05/23 12:30:46.98
  STEP: submitting the pod to kubernetes @ 08/05/23 12:30:46.98
  STEP: retrieving the pod @ 08/05/23 12:30:57.021
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:30:57.024
  Aug  5 12:30:57.033: INFO: DNS probes using dns-test-a672f9cf-bfc5-447c-8d95-20ef08a9ee7e succeeded

  STEP: changing the externalName to bar.example.com @ 08/05/23 12:30:57.033
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:30:57.043
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:30:57.043
  STEP: creating a second pod to probe DNS @ 08/05/23 12:30:57.043
  STEP: submitting the pod to kubernetes @ 08/05/23 12:30:57.043
  STEP: retrieving the pod @ 08/05/23 12:31:05.076
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:31:05.08
  Aug  5 12:31:05.090: INFO: DNS probes using dns-test-9f0544b4-17b2-4779-9350-a12c064375aa succeeded

  STEP: changing the service to type=ClusterIP @ 08/05/23 12:31:05.09
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:31:05.114
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9427.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9427.svc.cluster.local; sleep 1; done
   @ 08/05/23 12:31:05.114
  STEP: creating a third pod to probe DNS @ 08/05/23 12:31:05.114
  STEP: submitting the pod to kubernetes @ 08/05/23 12:31:05.119
  STEP: retrieving the pod @ 08/05/23 12:31:07.139
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:31:07.143
  Aug  5 12:31:07.152: INFO: DNS probes using dns-test-7592e578-1d23-41eb-a82d-987b21583287 succeeded

  Aug  5 12:31:07.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:31:07.156
  STEP: deleting the pod @ 08/05/23 12:31:07.169
  STEP: deleting the pod @ 08/05/23 12:31:07.184
  STEP: deleting the test externalName service @ 08/05/23 12:31:07.206
  STEP: Destroying namespace "dns-9427" for this suite. @ 08/05/23 12:31:07.225
• [20.282 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/05/23 12:31:07.234
  Aug  5 12:31:07.234: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:31:07.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:07.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:07.252
  STEP: Creating configMap with name projected-configmap-test-volume-5d5bc6ed-bfd6-485e-b70d-6a198cf61c82 @ 08/05/23 12:31:07.256
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:31:07.261
  STEP: Saw pod success @ 08/05/23 12:31:11.284
  Aug  5 12:31:11.288: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-c87fe50d-22fd-4299-8fc9-a93b921d4970 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:31:11.308
  Aug  5 12:31:11.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-567" for this suite. @ 08/05/23 12:31:11.33
• [4.103 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 08/05/23 12:31:11.337
  Aug  5 12:31:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 12:31:11.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:11.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:11.356
  Aug  5 12:31:11.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  W0805 12:31:11.363737      19 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000e79d00 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0805 12:31:13.919495      19 warnings.go:70] unknown field "alpha"
  W0805 12:31:13.919515      19 warnings.go:70] unknown field "beta"
  W0805 12:31:13.919522      19 warnings.go:70] unknown field "delta"
  W0805 12:31:13.919528      19 warnings.go:70] unknown field "epsilon"
  W0805 12:31:13.919540      19 warnings.go:70] unknown field "gamma"
  Aug  5 12:31:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7394" for this suite. @ 08/05/23 12:31:14.473
• [3.145 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/05/23 12:31:14.482
  Aug  5 12:31:14.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:31:14.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:14.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:14.498
  STEP: Creating configMap with name configmap-test-volume-864786fd-f922-472d-b6a1-d5b2013a638d @ 08/05/23 12:31:14.502
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:31:14.507
  STEP: Saw pod success @ 08/05/23 12:31:18.532
  Aug  5 12:31:18.535: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-70dfcab9-31fb-4815-a1b9-5594c5bd89a2 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:31:18.543
  Aug  5 12:31:18.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9924" for this suite. @ 08/05/23 12:31:18.567
• [4.091 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/05/23 12:31:18.574
  Aug  5 12:31:18.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 12:31:18.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:18.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:18.642
  STEP: creating a Service @ 08/05/23 12:31:18.649
  STEP: watching for the Service to be added @ 08/05/23 12:31:18.659
  Aug  5 12:31:18.661: INFO: Found Service test-service-gfrtf in namespace services-9520 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug  5 12:31:18.661: INFO: Service test-service-gfrtf created
  STEP: Getting /status @ 08/05/23 12:31:18.661
  Aug  5 12:31:18.668: INFO: Service test-service-gfrtf has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/05/23 12:31:18.668
  STEP: watching for the Service to be patched @ 08/05/23 12:31:18.674
  Aug  5 12:31:18.676: INFO: observed Service test-service-gfrtf in namespace services-9520 with annotations: map[] & LoadBalancer: {[]}
  Aug  5 12:31:18.676: INFO: Found Service test-service-gfrtf in namespace services-9520 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug  5 12:31:18.676: INFO: Service test-service-gfrtf has service status patched
  STEP: updating the ServiceStatus @ 08/05/23 12:31:18.676
  Aug  5 12:31:18.686: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/05/23 12:31:18.686
  Aug  5 12:31:18.688: INFO: Observed Service test-service-gfrtf in namespace services-9520 with annotations: map[] & Conditions: {[]}
  Aug  5 12:31:18.688: INFO: Observed event: &Service{ObjectMeta:{test-service-gfrtf  services-9520  633e4168-5faf-422a-82f0-115dbe16b6ed 13090 0 2023-08-05 12:31:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-05 12:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-05 12:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.146,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.146],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug  5 12:31:18.689: INFO: Found Service test-service-gfrtf in namespace services-9520 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  5 12:31:18.689: INFO: Service test-service-gfrtf has service status updated
  STEP: patching the service @ 08/05/23 12:31:18.689
  STEP: watching for the Service to be patched @ 08/05/23 12:31:18.701
  Aug  5 12:31:18.703: INFO: observed Service test-service-gfrtf in namespace services-9520 with labels: map[test-service-static:true]
  Aug  5 12:31:18.703: INFO: observed Service test-service-gfrtf in namespace services-9520 with labels: map[test-service-static:true]
  Aug  5 12:31:18.703: INFO: observed Service test-service-gfrtf in namespace services-9520 with labels: map[test-service-static:true]
  Aug  5 12:31:18.703: INFO: Found Service test-service-gfrtf in namespace services-9520 with labels: map[test-service:patched test-service-static:true]
  Aug  5 12:31:18.703: INFO: Service test-service-gfrtf patched
  STEP: deleting the service @ 08/05/23 12:31:18.703
  STEP: watching for the Service to be deleted @ 08/05/23 12:31:18.718
  Aug  5 12:31:18.720: INFO: Observed event: ADDED
  Aug  5 12:31:18.720: INFO: Observed event: MODIFIED
  Aug  5 12:31:18.720: INFO: Observed event: MODIFIED
  Aug  5 12:31:18.720: INFO: Observed event: MODIFIED
  Aug  5 12:31:18.720: INFO: Found Service test-service-gfrtf in namespace services-9520 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug  5 12:31:18.721: INFO: Service test-service-gfrtf deleted
  Aug  5 12:31:18.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9520" for this suite. @ 08/05/23 12:31:18.725
• [0.158 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/05/23 12:31:18.732
  Aug  5 12:31:18.732: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename podtemplate @ 08/05/23 12:31:18.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:18.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:18.75
  STEP: Create a pod template @ 08/05/23 12:31:18.753
  STEP: Replace a pod template @ 08/05/23 12:31:18.758
  Aug  5 12:31:18.768: INFO: Found updated podtemplate annotation: "true"

  Aug  5 12:31:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7104" for this suite. @ 08/05/23 12:31:18.772
• [0.048 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/05/23 12:31:18.78
  Aug  5 12:31:18.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename podtemplate @ 08/05/23 12:31:18.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:18.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:18.798
  Aug  5 12:31:18.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1550" for this suite. @ 08/05/23 12:31:18.835
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/05/23 12:31:18.844
  Aug  5 12:31:18.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:31:18.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:18.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:18.859
  STEP: Setting up server cert @ 08/05/23 12:31:18.884
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:31:19.157
  STEP: Deploying the webhook pod @ 08/05/23 12:31:19.166
  STEP: Wait for the deployment to be ready @ 08/05/23 12:31:19.179
  Aug  5 12:31:19.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:31:21.2
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:31:21.211
  Aug  5 12:31:22.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/05/23 12:31:22.215
  STEP: create a pod @ 08/05/23 12:31:22.234
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/05/23 12:31:24.252
  Aug  5 12:31:24.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=webhook-3579 attach --namespace=webhook-3579 to-be-attached-pod -i -c=container1'
  Aug  5 12:31:24.325: INFO: rc: 1
  Aug  5 12:31:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3579" for this suite. @ 08/05/23 12:31:24.39
  STEP: Destroying namespace "webhook-markers-9460" for this suite. @ 08/05/23 12:31:24.397
• [5.560 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/05/23 12:31:24.404
  Aug  5 12:31:24.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 12:31:24.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:24.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:24.422
  Aug  5 12:31:24.426: INFO: Creating deployment "test-recreate-deployment"
  Aug  5 12:31:24.432: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug  5 12:31:24.440: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Aug  5 12:31:26.449: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug  5 12:31:26.452: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug  5 12:31:26.462: INFO: Updating deployment test-recreate-deployment
  Aug  5 12:31:26.462: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug  5 12:31:26.547: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9709  727e99c8-3eee-4151-b810-eeceb78db9ba 13282 2 2023-08-05 12:31:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00468cdb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-05 12:31:26 +0000 UTC,LastTransitionTime:2023-08-05 12:31:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-05 12:31:26 +0000 UTC,LastTransitionTime:2023-08-05 12:31:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug  5 12:31:26.551: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-9709  3d614557-89d0-4bc2-9147-5063d3cd809d 13279 1 2023-08-05 12:31:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 727e99c8-3eee-4151-b810-eeceb78db9ba 0xc00468d147 0xc00468d148}] [] [{kube-controller-manager Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"727e99c8-3eee-4151-b810-eeceb78db9ba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00468d1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:31:26.551: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug  5 12:31:26.551: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-9709  9c9a70dc-25dd-4224-9b55-cd2ae40b7896 13271 2 2023-08-05 12:31:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 727e99c8-3eee-4151-b810-eeceb78db9ba 0xc00468d257 0xc00468d258}] [] [{kube-controller-manager Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"727e99c8-3eee-4151-b810-eeceb78db9ba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00468d308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:31:26.555: INFO: Pod "test-recreate-deployment-54757ffd6c-xv8pt" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-xv8pt test-recreate-deployment-54757ffd6c- deployment-9709  98a7a686-50f2-4410-8c18-eaf2dd4fe811 13283 0 2023-08-05 12:31:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 3d614557-89d0-4bc2-9147-5063d3cd809d 0xc0045cf437 0xc0045cf438}] [] [{kube-controller-manager Update v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d614557-89d0-4bc2-9147-5063d3cd809d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 12:31:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7hwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7hwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:,StartTime:2023-08-05 12:31:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 12:31:26.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9709" for this suite. @ 08/05/23 12:31:26.56
• [2.163 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/05/23 12:31:26.567
  Aug  5 12:31:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 12:31:26.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:26.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:26.587
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/05/23 12:31:26.591
  Aug  5 12:31:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:31:27.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:31:33.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4555" for this suite. @ 08/05/23 12:31:33.134
• [6.574 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/05/23 12:31:33.142
  Aug  5 12:31:33.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:31:33.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:33.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:33.168
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:31:33.17
  STEP: Saw pod success @ 08/05/23 12:31:37.192
  Aug  5 12:31:37.196: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-a31c8254-f578-47c4-9e4b-55ac74aecdac container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:31:37.212
  Aug  5 12:31:37.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7760" for this suite. @ 08/05/23 12:31:37.233
• [4.097 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/05/23 12:31:37.241
  Aug  5 12:31:37.241: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context @ 08/05/23 12:31:37.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:37.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:37.262
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/05/23 12:31:37.269
  STEP: Saw pod success @ 08/05/23 12:31:41.292
  Aug  5 12:31:41.296: INFO: Trying to get logs from node ip-172-31-95-133 pod security-context-b78befaa-ddac-46c1-a3f9-7420f4e7ab8d container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:31:41.304
  Aug  5 12:31:41.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4044" for this suite. @ 08/05/23 12:31:41.326
• [4.092 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/05/23 12:31:41.333
  Aug  5 12:31:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 12:31:41.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:31:41.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:31:41.352
  Aug  5 12:31:41.366: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Aug  5 12:31:46.371: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 12:31:46.371
  Aug  5 12:31:46.371: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Aug  5 12:31:48.376: INFO: Creating deployment "test-rollover-deployment"
  Aug  5 12:31:48.386: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Aug  5 12:31:50.396: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug  5 12:31:50.404: INFO: Ensure that both replica sets have 1 created replica
  Aug  5 12:31:50.410: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug  5 12:31:50.419: INFO: Updating deployment test-rollover-deployment
  Aug  5 12:31:50.419: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Aug  5 12:31:52.428: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug  5 12:31:52.435: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug  5 12:31:52.442: INFO: all replica sets need to contain the pod-template-hash label
  Aug  5 12:31:52.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug  5 12:31:54.450: INFO: all replica sets need to contain the pod-template-hash label
  Aug  5 12:31:54.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug  5 12:31:56.451: INFO: all replica sets need to contain the pod-template-hash label
  Aug  5 12:31:56.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug  5 12:31:58.451: INFO: all replica sets need to contain the pod-template-hash label
  Aug  5 12:31:58.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug  5 12:32:00.452: INFO: all replica sets need to contain the pod-template-hash label
  Aug  5 12:32:00.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 31, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 31, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug  5 12:32:02.450: INFO: 
  Aug  5 12:32:02.450: INFO: Ensure that both old replica sets have no replicas
  Aug  5 12:32:02.461: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4040  8af96f26-c094-4e2b-af41-643949361a9b 13592 2 2023-08-05 12:31:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-05 12:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:32:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c0e9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-05 12:31:48 +0000 UTC,LastTransitionTime:2023-08-05 12:31:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-05 12:32:01 +0000 UTC,LastTransitionTime:2023-08-05 12:31:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  5 12:32:02.464: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-4040  79c2f3d1-bd32-4193-95a8-37201764ac00 13582 2 2023-08-05 12:31:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8af96f26-c094-4e2b-af41-643949361a9b 0xc003170c87 0xc003170c88}] [] [{kube-controller-manager Update apps/v1 2023-08-05 12:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8af96f26-c094-4e2b-af41-643949361a9b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:32:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003170d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:32:02.464: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug  5 12:32:02.465: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4040  ed4947cc-dc3c-495c-bbb4-612cae6ccecd 13591 2 2023-08-05 12:31:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8af96f26-c094-4e2b-af41-643949361a9b 0xc003170b57 0xc003170b58}] [] [{e2e.test Update apps/v1 2023-08-05 12:31:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:32:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8af96f26-c094-4e2b-af41-643949361a9b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:32:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003170c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:32:02.465: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-4040  b8a38860-14ca-4225-97df-1b84432da615 13545 2 2023-08-05 12:31:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8af96f26-c094-4e2b-af41-643949361a9b 0xc003170da7 0xc003170da8}] [] [{kube-controller-manager Update apps/v1 2023-08-05 12:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8af96f26-c094-4e2b-af41-643949361a9b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:31:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003170e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:32:02.468: INFO: Pod "test-rollover-deployment-57777854c9-q5nlw" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-q5nlw test-rollover-deployment-57777854c9- deployment-4040  fcb34580-335e-4a76-8f60-b6a019eb527a 13560 0 2023-08-05 12:31:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 79c2f3d1-bd32-4193-95a8-37201764ac00 0xc004c0ed67 0xc004c0ed68}] [] [{kube-controller-manager Update v1 2023-08-05 12:31:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79c2f3d1-bd32-4193-95a8-37201764ac00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 12:31:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-snlsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-snlsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:31:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.180,StartTime:2023-08-05 12:31:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 12:31:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4e4c9537a361c37b3ce3b3ab3e797f9413cffa921de7ec66c7171c1adf137ce0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.180,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 12:32:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4040" for this suite. @ 08/05/23 12:32:02.473
• [21.147 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/05/23 12:32:02.481
  Aug  5 12:32:02.481: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 12:32:02.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:32:02.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:32:02.501
  Aug  5 12:32:02.503: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/05/23 12:32:03.518
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/05/23 12:32:03.526
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/05/23 12:32:04.534
  Aug  5 12:32:04.544: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/05/23 12:32:04.545
  Aug  5 12:32:05.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3263" for this suite. @ 08/05/23 12:32:05.557
• [3.083 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/05/23 12:32:05.565
  Aug  5 12:32:05.565: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:32:05.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:32:05.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:32:05.586
  STEP: Creating a ResourceQuota with terminating scope @ 08/05/23 12:32:05.589
  STEP: Ensuring ResourceQuota status is calculated @ 08/05/23 12:32:05.597
  STEP: Creating a ResourceQuota with not terminating scope @ 08/05/23 12:32:07.601
  STEP: Ensuring ResourceQuota status is calculated @ 08/05/23 12:32:07.606
  STEP: Creating a long running pod @ 08/05/23 12:32:09.611
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/05/23 12:32:09.627
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/05/23 12:32:11.631
  STEP: Deleting the pod @ 08/05/23 12:32:13.636
  STEP: Ensuring resource quota status released the pod usage @ 08/05/23 12:32:13.654
  STEP: Creating a terminating pod @ 08/05/23 12:32:15.658
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/05/23 12:32:15.669
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/05/23 12:32:17.674
  STEP: Deleting the pod @ 08/05/23 12:32:19.678
  STEP: Ensuring resource quota status released the pod usage @ 08/05/23 12:32:19.693
  Aug  5 12:32:21.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1441" for this suite. @ 08/05/23 12:32:21.703
• [16.145 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/05/23 12:32:21.71
  Aug  5 12:32:21.710: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption @ 08/05/23 12:32:21.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:32:21.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:32:21.731
  Aug  5 12:32:21.748: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug  5 12:33:21.764: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/05/23 12:33:21.768
  Aug  5 12:33:21.768: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/05/23 12:33:21.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:33:21.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:33:21.789
  STEP: Finding an available node @ 08/05/23 12:33:21.791
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/05/23 12:33:21.791
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/05/23 12:33:23.814
  Aug  5 12:33:23.827: INFO: found a healthy node: ip-172-31-95-133
  Aug  5 12:33:29.898: INFO: pods created so far: [1 1 1]
  Aug  5 12:33:29.898: INFO: length of pods created so far: 3
  Aug  5 12:33:31.908: INFO: pods created so far: [2 2 1]
  Aug  5 12:33:38.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 12:33:38.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8050" for this suite. @ 08/05/23 12:33:38.992
  STEP: Destroying namespace "sched-preemption-8271" for this suite. @ 08/05/23 12:33:38.999
• [77.295 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/05/23 12:33:39.007
  Aug  5 12:33:39.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/05/23 12:33:39.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:33:39.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:33:39.028
  Aug  5 12:33:39.031: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:33:40.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2867" for this suite. @ 08/05/23 12:33:40.059
• [1.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/05/23 12:33:40.069
  Aug  5 12:33:40.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-pred @ 08/05/23 12:33:40.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:33:40.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:33:40.091
  Aug  5 12:33:40.094: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  5 12:33:40.103: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 12:33:40.106: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-47 before test
  Aug  5 12:33:40.112: INFO: nginx-ingress-controller-kubernetes-worker-87f7r from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: coredns-5c7f76ccb8-8kfns from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container coredns ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: kube-state-metrics-5b95b4459c-p5wzf from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: metrics-server-v0.5.2-6cf8c8b69c-vnlhz from kube-system started at 2023-08-05 11:58:13 +0000 UTC (2 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container metrics-server ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: dashboard-metrics-scraper-6b8586b5c9-ghxkg from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: kubernetes-dashboard-6869f4cd5f-mr7dz from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2 from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:33:40.112: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 12:33:40.112: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-35-140 before test
  Aug  5 12:33:40.119: INFO: nginx-ingress-controller-kubernetes-worker-49tnn from ingress-nginx-kubernetes-worker started at 2023-08-05 12:02:34 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.119: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: sonobuoy from sonobuoy started at 2023-08-05 12:07:56 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: sonobuoy-e2e-job-6ea4f22076f544af from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:33:40.119: INFO: 	Container e2e ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-bxrtb from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:33:40.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 12:33:40.119: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-95-133 before test
  Aug  5 12:33:40.124: INFO: default-http-backend-kubernetes-worker-65fc475d49-l68k2 from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: nginx-ingress-controller-kubernetes-worker-hznm4 from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:19 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: calico-kube-controllers-5b8cb49547-z96wd from kube-system started at 2023-08-05 11:58:29 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: pod4 from sched-preemption-path-8050 started at 2023-08-05 12:33:31 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container pod4 ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: rs-pod3-h9btb from sched-preemption-path-8050 started at 2023-08-05 12:33:27 +0000 UTC (1 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container pod3 ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-kjhcj from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:33:40.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:33:40.124: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-1-47 @ 08/05/23 12:33:44.153
  STEP: verifying the node has the label node ip-172-31-35-140 @ 08/05/23 12:33:44.174
  STEP: verifying the node has the label node ip-172-31-95-133 @ 08/05/23 12:33:44.191
  Aug  5 12:33:44.203: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-l68k2 requesting resource cpu=10m on Node ip-172-31-95-133
  Aug  5 12:33:44.203: INFO: Pod nginx-ingress-controller-kubernetes-worker-49tnn requesting resource cpu=0m on Node ip-172-31-35-140
  Aug  5 12:33:44.203: INFO: Pod nginx-ingress-controller-kubernetes-worker-87f7r requesting resource cpu=0m on Node ip-172-31-1-47
  Aug  5 12:33:44.203: INFO: Pod nginx-ingress-controller-kubernetes-worker-hznm4 requesting resource cpu=0m on Node ip-172-31-95-133
  Aug  5 12:33:44.203: INFO: Pod calico-kube-controllers-5b8cb49547-z96wd requesting resource cpu=0m on Node ip-172-31-95-133
  Aug  5 12:33:44.203: INFO: Pod coredns-5c7f76ccb8-8kfns requesting resource cpu=100m on Node ip-172-31-1-47
  Aug  5 12:33:44.203: INFO: Pod kube-state-metrics-5b95b4459c-p5wzf requesting resource cpu=0m on Node ip-172-31-1-47
  Aug  5 12:33:44.203: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-vnlhz requesting resource cpu=5m on Node ip-172-31-1-47
  Aug  5 12:33:44.203: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-ghxkg requesting resource cpu=0m on Node ip-172-31-1-47
  Aug  5 12:33:44.204: INFO: Pod kubernetes-dashboard-6869f4cd5f-mr7dz requesting resource cpu=0m on Node ip-172-31-1-47
  Aug  5 12:33:44.204: INFO: Pod pod4 requesting resource cpu=0m on Node ip-172-31-95-133
  Aug  5 12:33:44.204: INFO: Pod rs-pod3-h9btb requesting resource cpu=0m on Node ip-172-31-95-133
  Aug  5 12:33:44.204: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-35-140
  Aug  5 12:33:44.204: INFO: Pod sonobuoy-e2e-job-6ea4f22076f544af requesting resource cpu=0m on Node ip-172-31-35-140
  Aug  5 12:33:44.204: INFO: Pod sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-bxrtb requesting resource cpu=0m on Node ip-172-31-35-140
  Aug  5 12:33:44.204: INFO: Pod sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-kjhcj requesting resource cpu=0m on Node ip-172-31-95-133
  Aug  5 12:33:44.204: INFO: Pod sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2 requesting resource cpu=0m on Node ip-172-31-1-47
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/05/23 12:33:44.204
  Aug  5 12:33:44.204: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-35-140
  Aug  5 12:33:44.214: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-95-133
  Aug  5 12:33:44.220: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-1-47
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/05/23 12:33:46.249
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a.17787cc0054f74fb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1745/filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a to ip-172-31-1-47] @ 08/05/23 12:33:46.253
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a.17787cc02af0d9a2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/05/23 12:33:46.253
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a.17787cc02bf726b6], Reason = [Created], Message = [Created container filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a.17787cc02fce0486], Reason = [Started], Message = [Started container filler-pod-4d24f36b-b345-4b0e-875b-c5081f78787a] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a01332cb-020c-4000-a982-171e0a088db8.17787cc00462adf5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1745/filler-pod-a01332cb-020c-4000-a982-171e0a088db8 to ip-172-31-95-133] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a01332cb-020c-4000-a982-171e0a088db8.17787cc036ac4e17], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a01332cb-020c-4000-a982-171e0a088db8.17787cc0385622dc], Reason = [Created], Message = [Created container filler-pod-a01332cb-020c-4000-a982-171e0a088db8] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a01332cb-020c-4000-a982-171e0a088db8.17787cc03e32d948], Reason = [Started], Message = [Started container filler-pod-a01332cb-020c-4000-a982-171e0a088db8] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24.17787cc00438959a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1745/filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24 to ip-172-31-35-140] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24.17787cc02991abf4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24.17787cc02aa18e4d], Reason = [Created], Message = [Created container filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24.17787cc02e999b79], Reason = [Started], Message = [Started container filler-pod-e3e18f37-73b6-40cf-a50a-4b6acafc7a24] @ 08/05/23 12:33:46.254
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17787cc07d908670], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 08/05/23 12:33:46.265
  STEP: removing the label node off the node ip-172-31-1-47 @ 08/05/23 12:33:47.265
  STEP: verifying the node doesn't have the label node @ 08/05/23 12:33:47.277
  STEP: removing the label node off the node ip-172-31-35-140 @ 08/05/23 12:33:47.281
  STEP: verifying the node doesn't have the label node @ 08/05/23 12:33:47.302
  STEP: removing the label node off the node ip-172-31-95-133 @ 08/05/23 12:33:47.307
  STEP: verifying the node doesn't have the label node @ 08/05/23 12:33:47.324
  Aug  5 12:33:47.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1745" for this suite. @ 08/05/23 12:33:47.335
• [7.276 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/05/23 12:33:47.346
  Aug  5 12:33:47.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:33:47.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:33:47.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:33:47.369
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/05/23 12:33:47.372
  STEP: Saw pod success @ 08/05/23 12:33:51.405
  Aug  5 12:33:51.408: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-d65cb89b-3e96-4191-b6cb-b35c643cbe97 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:33:51.425
  Aug  5 12:33:51.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1596" for this suite. @ 08/05/23 12:33:51.447
• [4.108 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/05/23 12:33:51.455
  Aug  5 12:33:51.455: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subpath @ 08/05/23 12:33:51.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:33:51.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:33:51.478
  STEP: Setting up data @ 08/05/23 12:33:51.48
  STEP: Creating pod pod-subpath-test-configmap-grd8 @ 08/05/23 12:33:51.492
  STEP: Creating a pod to test atomic-volume-subpath @ 08/05/23 12:33:51.492
  STEP: Saw pod success @ 08/05/23 12:34:15.565
  Aug  5 12:34:15.569: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-subpath-test-configmap-grd8 container test-container-subpath-configmap-grd8: <nil>
  STEP: delete the pod @ 08/05/23 12:34:15.577
  STEP: Deleting pod pod-subpath-test-configmap-grd8 @ 08/05/23 12:34:15.595
  Aug  5 12:34:15.595: INFO: Deleting pod "pod-subpath-test-configmap-grd8" in namespace "subpath-7050"
  Aug  5 12:34:15.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7050" for this suite. @ 08/05/23 12:34:15.603
• [24.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/05/23 12:34:15.612
  Aug  5 12:34:15.612: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:34:15.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:15.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:15.632
  STEP: validating cluster-info @ 08/05/23 12:34:15.634
  Aug  5 12:34:15.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-395 cluster-info'
  Aug  5 12:34:15.695: INFO: stderr: ""
  Aug  5 12:34:15.695: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug  5 12:34:15.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-395" for this suite. @ 08/05/23 12:34:15.699
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/05/23 12:34:15.707
  Aug  5 12:34:15.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 12:34:15.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:15.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:15.73
  Aug  5 12:34:15.732: INFO: Creating ReplicaSet my-hostname-basic-7220d024-1d03-4747-9886-911f4a6cc345
  Aug  5 12:34:15.740: INFO: Pod name my-hostname-basic-7220d024-1d03-4747-9886-911f4a6cc345: Found 0 pods out of 1
  Aug  5 12:34:20.744: INFO: Pod name my-hostname-basic-7220d024-1d03-4747-9886-911f4a6cc345: Found 1 pods out of 1
  Aug  5 12:34:20.744: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7220d024-1d03-4747-9886-911f4a6cc345" is running
  Aug  5 12:34:20.749: INFO: Pod "my-hostname-basic-7220d024-1d03-4747-9886-911f4a6cc345-tnph8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 12:34:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 12:34:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 12:34:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 12:34:15 +0000 UTC Reason: Message:}])
  Aug  5 12:34:20.749: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/05/23 12:34:20.749
  Aug  5 12:34:20.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3796" for this suite. @ 08/05/23 12:34:20.767
• [5.068 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/05/23 12:34:20.776
  Aug  5 12:34:20.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 12:34:20.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:20.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:20.797
  STEP: Creating a job @ 08/05/23 12:34:20.799
  STEP: Ensuring job reaches completions @ 08/05/23 12:34:20.806
  Aug  5 12:34:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3712" for this suite. @ 08/05/23 12:34:30.816
• [10.047 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/05/23 12:34:30.824
  Aug  5 12:34:30.825: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:34:30.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:30.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:30.846
  STEP: Setting up server cert @ 08/05/23 12:34:30.871
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:34:31.26
  STEP: Deploying the webhook pod @ 08/05/23 12:34:31.269
  STEP: Wait for the deployment to be ready @ 08/05/23 12:34:31.28
  Aug  5 12:34:31.288: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:34:33.3
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:34:33.312
  Aug  5 12:34:34.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/05/23 12:34:34.316
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/05/23 12:34:34.333
  STEP: Creating a dummy validating-webhook-configuration object @ 08/05/23 12:34:34.348
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/05/23 12:34:34.356
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/05/23 12:34:34.363
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/05/23 12:34:34.37
  Aug  5 12:34:34.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3474" for this suite. @ 08/05/23 12:34:34.437
  STEP: Destroying namespace "webhook-markers-3154" for this suite. @ 08/05/23 12:34:34.443
• [3.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/05/23 12:34:34.452
  Aug  5 12:34:34.452: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:34:34.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:34.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:34.471
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/05/23 12:34:34.473
  STEP: Saw pod success @ 08/05/23 12:34:38.497
  Aug  5 12:34:38.501: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-598ed4bb-0645-4071-8533-8d68ed531b35 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:34:38.509
  Aug  5 12:34:38.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7002" for this suite. @ 08/05/23 12:34:38.527
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/05/23 12:34:38.538
  Aug  5 12:34:38.538: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 12:34:38.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:38.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:38.557
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:34:38.559
  STEP: Saw pod success @ 08/05/23 12:34:42.582
  Aug  5 12:34:42.585: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-82d9800d-22db-46fd-ba4a-9591044d6219 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:34:42.593
  Aug  5 12:34:42.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4774" for this suite. @ 08/05/23 12:34:42.615
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/05/23 12:34:42.626
  Aug  5 12:34:42.626: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename runtimeclass @ 08/05/23 12:34:42.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:42.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:42.649
  Aug  5 12:34:42.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2473" for this suite. @ 08/05/23 12:34:42.685
• [0.067 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/05/23 12:34:42.693
  Aug  5 12:34:42.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:34:42.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:42.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:42.714
  STEP: Counting existing ResourceQuota @ 08/05/23 12:34:42.717
  STEP: Creating a ResourceQuota @ 08/05/23 12:34:47.721
  STEP: Ensuring resource quota status is calculated @ 08/05/23 12:34:47.727
  STEP: Creating a Pod that fits quota @ 08/05/23 12:34:49.731
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/05/23 12:34:49.748
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/05/23 12:34:51.752
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/05/23 12:34:51.755
  STEP: Ensuring a pod cannot update its resource requirements @ 08/05/23 12:34:51.756
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/05/23 12:34:51.761
  STEP: Deleting the pod @ 08/05/23 12:34:53.766
  STEP: Ensuring resource quota status released the pod usage @ 08/05/23 12:34:53.782
  Aug  5 12:34:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1969" for this suite. @ 08/05/23 12:34:55.791
• [13.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/05/23 12:34:55.801
  Aug  5 12:34:55.801: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:34:55.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:34:55.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:34:55.823
  STEP: creating a replication controller @ 08/05/23 12:34:55.826
  Aug  5 12:34:55.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 create -f -'
  Aug  5 12:34:56.072: INFO: stderr: ""
  Aug  5 12:34:56.072: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/05/23 12:34:56.072
  Aug  5 12:34:56.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:34:56.138: INFO: stderr: ""
  Aug  5 12:34:56.139: INFO: stdout: "update-demo-nautilus-pf4pm update-demo-nautilus-xctlm "
  Aug  5 12:34:56.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:34:56.199: INFO: stderr: ""
  Aug  5 12:34:56.199: INFO: stdout: ""
  Aug  5 12:34:56.199: INFO: update-demo-nautilus-pf4pm is created but not running
  Aug  5 12:35:01.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:35:01.266: INFO: stderr: ""
  Aug  5 12:35:01.266: INFO: stdout: "update-demo-nautilus-pf4pm update-demo-nautilus-xctlm "
  Aug  5 12:35:01.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:01.329: INFO: stderr: ""
  Aug  5 12:35:01.329: INFO: stdout: "true"
  Aug  5 12:35:01.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 12:35:01.389: INFO: stderr: ""
  Aug  5 12:35:01.389: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 12:35:01.389: INFO: validating pod update-demo-nautilus-pf4pm
  Aug  5 12:35:01.393: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 12:35:01.394: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 12:35:01.394: INFO: update-demo-nautilus-pf4pm is verified up and running
  Aug  5 12:35:01.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-xctlm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:01.456: INFO: stderr: ""
  Aug  5 12:35:01.456: INFO: stdout: "true"
  Aug  5 12:35:01.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-xctlm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 12:35:01.516: INFO: stderr: ""
  Aug  5 12:35:01.516: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 12:35:01.516: INFO: validating pod update-demo-nautilus-xctlm
  Aug  5 12:35:01.522: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 12:35:01.522: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 12:35:01.522: INFO: update-demo-nautilus-xctlm is verified up and running
  STEP: scaling down the replication controller @ 08/05/23 12:35:01.522
  Aug  5 12:35:01.523: INFO: scanned /root for discovery docs: <nil>
  Aug  5 12:35:01.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Aug  5 12:35:02.603: INFO: stderr: ""
  Aug  5 12:35:02.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/05/23 12:35:02.603
  Aug  5 12:35:02.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:35:02.668: INFO: stderr: ""
  Aug  5 12:35:02.668: INFO: stdout: "update-demo-nautilus-pf4pm update-demo-nautilus-xctlm "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 08/05/23 12:35:02.668
  Aug  5 12:35:07.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:35:07.734: INFO: stderr: ""
  Aug  5 12:35:07.734: INFO: stdout: "update-demo-nautilus-pf4pm "
  Aug  5 12:35:07.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:07.797: INFO: stderr: ""
  Aug  5 12:35:07.797: INFO: stdout: "true"
  Aug  5 12:35:07.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 12:35:07.858: INFO: stderr: ""
  Aug  5 12:35:07.858: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 12:35:07.858: INFO: validating pod update-demo-nautilus-pf4pm
  Aug  5 12:35:07.863: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 12:35:07.863: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 12:35:07.863: INFO: update-demo-nautilus-pf4pm is verified up and running
  STEP: scaling up the replication controller @ 08/05/23 12:35:07.863
  Aug  5 12:35:07.864: INFO: scanned /root for discovery docs: <nil>
  Aug  5 12:35:07.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Aug  5 12:35:08.947: INFO: stderr: ""
  Aug  5 12:35:08.947: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/05/23 12:35:08.947
  Aug  5 12:35:08.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:35:09.012: INFO: stderr: ""
  Aug  5 12:35:09.012: INFO: stdout: "update-demo-nautilus-5mfjv update-demo-nautilus-pf4pm "
  Aug  5 12:35:09.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-5mfjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:09.075: INFO: stderr: ""
  Aug  5 12:35:09.075: INFO: stdout: ""
  Aug  5 12:35:09.075: INFO: update-demo-nautilus-5mfjv is created but not running
  Aug  5 12:35:14.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 12:35:14.143: INFO: stderr: ""
  Aug  5 12:35:14.143: INFO: stdout: "update-demo-nautilus-5mfjv update-demo-nautilus-pf4pm "
  Aug  5 12:35:14.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-5mfjv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:14.205: INFO: stderr: ""
  Aug  5 12:35:14.205: INFO: stdout: "true"
  Aug  5 12:35:14.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-5mfjv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 12:35:14.266: INFO: stderr: ""
  Aug  5 12:35:14.266: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 12:35:14.266: INFO: validating pod update-demo-nautilus-5mfjv
  Aug  5 12:35:14.273: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 12:35:14.273: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 12:35:14.273: INFO: update-demo-nautilus-5mfjv is verified up and running
  Aug  5 12:35:14.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 12:35:14.337: INFO: stderr: ""
  Aug  5 12:35:14.337: INFO: stdout: "true"
  Aug  5 12:35:14.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods update-demo-nautilus-pf4pm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 12:35:14.401: INFO: stderr: ""
  Aug  5 12:35:14.401: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 12:35:14.401: INFO: validating pod update-demo-nautilus-pf4pm
  Aug  5 12:35:14.405: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 12:35:14.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 12:35:14.405: INFO: update-demo-nautilus-pf4pm is verified up and running
  STEP: using delete to clean up resources @ 08/05/23 12:35:14.405
  Aug  5 12:35:14.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 delete --grace-period=0 --force -f -'
  Aug  5 12:35:14.471: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 12:35:14.471: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug  5 12:35:14.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get rc,svc -l name=update-demo --no-headers'
  Aug  5 12:35:14.565: INFO: stderr: "No resources found in kubectl-5107 namespace.\n"
  Aug  5 12:35:14.565: INFO: stdout: ""
  Aug  5 12:35:14.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-5107 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  5 12:35:14.636: INFO: stderr: ""
  Aug  5 12:35:14.636: INFO: stdout: ""
  Aug  5 12:35:14.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5107" for this suite. @ 08/05/23 12:35:14.641
• [18.847 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/05/23 12:35:14.649
  Aug  5 12:35:14.649: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 12:35:14.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:14.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:14.672
  STEP: creating a Deployment @ 08/05/23 12:35:14.68
  Aug  5 12:35:14.680: INFO: Creating simple deployment test-deployment-pzhlw
  Aug  5 12:35:14.692: INFO: deployment "test-deployment-pzhlw" doesn't have the required revision set
  STEP: Getting /status @ 08/05/23 12:35:16.707
  Aug  5 12:35:16.711: INFO: Deployment test-deployment-pzhlw has Conditions: [{Available True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pzhlw-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/05/23 12:35:16.711
  Aug  5 12:35:16.721: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 35, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 35, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 12, 35, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 12, 35, 14, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pzhlw-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/05/23 12:35:16.721
  Aug  5 12:35:16.722: INFO: Observed &Deployment event: ADDED
  Aug  5 12:35:16.722: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pzhlw-5994cf9475"}
  Aug  5 12:35:16.722: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.722: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pzhlw-5994cf9475"}
  Aug  5 12:35:16.722: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  5 12:35:16.723: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pzhlw-5994cf9475" is progressing.}
  Aug  5 12:35:16.723: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pzhlw-5994cf9475" has successfully progressed.}
  Aug  5 12:35:16.723: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  5 12:35:16.723: INFO: Observed Deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pzhlw-5994cf9475" has successfully progressed.}
  Aug  5 12:35:16.724: INFO: Found Deployment test-deployment-pzhlw in namespace deployment-2253 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 12:35:16.724: INFO: Deployment test-deployment-pzhlw has an updated status
  STEP: patching the Statefulset Status @ 08/05/23 12:35:16.724
  Aug  5 12:35:16.724: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  5 12:35:16.736: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/05/23 12:35:16.736
  Aug  5 12:35:16.738: INFO: Observed &Deployment event: ADDED
  Aug  5 12:35:16.738: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pzhlw-5994cf9475"}
  Aug  5 12:35:16.738: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.738: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pzhlw-5994cf9475"}
  Aug  5 12:35:16.738: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  5 12:35:16.738: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.738: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  5 12:35:16.738: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:14 +0000 UTC 2023-08-05 12:35:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pzhlw-5994cf9475" is progressing.}
  Aug  5 12:35:16.739: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.739: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  5 12:35:16.739: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pzhlw-5994cf9475" has successfully progressed.}
  Aug  5 12:35:16.739: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.739: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  5 12:35:16.739: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-05 12:35:15 +0000 UTC 2023-08-05 12:35:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pzhlw-5994cf9475" has successfully progressed.}
  Aug  5 12:35:16.739: INFO: Observed deployment test-deployment-pzhlw in namespace deployment-2253 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 12:35:16.739: INFO: Observed &Deployment event: MODIFIED
  Aug  5 12:35:16.739: INFO: Found deployment test-deployment-pzhlw in namespace deployment-2253 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug  5 12:35:16.739: INFO: Deployment test-deployment-pzhlw has a patched status
  Aug  5 12:35:16.744: INFO: Deployment "test-deployment-pzhlw":
  &Deployment{ObjectMeta:{test-deployment-pzhlw  deployment-2253  2e4a34c1-20de-4ae4-b157-57c4008d6ed3 15060 1 2023-08-05 12:35:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-05 12:35:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-05 12:35:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-05 12:35:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051da3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pzhlw-5994cf9475",LastUpdateTime:2023-08-05 12:35:16 +0000 UTC,LastTransitionTime:2023-08-05 12:35:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  5 12:35:16.747: INFO: New ReplicaSet "test-deployment-pzhlw-5994cf9475" of Deployment "test-deployment-pzhlw":
  &ReplicaSet{ObjectMeta:{test-deployment-pzhlw-5994cf9475  deployment-2253  03e19a6b-6403-4e8f-b00e-425e1aabc047 15049 1 2023-08-05 12:35:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pzhlw 2e4a34c1-20de-4ae4-b157-57c4008d6ed3 0xc0051da7c0 0xc0051da7c1}] [] [{kube-controller-manager Update apps/v1 2023-08-05 12:35:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e4a34c1-20de-4ae4-b157-57c4008d6ed3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 12:35:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051da868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 12:35:16.754: INFO: Pod "test-deployment-pzhlw-5994cf9475-m8bc2" is available:
  &Pod{ObjectMeta:{test-deployment-pzhlw-5994cf9475-m8bc2 test-deployment-pzhlw-5994cf9475- deployment-2253  8e3484bd-60ee-4f55-b3de-0af734ecdf8d 15048 0 2023-08-05 12:35:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-pzhlw-5994cf9475 03e19a6b-6403-4e8f-b00e-425e1aabc047 0xc005224710 0xc005224711}] [] [{kube-controller-manager Update v1 2023-08-05 12:35:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03e19a6b-6403-4e8f-b00e-425e1aabc047\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 12:35:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbmjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbmjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:35:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:35:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:35:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 12:35:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.154,StartTime:2023-08-05 12:35:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 12:35:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f730bf71ab365c993fbaa509e5e0d0df58ea5078aa77ab0b7e6df701c130c163,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.154,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 12:35:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2253" for this suite. @ 08/05/23 12:35:16.758
• [2.116 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/05/23 12:35:16.766
  Aug  5 12:35:16.766: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubelet-test @ 08/05/23 12:35:16.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:16.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:16.786
  Aug  5 12:35:16.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5537" for this suite. @ 08/05/23 12:35:16.816
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/05/23 12:35:16.826
  Aug  5 12:35:16.826: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 12:35:16.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:16.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:16.853
  STEP: Create set of pods @ 08/05/23 12:35:16.856
  Aug  5 12:35:16.865: INFO: created test-pod-1
  Aug  5 12:35:16.872: INFO: created test-pod-2
  Aug  5 12:35:16.879: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/05/23 12:35:16.879
  STEP: waiting for all pods to be deleted @ 08/05/23 12:35:18.926
  Aug  5 12:35:18.930: INFO: Pod quantity 3 is different from expected quantity 0
  Aug  5 12:35:19.935: INFO: Pod quantity 3 is different from expected quantity 0
  Aug  5 12:35:20.934: INFO: Pod quantity 2 is different from expected quantity 0
  Aug  5 12:35:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4689" for this suite. @ 08/05/23 12:35:21.939
• [5.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/05/23 12:35:21.948
  Aug  5 12:35:21.948: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 12:35:21.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:21.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:21.972
  STEP: Creating secret with name secret-test-5adc9477-4080-4205-8a0b-2a76e1f7fc78 @ 08/05/23 12:35:21.975
  STEP: Creating a pod to test consume secrets @ 08/05/23 12:35:21.98
  STEP: Saw pod success @ 08/05/23 12:35:26.006
  Aug  5 12:35:26.011: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-76104db9-76b2-4422-9cb6-83c7129c6df2 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 12:35:26.02
  Aug  5 12:35:26.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4236" for this suite. @ 08/05/23 12:35:26.041
• [4.099 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/05/23 12:35:26.048
  Aug  5 12:35:26.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 12:35:26.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:26.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:26.08
  STEP: Creating a test headless service @ 08/05/23 12:35:26.082
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8588.svc.cluster.local;sleep 1; done
   @ 08/05/23 12:35:26.088
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8588.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8588.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8588.svc.cluster.local;sleep 1; done
   @ 08/05/23 12:35:26.088
  STEP: creating a pod to probe DNS @ 08/05/23 12:35:26.088
  STEP: submitting the pod to kubernetes @ 08/05/23 12:35:26.089
  STEP: retrieving the pod @ 08/05/23 12:35:28.114
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:35:28.118
  Aug  5 12:35:28.123: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.127: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.132: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.136: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.140: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.144: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.148: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8588.svc.cluster.local from pod dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690: the server could not find the requested resource (get pods dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690)
  Aug  5 12:35:28.152: INFO: Lookups using dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8588.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8588.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8588.svc.cluster.local jessie_udp@dns-test-service-2.dns-8588.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8588.svc.cluster.local]

  Aug  5 12:35:33.188: INFO: DNS probes using dns-8588/dns-test-2c96a1e1-0153-4a9d-a34c-d91a3891d690 succeeded

  Aug  5 12:35:33.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:35:33.192
  STEP: deleting the test headless service @ 08/05/23 12:35:33.209
  STEP: Destroying namespace "dns-8588" for this suite. @ 08/05/23 12:35:33.224
• [7.182 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/05/23 12:35:33.231
  Aug  5 12:35:33.231: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:35:33.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:33.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:33.25
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/05/23 12:35:33.252
  Aug  5 12:35:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-3833 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug  5 12:35:33.319: INFO: stderr: ""
  Aug  5 12:35:33.319: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/05/23 12:35:33.319
  Aug  5 12:35:33.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-3833 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Aug  5 12:35:33.384: INFO: stderr: ""
  Aug  5 12:35:33.384: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/05/23 12:35:33.384
  Aug  5 12:35:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-3833 delete pods e2e-test-httpd-pod'
  Aug  5 12:35:35.595: INFO: stderr: ""
  Aug  5 12:35:35.595: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  5 12:35:35.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3833" for this suite. @ 08/05/23 12:35:35.599
• [2.375 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/05/23 12:35:35.608
  Aug  5 12:35:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 12:35:35.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:35.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:35.627
  STEP: Creating a job @ 08/05/23 12:35:35.629
  STEP: Ensuring active pods == parallelism @ 08/05/23 12:35:35.634
  STEP: Orphaning one of the Job's Pods @ 08/05/23 12:35:37.639
  Aug  5 12:35:38.155: INFO: Successfully updated pod "adopt-release-drx8x"
  STEP: Checking that the Job readopts the Pod @ 08/05/23 12:35:38.155
  STEP: Removing the labels from the Job's Pod @ 08/05/23 12:35:40.163
  Aug  5 12:35:40.675: INFO: Successfully updated pod "adopt-release-drx8x"
  STEP: Checking that the Job releases the Pod @ 08/05/23 12:35:40.675
  Aug  5 12:35:42.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7310" for this suite. @ 08/05/23 12:35:42.688
• [7.087 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/05/23 12:35:42.696
  Aug  5 12:35:42.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 12:35:42.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:42.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:42.718
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/05/23 12:35:42.722
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/05/23 12:35:42.722
  STEP: creating a pod to probe DNS @ 08/05/23 12:35:42.722
  STEP: submitting the pod to kubernetes @ 08/05/23 12:35:42.722
  STEP: retrieving the pod @ 08/05/23 12:35:44.748
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:35:44.752
  Aug  5 12:35:44.770: INFO: DNS probes using dns-637/dns-test-cbc36890-6657-43ac-9894-e876b9c4d409 succeeded

  Aug  5 12:35:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:35:44.774
  STEP: Destroying namespace "dns-637" for this suite. @ 08/05/23 12:35:44.79
• [2.101 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/05/23 12:35:44.797
  Aug  5 12:35:44.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:35:44.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:44.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:44.821
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:35:44.824
  STEP: Saw pod success @ 08/05/23 12:35:48.845
  Aug  5 12:35:48.849: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-f760ec6c-9ef7-4df2-9753-fe28abb075f3 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:35:48.857
  Aug  5 12:35:48.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4445" for this suite. @ 08/05/23 12:35:48.876
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/05/23 12:35:48.889
  Aug  5 12:35:48.889: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 12:35:48.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:48.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:48.909
  STEP: Creating a job @ 08/05/23 12:35:48.917
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/05/23 12:35:48.922
  STEP: patching /status @ 08/05/23 12:35:50.927
  STEP: updating /status @ 08/05/23 12:35:50.934
  STEP: get /status @ 08/05/23 12:35:50.963
  Aug  5 12:35:50.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3765" for this suite. @ 08/05/23 12:35:50.971
• [2.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/05/23 12:35:50.982
  Aug  5 12:35:50.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:35:50.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:51.002
  Aug  5 12:35:51.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-7420 version'
  Aug  5 12:35:51.098: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug  5 12:35:51.098: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-20T02:05:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug  5 12:35:51.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7420" for this suite. @ 08/05/23 12:35:51.103
• [0.128 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/05/23 12:35:51.11
  Aug  5 12:35:51.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-pred @ 08/05/23 12:35:51.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:35:51.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:35:51.131
  Aug  5 12:35:51.133: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  5 12:35:51.145: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 12:35:51.149: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-47 before test
  Aug  5 12:35:51.155: INFO: nginx-ingress-controller-kubernetes-worker-87f7r from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.155: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: coredns-5c7f76ccb8-8kfns from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container coredns ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: kube-state-metrics-5b95b4459c-p5wzf from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: metrics-server-v0.5.2-6cf8c8b69c-vnlhz from kube-system started at 2023-08-05 11:58:13 +0000 UTC (2 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container metrics-server ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: dashboard-metrics-scraper-6b8586b5c9-ghxkg from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: kubernetes-dashboard-6869f4cd5f-mr7dz from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2 from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:35:51.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:35:51.156: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 12:35:51.157: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-35-140 before test
  Aug  5 12:35:51.163: INFO: nginx-ingress-controller-kubernetes-worker-49tnn from ingress-nginx-kubernetes-worker started at 2023-08-05 12:02:34 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: suspend-false-to-true-j7q5g from job-3765 started at 2023-08-05 12:35:48 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container c ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: adopt-release-7nlg8 from job-7310 started at 2023-08-05 12:35:41 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container c ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: sonobuoy from sonobuoy started at 2023-08-05 12:07:56 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: sonobuoy-e2e-job-6ea4f22076f544af from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container e2e ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:35:51.163: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-bxrtb from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:35:51.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:35:51.164: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 12:35:51.164: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-95-133 before test
  Aug  5 12:35:51.169: INFO: default-http-backend-kubernetes-worker-65fc475d49-l68k2 from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.169: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug  5 12:35:51.169: INFO: nginx-ingress-controller-kubernetes-worker-hznm4 from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:19 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: suspend-false-to-true-lvc8n from job-3765 started at 2023-08-05 12:35:48 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container c ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: adopt-release-drx8x from job-7310 started at 2023-08-05 12:35:35 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container c ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: adopt-release-kx8jv from job-7310 started at 2023-08-05 12:35:35 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container c ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: calico-kube-controllers-5b8cb49547-z96wd from kube-system started at 2023-08-05 11:58:29 +0000 UTC (1 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-kjhcj from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 12:35:51.170: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 12:35:51.170: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/05/23 12:35:51.17
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/05/23 12:35:53.191
  STEP: Trying to apply a random label on the found node. @ 08/05/23 12:35:53.208
  STEP: verifying the node has the label kubernetes.io/e2e-7bc27b58-681c-403b-b8e5-f561df7ad8f6 95 @ 08/05/23 12:35:53.217
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/05/23 12:35:53.22
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.95.133 on the node which pod4 resides and expect not scheduled @ 08/05/23 12:35:55.235
  STEP: removing the label kubernetes.io/e2e-7bc27b58-681c-403b-b8e5-f561df7ad8f6 off the node ip-172-31-95-133 @ 08/05/23 12:40:55.242
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-7bc27b58-681c-403b-b8e5-f561df7ad8f6 @ 08/05/23 12:40:55.257
  Aug  5 12:40:55.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8510" for this suite. @ 08/05/23 12:40:55.265
• [304.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/05/23 12:40:55.277
  Aug  5 12:40:55.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename cronjob @ 08/05/23 12:40:55.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:40:55.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:40:55.298
  STEP: Creating a ReplaceConcurrent cronjob @ 08/05/23 12:40:55.3
  STEP: Ensuring a job is scheduled @ 08/05/23 12:40:55.305
  STEP: Ensuring exactly one is scheduled @ 08/05/23 12:41:01.309
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/05/23 12:41:01.313
  STEP: Ensuring the job is replaced with a new one @ 08/05/23 12:41:01.316
  STEP: Removing cronjob @ 08/05/23 12:42:01.321
  Aug  5 12:42:01.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4294" for this suite. @ 08/05/23 12:42:01.334
• [66.065 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/05/23 12:42:01.342
  Aug  5 12:42:01.342: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:42:01.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:42:01.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:42:01.364
  STEP: Setting up server cert @ 08/05/23 12:42:01.387
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:42:02.057
  STEP: Deploying the webhook pod @ 08/05/23 12:42:02.066
  STEP: Wait for the deployment to be ready @ 08/05/23 12:42:02.078
  Aug  5 12:42:02.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:42:04.098
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:42:04.11
  Aug  5 12:42:05.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/05/23 12:42:05.114
  STEP: create a pod that should be updated by the webhook @ 08/05/23 12:42:05.128
  Aug  5 12:42:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4510" for this suite. @ 08/05/23 12:42:05.204
  STEP: Destroying namespace "webhook-markers-8683" for this suite. @ 08/05/23 12:42:05.211
• [3.875 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/05/23 12:42:05.222
  Aug  5 12:42:05.222: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 12:42:05.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:42:05.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:42:05.242
  Aug  5 12:42:05.244: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/05/23 12:42:06.568
  Aug  5 12:42:06.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9260 --namespace=crd-publish-openapi-9260 create -f -'
  Aug  5 12:42:07.199: INFO: stderr: ""
  Aug  5 12:42:07.199: INFO: stdout: "e2e-test-crd-publish-openapi-9407-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug  5 12:42:07.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9260 --namespace=crd-publish-openapi-9260 delete e2e-test-crd-publish-openapi-9407-crds test-cr'
  Aug  5 12:42:07.266: INFO: stderr: ""
  Aug  5 12:42:07.266: INFO: stdout: "e2e-test-crd-publish-openapi-9407-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug  5 12:42:07.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9260 --namespace=crd-publish-openapi-9260 apply -f -'
  Aug  5 12:42:07.494: INFO: stderr: ""
  Aug  5 12:42:07.494: INFO: stdout: "e2e-test-crd-publish-openapi-9407-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug  5 12:42:07.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9260 --namespace=crd-publish-openapi-9260 delete e2e-test-crd-publish-openapi-9407-crds test-cr'
  Aug  5 12:42:07.561: INFO: stderr: ""
  Aug  5 12:42:07.561: INFO: stdout: "e2e-test-crd-publish-openapi-9407-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/05/23 12:42:07.561
  Aug  5 12:42:07.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9260 explain e2e-test-crd-publish-openapi-9407-crds'
  Aug  5 12:42:07.754: INFO: stderr: ""
  Aug  5 12:42:07.754: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-9407-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Aug  5 12:42:09.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9260" for this suite. @ 08/05/23 12:42:09.147
• [3.933 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/05/23 12:42:09.16
  Aug  5 12:42:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:42:09.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:42:09.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:42:09.179
  STEP: Setting up server cert @ 08/05/23 12:42:09.207
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:42:09.813
  STEP: Deploying the webhook pod @ 08/05/23 12:42:09.819
  STEP: Wait for the deployment to be ready @ 08/05/23 12:42:09.831
  Aug  5 12:42:09.838: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:42:11.85
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:42:11.861
  Aug  5 12:42:12.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  5 12:42:12.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9003-crds.webhook.example.com via the AdmissionRegistration API @ 08/05/23 12:42:13.375
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/05/23 12:42:13.389
  Aug  5 12:42:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8523" for this suite. @ 08/05/23 12:42:15.99
  STEP: Destroying namespace "webhook-markers-9776" for this suite. @ 08/05/23 12:42:15.998
• [6.846 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/05/23 12:42:16.007
  Aug  5 12:42:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:42:16.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:42:16.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:42:16.031
  STEP: Creating configMap with name configmap-test-volume-2db09117-1e9c-4b78-8044-bd8417a518bc @ 08/05/23 12:42:16.033
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:42:16.039
  STEP: Saw pod success @ 08/05/23 12:42:20.061
  Aug  5 12:42:20.065: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-configmaps-377da1cf-5968-40d6-bb05-be0ac8e016e2 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:42:20.085
  Aug  5 12:42:20.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7160" for this suite. @ 08/05/23 12:42:20.104
• [4.104 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/05/23 12:42:20.112
  Aug  5 12:42:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 12:42:20.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:42:20.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:42:20.136
  STEP: Creating pod liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 in namespace container-probe-1064 @ 08/05/23 12:42:20.138
  Aug  5 12:42:22.156: INFO: Started pod liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 in namespace container-probe-1064
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 12:42:22.156
  Aug  5 12:42:22.160: INFO: Initial restart count of pod liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is 0
  Aug  5 12:42:42.213: INFO: Restart count of pod container-probe-1064/liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is now 1 (20.052732451s elapsed)
  Aug  5 12:43:02.258: INFO: Restart count of pod container-probe-1064/liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is now 2 (40.098237413s elapsed)
  Aug  5 12:43:22.306: INFO: Restart count of pod container-probe-1064/liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is now 3 (1m0.146076308s elapsed)
  Aug  5 12:43:42.354: INFO: Restart count of pod container-probe-1064/liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is now 4 (1m20.194098612s elapsed)
  Aug  5 12:44:42.507: INFO: Restart count of pod container-probe-1064/liveness-ae06d771-fb11-4e11-a8c2-0a2ecb534d20 is now 5 (2m20.346866251s elapsed)
  Aug  5 12:44:42.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:44:42.512
  STEP: Destroying namespace "container-probe-1064" for this suite. @ 08/05/23 12:44:42.524
• [142.419 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 08/05/23 12:44:42.531
  Aug  5 12:44:42.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 12:44:42.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:44:42.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:44:42.553
  Aug  5 12:44:42.578: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 12:44:42.585
  Aug  5 12:44:42.589: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:42.589: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:42.592: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:44:42.592: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:44:43.597: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:43.598: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:43.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:44:43.601: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:44:44.597: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:44.598: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:44.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:44:44.601: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/05/23 12:44:44.615
  STEP: Check that daemon pods images are updated. @ 08/05/23 12:44:44.626
  Aug  5 12:44:44.630: INFO: Wrong image for pod: daemon-set-b6xtd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:44.630: INFO: Wrong image for pod: daemon-set-gs2f2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:44.630: INFO: Wrong image for pod: daemon-set-xflct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:44.636: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:44.636: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:45.641: INFO: Pod daemon-set-fl4jl is not available
  Aug  5 12:44:45.641: INFO: Wrong image for pod: daemon-set-gs2f2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:45.641: INFO: Wrong image for pod: daemon-set-xflct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:45.645: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:45.645: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:46.641: INFO: Pod daemon-set-fl4jl is not available
  Aug  5 12:44:46.641: INFO: Wrong image for pod: daemon-set-gs2f2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:46.641: INFO: Wrong image for pod: daemon-set-xflct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:46.646: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:46.646: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:47.640: INFO: Pod daemon-set-5js4l is not available
  Aug  5 12:44:47.640: INFO: Wrong image for pod: daemon-set-xflct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:47.645: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:47.645: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:48.641: INFO: Pod daemon-set-5js4l is not available
  Aug  5 12:44:48.641: INFO: Wrong image for pod: daemon-set-xflct. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  5 12:44:48.644: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:48.645: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:49.640: INFO: Pod daemon-set-42hff is not available
  Aug  5 12:44:49.645: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:49.645: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/05/23 12:44:49.645
  Aug  5 12:44:49.649: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:49.649: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:49.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 12:44:49.653: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 12:44:50.657: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:50.657: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:44:50.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:44:50.661: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 12:44:50.679
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6253, will wait for the garbage collector to delete the pods @ 08/05/23 12:44:50.679
  Aug  5 12:44:50.740: INFO: Deleting DaemonSet.extensions daemon-set took: 7.083116ms
  Aug  5 12:44:50.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.704373ms
  Aug  5 12:44:52.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:44:52.744: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 12:44:52.748: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17335"},"items":null}

  Aug  5 12:44:52.751: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17335"},"items":null}

  Aug  5 12:44:52.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6253" for this suite. @ 08/05/23 12:44:52.772
• [10.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/05/23 12:44:52.782
  Aug  5 12:44:52.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:44:52.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:44:52.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:44:52.804
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:44:52.806
  STEP: Saw pod success @ 08/05/23 12:44:56.829
  Aug  5 12:44:56.832: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-584c3138-20e6-4d48-b248-8bafb147ea09 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:44:56.852
  Aug  5 12:44:56.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4505" for this suite. @ 08/05/23 12:44:56.875
• [4.100 seconds]
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/05/23 12:44:56.882
  Aug  5 12:44:56.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:44:56.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:44:56.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:44:56.901
  STEP: Creating configMap with name configmap-projected-all-test-volume-afcead94-762a-4d27-bffd-d2421e65ff8b @ 08/05/23 12:44:56.903
  STEP: Creating secret with name secret-projected-all-test-volume-71aa5ddf-4e5d-4122-ad7e-2dfe6714054b @ 08/05/23 12:44:56.908
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/05/23 12:44:56.912
  STEP: Saw pod success @ 08/05/23 12:45:00.936
  Aug  5 12:45:00.940: INFO: Trying to get logs from node ip-172-31-95-133 pod projected-volume-7a614ea9-74b7-436e-85e6-bc9ff111a56b container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 12:45:00.948
  Aug  5 12:45:00.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5765" for this suite. @ 08/05/23 12:45:00.969
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/05/23 12:45:00.978
  Aug  5 12:45:00.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-runtime @ 08/05/23 12:45:00.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:45:00.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:45:00.998
  STEP: create the container @ 08/05/23 12:45:01
  W0805 12:45:01.011607      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/05/23 12:45:01.011
  STEP: get the container status @ 08/05/23 12:45:04.027
  STEP: the container should be terminated @ 08/05/23 12:45:04.031
  STEP: the termination message should be set @ 08/05/23 12:45:04.031
  Aug  5 12:45:04.031: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/05/23 12:45:04.031
  Aug  5 12:45:04.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2779" for this suite. @ 08/05/23 12:45:04.05
• [3.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/05/23 12:45:04.059
  Aug  5 12:45:04.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename taint-single-pod @ 08/05/23 12:45:04.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:45:04.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:45:04.079
  Aug  5 12:45:04.081: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug  5 12:46:04.095: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 12:46:04.098: INFO: Starting informer...
  STEP: Starting pod... @ 08/05/23 12:46:04.099
  Aug  5 12:46:04.316: INFO: Pod is running on ip-172-31-95-133. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/05/23 12:46:04.316
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/05/23 12:46:04.326
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/05/23 12:46:04.333
  Aug  5 12:46:04.333: INFO: Pod wasn't evicted. Proceeding
  Aug  5 12:46:04.334: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/05/23 12:46:04.347
  STEP: Waiting some time to make sure that toleration time passed. @ 08/05/23 12:46:04.35
  Aug  5 12:47:19.353: INFO: Pod wasn't evicted. Test successful
  Aug  5 12:47:19.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3253" for this suite. @ 08/05/23 12:47:19.358
• [135.306 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/05/23 12:47:19.366
  Aug  5 12:47:19.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:47:19.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:47:19.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:47:19.399
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/05/23 12:47:19.401
  STEP: Saw pod success @ 08/05/23 12:47:23.425
  Aug  5 12:47:23.429: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-e34a51a3-d9c3-4b48-a903-470cee50c7c0 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:47:23.445
  Aug  5 12:47:23.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5329" for this suite. @ 08/05/23 12:47:23.466
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/05/23 12:47:23.475
  Aug  5 12:47:23.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pod-network-test @ 08/05/23 12:47:23.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:47:23.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:47:23.494
  STEP: Performing setup for networking test in namespace pod-network-test-8623 @ 08/05/23 12:47:23.496
  STEP: creating a selector @ 08/05/23 12:47:23.496
  STEP: Creating the service pods in kubernetes @ 08/05/23 12:47:23.496
  Aug  5 12:47:23.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/05/23 12:47:45.594
  Aug  5 12:47:47.615: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug  5 12:47:47.615: INFO: Breadth first check of 192.168.122.75 on host 172.31.1.47...
  Aug  5 12:47:47.618: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.140:9080/dial?request=hostname&protocol=http&host=192.168.122.75&port=8083&tries=1'] Namespace:pod-network-test-8623 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:47:47.618: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:47:47.619: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:47:47.619: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8623/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.140%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.122.75%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 12:47:47.689: INFO: Waiting for responses: map[]
  Aug  5 12:47:47.689: INFO: reached 192.168.122.75 after 0/1 tries
  Aug  5 12:47:47.689: INFO: Breadth first check of 192.168.166.232 on host 172.31.35.140...
  Aug  5 12:47:47.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.140:9080/dial?request=hostname&protocol=http&host=192.168.166.232&port=8083&tries=1'] Namespace:pod-network-test-8623 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:47:47.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:47:47.694: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:47:47.694: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8623/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.140%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.166.232%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 12:47:47.766: INFO: Waiting for responses: map[]
  Aug  5 12:47:47.766: INFO: reached 192.168.166.232 after 0/1 tries
  Aug  5 12:47:47.766: INFO: Breadth first check of 192.168.52.146 on host 172.31.95.133...
  Aug  5 12:47:47.769: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.140:9080/dial?request=hostname&protocol=http&host=192.168.52.146&port=8083&tries=1'] Namespace:pod-network-test-8623 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 12:47:47.769: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 12:47:47.770: INFO: ExecWithOptions: Clientset creation
  Aug  5 12:47:47.770: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8623/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.140%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.52.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 12:47:47.836: INFO: Waiting for responses: map[]
  Aug  5 12:47:47.836: INFO: reached 192.168.52.146 after 0/1 tries
  Aug  5 12:47:47.836: INFO: Going to retry 0 out of 3 pods....
  Aug  5 12:47:47.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8623" for this suite. @ 08/05/23 12:47:47.84
• [24.372 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/05/23 12:47:47.848
  Aug  5 12:47:47.849: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename proxy @ 08/05/23 12:47:47.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:47:47.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:47:47.872
  Aug  5 12:47:47.874: INFO: Creating pod...
  Aug  5 12:47:49.892: INFO: Creating service...
  Aug  5 12:47:49.902: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/DELETE
  Aug  5 12:47:49.909: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  5 12:47:49.909: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/GET
  Aug  5 12:47:49.915: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug  5 12:47:49.915: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/HEAD
  Aug  5 12:47:49.919: INFO: http.Client request:HEAD | StatusCode:200
  Aug  5 12:47:49.919: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug  5 12:47:49.923: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  5 12:47:49.923: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/PATCH
  Aug  5 12:47:49.927: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  5 12:47:49.927: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/POST
  Aug  5 12:47:49.932: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  5 12:47:49.932: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/pods/agnhost/proxy/some/path/with/PUT
  Aug  5 12:47:49.936: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  5 12:47:49.936: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/DELETE
  Aug  5 12:47:49.942: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  5 12:47:49.942: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/GET
  Aug  5 12:47:49.949: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug  5 12:47:49.949: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/HEAD
  Aug  5 12:47:49.955: INFO: http.Client request:HEAD | StatusCode:200
  Aug  5 12:47:49.955: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/OPTIONS
  Aug  5 12:47:49.961: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  5 12:47:49.961: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/PATCH
  Aug  5 12:47:49.967: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  5 12:47:49.967: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/POST
  Aug  5 12:47:49.974: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  5 12:47:49.974: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1849/services/test-service/proxy/some/path/with/PUT
  Aug  5 12:47:49.979: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  5 12:47:49.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1849" for this suite. @ 08/05/23 12:47:49.983
• [2.142 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/05/23 12:47:49.991
  Aug  5 12:47:49.991: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:47:49.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:47:50.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:47:50.011
  STEP: Starting the proxy @ 08/05/23 12:47:50.013
  Aug  5 12:47:50.014: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-765 proxy --unix-socket=/tmp/kubectl-proxy-unix4056702469/test'
  STEP: retrieving proxy /api/ output @ 08/05/23 12:47:50.06
  Aug  5 12:47:50.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-765" for this suite. @ 08/05/23 12:47:50.064
• [0.080 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/05/23 12:47:50.072
  Aug  5 12:47:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename watch @ 08/05/23 12:47:50.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:47:50.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:47:50.09
  STEP: creating a watch on configmaps with a certain label @ 08/05/23 12:47:50.092
  STEP: creating a new configmap @ 08/05/23 12:47:50.093
  STEP: modifying the configmap once @ 08/05/23 12:47:50.099
  STEP: changing the label value of the configmap @ 08/05/23 12:47:50.107
  STEP: Expecting to observe a delete notification for the watched object @ 08/05/23 12:47:50.114
  Aug  5 12:47:50.114: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18077 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:47:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:47:50.114: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18078 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:47:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:47:50.115: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18079 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:47:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/05/23 12:47:50.115
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/05/23 12:47:50.122
  STEP: changing the label value of the configmap back @ 08/05/23 12:48:00.123
  STEP: modifying the configmap a third time @ 08/05/23 12:48:00.132
  STEP: deleting the configmap @ 08/05/23 12:48:00.14
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/05/23 12:48:00.146
  Aug  5 12:48:00.146: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18181 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:48:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:48:00.147: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18182 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:48:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:48:00.147: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-189  605cd017-1a33-46d9-b4e7-5e34c5b2efa7 18183 0 2023-08-05 12:47:50 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-05 12:48:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 12:48:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-189" for this suite. @ 08/05/23 12:48:00.151
• [10.086 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/05/23 12:48:00.158
  Aug  5 12:48:00.158: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:48:00.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:48:00.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:48:00.179
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/05/23 12:48:00.181
  STEP: Saw pod success @ 08/05/23 12:48:04.206
  Aug  5 12:48:04.209: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-191d6da7-7747-4f76-96a0-d320ad4bb869 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:48:04.217
  Aug  5 12:48:04.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1401" for this suite. @ 08/05/23 12:48:04.239
• [4.087 seconds]
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/05/23 12:48:04.245
  Aug  5 12:48:04.245: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename cronjob @ 08/05/23 12:48:04.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:48:04.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:48:04.268
  STEP: Creating a suspended cronjob @ 08/05/23 12:48:04.271
  STEP: Ensuring no jobs are scheduled @ 08/05/23 12:48:04.276
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/05/23 12:53:04.285
  STEP: Removing cronjob @ 08/05/23 12:53:04.288
  Aug  5 12:53:04.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7872" for this suite. @ 08/05/23 12:53:04.299
• [300.060 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/05/23 12:53:04.305
  Aug  5 12:53:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:53:04.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:04.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:04.326
  STEP: Counting existing ResourceQuota @ 08/05/23 12:53:21.333
  STEP: Creating a ResourceQuota @ 08/05/23 12:53:26.338
  STEP: Ensuring resource quota status is calculated @ 08/05/23 12:53:26.343
  STEP: Creating a ConfigMap @ 08/05/23 12:53:28.349
  STEP: Ensuring resource quota status captures configMap creation @ 08/05/23 12:53:28.36
  STEP: Deleting a ConfigMap @ 08/05/23 12:53:30.365
  STEP: Ensuring resource quota status released usage @ 08/05/23 12:53:30.373
  Aug  5 12:53:32.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6566" for this suite. @ 08/05/23 12:53:32.382
• [28.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/05/23 12:53:32.392
  Aug  5 12:53:32.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:53:32.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:32.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:32.411
  STEP: Setting up server cert @ 08/05/23 12:53:32.444
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:53:32.728
  STEP: Deploying the webhook pod @ 08/05/23 12:53:32.737
  STEP: Wait for the deployment to be ready @ 08/05/23 12:53:32.75
  Aug  5 12:53:32.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:53:34.768
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:53:34.778
  Aug  5 12:53:35.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/05/23 12:53:35.783
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/05/23 12:53:35.784
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/05/23 12:53:35.784
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/05/23 12:53:35.784
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/05/23 12:53:35.785
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/05/23 12:53:35.785
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/05/23 12:53:35.786
  Aug  5 12:53:35.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9085" for this suite. @ 08/05/23 12:53:35.829
  STEP: Destroying namespace "webhook-markers-4857" for this suite. @ 08/05/23 12:53:35.836
• [3.453 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/05/23 12:53:35.845
  Aug  5 12:53:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 12:53:35.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:35.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:35.866
  STEP: Setting up server cert @ 08/05/23 12:53:35.889
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 12:53:36.114
  STEP: Deploying the webhook pod @ 08/05/23 12:53:36.119
  STEP: Wait for the deployment to be ready @ 08/05/23 12:53:36.131
  Aug  5 12:53:36.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 12:53:38.149
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 12:53:38.162
  Aug  5 12:53:39.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/05/23 12:53:39.228
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/05/23 12:53:39.27
  STEP: Deleting the collection of validation webhooks @ 08/05/23 12:53:39.305
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/05/23 12:53:39.362
  Aug  5 12:53:39.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1429" for this suite. @ 08/05/23 12:53:39.414
  STEP: Destroying namespace "webhook-markers-725" for this suite. @ 08/05/23 12:53:39.421
• [3.582 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/05/23 12:53:39.429
  Aug  5 12:53:39.429: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 12:53:39.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:39.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:39.447
  STEP: create deployment with httpd image @ 08/05/23 12:53:39.449
  Aug  5 12:53:39.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-8264 create -f -'
  Aug  5 12:53:39.708: INFO: stderr: ""
  Aug  5 12:53:39.708: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/05/23 12:53:39.708
  Aug  5 12:53:39.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-8264 diff -f -'
  Aug  5 12:53:40.317: INFO: rc: 1
  Aug  5 12:53:40.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-8264 delete -f -'
  Aug  5 12:53:40.381: INFO: stderr: ""
  Aug  5 12:53:40.381: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug  5 12:53:40.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8264" for this suite. @ 08/05/23 12:53:40.385
• [0.964 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/05/23 12:53:40.393
  Aug  5 12:53:40.393: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename events @ 08/05/23 12:53:40.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:40.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:40.414
  STEP: Create set of events @ 08/05/23 12:53:40.416
  Aug  5 12:53:40.421: INFO: created test-event-1
  Aug  5 12:53:40.427: INFO: created test-event-2
  Aug  5 12:53:40.433: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/05/23 12:53:40.433
  STEP: delete collection of events @ 08/05/23 12:53:40.437
  Aug  5 12:53:40.437: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/05/23 12:53:40.469
  Aug  5 12:53:40.469: INFO: requesting list of events to confirm quantity
  Aug  5 12:53:40.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1722" for this suite. @ 08/05/23 12:53:40.476
• [0.091 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/05/23 12:53:40.484
  Aug  5 12:53:40.484: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename endpointslice @ 08/05/23 12:53:40.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:40.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:40.512
  Aug  5 12:53:42.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4793" for this suite. @ 08/05/23 12:53:42.571
• [2.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/05/23 12:53:42.581
  Aug  5 12:53:42.581: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 12:53:42.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:42.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:42.603
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/05/23 12:53:42.607
  STEP: When a replicaset with a matching selector is created @ 08/05/23 12:53:44.628
  STEP: Then the orphan pod is adopted @ 08/05/23 12:53:44.633
  STEP: When the matched label of one of its pods change @ 08/05/23 12:53:45.642
  Aug  5 12:53:45.646: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/05/23 12:53:45.656
  Aug  5 12:53:46.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5922" for this suite. @ 08/05/23 12:53:46.669
• [4.095 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/05/23 12:53:46.676
  Aug  5 12:53:46.676: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:53:46.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:46.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:46.695
  STEP: Creating configMap with name configmap-test-volume-ab16d36d-bbe9-488f-a8b5-48e45ebd729b @ 08/05/23 12:53:46.697
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:53:46.702
  STEP: Saw pod success @ 08/05/23 12:53:50.726
  Aug  5 12:53:50.730: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-configmaps-90cac95c-40e6-4914-86a0-07e6da2eebe7 container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 12:53:50.748
  Aug  5 12:53:50.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-220" for this suite. @ 08/05/23 12:53:50.77
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 08/05/23 12:53:50.781
  Aug  5 12:53:50.781: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 12:53:50.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:50.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:50.801
  STEP: Creating simple DaemonSet "daemon-set" @ 08/05/23 12:53:50.823
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 12:53:50.83
  Aug  5 12:53:50.834: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:50.834: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:50.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:53:50.838: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:53:51.843: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:51.843: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:51.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 12:53:51.846: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 12:53:52.843: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:52.843: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:52.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 12:53:52.847: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 12:53:53.843: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:53.843: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:53:53.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:53:53.847: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/05/23 12:53:53.85
  STEP: DeleteCollection of the DaemonSets @ 08/05/23 12:53:53.854
  STEP: Verify that ReplicaSets have been deleted @ 08/05/23 12:53:53.865
  Aug  5 12:53:53.875: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19325"},"items":null}

  Aug  5 12:53:53.885: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19326"},"items":[{"metadata":{"name":"daemon-set-7q56m","generateName":"daemon-set-","namespace":"daemonsets-6466","uid":"3469271f-5d3c-4dbc-a432-3a4116d9a8a3","resourceVersion":"19287","creationTimestamp":"2023-08-05T12:53:50Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4e8ba38-e3d2-4204-b7ae-151c77d3c000","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4e8ba38-e3d2-4204-b7ae-151c77d3c000\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rtc2l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rtc2l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-35-140","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-35-140"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"}],"hostIP":"172.31.35.140","podIP":"192.168.166.234","podIPs":[{"ip":"192.168.166.234"}],"startTime":"2023-08-05T12:53:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-05T12:53:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://86b6a932a0bb33f50547e9967750b96b677ace10c443ccc257fd4c015ea1948a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lcpxg","generateName":"daemon-set-","namespace":"daemonsets-6466","uid":"1b2ce042-b1d4-46a6-bf4f-234c877b4afc","resourceVersion":"19323","creationTimestamp":"2023-08-05T12:53:50Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4e8ba38-e3d2-4204-b7ae-151c77d3c000","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4e8ba38-e3d2-4204-b7ae-151c77d3c000\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-stj4d","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-stj4d","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-95-133","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-95-133"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"}],"hostIP":"172.31.95.133","podIP":"192.168.52.147","podIPs":[{"ip":"192.168.52.147"}],"startTime":"2023-08-05T12:53:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-05T12:53:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6a7f6a6073e377c5a226d050de32946cfed7d5fb5d70e83d4dfa35cb484c5aa5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qwl92","generateName":"daemon-set-","namespace":"daemonsets-6466","uid":"d452a0b9-b899-48b3-9cf1-de2ca3bfa796","resourceVersion":"19326","creationTimestamp":"2023-08-05T12:53:50Z","deletionTimestamp":"2023-08-05T12:54:23Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4e8ba38-e3d2-4204-b7ae-151c77d3c000","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4e8ba38-e3d2-4204-b7ae-151c77d3c000\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-05T12:53:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n8fcl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n8fcl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-1-47","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-1-47"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-05T12:53:50Z"}],"hostIP":"172.31.1.47","podIP":"192.168.122.87","podIPs":[{"ip":"192.168.122.87"}],"startTime":"2023-08-05T12:53:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-05T12:53:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ae6e645304e921ebd499c35b0ece2d8dc3675e5f50340908dea7f148ab642c7b","started":true}],"qosClass":"BestEffort"}}]}

  Aug  5 12:53:53.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6466" for this suite. @ 08/05/23 12:53:53.906
• [3.131 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/05/23 12:53:53.913
  Aug  5 12:53:53.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 12:53:53.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:53.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:53.936
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-211.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-211.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/05/23 12:53:53.94
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-211.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-211.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/05/23 12:53:53.94
  STEP: creating a pod to probe /etc/hosts @ 08/05/23 12:53:53.94
  STEP: submitting the pod to kubernetes @ 08/05/23 12:53:53.94
  STEP: retrieving the pod @ 08/05/23 12:53:55.959
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:53:55.962
  Aug  5 12:53:55.981: INFO: DNS probes using dns-211/dns-test-488a432e-9241-4e78-a33d-9b005bd0cee1 succeeded

  Aug  5 12:53:55.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:53:55.984
  STEP: Destroying namespace "dns-211" for this suite. @ 08/05/23 12:53:56
• [2.096 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/05/23 12:53:56.009
  Aug  5 12:53:56.009: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 12:53:56.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:56.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:56.031
  STEP: Creating namespace "e2e-ns-ssk2d" @ 08/05/23 12:53:56.033
  Aug  5 12:53:56.049: INFO: Namespace "e2e-ns-ssk2d-3232" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-ssk2d-3232" @ 08/05/23 12:53:56.05
  Aug  5 12:53:56.058: INFO: Namespace "e2e-ns-ssk2d-3232" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-ssk2d-3232" @ 08/05/23 12:53:56.058
  Aug  5 12:53:56.065: INFO: Namespace "e2e-ns-ssk2d-3232" has []v1.FinalizerName{"kubernetes"}
  Aug  5 12:53:56.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7431" for this suite. @ 08/05/23 12:53:56.07
  STEP: Destroying namespace "e2e-ns-ssk2d-3232" for this suite. @ 08/05/23 12:53:56.078
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/05/23 12:53:56.087
  Aug  5 12:53:56.087: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 12:53:56.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:56.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:56.107
  STEP: Creating a test headless service @ 08/05/23 12:53:56.109
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8849.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8849.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/05/23 12:53:56.115
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8849.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8849.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/05/23 12:53:56.115
  STEP: creating a pod to probe DNS @ 08/05/23 12:53:56.115
  STEP: submitting the pod to kubernetes @ 08/05/23 12:53:56.115
  STEP: retrieving the pod @ 08/05/23 12:53:58.138
  STEP: looking for the results for each expected name from probers @ 08/05/23 12:53:58.141
  Aug  5 12:53:58.159: INFO: DNS probes using dns-8849/dns-test-1747febe-7546-4d37-b4b7-4a8b5891bb23 succeeded

  Aug  5 12:53:58.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:53:58.163
  STEP: deleting the test headless service @ 08/05/23 12:53:58.18
  STEP: Destroying namespace "dns-8849" for this suite. @ 08/05/23 12:53:58.196
• [2.116 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/05/23 12:53:58.203
  Aug  5 12:53:58.203: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:53:58.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:53:58.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:53:58.222
  STEP: Creating configMap with name configmap-test-volume-map-a5093219-968b-4d4c-92a2-b57ec0679466 @ 08/05/23 12:53:58.224
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:53:58.23
  STEP: Saw pod success @ 08/05/23 12:54:02.254
  Aug  5 12:54:02.258: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-c90b3916-10e9-4218-80ce-e59acdef61b0 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:54:02.279
  Aug  5 12:54:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4918" for this suite. @ 08/05/23 12:54:02.301
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/05/23 12:54:02.31
  Aug  5 12:54:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/05/23 12:54:02.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:02.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:02.329
  STEP: fetching the /apis discovery document @ 08/05/23 12:54:02.332
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/05/23 12:54:02.333
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/05/23 12:54:02.333
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/05/23 12:54:02.333
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/05/23 12:54:02.334
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/05/23 12:54:02.334
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/05/23 12:54:02.335
  Aug  5 12:54:02.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9900" for this suite. @ 08/05/23 12:54:02.339
• [0.036 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/05/23 12:54:02.347
  Aug  5 12:54:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:54:02.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:02.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:02.365
  STEP: Creating configMap with name projected-configmap-test-volume-map-a9b52a03-9390-449d-8c10-473052ea3953 @ 08/05/23 12:54:02.368
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:54:02.374
  STEP: Saw pod success @ 08/05/23 12:54:06.396
  Aug  5 12:54:06.399: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-9176eaef-f739-48bd-86f3-6490aa9d9d0b container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:54:06.407
  Aug  5 12:54:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9021" for this suite. @ 08/05/23 12:54:06.429
• [4.089 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/05/23 12:54:06.436
  Aug  5 12:54:06.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename prestop @ 08/05/23 12:54:06.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:06.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:06.456
  STEP: Creating server pod server in namespace prestop-827 @ 08/05/23 12:54:06.458
  STEP: Waiting for pods to come up. @ 08/05/23 12:54:06.466
  STEP: Creating tester pod tester in namespace prestop-827 @ 08/05/23 12:54:08.478
  STEP: Deleting pre-stop pod @ 08/05/23 12:54:10.493
  Aug  5 12:54:15.512: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug  5 12:54:15.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/05/23 12:54:15.516
  STEP: Destroying namespace "prestop-827" for this suite. @ 08/05/23 12:54:15.531
• [9.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/05/23 12:54:15.543
  Aug  5 12:54:15.543: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svc-latency @ 08/05/23 12:54:15.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:15.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:15.563
  Aug  5 12:54:15.565: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-2509 @ 08/05/23 12:54:15.566
  I0805 12:54:15.572023      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2509, replica count: 1
  I0805 12:54:16.622718      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0805 12:54:17.622853      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 12:54:17.734: INFO: Created: latency-svc-tfz86
  Aug  5 12:54:17.744: INFO: Got endpoints: latency-svc-tfz86 [21.549336ms]
  Aug  5 12:54:17.756: INFO: Created: latency-svc-pzlb5
  Aug  5 12:54:17.766: INFO: Created: latency-svc-dqkc9
  Aug  5 12:54:17.766: INFO: Got endpoints: latency-svc-pzlb5 [21.521436ms]
  Aug  5 12:54:17.771: INFO: Got endpoints: latency-svc-dqkc9 [26.218152ms]
  Aug  5 12:54:17.772: INFO: Created: latency-svc-7dvqv
  Aug  5 12:54:17.778: INFO: Got endpoints: latency-svc-7dvqv [33.16345ms]
  Aug  5 12:54:17.782: INFO: Created: latency-svc-k4q4t
  Aug  5 12:54:17.788: INFO: Got endpoints: latency-svc-k4q4t [42.630705ms]
  Aug  5 12:54:17.789: INFO: Created: latency-svc-d2wms
  Aug  5 12:54:17.796: INFO: Created: latency-svc-dx9t6
  Aug  5 12:54:17.796: INFO: Got endpoints: latency-svc-d2wms [50.957572ms]
  Aug  5 12:54:17.803: INFO: Got endpoints: latency-svc-dx9t6 [57.602521ms]
  Aug  5 12:54:17.804: INFO: Created: latency-svc-d4pxx
  Aug  5 12:54:17.808: INFO: Got endpoints: latency-svc-d4pxx [62.883421ms]
  Aug  5 12:54:17.810: INFO: Created: latency-svc-6smnc
  Aug  5 12:54:17.821: INFO: Got endpoints: latency-svc-6smnc [76.114894ms]
  Aug  5 12:54:17.822: INFO: Created: latency-svc-fq7mn
  Aug  5 12:54:17.823: INFO: Got endpoints: latency-svc-fq7mn [78.021113ms]
  Aug  5 12:54:17.827: INFO: Created: latency-svc-gl62h
  Aug  5 12:54:17.831: INFO: Got endpoints: latency-svc-gl62h [85.443482ms]
  Aug  5 12:54:17.834: INFO: Created: latency-svc-zdtrw
  Aug  5 12:54:17.838: INFO: Got endpoints: latency-svc-zdtrw [92.706092ms]
  Aug  5 12:54:17.840: INFO: Created: latency-svc-tn98m
  Aug  5 12:54:17.846: INFO: Got endpoints: latency-svc-tn98m [101.006664ms]
  Aug  5 12:54:17.848: INFO: Created: latency-svc-69d27
  Aug  5 12:54:17.855: INFO: Got endpoints: latency-svc-69d27 [109.168209ms]
  Aug  5 12:54:17.856: INFO: Created: latency-svc-hwwlv
  Aug  5 12:54:17.862: INFO: Got endpoints: latency-svc-hwwlv [116.711543ms]
  Aug  5 12:54:17.863: INFO: Created: latency-svc-9wzqx
  Aug  5 12:54:17.867: INFO: Got endpoints: latency-svc-9wzqx [121.593208ms]
  Aug  5 12:54:17.872: INFO: Created: latency-svc-l44zw
  Aug  5 12:54:17.876: INFO: Got endpoints: latency-svc-l44zw [110.258546ms]
  Aug  5 12:54:17.886: INFO: Created: latency-svc-rgh2x
  Aug  5 12:54:17.890: INFO: Got endpoints: latency-svc-rgh2x [112.214486ms]
  Aug  5 12:54:17.890: INFO: Created: latency-svc-b2ktf
  Aug  5 12:54:17.891: INFO: Got endpoints: latency-svc-b2ktf [120.246168ms]
  Aug  5 12:54:17.900: INFO: Created: latency-svc-9kl9b
  Aug  5 12:54:17.906: INFO: Got endpoints: latency-svc-9kl9b [118.025027ms]
  Aug  5 12:54:17.909: INFO: Created: latency-svc-wsf65
  Aug  5 12:54:17.913: INFO: Got endpoints: latency-svc-wsf65 [116.806781ms]
  Aug  5 12:54:17.916: INFO: Created: latency-svc-ghmxx
  Aug  5 12:54:17.920: INFO: Got endpoints: latency-svc-ghmxx [117.709725ms]
  Aug  5 12:54:17.923: INFO: Created: latency-svc-qg4m5
  Aug  5 12:54:17.929: INFO: Got endpoints: latency-svc-qg4m5 [120.40241ms]
  Aug  5 12:54:17.931: INFO: Created: latency-svc-n5tqm
  Aug  5 12:54:17.936: INFO: Got endpoints: latency-svc-n5tqm [115.072412ms]
  Aug  5 12:54:17.939: INFO: Created: latency-svc-wkbht
  Aug  5 12:54:17.945: INFO: Got endpoints: latency-svc-wkbht [121.349256ms]
  Aug  5 12:54:17.947: INFO: Created: latency-svc-lwblm
  Aug  5 12:54:17.950: INFO: Got endpoints: latency-svc-lwblm [119.364723ms]
  Aug  5 12:54:17.953: INFO: Created: latency-svc-ln27k
  Aug  5 12:54:17.959: INFO: Got endpoints: latency-svc-ln27k [120.778237ms]
  Aug  5 12:54:17.962: INFO: Created: latency-svc-5rt7b
  Aug  5 12:54:17.965: INFO: Got endpoints: latency-svc-5rt7b [118.506022ms]
  Aug  5 12:54:17.968: INFO: Created: latency-svc-5t9ww
  Aug  5 12:54:17.975: INFO: Created: latency-svc-gf4gq
  Aug  5 12:54:17.976: INFO: Got endpoints: latency-svc-5t9ww [120.858643ms]
  Aug  5 12:54:17.980: INFO: Created: latency-svc-8fvnm
  Aug  5 12:54:17.981: INFO: Got endpoints: latency-svc-gf4gq [118.620841ms]
  Aug  5 12:54:17.986: INFO: Created: latency-svc-l2tmz
  Aug  5 12:54:17.993: INFO: Created: latency-svc-jv2ds
  Aug  5 12:54:17.998: INFO: Created: latency-svc-tcmx4
  Aug  5 12:54:17.998: INFO: Got endpoints: latency-svc-8fvnm [131.283444ms]
  Aug  5 12:54:18.004: INFO: Created: latency-svc-rtnfb
  Aug  5 12:54:18.007: INFO: Got endpoints: latency-svc-l2tmz [130.660622ms]
  Aug  5 12:54:18.008: INFO: Got endpoints: latency-svc-jv2ds [117.483434ms]
  Aug  5 12:54:18.009: INFO: Got endpoints: latency-svc-tcmx4 [118.119705ms]
  Aug  5 12:54:18.012: INFO: Got endpoints: latency-svc-rtnfb [105.927945ms]
  Aug  5 12:54:18.014: INFO: Created: latency-svc-bpmgm
  Aug  5 12:54:18.022: INFO: Created: latency-svc-hm655
  Aug  5 12:54:18.026: INFO: Created: latency-svc-hgtsc
  Aug  5 12:54:18.032: INFO: Created: latency-svc-k9mzf
  Aug  5 12:54:18.039: INFO: Got endpoints: latency-svc-bpmgm [126.323021ms]
  Aug  5 12:54:18.040: INFO: Created: latency-svc-vmzbs
  Aug  5 12:54:18.048: INFO: Created: latency-svc-nlxj5
  Aug  5 12:54:18.055: INFO: Created: latency-svc-nxtp2
  Aug  5 12:54:18.060: INFO: Created: latency-svc-248q9
  Aug  5 12:54:18.066: INFO: Created: latency-svc-jj7zx
  Aug  5 12:54:18.073: INFO: Created: latency-svc-bhfrj
  Aug  5 12:54:18.079: INFO: Created: latency-svc-929md
  Aug  5 12:54:18.085: INFO: Created: latency-svc-4b7jx
  Aug  5 12:54:18.091: INFO: Got endpoints: latency-svc-hm655 [170.610179ms]
  Aug  5 12:54:18.094: INFO: Created: latency-svc-sr5gk
  Aug  5 12:54:18.098: INFO: Created: latency-svc-wvsq4
  Aug  5 12:54:18.113: INFO: Created: latency-svc-zr5hn
  Aug  5 12:54:18.115: INFO: Created: latency-svc-8crsg
  Aug  5 12:54:18.116: INFO: Created: latency-svc-vlftp
  Aug  5 12:54:18.139: INFO: Got endpoints: latency-svc-hgtsc [210.340807ms]
  Aug  5 12:54:18.150: INFO: Created: latency-svc-fxg7c
  Aug  5 12:54:18.189: INFO: Got endpoints: latency-svc-k9mzf [252.429026ms]
  Aug  5 12:54:18.200: INFO: Created: latency-svc-mqsfs
  Aug  5 12:54:18.238: INFO: Got endpoints: latency-svc-vmzbs [293.046334ms]
  Aug  5 12:54:18.249: INFO: Created: latency-svc-zmqm8
  Aug  5 12:54:18.290: INFO: Got endpoints: latency-svc-nlxj5 [339.40961ms]
  Aug  5 12:54:18.300: INFO: Created: latency-svc-hhxlj
  Aug  5 12:54:18.340: INFO: Got endpoints: latency-svc-nxtp2 [380.647176ms]
  Aug  5 12:54:18.351: INFO: Created: latency-svc-glnfs
  Aug  5 12:54:18.391: INFO: Got endpoints: latency-svc-248q9 [425.599288ms]
  Aug  5 12:54:18.401: INFO: Created: latency-svc-qldpl
  Aug  5 12:54:18.440: INFO: Got endpoints: latency-svc-jj7zx [464.534829ms]
  Aug  5 12:54:18.451: INFO: Created: latency-svc-jmbg9
  Aug  5 12:54:18.488: INFO: Got endpoints: latency-svc-bhfrj [506.853222ms]
  Aug  5 12:54:18.501: INFO: Created: latency-svc-qfsx2
  Aug  5 12:54:18.538: INFO: Got endpoints: latency-svc-929md [540.002202ms]
  Aug  5 12:54:18.550: INFO: Created: latency-svc-lfx5x
  Aug  5 12:54:18.590: INFO: Got endpoints: latency-svc-4b7jx [583.076625ms]
  Aug  5 12:54:18.600: INFO: Created: latency-svc-2l7fz
  Aug  5 12:54:18.640: INFO: Got endpoints: latency-svc-sr5gk [631.732092ms]
  Aug  5 12:54:18.651: INFO: Created: latency-svc-rqjgl
  Aug  5 12:54:18.689: INFO: Got endpoints: latency-svc-wvsq4 [679.993938ms]
  Aug  5 12:54:18.700: INFO: Created: latency-svc-gxxzn
  Aug  5 12:54:18.738: INFO: Got endpoints: latency-svc-zr5hn [725.880322ms]
  Aug  5 12:54:18.750: INFO: Created: latency-svc-ltrnk
  Aug  5 12:54:18.789: INFO: Got endpoints: latency-svc-8crsg [749.351249ms]
  Aug  5 12:54:18.799: INFO: Created: latency-svc-lhwfk
  Aug  5 12:54:18.839: INFO: Got endpoints: latency-svc-vlftp [747.24154ms]
  Aug  5 12:54:18.850: INFO: Created: latency-svc-j5sqt
  Aug  5 12:54:18.889: INFO: Got endpoints: latency-svc-fxg7c [750.580732ms]
  Aug  5 12:54:18.900: INFO: Created: latency-svc-4qcj5
  Aug  5 12:54:18.938: INFO: Got endpoints: latency-svc-mqsfs [749.009402ms]
  Aug  5 12:54:18.950: INFO: Created: latency-svc-tnxfr
  Aug  5 12:54:18.989: INFO: Got endpoints: latency-svc-zmqm8 [750.869923ms]
  Aug  5 12:54:19.001: INFO: Created: latency-svc-shcp8
  Aug  5 12:54:19.038: INFO: Got endpoints: latency-svc-hhxlj [748.465989ms]
  Aug  5 12:54:19.050: INFO: Created: latency-svc-b4k67
  Aug  5 12:54:19.090: INFO: Got endpoints: latency-svc-glnfs [750.286942ms]
  Aug  5 12:54:19.103: INFO: Created: latency-svc-2wkkz
  Aug  5 12:54:19.140: INFO: Got endpoints: latency-svc-qldpl [749.487302ms]
  Aug  5 12:54:19.156: INFO: Created: latency-svc-td8vz
  Aug  5 12:54:19.188: INFO: Got endpoints: latency-svc-jmbg9 [747.719049ms]
  Aug  5 12:54:19.200: INFO: Created: latency-svc-g9ggd
  Aug  5 12:54:19.238: INFO: Got endpoints: latency-svc-qfsx2 [750.347944ms]
  Aug  5 12:54:19.249: INFO: Created: latency-svc-rsnkh
  Aug  5 12:54:19.289: INFO: Got endpoints: latency-svc-lfx5x [750.166862ms]
  Aug  5 12:54:19.299: INFO: Created: latency-svc-z22zf
  Aug  5 12:54:19.340: INFO: Got endpoints: latency-svc-2l7fz [749.817985ms]
  Aug  5 12:54:19.350: INFO: Created: latency-svc-sl887
  Aug  5 12:54:19.389: INFO: Got endpoints: latency-svc-rqjgl [748.709078ms]
  Aug  5 12:54:19.399: INFO: Created: latency-svc-sfmkl
  Aug  5 12:54:19.438: INFO: Got endpoints: latency-svc-gxxzn [748.729899ms]
  Aug  5 12:54:19.450: INFO: Created: latency-svc-5g5wp
  Aug  5 12:54:19.488: INFO: Got endpoints: latency-svc-ltrnk [750.339678ms]
  Aug  5 12:54:19.498: INFO: Created: latency-svc-pd98q
  Aug  5 12:54:19.540: INFO: Got endpoints: latency-svc-lhwfk [751.278008ms]
  Aug  5 12:54:19.551: INFO: Created: latency-svc-zl58x
  Aug  5 12:54:19.590: INFO: Got endpoints: latency-svc-j5sqt [751.398478ms]
  Aug  5 12:54:19.603: INFO: Created: latency-svc-26bsh
  Aug  5 12:54:19.637: INFO: Got endpoints: latency-svc-4qcj5 [747.97317ms]
  Aug  5 12:54:19.649: INFO: Created: latency-svc-hrd9h
  Aug  5 12:54:19.689: INFO: Got endpoints: latency-svc-tnxfr [750.596771ms]
  Aug  5 12:54:19.700: INFO: Created: latency-svc-5lbsb
  Aug  5 12:54:19.738: INFO: Got endpoints: latency-svc-shcp8 [749.348636ms]
  Aug  5 12:54:19.753: INFO: Created: latency-svc-nm55n
  Aug  5 12:54:19.789: INFO: Got endpoints: latency-svc-b4k67 [750.991344ms]
  Aug  5 12:54:19.799: INFO: Created: latency-svc-c2bld
  Aug  5 12:54:19.840: INFO: Got endpoints: latency-svc-2wkkz [749.575695ms]
  Aug  5 12:54:19.852: INFO: Created: latency-svc-gcv7f
  Aug  5 12:54:19.890: INFO: Got endpoints: latency-svc-td8vz [749.561137ms]
  Aug  5 12:54:19.901: INFO: Created: latency-svc-nwxzd
  Aug  5 12:54:19.939: INFO: Got endpoints: latency-svc-g9ggd [750.929992ms]
  Aug  5 12:54:19.950: INFO: Created: latency-svc-thh6j
  Aug  5 12:54:19.988: INFO: Got endpoints: latency-svc-rsnkh [750.080116ms]
  Aug  5 12:54:20.000: INFO: Created: latency-svc-gn8ht
  Aug  5 12:54:20.039: INFO: Got endpoints: latency-svc-z22zf [749.554153ms]
  Aug  5 12:54:20.049: INFO: Created: latency-svc-wqfgq
  Aug  5 12:54:20.090: INFO: Got endpoints: latency-svc-sl887 [750.141396ms]
  Aug  5 12:54:20.100: INFO: Created: latency-svc-zdf7h
  Aug  5 12:54:20.139: INFO: Got endpoints: latency-svc-sfmkl [750.4396ms]
  Aug  5 12:54:20.150: INFO: Created: latency-svc-62tqh
  Aug  5 12:54:20.188: INFO: Got endpoints: latency-svc-5g5wp [749.541506ms]
  Aug  5 12:54:20.200: INFO: Created: latency-svc-cx447
  Aug  5 12:54:20.240: INFO: Got endpoints: latency-svc-pd98q [751.997857ms]
  Aug  5 12:54:20.250: INFO: Created: latency-svc-vgdcx
  Aug  5 12:54:20.289: INFO: Got endpoints: latency-svc-zl58x [749.268131ms]
  Aug  5 12:54:20.301: INFO: Created: latency-svc-qm4sz
  Aug  5 12:54:20.340: INFO: Got endpoints: latency-svc-26bsh [749.607898ms]
  Aug  5 12:54:20.350: INFO: Created: latency-svc-d788l
  Aug  5 12:54:20.390: INFO: Got endpoints: latency-svc-hrd9h [751.946554ms]
  Aug  5 12:54:20.400: INFO: Created: latency-svc-2qv9f
  Aug  5 12:54:20.440: INFO: Got endpoints: latency-svc-5lbsb [750.375674ms]
  Aug  5 12:54:20.451: INFO: Created: latency-svc-7kknl
  Aug  5 12:54:20.488: INFO: Got endpoints: latency-svc-nm55n [749.778981ms]
  Aug  5 12:54:20.500: INFO: Created: latency-svc-rchtt
  Aug  5 12:54:20.539: INFO: Got endpoints: latency-svc-c2bld [749.8725ms]
  Aug  5 12:54:20.549: INFO: Created: latency-svc-66pt2
  Aug  5 12:54:20.589: INFO: Got endpoints: latency-svc-gcv7f [748.990437ms]
  Aug  5 12:54:20.602: INFO: Created: latency-svc-dpcbv
  Aug  5 12:54:20.638: INFO: Got endpoints: latency-svc-nwxzd [747.896632ms]
  Aug  5 12:54:20.649: INFO: Created: latency-svc-j62tl
  Aug  5 12:54:20.688: INFO: Got endpoints: latency-svc-thh6j [749.248606ms]
  Aug  5 12:54:20.701: INFO: Created: latency-svc-44rsd
  Aug  5 12:54:20.739: INFO: Got endpoints: latency-svc-gn8ht [749.955684ms]
  Aug  5 12:54:20.754: INFO: Created: latency-svc-jgfdm
  Aug  5 12:54:20.790: INFO: Got endpoints: latency-svc-wqfgq [751.331918ms]
  Aug  5 12:54:20.801: INFO: Created: latency-svc-88wxs
  Aug  5 12:54:20.837: INFO: Got endpoints: latency-svc-zdf7h [747.181448ms]
  Aug  5 12:54:20.849: INFO: Created: latency-svc-tpsd8
  Aug  5 12:54:20.889: INFO: Got endpoints: latency-svc-62tqh [749.16341ms]
  Aug  5 12:54:20.900: INFO: Created: latency-svc-5w8jx
  Aug  5 12:54:20.940: INFO: Got endpoints: latency-svc-cx447 [752.384931ms]
  Aug  5 12:54:20.952: INFO: Created: latency-svc-j25pd
  Aug  5 12:54:20.990: INFO: Got endpoints: latency-svc-vgdcx [750.068309ms]
  Aug  5 12:54:21.001: INFO: Created: latency-svc-vsvct
  Aug  5 12:54:21.038: INFO: Got endpoints: latency-svc-qm4sz [748.652728ms]
  Aug  5 12:54:21.051: INFO: Created: latency-svc-qs9dj
  Aug  5 12:54:21.088: INFO: Got endpoints: latency-svc-d788l [748.451947ms]
  Aug  5 12:54:21.098: INFO: Created: latency-svc-grn4l
  Aug  5 12:54:21.140: INFO: Got endpoints: latency-svc-2qv9f [750.077307ms]
  Aug  5 12:54:21.151: INFO: Created: latency-svc-2wpq5
  Aug  5 12:54:21.194: INFO: Got endpoints: latency-svc-7kknl [754.271419ms]
  Aug  5 12:54:21.205: INFO: Created: latency-svc-q6gv4
  Aug  5 12:54:21.240: INFO: Got endpoints: latency-svc-rchtt [751.692621ms]
  Aug  5 12:54:21.251: INFO: Created: latency-svc-m5bjg
  Aug  5 12:54:21.289: INFO: Got endpoints: latency-svc-66pt2 [749.931596ms]
  Aug  5 12:54:21.300: INFO: Created: latency-svc-vbzrv
  Aug  5 12:54:21.339: INFO: Got endpoints: latency-svc-dpcbv [749.747868ms]
  Aug  5 12:54:21.350: INFO: Created: latency-svc-psxc7
  Aug  5 12:54:21.388: INFO: Got endpoints: latency-svc-j62tl [750.451357ms]
  Aug  5 12:54:21.400: INFO: Created: latency-svc-8gncc
  Aug  5 12:54:21.438: INFO: Got endpoints: latency-svc-44rsd [750.257078ms]
  Aug  5 12:54:21.449: INFO: Created: latency-svc-s8mfj
  Aug  5 12:54:21.489: INFO: Got endpoints: latency-svc-jgfdm [749.785458ms]
  Aug  5 12:54:21.499: INFO: Created: latency-svc-4hs4f
  Aug  5 12:54:21.540: INFO: Got endpoints: latency-svc-88wxs [749.695945ms]
  Aug  5 12:54:21.552: INFO: Created: latency-svc-w22rh
  Aug  5 12:54:21.587: INFO: Got endpoints: latency-svc-tpsd8 [750.125457ms]
  Aug  5 12:54:21.600: INFO: Created: latency-svc-7w4bk
  Aug  5 12:54:21.638: INFO: Got endpoints: latency-svc-5w8jx [749.384057ms]
  Aug  5 12:54:21.650: INFO: Created: latency-svc-8b6gs
  Aug  5 12:54:21.689: INFO: Got endpoints: latency-svc-j25pd [748.539238ms]
  Aug  5 12:54:21.699: INFO: Created: latency-svc-gbq26
  Aug  5 12:54:21.739: INFO: Got endpoints: latency-svc-vsvct [748.372354ms]
  Aug  5 12:54:21.751: INFO: Created: latency-svc-hzwcm
  Aug  5 12:54:21.790: INFO: Got endpoints: latency-svc-qs9dj [751.811534ms]
  Aug  5 12:54:21.801: INFO: Created: latency-svc-vlqvn
  Aug  5 12:54:21.840: INFO: Got endpoints: latency-svc-grn4l [751.408411ms]
  Aug  5 12:54:21.850: INFO: Created: latency-svc-c9skn
  Aug  5 12:54:21.887: INFO: Got endpoints: latency-svc-2wpq5 [747.557023ms]
  Aug  5 12:54:21.899: INFO: Created: latency-svc-m48w9
  Aug  5 12:54:21.938: INFO: Got endpoints: latency-svc-q6gv4 [743.469663ms]
  Aug  5 12:54:21.950: INFO: Created: latency-svc-kd4ps
  Aug  5 12:54:21.990: INFO: Got endpoints: latency-svc-m5bjg [749.786016ms]
  Aug  5 12:54:22.001: INFO: Created: latency-svc-m26xc
  Aug  5 12:54:22.039: INFO: Got endpoints: latency-svc-vbzrv [749.892714ms]
  Aug  5 12:54:22.050: INFO: Created: latency-svc-hdp47
  Aug  5 12:54:22.090: INFO: Got endpoints: latency-svc-psxc7 [750.829666ms]
  Aug  5 12:54:22.101: INFO: Created: latency-svc-m6gzj
  Aug  5 12:54:22.139: INFO: Got endpoints: latency-svc-8gncc [751.228489ms]
  Aug  5 12:54:22.151: INFO: Created: latency-svc-qckz4
  Aug  5 12:54:22.189: INFO: Got endpoints: latency-svc-s8mfj [750.091161ms]
  Aug  5 12:54:22.201: INFO: Created: latency-svc-xqncg
  Aug  5 12:54:22.240: INFO: Got endpoints: latency-svc-4hs4f [751.235169ms]
  Aug  5 12:54:22.250: INFO: Created: latency-svc-vxj42
  Aug  5 12:54:22.290: INFO: Got endpoints: latency-svc-w22rh [750.121833ms]
  Aug  5 12:54:22.301: INFO: Created: latency-svc-6b4xr
  Aug  5 12:54:22.338: INFO: Got endpoints: latency-svc-7w4bk [750.385353ms]
  Aug  5 12:54:22.350: INFO: Created: latency-svc-hm9ws
  Aug  5 12:54:22.388: INFO: Got endpoints: latency-svc-8b6gs [750.153323ms]
  Aug  5 12:54:22.401: INFO: Created: latency-svc-9llc7
  Aug  5 12:54:22.439: INFO: Got endpoints: latency-svc-gbq26 [750.02693ms]
  Aug  5 12:54:22.451: INFO: Created: latency-svc-vk7qr
  Aug  5 12:54:22.488: INFO: Got endpoints: latency-svc-hzwcm [749.145166ms]
  Aug  5 12:54:22.500: INFO: Created: latency-svc-q7vq4
  Aug  5 12:54:22.539: INFO: Got endpoints: latency-svc-vlqvn [749.273263ms]
  Aug  5 12:54:22.551: INFO: Created: latency-svc-s87s7
  Aug  5 12:54:22.593: INFO: Got endpoints: latency-svc-c9skn [752.764153ms]
  Aug  5 12:54:22.604: INFO: Created: latency-svc-m4tr7
  Aug  5 12:54:22.639: INFO: Got endpoints: latency-svc-m48w9 [751.356288ms]
  Aug  5 12:54:22.651: INFO: Created: latency-svc-8pg2v
  Aug  5 12:54:22.688: INFO: Got endpoints: latency-svc-kd4ps [749.959241ms]
  Aug  5 12:54:22.699: INFO: Created: latency-svc-59rzr
  Aug  5 12:54:22.739: INFO: Got endpoints: latency-svc-m26xc [749.169269ms]
  Aug  5 12:54:22.753: INFO: Created: latency-svc-2j5xz
  Aug  5 12:54:22.789: INFO: Got endpoints: latency-svc-hdp47 [749.316597ms]
  Aug  5 12:54:22.800: INFO: Created: latency-svc-6bwlf
  Aug  5 12:54:22.840: INFO: Got endpoints: latency-svc-m6gzj [750.069332ms]
  Aug  5 12:54:22.851: INFO: Created: latency-svc-gnmkr
  Aug  5 12:54:22.889: INFO: Got endpoints: latency-svc-qckz4 [749.78928ms]
  Aug  5 12:54:22.900: INFO: Created: latency-svc-lk96v
  Aug  5 12:54:22.938: INFO: Got endpoints: latency-svc-xqncg [749.623754ms]
  Aug  5 12:54:22.952: INFO: Created: latency-svc-ddldq
  Aug  5 12:54:22.988: INFO: Got endpoints: latency-svc-vxj42 [748.217309ms]
  Aug  5 12:54:23.000: INFO: Created: latency-svc-q684m
  Aug  5 12:54:23.039: INFO: Got endpoints: latency-svc-6b4xr [749.02652ms]
  Aug  5 12:54:23.053: INFO: Created: latency-svc-rhvz7
  Aug  5 12:54:23.089: INFO: Got endpoints: latency-svc-hm9ws [751.053599ms]
  Aug  5 12:54:23.100: INFO: Created: latency-svc-86rfc
  Aug  5 12:54:23.145: INFO: Got endpoints: latency-svc-9llc7 [756.42685ms]
  Aug  5 12:54:23.156: INFO: Created: latency-svc-7htdd
  Aug  5 12:54:23.189: INFO: Got endpoints: latency-svc-vk7qr [750.639356ms]
  Aug  5 12:54:23.202: INFO: Created: latency-svc-7lnm4
  Aug  5 12:54:23.238: INFO: Got endpoints: latency-svc-q7vq4 [750.008946ms]
  Aug  5 12:54:23.249: INFO: Created: latency-svc-g4m28
  Aug  5 12:54:23.289: INFO: Got endpoints: latency-svc-s87s7 [750.212889ms]
  Aug  5 12:54:23.300: INFO: Created: latency-svc-66nfv
  Aug  5 12:54:23.339: INFO: Got endpoints: latency-svc-m4tr7 [746.140592ms]
  Aug  5 12:54:23.350: INFO: Created: latency-svc-xnv9m
  Aug  5 12:54:23.390: INFO: Got endpoints: latency-svc-8pg2v [750.985522ms]
  Aug  5 12:54:23.401: INFO: Created: latency-svc-skp9c
  Aug  5 12:54:23.439: INFO: Got endpoints: latency-svc-59rzr [751.069384ms]
  Aug  5 12:54:23.451: INFO: Created: latency-svc-k5vf9
  Aug  5 12:54:23.488: INFO: Got endpoints: latency-svc-2j5xz [748.740912ms]
  Aug  5 12:54:23.499: INFO: Created: latency-svc-9kkqf
  Aug  5 12:54:23.540: INFO: Got endpoints: latency-svc-6bwlf [751.085975ms]
  Aug  5 12:54:23.552: INFO: Created: latency-svc-j794b
  Aug  5 12:54:23.590: INFO: Got endpoints: latency-svc-gnmkr [750.605275ms]
  Aug  5 12:54:23.601: INFO: Created: latency-svc-stvpl
  Aug  5 12:54:23.640: INFO: Got endpoints: latency-svc-lk96v [750.552788ms]
  Aug  5 12:54:23.650: INFO: Created: latency-svc-jx6bw
  Aug  5 12:54:23.688: INFO: Got endpoints: latency-svc-ddldq [749.480423ms]
  Aug  5 12:54:23.702: INFO: Created: latency-svc-7gksz
  Aug  5 12:54:23.738: INFO: Got endpoints: latency-svc-q684m [749.459784ms]
  Aug  5 12:54:23.751: INFO: Created: latency-svc-hcj9w
  Aug  5 12:54:23.789: INFO: Got endpoints: latency-svc-rhvz7 [749.741845ms]
  Aug  5 12:54:23.801: INFO: Created: latency-svc-r5757
  Aug  5 12:54:23.839: INFO: Got endpoints: latency-svc-86rfc [749.995169ms]
  Aug  5 12:54:23.850: INFO: Created: latency-svc-zwqjw
  Aug  5 12:54:23.888: INFO: Got endpoints: latency-svc-7htdd [743.42596ms]
  Aug  5 12:54:23.902: INFO: Created: latency-svc-t9tlh
  Aug  5 12:54:23.938: INFO: Got endpoints: latency-svc-7lnm4 [748.629412ms]
  Aug  5 12:54:23.949: INFO: Created: latency-svc-b6c8z
  Aug  5 12:54:23.989: INFO: Got endpoints: latency-svc-g4m28 [750.753075ms]
  Aug  5 12:54:24.001: INFO: Created: latency-svc-s67ch
  Aug  5 12:54:24.041: INFO: Got endpoints: latency-svc-66nfv [751.825362ms]
  Aug  5 12:54:24.052: INFO: Created: latency-svc-687tb
  Aug  5 12:54:24.089: INFO: Got endpoints: latency-svc-xnv9m [750.133332ms]
  Aug  5 12:54:24.099: INFO: Created: latency-svc-h7qz9
  Aug  5 12:54:24.138: INFO: Got endpoints: latency-svc-skp9c [747.566589ms]
  Aug  5 12:54:24.150: INFO: Created: latency-svc-2txbj
  Aug  5 12:54:24.189: INFO: Got endpoints: latency-svc-k5vf9 [749.558752ms]
  Aug  5 12:54:24.200: INFO: Created: latency-svc-9pzfs
  Aug  5 12:54:24.240: INFO: Got endpoints: latency-svc-9kkqf [751.851585ms]
  Aug  5 12:54:24.251: INFO: Created: latency-svc-99wgw
  Aug  5 12:54:24.291: INFO: Got endpoints: latency-svc-j794b [750.182908ms]
  Aug  5 12:54:24.303: INFO: Created: latency-svc-pgwgr
  Aug  5 12:54:24.339: INFO: Got endpoints: latency-svc-stvpl [748.956621ms]
  Aug  5 12:54:24.353: INFO: Created: latency-svc-qcwl2
  Aug  5 12:54:24.389: INFO: Got endpoints: latency-svc-jx6bw [749.269853ms]
  Aug  5 12:54:24.401: INFO: Created: latency-svc-zjfh2
  Aug  5 12:54:24.438: INFO: Got endpoints: latency-svc-7gksz [750.615851ms]
  Aug  5 12:54:24.450: INFO: Created: latency-svc-bfcr2
  Aug  5 12:54:24.490: INFO: Got endpoints: latency-svc-hcj9w [751.768207ms]
  Aug  5 12:54:24.500: INFO: Created: latency-svc-7ld2l
  Aug  5 12:54:24.539: INFO: Got endpoints: latency-svc-r5757 [749.548449ms]
  Aug  5 12:54:24.549: INFO: Created: latency-svc-xjgnj
  Aug  5 12:54:24.590: INFO: Got endpoints: latency-svc-zwqjw [750.776578ms]
  Aug  5 12:54:24.602: INFO: Created: latency-svc-qv9fg
  Aug  5 12:54:24.639: INFO: Got endpoints: latency-svc-t9tlh [751.280348ms]
  Aug  5 12:54:24.651: INFO: Created: latency-svc-h7hc4
  Aug  5 12:54:24.688: INFO: Got endpoints: latency-svc-b6c8z [749.637487ms]
  Aug  5 12:54:24.699: INFO: Created: latency-svc-x8vcb
  Aug  5 12:54:24.738: INFO: Got endpoints: latency-svc-s67ch [749.047555ms]
  Aug  5 12:54:24.751: INFO: Created: latency-svc-psbt8
  Aug  5 12:54:24.790: INFO: Got endpoints: latency-svc-687tb [748.321374ms]
  Aug  5 12:54:24.800: INFO: Created: latency-svc-fzwkf
  Aug  5 12:54:24.840: INFO: Got endpoints: latency-svc-h7qz9 [750.305048ms]
  Aug  5 12:54:24.852: INFO: Created: latency-svc-ghr4v
  Aug  5 12:54:24.889: INFO: Got endpoints: latency-svc-2txbj [751.105342ms]
  Aug  5 12:54:24.900: INFO: Created: latency-svc-8d2j2
  Aug  5 12:54:24.939: INFO: Got endpoints: latency-svc-9pzfs [749.870777ms]
  Aug  5 12:54:24.951: INFO: Created: latency-svc-fr58f
  Aug  5 12:54:24.988: INFO: Got endpoints: latency-svc-99wgw [747.552661ms]
  Aug  5 12:54:25.000: INFO: Created: latency-svc-f4qpt
  Aug  5 12:54:25.038: INFO: Got endpoints: latency-svc-pgwgr [747.760101ms]
  Aug  5 12:54:25.050: INFO: Created: latency-svc-sg2f7
  Aug  5 12:54:25.090: INFO: Got endpoints: latency-svc-qcwl2 [750.869598ms]
  Aug  5 12:54:25.101: INFO: Created: latency-svc-bsz4c
  Aug  5 12:54:25.139: INFO: Got endpoints: latency-svc-zjfh2 [749.566064ms]
  Aug  5 12:54:25.149: INFO: Created: latency-svc-8gj92
  Aug  5 12:54:25.188: INFO: Got endpoints: latency-svc-bfcr2 [750.049471ms]
  Aug  5 12:54:25.200: INFO: Created: latency-svc-c9hnb
  Aug  5 12:54:25.239: INFO: Got endpoints: latency-svc-7ld2l [749.588315ms]
  Aug  5 12:54:25.251: INFO: Created: latency-svc-cv54q
  Aug  5 12:54:25.288: INFO: Got endpoints: latency-svc-xjgnj [749.351759ms]
  Aug  5 12:54:25.299: INFO: Created: latency-svc-76rdc
  Aug  5 12:54:25.340: INFO: Got endpoints: latency-svc-qv9fg [749.911526ms]
  Aug  5 12:54:25.351: INFO: Created: latency-svc-689l4
  Aug  5 12:54:25.388: INFO: Got endpoints: latency-svc-h7hc4 [748.872961ms]
  Aug  5 12:54:25.400: INFO: Created: latency-svc-hmp68
  Aug  5 12:54:25.438: INFO: Got endpoints: latency-svc-x8vcb [750.056628ms]
  Aug  5 12:54:25.449: INFO: Created: latency-svc-fz5g5
  Aug  5 12:54:25.488: INFO: Got endpoints: latency-svc-psbt8 [749.354884ms]
  Aug  5 12:54:25.500: INFO: Created: latency-svc-s5nq2
  Aug  5 12:54:25.539: INFO: Got endpoints: latency-svc-fzwkf [749.788259ms]
  Aug  5 12:54:25.551: INFO: Created: latency-svc-7m6b5
  Aug  5 12:54:25.589: INFO: Got endpoints: latency-svc-ghr4v [748.917341ms]
  Aug  5 12:54:25.639: INFO: Got endpoints: latency-svc-8d2j2 [750.061918ms]
  Aug  5 12:54:25.689: INFO: Got endpoints: latency-svc-fr58f [750.123282ms]
  Aug  5 12:54:25.738: INFO: Got endpoints: latency-svc-f4qpt [749.635436ms]
  Aug  5 12:54:25.791: INFO: Got endpoints: latency-svc-sg2f7 [752.150716ms]
  Aug  5 12:54:25.839: INFO: Got endpoints: latency-svc-bsz4c [748.675585ms]
  Aug  5 12:54:25.890: INFO: Got endpoints: latency-svc-8gj92 [750.926304ms]
  Aug  5 12:54:25.939: INFO: Got endpoints: latency-svc-c9hnb [750.385261ms]
  Aug  5 12:54:25.990: INFO: Got endpoints: latency-svc-cv54q [749.876762ms]
  Aug  5 12:54:26.039: INFO: Got endpoints: latency-svc-76rdc [750.146447ms]
  Aug  5 12:54:26.090: INFO: Got endpoints: latency-svc-689l4 [750.232726ms]
  Aug  5 12:54:26.138: INFO: Got endpoints: latency-svc-hmp68 [749.737328ms]
  Aug  5 12:54:26.189: INFO: Got endpoints: latency-svc-fz5g5 [751.593783ms]
  Aug  5 12:54:26.239: INFO: Got endpoints: latency-svc-s5nq2 [751.048492ms]
  Aug  5 12:54:26.290: INFO: Got endpoints: latency-svc-7m6b5 [750.078557ms]
  Aug  5 12:54:26.290: INFO: Latencies: [21.521436ms 26.218152ms 33.16345ms 42.630705ms 50.957572ms 57.602521ms 62.883421ms 76.114894ms 78.021113ms 85.443482ms 92.706092ms 101.006664ms 105.927945ms 109.168209ms 110.258546ms 112.214486ms 115.072412ms 116.711543ms 116.806781ms 117.483434ms 117.709725ms 118.025027ms 118.119705ms 118.506022ms 118.620841ms 119.364723ms 120.246168ms 120.40241ms 120.778237ms 120.858643ms 121.349256ms 121.593208ms 126.323021ms 130.660622ms 131.283444ms 170.610179ms 210.340807ms 252.429026ms 293.046334ms 339.40961ms 380.647176ms 425.599288ms 464.534829ms 506.853222ms 540.002202ms 583.076625ms 631.732092ms 679.993938ms 725.880322ms 743.42596ms 743.469663ms 746.140592ms 747.181448ms 747.24154ms 747.552661ms 747.557023ms 747.566589ms 747.719049ms 747.760101ms 747.896632ms 747.97317ms 748.217309ms 748.321374ms 748.372354ms 748.451947ms 748.465989ms 748.539238ms 748.629412ms 748.652728ms 748.675585ms 748.709078ms 748.729899ms 748.740912ms 748.872961ms 748.917341ms 748.956621ms 748.990437ms 749.009402ms 749.02652ms 749.047555ms 749.145166ms 749.16341ms 749.169269ms 749.248606ms 749.268131ms 749.269853ms 749.273263ms 749.316597ms 749.348636ms 749.351249ms 749.351759ms 749.354884ms 749.384057ms 749.459784ms 749.480423ms 749.487302ms 749.541506ms 749.548449ms 749.554153ms 749.558752ms 749.561137ms 749.566064ms 749.575695ms 749.588315ms 749.607898ms 749.623754ms 749.635436ms 749.637487ms 749.695945ms 749.737328ms 749.741845ms 749.747868ms 749.778981ms 749.785458ms 749.786016ms 749.788259ms 749.78928ms 749.817985ms 749.870777ms 749.8725ms 749.876762ms 749.892714ms 749.911526ms 749.931596ms 749.955684ms 749.959241ms 749.995169ms 750.008946ms 750.02693ms 750.049471ms 750.056628ms 750.061918ms 750.068309ms 750.069332ms 750.077307ms 750.078557ms 750.080116ms 750.091161ms 750.121833ms 750.123282ms 750.125457ms 750.133332ms 750.141396ms 750.146447ms 750.153323ms 750.166862ms 750.182908ms 750.212889ms 750.232726ms 750.257078ms 750.286942ms 750.305048ms 750.339678ms 750.347944ms 750.375674ms 750.385261ms 750.385353ms 750.4396ms 750.451357ms 750.552788ms 750.580732ms 750.596771ms 750.605275ms 750.615851ms 750.639356ms 750.753075ms 750.776578ms 750.829666ms 750.869598ms 750.869923ms 750.926304ms 750.929992ms 750.985522ms 750.991344ms 751.048492ms 751.053599ms 751.069384ms 751.085975ms 751.105342ms 751.228489ms 751.235169ms 751.278008ms 751.280348ms 751.331918ms 751.356288ms 751.398478ms 751.408411ms 751.593783ms 751.692621ms 751.768207ms 751.811534ms 751.825362ms 751.851585ms 751.946554ms 751.997857ms 752.150716ms 752.384931ms 752.764153ms 754.271419ms 756.42685ms]
  Aug  5 12:54:26.290: INFO: 50 %ile: 749.561137ms
  Aug  5 12:54:26.290: INFO: 90 %ile: 751.235169ms
  Aug  5 12:54:26.290: INFO: 99 %ile: 754.271419ms
  Aug  5 12:54:26.290: INFO: Total sample count: 200
  Aug  5 12:54:26.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-2509" for this suite. @ 08/05/23 12:54:26.296
• [10.760 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/05/23 12:54:26.306
  Aug  5 12:54:26.306: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 12:54:26.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:26.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:26.325
  STEP: creating an Endpoint @ 08/05/23 12:54:26.339
  STEP: waiting for available Endpoint @ 08/05/23 12:54:26.344
  STEP: listing all Endpoints @ 08/05/23 12:54:26.346
  STEP: updating the Endpoint @ 08/05/23 12:54:26.353
  STEP: fetching the Endpoint @ 08/05/23 12:54:26.358
  STEP: patching the Endpoint @ 08/05/23 12:54:26.362
  STEP: fetching the Endpoint @ 08/05/23 12:54:26.371
  STEP: deleting the Endpoint by Collection @ 08/05/23 12:54:26.374
  STEP: waiting for Endpoint deletion @ 08/05/23 12:54:26.386
  STEP: fetching the Endpoint @ 08/05/23 12:54:26.388
  Aug  5 12:54:26.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1275" for this suite. @ 08/05/23 12:54:26.395
• [0.097 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/05/23 12:54:26.404
  Aug  5 12:54:26.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:54:26.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:26.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:26.426
  STEP: Counting existing ResourceQuota @ 08/05/23 12:54:26.428
  STEP: Creating a ResourceQuota @ 08/05/23 12:54:31.433
  STEP: Ensuring resource quota status is calculated @ 08/05/23 12:54:31.439
  STEP: Creating a ReplicaSet @ 08/05/23 12:54:33.444
  STEP: Ensuring resource quota status captures replicaset creation @ 08/05/23 12:54:33.455
  STEP: Deleting a ReplicaSet @ 08/05/23 12:54:35.46
  STEP: Ensuring resource quota status released usage @ 08/05/23 12:54:35.467
  Aug  5 12:54:37.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9646" for this suite. @ 08/05/23 12:54:37.476
• [11.079 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/05/23 12:54:37.483
  Aug  5 12:54:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:54:37.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:37.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:37.502
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/05/23 12:54:37.504
  STEP: Saw pod success @ 08/05/23 12:54:41.525
  Aug  5 12:54:41.529: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-b3e57697-a663-4713-b5cd-9279a4f79b7d container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:54:41.536
  Aug  5 12:54:41.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1080" for this suite. @ 08/05/23 12:54:41.555
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/05/23 12:54:41.562
  Aug  5 12:54:41.563: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 12:54:41.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:54:41.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:54:41.582
  STEP: Creating pod test-grpc-eadd0eb3-01d9-47ed-af74-995137a0ca63 in namespace container-probe-4111 @ 08/05/23 12:54:41.585
  Aug  5 12:54:43.602: INFO: Started pod test-grpc-eadd0eb3-01d9-47ed-af74-995137a0ca63 in namespace container-probe-4111
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 12:54:43.602
  Aug  5 12:54:43.606: INFO: Initial restart count of pod test-grpc-eadd0eb3-01d9-47ed-af74-995137a0ca63 is 0
  Aug  5 12:55:57.792: INFO: Restart count of pod container-probe-4111/test-grpc-eadd0eb3-01d9-47ed-af74-995137a0ca63 is now 1 (1m14.18626675s elapsed)
  Aug  5 12:55:57.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:55:57.796
  STEP: Destroying namespace "container-probe-4111" for this suite. @ 08/05/23 12:55:57.811
• [76.255 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/05/23 12:55:57.818
  Aug  5 12:55:57.818: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 12:55:57.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:55:57.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:55:57.839
  STEP: creating the pod @ 08/05/23 12:55:57.841
  STEP: submitting the pod to kubernetes @ 08/05/23 12:55:57.841
  STEP: verifying the pod is in kubernetes @ 08/05/23 12:55:59.866
  STEP: updating the pod @ 08/05/23 12:55:59.87
  Aug  5 12:56:00.385: INFO: Successfully updated pod "pod-update-722be93b-84fc-4f28-994a-310829025b14"
  STEP: verifying the updated pod is in kubernetes @ 08/05/23 12:56:00.388
  Aug  5 12:56:00.394: INFO: Pod update OK
  Aug  5 12:56:00.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1973" for this suite. @ 08/05/23 12:56:00.398
• [2.587 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 08/05/23 12:56:00.408
  Aug  5 12:56:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 12:56:00.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:00.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:00.431
  STEP: Creating simple DaemonSet "daemon-set" @ 08/05/23 12:56:00.455
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 12:56:00.461
  Aug  5 12:56:00.468: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:00.468: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:00.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:56:00.471: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:56:01.476: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:01.476: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:01.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:56:01.479: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:56:02.476: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:02.476: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:56:02.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 12:56:02.479: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 08/05/23 12:56:02.483
  Aug  5 12:56:02.487: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/05/23 12:56:02.487
  Aug  5 12:56:02.499: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/05/23 12:56:02.499
  Aug  5 12:56:02.500: INFO: Observed &DaemonSet event: ADDED
  Aug  5 12:56:02.501: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.501: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.501: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.501: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.502: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.502: INFO: Found daemon set daemon-set in namespace daemonsets-1189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  5 12:56:02.502: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/05/23 12:56:02.502
  STEP: watching for the daemon set status to be patched @ 08/05/23 12:56:02.509
  Aug  5 12:56:02.511: INFO: Observed &DaemonSet event: ADDED
  Aug  5 12:56:02.511: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.511: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.512: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.512: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.512: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.512: INFO: Observed daemon set daemon-set in namespace daemonsets-1189 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  5 12:56:02.513: INFO: Observed &DaemonSet event: MODIFIED
  Aug  5 12:56:02.513: INFO: Found daemon set daemon-set in namespace daemonsets-1189 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug  5 12:56:02.513: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 12:56:02.516
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1189, will wait for the garbage collector to delete the pods @ 08/05/23 12:56:02.516
  Aug  5 12:56:02.577: INFO: Deleting DaemonSet.extensions daemon-set took: 6.60831ms
  Aug  5 12:56:02.677: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.448494ms
  Aug  5 12:56:04.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 12:56:04.181: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 12:56:04.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21774"},"items":null}

  Aug  5 12:56:04.188: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21774"},"items":null}

  Aug  5 12:56:04.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1189" for this suite. @ 08/05/23 12:56:04.206
• [3.804 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/05/23 12:56:04.213
  Aug  5 12:56:04.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 12:56:04.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:04.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:04.233
  STEP: creating the pod @ 08/05/23 12:56:04.235
  STEP: setting up watch @ 08/05/23 12:56:04.235
  STEP: submitting the pod to kubernetes @ 08/05/23 12:56:04.341
  STEP: verifying the pod is in kubernetes @ 08/05/23 12:56:04.351
  STEP: verifying pod creation was observed @ 08/05/23 12:56:04.356
  STEP: deleting the pod gracefully @ 08/05/23 12:56:06.368
  STEP: verifying pod deletion was observed @ 08/05/23 12:56:06.375
  Aug  5 12:56:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9518" for this suite. @ 08/05/23 12:56:09.128
• [4.922 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/05/23 12:56:09.14
  Aug  5 12:56:09.140: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 12:56:09.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:09.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:09.16
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 12:56:09.163
  STEP: Saw pod success @ 08/05/23 12:56:13.185
  Aug  5 12:56:13.189: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-c356abb0-fbb9-44fb-9b70-d344cf7e8cb4 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 12:56:13.205
  Aug  5 12:56:13.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8987" for this suite. @ 08/05/23 12:56:13.224
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/05/23 12:56:13.232
  Aug  5 12:56:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:56:13.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:13.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:13.253
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/05/23 12:56:13.255
  STEP: Saw pod success @ 08/05/23 12:56:17.279
  Aug  5 12:56:17.282: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-c479e72d-34d0-41ba-a952-30b52f344dcb container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:56:17.29
  Aug  5 12:56:17.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-35" for this suite. @ 08/05/23 12:56:17.311
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/05/23 12:56:17.319
  Aug  5 12:56:17.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 12:56:17.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:17.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:17.34
  Aug  5 12:56:17.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-111" for this suite. @ 08/05/23 12:56:17.39
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/05/23 12:56:17.398
  Aug  5 12:56:17.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:56:17.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:17.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:17.417
  Aug  5 12:56:17.423: INFO: Got root ca configmap in namespace "svcaccounts-3166"
  Aug  5 12:56:17.429: INFO: Deleted root ca configmap in namespace "svcaccounts-3166"
  STEP: waiting for a new root ca configmap created @ 08/05/23 12:56:17.93
  Aug  5 12:56:17.934: INFO: Recreated root ca configmap in namespace "svcaccounts-3166"
  Aug  5 12:56:17.939: INFO: Updated root ca configmap in namespace "svcaccounts-3166"
  STEP: waiting for the root ca configmap reconciled @ 08/05/23 12:56:18.44
  Aug  5 12:56:18.444: INFO: Reconciled root ca configmap in namespace "svcaccounts-3166"
  Aug  5 12:56:18.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3166" for this suite. @ 08/05/23 12:56:18.448
• [1.057 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/05/23 12:56:18.455
  Aug  5 12:56:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 12:56:18.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:18.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:18.474
  STEP: Creating the pod @ 08/05/23 12:56:18.476
  Aug  5 12:56:21.022: INFO: Successfully updated pod "labelsupdatee26aac54-f89f-4ac5-b5e5-a7a91296c330"
  Aug  5 12:56:23.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5509" for this suite. @ 08/05/23 12:56:23.048
• [4.599 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/05/23 12:56:23.056
  Aug  5 12:56:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 12:56:23.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:23.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:23.077
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5018 @ 08/05/23 12:56:23.079
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/05/23 12:56:23.09
  STEP: creating service externalsvc in namespace services-5018 @ 08/05/23 12:56:23.09
  STEP: creating replication controller externalsvc in namespace services-5018 @ 08/05/23 12:56:23.104
  I0805 12:56:23.111423      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5018, replica count: 2
  I0805 12:56:26.162305      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/05/23 12:56:26.168
  Aug  5 12:56:26.187: INFO: Creating new exec pod
  Aug  5 12:56:28.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-5018 exec execpodlwr84 -- /bin/sh -x -c nslookup clusterip-service.services-5018.svc.cluster.local'
  Aug  5 12:56:28.367: INFO: stderr: "+ nslookup clusterip-service.services-5018.svc.cluster.local\n"
  Aug  5 12:56:28.367: INFO: stdout: "Server:\t\t10.152.183.228\nAddress:\t10.152.183.228#53\n\nclusterip-service.services-5018.svc.cluster.local\tcanonical name = externalsvc.services-5018.svc.cluster.local.\nName:\texternalsvc.services-5018.svc.cluster.local\nAddress: 10.152.183.38\n\n"
  Aug  5 12:56:28.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5018, will wait for the garbage collector to delete the pods @ 08/05/23 12:56:28.377
  Aug  5 12:56:28.442: INFO: Deleting ReplicationController externalsvc took: 7.34856ms
  Aug  5 12:56:28.542: INFO: Terminating ReplicationController externalsvc pods took: 100.507592ms
  Aug  5 12:56:30.261: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-5018" for this suite. @ 08/05/23 12:56:30.273
• [7.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/05/23 12:56:30.283
  Aug  5 12:56:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename runtimeclass @ 08/05/23 12:56:30.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:30.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:30.301
  Aug  5 12:56:30.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4395" for this suite. @ 08/05/23 12:56:30.332
• [0.058 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/05/23 12:56:30.341
  Aug  5 12:56:30.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 12:56:30.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:30.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:30.361
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/05/23 12:56:30.363
  STEP: Saw pod success @ 08/05/23 12:56:34.386
  Aug  5 12:56:34.390: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-ffc7b236-74f9-4f0d-a2da-337741879321 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 12:56:34.404
  Aug  5 12:56:34.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7533" for this suite. @ 08/05/23 12:56:34.425
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/05/23 12:56:34.433
  Aug  5 12:56:34.433: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 12:56:34.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:34.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:34.454
  STEP: creating a ServiceAccount @ 08/05/23 12:56:34.456
  STEP: watching for the ServiceAccount to be added @ 08/05/23 12:56:34.464
  STEP: patching the ServiceAccount @ 08/05/23 12:56:34.468
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/05/23 12:56:34.474
  STEP: deleting the ServiceAccount @ 08/05/23 12:56:34.478
  Aug  5 12:56:34.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8352" for this suite. @ 08/05/23 12:56:34.497
• [0.071 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/05/23 12:56:34.505
  Aug  5 12:56:34.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 12:56:34.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:34.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:34.524
  STEP: apply creating a deployment @ 08/05/23 12:56:34.527
  Aug  5 12:56:34.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1088" for this suite. @ 08/05/23 12:56:34.544
• [0.046 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/05/23 12:56:34.552
  Aug  5 12:56:34.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 12:56:34.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:34.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:34.574
  STEP: Creating configMap with name projected-configmap-test-volume-64805def-7279-4a02-a3b4-9b3a3b95401d @ 08/05/23 12:56:34.576
  STEP: Creating a pod to test consume configMaps @ 08/05/23 12:56:34.582
  STEP: Saw pod success @ 08/05/23 12:56:38.605
  Aug  5 12:56:38.609: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-projected-configmaps-a336bb2b-63ba-475c-bfd0-a51eb86cfde2 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 12:56:38.616
  Aug  5 12:56:38.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4582" for this suite. @ 08/05/23 12:56:38.642
• [4.097 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/05/23 12:56:38.65
  Aug  5 12:56:38.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 12:56:38.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:56:38.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:56:38.671
  STEP: Creating pod busybox-d1adf6a9-9f87-4144-b71a-178402706201 in namespace container-probe-9927 @ 08/05/23 12:56:38.674
  Aug  5 12:56:40.693: INFO: Started pod busybox-d1adf6a9-9f87-4144-b71a-178402706201 in namespace container-probe-9927
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 12:56:40.693
  Aug  5 12:56:40.696: INFO: Initial restart count of pod busybox-d1adf6a9-9f87-4144-b71a-178402706201 is 0
  Aug  5 12:57:30.819: INFO: Restart count of pod container-probe-9927/busybox-d1adf6a9-9f87-4144-b71a-178402706201 is now 1 (50.122913311s elapsed)
  Aug  5 12:57:30.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 12:57:30.824
  STEP: Destroying namespace "container-probe-9927" for this suite. @ 08/05/23 12:57:30.846
• [52.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/05/23 12:57:30.857
  Aug  5 12:57:30.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 12:57:30.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:57:30.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:57:30.879
  STEP: Creating secret with name s-test-opt-del-63d796f0-aec6-4486-b0d0-d86f55dc9acb @ 08/05/23 12:57:30.886
  STEP: Creating secret with name s-test-opt-upd-989f7334-f939-4dbf-a912-93893f04ed9c @ 08/05/23 12:57:30.89
  STEP: Creating the pod @ 08/05/23 12:57:30.894
  STEP: Deleting secret s-test-opt-del-63d796f0-aec6-4486-b0d0-d86f55dc9acb @ 08/05/23 12:57:32.939
  STEP: Updating secret s-test-opt-upd-989f7334-f939-4dbf-a912-93893f04ed9c @ 08/05/23 12:57:32.946
  STEP: Creating secret with name s-test-opt-create-6ca118a9-c146-4ba9-bc3b-1e4a9cfbe579 @ 08/05/23 12:57:32.951
  STEP: waiting to observe update in volume @ 08/05/23 12:57:32.956
  Aug  5 12:58:55.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7845" for this suite. @ 08/05/23 12:58:55.389
• [84.539 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/05/23 12:58:55.397
  Aug  5 12:58:55.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename cronjob @ 08/05/23 12:58:55.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:58:55.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:58:55.418
  STEP: Creating a cronjob @ 08/05/23 12:58:55.421
  STEP: creating @ 08/05/23 12:58:55.421
  STEP: getting @ 08/05/23 12:58:55.428
  STEP: listing @ 08/05/23 12:58:55.431
  STEP: watching @ 08/05/23 12:58:55.435
  Aug  5 12:58:55.435: INFO: starting watch
  STEP: cluster-wide listing @ 08/05/23 12:58:55.436
  STEP: cluster-wide watching @ 08/05/23 12:58:55.439
  Aug  5 12:58:55.439: INFO: starting watch
  STEP: patching @ 08/05/23 12:58:55.44
  STEP: updating @ 08/05/23 12:58:55.447
  Aug  5 12:58:55.455: INFO: waiting for watch events with expected annotations
  Aug  5 12:58:55.455: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/05/23 12:58:55.455
  STEP: updating /status @ 08/05/23 12:58:55.461
  STEP: get /status @ 08/05/23 12:58:55.468
  STEP: deleting @ 08/05/23 12:58:55.472
  STEP: deleting a collection @ 08/05/23 12:58:55.495
  Aug  5 12:58:55.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8601" for this suite. @ 08/05/23 12:58:55.511
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/05/23 12:58:55.521
  Aug  5 12:58:55.521: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 12:58:55.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:58:55.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:58:55.54
  STEP: Discovering how many secrets are in namespace by default @ 08/05/23 12:58:55.542
  STEP: Counting existing ResourceQuota @ 08/05/23 12:59:00.548
  STEP: Creating a ResourceQuota @ 08/05/23 12:59:05.552
  STEP: Ensuring resource quota status is calculated @ 08/05/23 12:59:05.557
  STEP: Creating a Secret @ 08/05/23 12:59:07.562
  STEP: Ensuring resource quota status captures secret creation @ 08/05/23 12:59:07.577
  STEP: Deleting a secret @ 08/05/23 12:59:09.582
  STEP: Ensuring resource quota status released usage @ 08/05/23 12:59:09.589
  Aug  5 12:59:11.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1697" for this suite. @ 08/05/23 12:59:11.597
• [16.085 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/05/23 12:59:11.606
  Aug  5 12:59:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/05/23 12:59:11.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:11.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:11.628
  STEP: create the container to handle the HTTPGet hook request. @ 08/05/23 12:59:11.635
  STEP: create the pod with lifecycle hook @ 08/05/23 12:59:13.659
  STEP: check poststart hook @ 08/05/23 12:59:15.678
  STEP: delete the pod with lifecycle hook @ 08/05/23 12:59:15.696
  Aug  5 12:59:17.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8289" for this suite. @ 08/05/23 12:59:17.718
• [6.120 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/05/23 12:59:17.727
  Aug  5 12:59:17.727: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 12:59:17.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:17.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:17.748
  STEP: creating service endpoint-test2 in namespace services-2924 @ 08/05/23 12:59:17.756
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2924 to expose endpoints map[] @ 08/05/23 12:59:17.766
  Aug  5 12:59:17.770: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Aug  5 12:59:18.780: INFO: successfully validated that service endpoint-test2 in namespace services-2924 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2924 @ 08/05/23 12:59:18.781
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2924 to expose endpoints map[pod1:[80]] @ 08/05/23 12:59:20.803
  Aug  5 12:59:20.814: INFO: successfully validated that service endpoint-test2 in namespace services-2924 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/05/23 12:59:20.814
  Aug  5 12:59:20.814: INFO: Creating new exec pod
  Aug  5 12:59:23.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug  5 12:59:23.951: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  5 12:59:23.951: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 12:59:23.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.121 80'
  Aug  5 12:59:24.070: INFO: stderr: "+ nc -v -t -w 2 10.152.183.121 80\nConnection to 10.152.183.121 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug  5 12:59:24.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2924 @ 08/05/23 12:59:24.07
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2924 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/05/23 12:59:26.09
  Aug  5 12:59:26.104: INFO: successfully validated that service endpoint-test2 in namespace services-2924 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/05/23 12:59:26.104
  Aug  5 12:59:27.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug  5 12:59:27.223: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  5 12:59:27.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 12:59:27.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.121 80'
  Aug  5 12:59:27.346: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.121 80\nConnection to 10.152.183.121 80 port [tcp/http] succeeded!\n"
  Aug  5 12:59:27.346: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2924 @ 08/05/23 12:59:27.346
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2924 to expose endpoints map[pod2:[80]] @ 08/05/23 12:59:27.363
  Aug  5 12:59:27.377: INFO: successfully validated that service endpoint-test2 in namespace services-2924 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/05/23 12:59:27.377
  Aug  5 12:59:28.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug  5 12:59:28.504: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  5 12:59:28.504: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 12:59:28.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2924 exec execpodqfqjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.121 80'
  Aug  5 12:59:28.631: INFO: stderr: "+ nc -v -t -w 2 10.152.183.121 80\n+ echo hostName\nConnection to 10.152.183.121 80 port [tcp/http] succeeded!\n"
  Aug  5 12:59:28.631: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2924 @ 08/05/23 12:59:28.631
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2924 to expose endpoints map[] @ 08/05/23 12:59:28.656
  Aug  5 12:59:28.664: INFO: successfully validated that service endpoint-test2 in namespace services-2924 exposes endpoints map[]
  Aug  5 12:59:28.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2924" for this suite. @ 08/05/23 12:59:28.688
• [10.967 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/05/23 12:59:28.696
  Aug  5 12:59:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 12:59:28.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:28.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:28.717
  STEP: create the deployment @ 08/05/23 12:59:28.72
  W0805 12:59:28.726125      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/05/23 12:59:28.726
  STEP: delete the deployment @ 08/05/23 12:59:29.234
  STEP: wait for all rs to be garbage collected @ 08/05/23 12:59:29.245
  STEP: expected 0 rs, got 1 rs @ 08/05/23 12:59:29.256
  STEP: expected 0 pods, got 2 pods @ 08/05/23 12:59:29.26
  STEP: Gathering metrics @ 08/05/23 12:59:29.771
  W0805 12:59:29.775038      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 12:59:29.775: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 12:59:29.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6046" for this suite. @ 08/05/23 12:59:29.78
• [1.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/05/23 12:59:29.789
  Aug  5 12:59:29.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename controllerrevisions @ 08/05/23 12:59:29.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:29.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:29.808
  STEP: Creating DaemonSet "e2e-6cx6h-daemon-set" @ 08/05/23 12:59:29.829
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 12:59:29.835
  Aug  5 12:59:29.840: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:29.840: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:29.844: INFO: Number of nodes with available pods controlled by daemonset e2e-6cx6h-daemon-set: 0
  Aug  5 12:59:29.844: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:59:30.849: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:30.849: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:30.853: INFO: Number of nodes with available pods controlled by daemonset e2e-6cx6h-daemon-set: 0
  Aug  5 12:59:30.853: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  Aug  5 12:59:31.848: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:31.848: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 12:59:31.853: INFO: Number of nodes with available pods controlled by daemonset e2e-6cx6h-daemon-set: 3
  Aug  5 12:59:31.853: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-6cx6h-daemon-set
  STEP: Confirm DaemonSet "e2e-6cx6h-daemon-set" successfully created with "daemonset-name=e2e-6cx6h-daemon-set" label @ 08/05/23 12:59:31.857
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-6cx6h-daemon-set" @ 08/05/23 12:59:31.864
  Aug  5 12:59:31.868: INFO: Located ControllerRevision: "e2e-6cx6h-daemon-set-7f5cf84f8b"
  STEP: Patching ControllerRevision "e2e-6cx6h-daemon-set-7f5cf84f8b" @ 08/05/23 12:59:31.871
  Aug  5 12:59:31.879: INFO: e2e-6cx6h-daemon-set-7f5cf84f8b has been patched
  STEP: Create a new ControllerRevision @ 08/05/23 12:59:31.879
  Aug  5 12:59:31.884: INFO: Created ControllerRevision: e2e-6cx6h-daemon-set-845555ff64
  STEP: Confirm that there are two ControllerRevisions @ 08/05/23 12:59:31.884
  Aug  5 12:59:31.884: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  5 12:59:31.887: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-6cx6h-daemon-set-7f5cf84f8b" @ 08/05/23 12:59:31.888
  STEP: Confirm that there is only one ControllerRevision @ 08/05/23 12:59:31.894
  Aug  5 12:59:31.894: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  5 12:59:31.898: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-6cx6h-daemon-set-845555ff64" @ 08/05/23 12:59:31.901
  Aug  5 12:59:31.911: INFO: e2e-6cx6h-daemon-set-845555ff64 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/05/23 12:59:31.911
  W0805 12:59:31.918149      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/05/23 12:59:31.918
  Aug  5 12:59:31.918: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  5 12:59:32.923: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  5 12:59:32.927: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-6cx6h-daemon-set-845555ff64=updated" @ 08/05/23 12:59:32.927
  STEP: Confirm that there is only one ControllerRevision @ 08/05/23 12:59:32.936
  Aug  5 12:59:32.936: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  5 12:59:32.939: INFO: Found 1 ControllerRevisions
  Aug  5 12:59:32.943: INFO: ControllerRevision "e2e-6cx6h-daemon-set-99fcdb9cc" has revision 3
  STEP: Deleting DaemonSet "e2e-6cx6h-daemon-set" @ 08/05/23 12:59:32.946
  STEP: deleting DaemonSet.extensions e2e-6cx6h-daemon-set in namespace controllerrevisions-3867, will wait for the garbage collector to delete the pods @ 08/05/23 12:59:32.946
  Aug  5 12:59:33.007: INFO: Deleting DaemonSet.extensions e2e-6cx6h-daemon-set took: 6.704994ms
  Aug  5 12:59:33.108: INFO: Terminating DaemonSet.extensions e2e-6cx6h-daemon-set pods took: 101.045047ms
  Aug  5 12:59:34.613: INFO: Number of nodes with available pods controlled by daemonset e2e-6cx6h-daemon-set: 0
  Aug  5 12:59:34.613: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-6cx6h-daemon-set
  Aug  5 12:59:34.616: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23073"},"items":null}

  Aug  5 12:59:34.620: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23073"},"items":null}

  Aug  5 12:59:34.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-3867" for this suite. @ 08/05/23 12:59:34.637
• [4.855 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/05/23 12:59:34.645
  Aug  5 12:59:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 12:59:34.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:34.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:34.668
  STEP: Creating a pod to test substitution in container's args @ 08/05/23 12:59:34.67
  STEP: Saw pod success @ 08/05/23 12:59:38.696
  Aug  5 12:59:38.699: INFO: Trying to get logs from node ip-172-31-95-133 pod var-expansion-c1dbf878-6e4e-47c9-bb53-260c612ebd13 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 12:59:38.719
  Aug  5 12:59:38.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1457" for this suite. @ 08/05/23 12:59:38.746
• [4.108 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/05/23 12:59:38.753
  Aug  5 12:59:38.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 12:59:38.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:38.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:38.773
  STEP: Creating service test in namespace statefulset-345 @ 08/05/23 12:59:38.775
  STEP: Creating statefulset ss in namespace statefulset-345 @ 08/05/23 12:59:38.781
  Aug  5 12:59:38.791: INFO: Found 0 stateful pods, waiting for 1
  Aug  5 12:59:48.799: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/05/23 12:59:48.807
  STEP: updating a scale subresource @ 08/05/23 12:59:48.815
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/05/23 12:59:48.823
  STEP: Patch a scale subresource @ 08/05/23 12:59:48.829
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/05/23 12:59:48.844
  Aug  5 12:59:48.848: INFO: Deleting all statefulset in ns statefulset-345
  Aug  5 12:59:48.852: INFO: Scaling statefulset ss to 0
  Aug  5 12:59:58.895: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 12:59:58.899: INFO: Deleting statefulset ss
  Aug  5 12:59:58.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-345" for this suite. @ 08/05/23 12:59:58.925
• [20.178 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/05/23 12:59:58.933
  Aug  5 12:59:58.933: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 12:59:58.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 12:59:58.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 12:59:58.955
  STEP: Creating service test in namespace statefulset-3768 @ 08/05/23 12:59:58.959
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/05/23 12:59:58.966
  STEP: Creating stateful set ss in namespace statefulset-3768 @ 08/05/23 12:59:58.971
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3768 @ 08/05/23 12:59:58.98
  Aug  5 12:59:58.985: INFO: Found 0 stateful pods, waiting for 1
  Aug  5 13:00:08.990: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/05/23 13:00:08.99
  Aug  5 13:00:08.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:00:09.120: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:00:09.120: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:00:09.120: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:00:09.124: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Aug  5 13:00:19.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:00:19.132: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:00:19.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999816s
  Aug  5 13:00:20.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995513497s
  Aug  5 13:00:21.159: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991168489s
  Aug  5 13:00:22.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986906295s
  Aug  5 13:00:23.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982508457s
  Aug  5 13:00:24.172: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97823122s
  Aug  5 13:00:25.176: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973777592s
  Aug  5 13:00:26.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968423893s
  Aug  5 13:00:27.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963441326s
  Aug  5 13:00:28.192: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.558164ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3768 @ 08/05/23 13:00:29.192
  Aug  5 13:00:29.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:00:29.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 13:00:29.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:00:29.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:00:29.331: INFO: Found 1 stateful pods, waiting for 3
  Aug  5 13:00:39.339: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:00:39.339: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:00:39.339: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/05/23 13:00:39.339
  STEP: Scale down will halt with unhealthy stateful pod @ 08/05/23 13:00:39.339
  Aug  5 13:00:39.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:00:39.473: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:00:39.473: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:00:39.473: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:00:39.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:00:39.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:00:39.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:00:39.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:00:39.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:00:39.750: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:00:39.750: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:00:39.750: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:00:39.750: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:00:39.754: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Aug  5 13:00:49.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:00:49.766: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:00:49.766: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:00:49.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999822s
  Aug  5 13:00:50.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996101384s
  Aug  5 13:00:51.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991517452s
  Aug  5 13:00:52.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98600462s
  Aug  5 13:00:53.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981491452s
  Aug  5 13:00:54.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976875754s
  Aug  5 13:00:55.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97182069s
  Aug  5 13:00:56.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965996098s
  Aug  5 13:00:57.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960768863s
  Aug  5 13:00:58.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.026719ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3768 @ 08/05/23 13:00:59.826
  Aug  5 13:00:59.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:00:59.950: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 13:00:59.950: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:00:59.950: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:00:59.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:01:00.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 13:01:00.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:01:00.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:01:00.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-3768 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:01:00.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 13:01:00.187: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:01:00.187: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:01:00.188: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/05/23 13:01:10.205
  Aug  5 13:01:10.205: INFO: Deleting all statefulset in ns statefulset-3768
  Aug  5 13:01:10.209: INFO: Scaling statefulset ss to 0
  Aug  5 13:01:10.220: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:01:10.223: INFO: Deleting statefulset ss
  Aug  5 13:01:10.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3768" for this suite. @ 08/05/23 13:01:10.24
• [71.315 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/05/23 13:01:10.249
  Aug  5 13:01:10.249: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context @ 08/05/23 13:01:10.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:01:10.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:01:10.271
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/05/23 13:01:10.274
  STEP: Saw pod success @ 08/05/23 13:01:14.3
  Aug  5 13:01:14.304: INFO: Trying to get logs from node ip-172-31-95-133 pod security-context-58224743-f900-4828-bf30-75f4de2c19c0 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:01:14.321
  Aug  5 13:01:14.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3769" for this suite. @ 08/05/23 13:01:14.345
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/05/23 13:01:14.355
  Aug  5 13:01:14.355: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:01:14.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:01:14.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:01:14.376
  STEP: Creating configMap with name cm-test-opt-del-91554868-c883-4c79-88c5-c3377fe7e261 @ 08/05/23 13:01:14.382
  STEP: Creating configMap with name cm-test-opt-upd-b9b1aedb-50f5-4b3a-9608-31e5f66249c2 @ 08/05/23 13:01:14.387
  STEP: Creating the pod @ 08/05/23 13:01:14.393
  STEP: Deleting configmap cm-test-opt-del-91554868-c883-4c79-88c5-c3377fe7e261 @ 08/05/23 13:01:16.44
  STEP: Updating configmap cm-test-opt-upd-b9b1aedb-50f5-4b3a-9608-31e5f66249c2 @ 08/05/23 13:01:16.446
  STEP: Creating configMap with name cm-test-opt-create-32f931f4-5628-41d0-9dc7-39a82a36de72 @ 08/05/23 13:01:16.451
  STEP: waiting to observe update in volume @ 08/05/23 13:01:16.458
  Aug  5 13:02:46.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8285" for this suite. @ 08/05/23 13:02:46.934
• [92.586 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/05/23 13:02:46.942
  Aug  5 13:02:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:02:46.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:02:46.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:02:46.965
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:02:46.967
  STEP: Saw pod success @ 08/05/23 13:02:50.989
  Aug  5 13:02:50.994: INFO: Trying to get logs from node ip-172-31-35-140 pod downwardapi-volume-7199c0d6-30f9-4be9-858e-123dd5b0668e container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:02:51.012
  Aug  5 13:02:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5412" for this suite. @ 08/05/23 13:02:51.042
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/05/23 13:02:51.051
  Aug  5 13:02:51.051: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption @ 08/05/23 13:02:51.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:02:51.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:02:51.07
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/05/23 13:02:51.073
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:02:51.078
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/05/23 13:02:53.092
  STEP: Waiting for all pods to be running @ 08/05/23 13:02:53.092
  Aug  5 13:02:53.096: INFO: pods: 0 < 3
  STEP: locating a running pod @ 08/05/23 13:02:55.101
  STEP: Updating the pdb to allow a pod to be evicted @ 08/05/23 13:02:55.112
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:02:55.122
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/05/23 13:02:57.131
  STEP: Waiting for all pods to be running @ 08/05/23 13:02:57.131
  STEP: Waiting for the pdb to observed all healthy pods @ 08/05/23 13:02:57.135
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/05/23 13:02:57.163
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:02:57.188
  STEP: Waiting for all pods to be running @ 08/05/23 13:02:59.2
  STEP: locating a running pod @ 08/05/23 13:02:59.204
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/05/23 13:02:59.218
  STEP: Waiting for the pdb to be deleted @ 08/05/23 13:02:59.225
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/05/23 13:02:59.228
  STEP: Waiting for all pods to be running @ 08/05/23 13:02:59.228
  Aug  5 13:02:59.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3615" for this suite. @ 08/05/23 13:02:59.254
• [8.210 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/05/23 13:02:59.261
  Aug  5 13:02:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename containers @ 08/05/23 13:02:59.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:02:59.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:02:59.284
  STEP: Creating a pod to test override all @ 08/05/23 13:02:59.292
  STEP: Saw pod success @ 08/05/23 13:03:03.315
  Aug  5 13:03:03.319: INFO: Trying to get logs from node ip-172-31-95-133 pod client-containers-e34d6f44-90fd-49f3-82f2-363417cc25f2 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:03:03.327
  Aug  5 13:03:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6760" for this suite. @ 08/05/23 13:03:03.35
• [4.096 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/05/23 13:03:03.357
  Aug  5 13:03:03.357: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/05/23 13:03:03.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:03.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:03.379
  Aug  5 13:03:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:03:09.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9497" for this suite. @ 08/05/23 13:03:09.605
• [6.255 seconds]
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/05/23 13:03:09.612
  Aug  5 13:03:09.612: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename events @ 08/05/23 13:03:09.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:09.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:09.642
  STEP: creating a test event @ 08/05/23 13:03:09.646
  STEP: listing all events in all namespaces @ 08/05/23 13:03:09.652
  STEP: patching the test event @ 08/05/23 13:03:09.659
  STEP: fetching the test event @ 08/05/23 13:03:09.667
  STEP: updating the test event @ 08/05/23 13:03:09.67
  STEP: getting the test event @ 08/05/23 13:03:09.681
  STEP: deleting the test event @ 08/05/23 13:03:09.684
  STEP: listing all events in all namespaces @ 08/05/23 13:03:09.694
  Aug  5 13:03:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5859" for this suite. @ 08/05/23 13:03:09.704
• [0.099 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/05/23 13:03:09.712
  Aug  5 13:03:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename containers @ 08/05/23 13:03:09.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:09.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:09.737
  Aug  5 13:03:11.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4935" for this suite. @ 08/05/23 13:03:11.774
• [2.069 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/05/23 13:03:11.782
  Aug  5 13:03:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:03:11.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:11.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:11.806
  STEP: creating a Pod with a static label @ 08/05/23 13:03:11.814
  STEP: watching for Pod to be ready @ 08/05/23 13:03:11.823
  Aug  5 13:03:11.824: INFO: observed Pod pod-test in namespace pods-9895 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug  5 13:03:11.829: INFO: observed Pod pod-test in namespace pods-9895 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC  }]
  Aug  5 13:03:11.848: INFO: observed Pod pod-test in namespace pods-9895 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC  }]
  Aug  5 13:03:12.728: INFO: Found Pod pod-test in namespace pods-9895 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:03:11 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/05/23 13:03:12.732
  STEP: getting the Pod and ensuring that it's patched @ 08/05/23 13:03:12.742
  STEP: replacing the Pod's status Ready condition to False @ 08/05/23 13:03:12.746
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/05/23 13:03:12.759
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/05/23 13:03:12.759
  STEP: watching for the Pod to be deleted @ 08/05/23 13:03:12.771
  Aug  5 13:03:12.773: INFO: observed event type MODIFIED
  Aug  5 13:03:14.739: INFO: observed event type MODIFIED
  Aug  5 13:03:15.070: INFO: observed event type MODIFIED
  Aug  5 13:03:15.741: INFO: observed event type MODIFIED
  Aug  5 13:03:15.751: INFO: observed event type MODIFIED
  Aug  5 13:03:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9895" for this suite. @ 08/05/23 13:03:15.766
• [3.991 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/05/23 13:03:15.773
  Aug  5 13:03:15.773: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:03:15.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:15.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:15.793
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/05/23 13:03:15.796
  STEP: Saw pod success @ 08/05/23 13:03:19.821
  Aug  5 13:03:19.825: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-2b752497-f48c-4ac7-85a6-826792277912 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:03:19.833
  Aug  5 13:03:19.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7111" for this suite. @ 08/05/23 13:03:19.853
• [4.087 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/05/23 13:03:19.861
  Aug  5 13:03:19.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 13:03:19.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:19.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:19.883
  STEP: Creating a test headless service @ 08/05/23 13:03:19.886
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7698 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7698;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7698 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7698;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7698.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7698.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7698.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7698.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7698.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7698.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7698.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7698.svc;check="$$(dig +notcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_tcp@PTR;sleep 1; done
   @ 08/05/23 13:03:19.905
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7698 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7698;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7698 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7698;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7698.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7698.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7698.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7698.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7698.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7698.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7698.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7698.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7698.svc;check="$$(dig +notcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.232_tcp@PTR;sleep 1; done
   @ 08/05/23 13:03:19.905
  STEP: creating a pod to probe DNS @ 08/05/23 13:03:19.906
  STEP: submitting the pod to kubernetes @ 08/05/23 13:03:19.906
  STEP: retrieving the pod @ 08/05/23 13:03:21.93
  STEP: looking for the results for each expected name from probers @ 08/05/23 13:03:21.934
  Aug  5 13:03:21.939: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.944: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.948: INFO: Unable to read wheezy_udp@dns-test-service.dns-7698 from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.953: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7698 from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.966: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.971: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.993: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:21.997: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.002: INFO: Unable to read jessie_udp@dns-test-service.dns-7698 from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.008: INFO: Unable to read jessie_tcp@dns-test-service.dns-7698 from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.012: INFO: Unable to read jessie_udp@dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.022: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.027: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7698.svc from pod dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661: the server could not find the requested resource (get pods dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661)
  Aug  5 13:03:22.044: INFO: Lookups using dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7698 wheezy_tcp@dns-test-service.dns-7698 wheezy_udp@dns-test-service.dns-7698.svc wheezy_tcp@dns-test-service.dns-7698.svc wheezy_udp@_http._tcp.dns-test-service.dns-7698.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7698.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7698 jessie_tcp@dns-test-service.dns-7698 jessie_udp@dns-test-service.dns-7698.svc jessie_tcp@dns-test-service.dns-7698.svc jessie_udp@_http._tcp.dns-test-service.dns-7698.svc jessie_tcp@_http._tcp.dns-test-service.dns-7698.svc]

  Aug  5 13:03:27.163: INFO: DNS probes using dns-7698/dns-test-28d4503e-bfd8-48c4-8c7e-19b629af2661 succeeded

  Aug  5 13:03:27.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 13:03:27.169
  STEP: deleting the test service @ 08/05/23 13:03:27.183
  STEP: deleting the test headless service @ 08/05/23 13:03:27.211
  STEP: Destroying namespace "dns-7698" for this suite. @ 08/05/23 13:03:27.226
• [7.374 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/05/23 13:03:27.235
  Aug  5 13:03:27.235: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:03:27.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:27.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:27.254
  STEP: Creating configMap with name projected-configmap-test-volume-map-a84e9495-093c-41ef-a92a-8491d1ca478c @ 08/05/23 13:03:27.256
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:03:27.261
  STEP: Saw pod success @ 08/05/23 13:03:31.283
  Aug  5 13:03:31.287: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-e18242e5-0b53-4df1-a7f8-f927ee555df8 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:03:31.294
  Aug  5 13:03:31.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5086" for this suite. @ 08/05/23 13:03:31.318
• [4.090 seconds]
------------------------------
SSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/05/23 13:03:31.325
  Aug  5 13:03:31.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename lease-test @ 08/05/23 13:03:31.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:31.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:31.345
  Aug  5 13:03:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-4331" for this suite. @ 08/05/23 13:03:31.434
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/05/23 13:03:31.452
  Aug  5 13:03:31.452: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 13:03:31.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:31.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:31.48
  STEP: creating a Deployment @ 08/05/23 13:03:31.486
  STEP: waiting for Deployment to be created @ 08/05/23 13:03:31.493
  STEP: waiting for all Replicas to be Ready @ 08/05/23 13:03:31.494
  Aug  5 13:03:31.495: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.495: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.504: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.505: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.522: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.522: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.549: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:31.549: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  5 13:03:32.784: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug  5 13:03:32.785: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug  5 13:03:33.065: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/05/23 13:03:33.065
  W0805 13:03:33.075761      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  5 13:03:33.077: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/05/23 13:03:33.077
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 0
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.079: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.098: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.098: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.143: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.143: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:33.159: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:33.159: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:33.173: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:33.173: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:34.081: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:34.081: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:34.104: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  STEP: listing Deployments @ 08/05/23 13:03:34.105
  Aug  5 13:03:34.109: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/05/23 13:03:34.109
  Aug  5 13:03:34.121: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/05/23 13:03:34.121
  Aug  5 13:03:34.128: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:34.134: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:34.155: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:34.174: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:34.186: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:34.190: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:35.106: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:35.145: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:35.166: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  5 13:03:36.828: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/05/23 13:03:36.857
  STEP: fetching the DeploymentStatus @ 08/05/23 13:03:36.864
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.869: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 1
  Aug  5 13:03:36.870: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:36.870: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:36.870: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 2
  Aug  5 13:03:36.870: INFO: observed Deployment test-deployment in namespace deployment-8230 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/05/23 13:03:36.87
  Aug  5 13:03:36.880: INFO: observed event type MODIFIED
  Aug  5 13:03:36.880: INFO: observed event type MODIFIED
  Aug  5 13:03:36.880: INFO: observed event type MODIFIED
  Aug  5 13:03:36.881: INFO: observed event type MODIFIED
  Aug  5 13:03:36.881: INFO: observed event type MODIFIED
  Aug  5 13:03:36.881: INFO: observed event type MODIFIED
  Aug  5 13:03:36.881: INFO: observed event type MODIFIED
  Aug  5 13:03:36.881: INFO: observed event type MODIFIED
  Aug  5 13:03:36.882: INFO: observed event type MODIFIED
  Aug  5 13:03:36.882: INFO: observed event type MODIFIED
  Aug  5 13:03:36.882: INFO: observed event type MODIFIED
  Aug  5 13:03:36.885: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug  5 13:03:36.891: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-8230  6a8daa5c-ded1-4bb3-9833-a1ddb6470b8d 24788 2 2023-08-05 13:03:34 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ddc54ae6-2a97-4bf7-8304-75720b33ff59 0xc004b91db7 0xc004b91db8}] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:03:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc54ae6-2a97-4bf7-8304-75720b33ff59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:03:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b91e40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Aug  5 13:03:36.895: INFO: pod: "test-deployment-6fc78d85c6-5mtvh":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-5mtvh test-deployment-6fc78d85c6- deployment-8230  fd4aa2e8-386d-4afd-a3df-0bba1c03bde7 24787 0 2023-08-05 13:03:35 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 6a8daa5c-ded1-4bb3-9833-a1ddb6470b8d 0xc002c93b87 0xc002c93b88}] [] [{kube-controller-manager Update v1 2023-08-05 13:03:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8daa5c-ded1-4bb3-9833-a1ddb6470b8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:03:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sglcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sglcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.251,StartTime:2023-08-05 13:03:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:03:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0fe67627350804f3c766cd395f6aec43792de13ddefd6226f5f648487e690c4b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.251,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  5 13:03:36.895: INFO: pod: "test-deployment-6fc78d85c6-vq6v9":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-vq6v9 test-deployment-6fc78d85c6- deployment-8230  097310e0-4112-40b5-8384-6796395fd01a 24735 0 2023-08-05 13:03:34 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 6a8daa5c-ded1-4bb3-9833-a1ddb6470b8d 0xc002c93d77 0xc002c93d78}] [] [{kube-controller-manager Update v1 2023-08-05 13:03:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8daa5c-ded1-4bb3-9833-a1ddb6470b8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:03:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4g5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4g5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:03:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.159,StartTime:2023-08-05 13:03:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:03:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://946f1281315103eb50a9b628bef1cac5d325bd584996406601fe405dd3443c98,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.159,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  5 13:03:36.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8230" for this suite. @ 08/05/23 13:03:36.903
• [5.461 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/05/23 13:03:36.914
  Aug  5 13:03:36.914: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:03:36.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:36.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:36.935
  STEP: validating api versions @ 08/05/23 13:03:36.937
  Aug  5 13:03:36.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-4964 api-versions'
  Aug  5 13:03:36.997: INFO: stderr: ""
  Aug  5 13:03:36.997: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug  5 13:03:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4964" for this suite. @ 08/05/23 13:03:37.003
• [0.096 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/05/23 13:03:37.011
  Aug  5 13:03:37.011: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pod-network-test @ 08/05/23 13:03:37.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:37.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:37.038
  STEP: Performing setup for networking test in namespace pod-network-test-4558 @ 08/05/23 13:03:37.04
  STEP: creating a selector @ 08/05/23 13:03:37.04
  STEP: Creating the service pods in kubernetes @ 08/05/23 13:03:37.04
  Aug  5 13:03:37.040: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/05/23 13:03:49.125
  Aug  5 13:03:51.144: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug  5 13:03:51.144: INFO: Breadth first check of 192.168.122.97 on host 172.31.1.47...
  Aug  5 13:03:51.148: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.134:9080/dial?request=hostname&protocol=udp&host=192.168.122.97&port=8081&tries=1'] Namespace:pod-network-test-4558 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:03:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:03:51.148: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:03:51.148: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4558/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.122.97%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 13:03:51.212: INFO: Waiting for responses: map[]
  Aug  5 13:03:51.212: INFO: reached 192.168.122.97 after 0/1 tries
  Aug  5 13:03:51.212: INFO: Breadth first check of 192.168.166.222 on host 172.31.35.140...
  Aug  5 13:03:51.217: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.134:9080/dial?request=hostname&protocol=udp&host=192.168.166.222&port=8081&tries=1'] Namespace:pod-network-test-4558 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:03:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:03:51.217: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:03:51.217: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4558/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.166.222%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 13:03:51.284: INFO: Waiting for responses: map[]
  Aug  5 13:03:51.284: INFO: reached 192.168.166.222 after 0/1 tries
  Aug  5 13:03:51.284: INFO: Breadth first check of 192.168.52.164 on host 172.31.95.133...
  Aug  5 13:03:51.288: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.52.134:9080/dial?request=hostname&protocol=udp&host=192.168.52.164&port=8081&tries=1'] Namespace:pod-network-test-4558 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:03:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:03:51.288: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:03:51.289: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4558/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.52.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.52.164%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  5 13:03:51.351: INFO: Waiting for responses: map[]
  Aug  5 13:03:51.351: INFO: reached 192.168.52.164 after 0/1 tries
  Aug  5 13:03:51.351: INFO: Going to retry 0 out of 3 pods....
  Aug  5 13:03:51.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4558" for this suite. @ 08/05/23 13:03:51.356
• [14.353 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/05/23 13:03:51.365
  Aug  5 13:03:51.365: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:03:51.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:51.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:51.39
  STEP: Creating secret with name projected-secret-test-596240e9-137d-48a7-a48e-3af1d61b54c4 @ 08/05/23 13:03:51.393
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:03:51.397
  STEP: Saw pod success @ 08/05/23 13:03:55.419
  Aug  5 13:03:55.423: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-projected-secrets-de77732f-2d75-42b9-a804-101fb5c31274 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:03:55.43
  Aug  5 13:03:55.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4499" for this suite. @ 08/05/23 13:03:55.453
• [4.095 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 08/05/23 13:03:55.461
  Aug  5 13:03:55.461: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 13:03:55.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:03:55.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:03:55.484
  Aug  5 13:03:55.506: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/05/23 13:03:55.51
  Aug  5 13:03:55.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:55.514: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/05/23 13:03:55.514
  Aug  5 13:03:55.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:55.534: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 13:03:56.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:56.539: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 13:03:57.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  5 13:03:57.538: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/05/23 13:03:57.542
  Aug  5 13:03:57.557: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  5 13:03:57.557: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Aug  5 13:03:58.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:58.561: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/05/23 13:03:58.561
  Aug  5 13:03:58.571: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:58.571: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 13:03:59.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:03:59.576: INFO: Node ip-172-31-95-133 is running 0 daemon pod, expected 1
  Aug  5 13:04:00.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  5 13:04:00.575: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 13:04:00.583
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-134, will wait for the garbage collector to delete the pods @ 08/05/23 13:04:00.583
  Aug  5 13:04:00.648: INFO: Deleting DaemonSet.extensions daemon-set took: 10.043415ms
  Aug  5 13:04:00.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.202417ms
  Aug  5 13:04:02.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:04:02.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 13:04:02.255: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25154"},"items":null}

  Aug  5 13:04:02.259: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25154"},"items":null}

  Aug  5 13:04:02.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-134" for this suite. @ 08/05/23 13:04:02.286
• [6.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/05/23 13:04:02.295
  Aug  5 13:04:02.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 13:04:02.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:02.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:02.316
  Aug  5 13:04:24.386: INFO: Container started at 2023-08-05 13:04:03 +0000 UTC, pod became ready at 2023-08-05 13:04:22 +0000 UTC
  Aug  5 13:04:24.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-952" for this suite. @ 08/05/23 13:04:24.391
• [22.103 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/05/23 13:04:24.398
  Aug  5 13:04:24.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 13:04:24.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:24.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:24.42
  STEP: creating a Namespace @ 08/05/23 13:04:24.422
  STEP: patching the Namespace @ 08/05/23 13:04:24.44
  STEP: get the Namespace and ensuring it has the label @ 08/05/23 13:04:24.447
  Aug  5 13:04:24.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4026" for this suite. @ 08/05/23 13:04:24.455
  STEP: Destroying namespace "nspatchtest-c0d69c40-3900-4047-a10a-570924a8b2d3-3391" for this suite. @ 08/05/23 13:04:24.463
• [0.071 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/05/23 13:04:24.47
  Aug  5 13:04:24.470: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:04:24.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:24.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:24.489
  STEP: Creating a ResourceQuota with best effort scope @ 08/05/23 13:04:24.492
  STEP: Ensuring ResourceQuota status is calculated @ 08/05/23 13:04:24.497
  STEP: Creating a ResourceQuota with not best effort scope @ 08/05/23 13:04:26.502
  STEP: Ensuring ResourceQuota status is calculated @ 08/05/23 13:04:26.509
  STEP: Creating a best-effort pod @ 08/05/23 13:04:28.514
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/05/23 13:04:28.528
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/05/23 13:04:30.533
  STEP: Deleting the pod @ 08/05/23 13:04:32.537
  STEP: Ensuring resource quota status released the pod usage @ 08/05/23 13:04:32.55
  STEP: Creating a not best-effort pod @ 08/05/23 13:04:34.554
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/05/23 13:04:34.568
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/05/23 13:04:36.572
  STEP: Deleting the pod @ 08/05/23 13:04:38.576
  STEP: Ensuring resource quota status released the pod usage @ 08/05/23 13:04:38.592
  Aug  5 13:04:40.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3789" for this suite. @ 08/05/23 13:04:40.602
• [16.139 seconds]
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/05/23 13:04:40.609
  Aug  5 13:04:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/05/23 13:04:40.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:40.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:40.631
  STEP: Setting up the test @ 08/05/23 13:04:40.633
  STEP: Creating hostNetwork=false pod @ 08/05/23 13:04:40.634
  STEP: Creating hostNetwork=true pod @ 08/05/23 13:04:42.662
  STEP: Running the test @ 08/05/23 13:04:44.683
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/05/23 13:04:44.683
  Aug  5 13:04:44.683: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:44.683: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:44.683: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:44.683: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  5 13:04:44.744: INFO: Exec stderr: ""
  Aug  5 13:04:44.745: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:44.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:44.745: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:44.745: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  5 13:04:44.827: INFO: Exec stderr: ""
  Aug  5 13:04:44.827: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:44.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:44.828: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:44.828: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  5 13:04:44.896: INFO: Exec stderr: ""
  Aug  5 13:04:44.896: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:44.897: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:44.897: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  5 13:04:44.959: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/05/23 13:04:44.96
  Aug  5 13:04:44.960: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:44.960: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:44.960: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:44.960: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug  5 13:04:45.020: INFO: Exec stderr: ""
  Aug  5 13:04:45.020: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:45.021: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:45.021: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:45.021: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug  5 13:04:45.071: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/05/23 13:04:45.071
  Aug  5 13:04:45.071: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:45.071: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:45.072: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:45.072: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  5 13:04:45.134: INFO: Exec stderr: ""
  Aug  5 13:04:45.134: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:45.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:45.134: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:45.134: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  5 13:04:45.189: INFO: Exec stderr: ""
  Aug  5 13:04:45.189: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:45.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:45.190: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:45.190: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  5 13:04:45.249: INFO: Exec stderr: ""
  Aug  5 13:04:45.249: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2830 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:04:45.249: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:04:45.250: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:04:45.250: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2830/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  5 13:04:45.319: INFO: Exec stderr: ""
  Aug  5 13:04:45.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2830" for this suite. @ 08/05/23 13:04:45.324
• [4.722 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 08/05/23 13:04:45.333
  Aug  5 13:04:45.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:04:45.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:45.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:45.352
  STEP: Creating secret with name secret-test-map-cf277f8e-bb88-4371-9724-0381e8d3d73d @ 08/05/23 13:04:45.355
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:04:45.359
  STEP: Saw pod success @ 08/05/23 13:04:49.381
  Aug  5 13:04:49.385: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-secrets-9c88878e-1582-4de6-a4f0-8a5a74be4588 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:04:49.393
  Aug  5 13:04:49.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9146" for this suite. @ 08/05/23 13:04:49.415
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/05/23 13:04:49.426
  Aug  5 13:04:49.426: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:04:49.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:04:49.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:04:49.447
  STEP: Setting up server cert @ 08/05/23 13:04:49.475
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:04:49.705
  STEP: Deploying the webhook pod @ 08/05/23 13:04:49.713
  STEP: Wait for the deployment to be ready @ 08/05/23 13:04:49.726
  Aug  5 13:04:49.734: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 13:04:51.745
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:04:51.758
  Aug  5 13:04:52.759: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/05/23 13:04:52.762
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/05/23 13:04:52.763
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/05/23 13:04:52.777
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/05/23 13:04:53.79
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/05/23 13:04:53.79
  STEP: Having no error when timeout is longer than webhook latency @ 08/05/23 13:04:54.82
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/05/23 13:04:54.82
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/05/23 13:04:59.853
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/05/23 13:04:59.853
  Aug  5 13:05:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1047" for this suite. @ 08/05/23 13:05:04.947
  STEP: Destroying namespace "webhook-markers-969" for this suite. @ 08/05/23 13:05:04.954
• [15.535 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/05/23 13:05:04.962
  Aug  5 13:05:04.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:05:04.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:04.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:04.987
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:05:04.989
  STEP: Saw pod success @ 08/05/23 13:05:09.014
  Aug  5 13:05:09.018: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-2a7e2531-aab0-42c0-97bd-bb8fc7c80bbe container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:05:09.038
  Aug  5 13:05:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6709" for this suite. @ 08/05/23 13:05:09.061
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/05/23 13:05:09.069
  Aug  5 13:05:09.069: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:05:09.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:09.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:09.089
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8580 @ 08/05/23 13:05:09.096
  STEP: changing the ExternalName service to type=ClusterIP @ 08/05/23 13:05:09.101
  STEP: creating replication controller externalname-service in namespace services-8580 @ 08/05/23 13:05:09.116
  I0805 13:05:09.126395      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8580, replica count: 2
  I0805 13:05:12.178648      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:05:12.178: INFO: Creating new exec pod
  Aug  5 13:05:15.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8580 exec execpodvjp5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:05:15.325: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:05:15.325: INFO: stdout: ""
  Aug  5 13:05:16.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8580 exec execpodvjp5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:05:16.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:05:16.447: INFO: stdout: ""
  Aug  5 13:05:17.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8580 exec execpodvjp5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:05:17.451: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:05:17.451: INFO: stdout: ""
  Aug  5 13:05:18.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8580 exec execpodvjp5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:05:18.455: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:05:18.455: INFO: stdout: "externalname-service-j4cbx"
  Aug  5 13:05:18.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8580 exec execpodvjp5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.61 80'
  Aug  5 13:05:18.580: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.61 80\nConnection to 10.152.183.61 80 port [tcp/http] succeeded!\n"
  Aug  5 13:05:18.580: INFO: stdout: "externalname-service-plw8q"
  Aug  5 13:05:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:05:18.585: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-8580" for this suite. @ 08/05/23 13:05:18.604
• [9.542 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/05/23 13:05:18.611
  Aug  5 13:05:18.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename limitrange @ 08/05/23 13:05:18.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:18.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:18.632
  STEP: Creating a LimitRange @ 08/05/23 13:05:18.635
  STEP: Setting up watch @ 08/05/23 13:05:18.635
  STEP: Submitting a LimitRange @ 08/05/23 13:05:18.739
  STEP: Verifying LimitRange creation was observed @ 08/05/23 13:05:18.745
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/05/23 13:05:18.745
  Aug  5 13:05:18.749: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug  5 13:05:18.749: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/05/23 13:05:18.749
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/05/23 13:05:18.754
  Aug  5 13:05:18.757: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug  5 13:05:18.757: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/05/23 13:05:18.758
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/05/23 13:05:18.763
  Aug  5 13:05:18.769: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug  5 13:05:18.769: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/05/23 13:05:18.769
  STEP: Failing to create a Pod with more than max resources @ 08/05/23 13:05:18.771
  STEP: Updating a LimitRange @ 08/05/23 13:05:18.773
  STEP: Verifying LimitRange updating is effective @ 08/05/23 13:05:18.777
  STEP: Creating a Pod with less than former min resources @ 08/05/23 13:05:20.782
  STEP: Failing to create a Pod with more than max resources @ 08/05/23 13:05:20.789
  STEP: Deleting a LimitRange @ 08/05/23 13:05:20.791
  STEP: Verifying the LimitRange was deleted @ 08/05/23 13:05:20.799
  Aug  5 13:05:25.803: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/05/23 13:05:25.803
  Aug  5 13:05:25.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1098" for this suite. @ 08/05/23 13:05:25.817
• [7.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/05/23 13:05:25.828
  Aug  5 13:05:25.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename watch @ 08/05/23 13:05:25.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:25.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:25.851
  STEP: getting a starting resourceVersion @ 08/05/23 13:05:25.853
  STEP: starting a background goroutine to produce watch events @ 08/05/23 13:05:25.857
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/05/23 13:05:25.857
  Aug  5 13:05:28.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2577" for this suite. @ 08/05/23 13:05:28.684
• [2.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/05/23 13:05:28.741
  Aug  5 13:05:28.741: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:05:28.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:28.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:28.764
  STEP: Creating a pod to test downward api env vars @ 08/05/23 13:05:28.766
  STEP: Saw pod success @ 08/05/23 13:05:32.789
  Aug  5 13:05:32.792: INFO: Trying to get logs from node ip-172-31-95-133 pod downward-api-19bd866e-9dd2-43c2-a73e-7bb0c130f9b1 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:05:32.801
  Aug  5 13:05:32.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8965" for this suite. @ 08/05/23 13:05:32.823
• [4.092 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/05/23 13:05:32.833
  Aug  5 13:05:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:05:32.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:32.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:32.853
  STEP: Creating a pod to test substitution in volume subpath @ 08/05/23 13:05:32.855
  STEP: Saw pod success @ 08/05/23 13:05:36.88
  Aug  5 13:05:36.883: INFO: Trying to get logs from node ip-172-31-95-133 pod var-expansion-6282004d-bcd8-4e9e-afd0-d6289376fe17 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:05:36.892
  Aug  5 13:05:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4978" for this suite. @ 08/05/23 13:05:36.915
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/05/23 13:05:36.925
  Aug  5 13:05:36.925: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 13:05:36.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:36.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:36.947
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/05/23 13:05:36.954
  Aug  5 13:05:36.963: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5365  da2139aa-207e-4298-bb9c-749d0c4c65ff 26017 0 2023-08-05 13:05:36 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-05 13:05:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcml6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcml6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/05/23 13:05:38.975
  Aug  5 13:05:38.975: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5365 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:05:38.975: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:05:38.975: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:05:38.976: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5365/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 08/05/23 13:05:39.081
  Aug  5 13:05:39.081: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5365 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:05:39.081: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:05:39.082: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:05:39.082: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-5365/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  5 13:05:39.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:05:39.182: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-5365" for this suite. @ 08/05/23 13:05:39.198
• [2.282 seconds]
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/05/23 13:05:39.207
  Aug  5 13:05:39.207: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:05:39.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:39.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:39.231
  STEP: Saw pod success @ 08/05/23 13:05:45.313
  Aug  5 13:05:45.317: INFO: Trying to get logs from node ip-172-31-95-133 pod client-envvars-87b51626-40e3-4744-8ca1-77496aa23744 container env3cont: <nil>
  STEP: delete the pod @ 08/05/23 13:05:45.325
  Aug  5 13:05:45.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4583" for this suite. @ 08/05/23 13:05:45.345
• [6.145 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/05/23 13:05:45.352
  Aug  5 13:05:45.352: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:05:45.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:45.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:45.374
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:05:45.377
  STEP: Saw pod success @ 08/05/23 13:05:49.402
  Aug  5 13:05:49.406: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-a1e58de2-cb06-4611-82b4-e135ad659d7e container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:05:49.414
  Aug  5 13:05:49.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1418" for this suite. @ 08/05/23 13:05:49.434
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/05/23 13:05:49.444
  Aug  5 13:05:49.444: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:05:49.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:49.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:49.467
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:05:49.471
  STEP: Saw pod success @ 08/05/23 13:05:53.502
  Aug  5 13:05:53.506: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-bb5de643-2db2-4ce1-8454-74c580dcff06 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:05:53.517
  Aug  5 13:05:53.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2846" for this suite. @ 08/05/23 13:05:53.541
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/05/23 13:05:53.551
  Aug  5 13:05:53.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 13:05:53.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:05:53.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:05:53.578
  STEP: Creating service test in namespace statefulset-9120 @ 08/05/23 13:05:53.581
  STEP: Creating a new StatefulSet @ 08/05/23 13:05:53.588
  Aug  5 13:05:53.601: INFO: Found 0 stateful pods, waiting for 3
  Aug  5 13:06:03.606: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:06:03.606: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:06:03.606: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/05/23 13:06:03.617
  Aug  5 13:06:03.637: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/05/23 13:06:03.637
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/05/23 13:06:13.653
  STEP: Performing a canary update @ 08/05/23 13:06:13.653
  Aug  5 13:06:13.674: INFO: Updating stateful set ss2
  Aug  5 13:06:13.683: INFO: Waiting for Pod statefulset-9120/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/05/23 13:06:23.694
  Aug  5 13:06:23.734: INFO: Found 1 stateful pods, waiting for 3
  Aug  5 13:06:33.739: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:06:33.739: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:06:33.739: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/05/23 13:06:33.747
  Aug  5 13:06:33.769: INFO: Updating stateful set ss2
  Aug  5 13:06:33.778: INFO: Waiting for Pod statefulset-9120/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug  5 13:06:43.808: INFO: Updating stateful set ss2
  Aug  5 13:06:43.817: INFO: Waiting for StatefulSet statefulset-9120/ss2 to complete update
  Aug  5 13:06:43.817: INFO: Waiting for Pod statefulset-9120/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug  5 13:06:53.826: INFO: Deleting all statefulset in ns statefulset-9120
  Aug  5 13:06:53.830: INFO: Scaling statefulset ss2 to 0
  Aug  5 13:07:03.850: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:07:03.854: INFO: Deleting statefulset ss2
  Aug  5 13:07:03.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9120" for this suite. @ 08/05/23 13:07:03.875
• [70.332 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/05/23 13:07:03.885
  Aug  5 13:07:03.885: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:07:03.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:07:03.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:07:03.908
  STEP: Creating configMap with name configmap-test-upd-fa8538b2-f3b5-44cb-b062-8f04f7b356de @ 08/05/23 13:07:03.915
  STEP: Creating the pod @ 08/05/23 13:07:03.92
  STEP: Updating configmap configmap-test-upd-fa8538b2-f3b5-44cb-b062-8f04f7b356de @ 08/05/23 13:07:05.95
  STEP: waiting to observe update in volume @ 08/05/23 13:07:05.955
  Aug  5 13:08:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5085" for this suite. @ 08/05/23 13:08:36.408
• [92.529 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/05/23 13:08:36.416
  Aug  5 13:08:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:08:36.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:08:36.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:08:36.436
  STEP: Creating secret with name secret-test-8d68f3cd-8aea-451e-9d0c-5c25bd136ba5 @ 08/05/23 13:08:36.46
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:08:36.468
  STEP: Saw pod success @ 08/05/23 13:08:40.493
  Aug  5 13:08:40.497: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-b6566a84-69fb-4deb-96d6-c441fb389ce0 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:08:40.505
  Aug  5 13:08:40.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3446" for this suite. @ 08/05/23 13:08:40.524
  STEP: Destroying namespace "secret-namespace-4138" for this suite. @ 08/05/23 13:08:40.53
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/05/23 13:08:40.539
  Aug  5 13:08:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:08:40.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:08:40.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:08:40.56
  STEP: Creating configMap with name projected-configmap-test-volume-3ac64e81-9cba-4786-897b-9ddc8a8df1a7 @ 08/05/23 13:08:40.562
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:08:40.567
  STEP: Saw pod success @ 08/05/23 13:08:44.59
  Aug  5 13:08:44.599: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-9636ed03-8f01-40fe-bc9c-881422159c98 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:08:44.606
  Aug  5 13:08:44.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-747" for this suite. @ 08/05/23 13:08:44.631
• [4.099 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/05/23 13:08:44.638
  Aug  5 13:08:44.638: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:08:44.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:08:44.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:08:44.663
  STEP: creating a ConfigMap @ 08/05/23 13:08:44.666
  STEP: fetching the ConfigMap @ 08/05/23 13:08:44.671
  STEP: patching the ConfigMap @ 08/05/23 13:08:44.674
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/05/23 13:08:44.68
  STEP: deleting the ConfigMap by collection with a label selector @ 08/05/23 13:08:44.684
  STEP: listing all ConfigMaps in test namespace @ 08/05/23 13:08:44.693
  Aug  5 13:08:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1534" for this suite. @ 08/05/23 13:08:44.7
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/05/23 13:08:44.708
  Aug  5 13:08:44.708: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:08:44.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:08:44.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:08:44.73
  STEP: Creating resourceQuota "e2e-rq-status-mh5kp" @ 08/05/23 13:08:44.735
  Aug  5 13:08:44.743: INFO: Resource quota "e2e-rq-status-mh5kp" reports spec: hard cpu limit of 500m
  Aug  5 13:08:44.743: INFO: Resource quota "e2e-rq-status-mh5kp" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-mh5kp" /status @ 08/05/23 13:08:44.743
  STEP: Confirm /status for "e2e-rq-status-mh5kp" resourceQuota via watch @ 08/05/23 13:08:44.751
  Aug  5 13:08:44.752: INFO: observed resourceQuota "e2e-rq-status-mh5kp" in namespace "resourcequota-6644" with hard status: v1.ResourceList(nil)
  Aug  5 13:08:44.753: INFO: Found resourceQuota "e2e-rq-status-mh5kp" in namespace "resourcequota-6644" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug  5 13:08:44.753: INFO: ResourceQuota "e2e-rq-status-mh5kp" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/05/23 13:08:44.756
  Aug  5 13:08:44.763: INFO: Resource quota "e2e-rq-status-mh5kp" reports spec: hard cpu limit of 1
  Aug  5 13:08:44.763: INFO: Resource quota "e2e-rq-status-mh5kp" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-mh5kp" /status @ 08/05/23 13:08:44.763
  STEP: Confirm /status for "e2e-rq-status-mh5kp" resourceQuota via watch @ 08/05/23 13:08:44.77
  Aug  5 13:08:44.771: INFO: observed resourceQuota "e2e-rq-status-mh5kp" in namespace "resourcequota-6644" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug  5 13:08:44.771: INFO: Found resourceQuota "e2e-rq-status-mh5kp" in namespace "resourcequota-6644" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug  5 13:08:44.771: INFO: ResourceQuota "e2e-rq-status-mh5kp" /status was patched
  STEP: Get "e2e-rq-status-mh5kp" /status @ 08/05/23 13:08:44.771
  Aug  5 13:08:44.775: INFO: Resourcequota "e2e-rq-status-mh5kp" reports status: hard cpu of 1
  Aug  5 13:08:44.775: INFO: Resourcequota "e2e-rq-status-mh5kp" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-mh5kp" /status before checking Spec is unchanged @ 08/05/23 13:08:44.778
  Aug  5 13:08:44.783: INFO: Resourcequota "e2e-rq-status-mh5kp" reports status: hard cpu of 2
  Aug  5 13:08:44.783: INFO: Resourcequota "e2e-rq-status-mh5kp" reports status: hard memory of 2Gi
  Aug  5 13:08:44.784: INFO: Found resourceQuota "e2e-rq-status-mh5kp" in namespace "resourcequota-6644" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Aug  5 13:12:29.793: INFO: ResourceQuota "e2e-rq-status-mh5kp" Spec was unchanged and /status reset
  Aug  5 13:12:29.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6644" for this suite. @ 08/05/23 13:12:29.797
• [225.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/05/23 13:12:29.805
  Aug  5 13:12:29.805: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:12:29.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:12:29.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:12:29.828
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:12:29.831
  STEP: Saw pod success @ 08/05/23 13:12:33.854
  Aug  5 13:12:33.858: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-6a6bbc04-7b13-4d1e-b531-7b89be40903c container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:12:33.878
  Aug  5 13:12:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5607" for this suite. @ 08/05/23 13:12:33.902
• [4.104 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/05/23 13:12:33.91
  Aug  5 13:12:33.910: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename runtimeclass @ 08/05/23 13:12:33.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:12:33.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:12:33.931
  STEP: Deleting RuntimeClass runtimeclass-8516-delete-me @ 08/05/23 13:12:33.938
  STEP: Waiting for the RuntimeClass to disappear @ 08/05/23 13:12:33.948
  Aug  5 13:12:33.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8516" for this suite. @ 08/05/23 13:12:33.964
• [0.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/05/23 13:12:33.974
  Aug  5 13:12:33.974: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/05/23 13:12:33.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:12:33.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:12:33.994
  STEP: creating @ 08/05/23 13:12:33.997
  STEP: getting @ 08/05/23 13:12:34.014
  STEP: listing @ 08/05/23 13:12:34.02
  STEP: deleting @ 08/05/23 13:12:34.023
  Aug  5 13:12:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6424" for this suite. @ 08/05/23 13:12:34.047
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/05/23 13:12:34.056
  Aug  5 13:12:34.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption @ 08/05/23 13:12:34.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:12:34.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:12:34.076
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:12:34.082
  STEP: Waiting for all pods to be running @ 08/05/23 13:12:36.114
  Aug  5 13:12:36.121: INFO: running pods: 0 < 3
  Aug  5 13:12:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2245" for this suite. @ 08/05/23 13:12:38.134
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/05/23 13:12:38.142
  Aug  5 13:12:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subpath @ 08/05/23 13:12:38.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:12:38.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:12:38.163
  STEP: Setting up data @ 08/05/23 13:12:38.165
  STEP: Creating pod pod-subpath-test-downwardapi-xbm5 @ 08/05/23 13:12:38.174
  STEP: Creating a pod to test atomic-volume-subpath @ 08/05/23 13:12:38.174
  STEP: Saw pod success @ 08/05/23 13:13:02.244
  Aug  5 13:13:02.247: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-subpath-test-downwardapi-xbm5 container test-container-subpath-downwardapi-xbm5: <nil>
  STEP: delete the pod @ 08/05/23 13:13:02.256
  STEP: Deleting pod pod-subpath-test-downwardapi-xbm5 @ 08/05/23 13:13:02.274
  Aug  5 13:13:02.274: INFO: Deleting pod "pod-subpath-test-downwardapi-xbm5" in namespace "subpath-7440"
  Aug  5 13:13:02.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7440" for this suite. @ 08/05/23 13:13:02.281
• [24.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/05/23 13:13:02.292
  Aug  5 13:13:02.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename svcaccounts @ 08/05/23 13:13:02.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:13:02.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:13:02.312
  Aug  5 13:13:02.332: INFO: created pod pod-service-account-defaultsa
  Aug  5 13:13:02.332: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug  5 13:13:02.339: INFO: created pod pod-service-account-mountsa
  Aug  5 13:13:02.340: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug  5 13:13:02.346: INFO: created pod pod-service-account-nomountsa
  Aug  5 13:13:02.346: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Aug  5 13:13:02.354: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug  5 13:13:02.354: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug  5 13:13:02.362: INFO: created pod pod-service-account-mountsa-mountspec
  Aug  5 13:13:02.362: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug  5 13:13:02.370: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug  5 13:13:02.370: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug  5 13:13:02.376: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug  5 13:13:02.376: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug  5 13:13:02.391: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug  5 13:13:02.391: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug  5 13:13:02.400: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug  5 13:13:02.400: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug  5 13:13:02.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-553" for this suite. @ 08/05/23 13:13:02.405
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/05/23 13:13:02.413
  Aug  5 13:13:02.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:13:02.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:13:02.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:13:02.437
  STEP: Creating configMap with name cm-test-opt-del-0a82d873-a361-4c6b-8383-bf1e9c971385 @ 08/05/23 13:13:02.443
  STEP: Creating configMap with name cm-test-opt-upd-639e356d-dfd0-4899-8385-fa04d5fd1c40 @ 08/05/23 13:13:02.448
  STEP: Creating the pod @ 08/05/23 13:13:02.452
  STEP: Deleting configmap cm-test-opt-del-0a82d873-a361-4c6b-8383-bf1e9c971385 @ 08/05/23 13:13:04.51
  STEP: Updating configmap cm-test-opt-upd-639e356d-dfd0-4899-8385-fa04d5fd1c40 @ 08/05/23 13:13:04.517
  STEP: Creating configMap with name cm-test-opt-create-abaffd36-e8e6-4328-94e5-8914493e7b79 @ 08/05/23 13:13:04.521
  STEP: waiting to observe update in volume @ 08/05/23 13:13:04.526
  Aug  5 13:14:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3883" for this suite. @ 08/05/23 13:14:22.921
• [80.515 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/05/23 13:14:22.929
  Aug  5 13:14:22.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:14:22.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:14:22.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:14:22.954
  STEP: creating a replication controller @ 08/05/23 13:14:22.957
  Aug  5 13:14:22.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 create -f -'
  Aug  5 13:14:23.205: INFO: stderr: ""
  Aug  5 13:14:23.205: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/05/23 13:14:23.205
  Aug  5 13:14:23.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 13:14:23.271: INFO: stderr: ""
  Aug  5 13:14:23.271: INFO: stdout: "update-demo-nautilus-ms5w9 update-demo-nautilus-nj8g2 "
  Aug  5 13:14:23.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods update-demo-nautilus-ms5w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 13:14:23.336: INFO: stderr: ""
  Aug  5 13:14:23.336: INFO: stdout: ""
  Aug  5 13:14:23.336: INFO: update-demo-nautilus-ms5w9 is created but not running
  Aug  5 13:14:28.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  5 13:14:28.402: INFO: stderr: ""
  Aug  5 13:14:28.402: INFO: stdout: "update-demo-nautilus-ms5w9 update-demo-nautilus-nj8g2 "
  Aug  5 13:14:28.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods update-demo-nautilus-ms5w9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 13:14:28.466: INFO: stderr: ""
  Aug  5 13:14:28.466: INFO: stdout: "true"
  Aug  5 13:14:28.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods update-demo-nautilus-ms5w9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 13:14:28.527: INFO: stderr: ""
  Aug  5 13:14:28.527: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 13:14:28.527: INFO: validating pod update-demo-nautilus-ms5w9
  Aug  5 13:14:28.533: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 13:14:28.534: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 13:14:28.534: INFO: update-demo-nautilus-ms5w9 is verified up and running
  Aug  5 13:14:28.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods update-demo-nautilus-nj8g2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  5 13:14:28.594: INFO: stderr: ""
  Aug  5 13:14:28.594: INFO: stdout: "true"
  Aug  5 13:14:28.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods update-demo-nautilus-nj8g2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  5 13:14:28.657: INFO: stderr: ""
  Aug  5 13:14:28.657: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  5 13:14:28.657: INFO: validating pod update-demo-nautilus-nj8g2
  Aug  5 13:14:28.662: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  5 13:14:28.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  5 13:14:28.662: INFO: update-demo-nautilus-nj8g2 is verified up and running
  STEP: using delete to clean up resources @ 08/05/23 13:14:28.663
  Aug  5 13:14:28.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 delete --grace-period=0 --force -f -'
  Aug  5 13:14:28.730: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:14:28.730: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug  5 13:14:28.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get rc,svc -l name=update-demo --no-headers'
  Aug  5 13:14:28.844: INFO: stderr: "No resources found in kubectl-2471 namespace.\n"
  Aug  5 13:14:28.844: INFO: stdout: ""
  Aug  5 13:14:28.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2471 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  5 13:14:28.939: INFO: stderr: ""
  Aug  5 13:14:28.939: INFO: stdout: ""
  Aug  5 13:14:28.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2471" for this suite. @ 08/05/23 13:14:28.944
• [6.024 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/05/23 13:14:28.953
  Aug  5 13:14:28.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:14:28.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:14:28.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:14:28.974
  Aug  5 13:14:30.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:14:31.003: INFO: Deleting pod "var-expansion-551a7286-5b38-4cea-80bb-dccfea96002a" in namespace "var-expansion-8919"
  Aug  5 13:14:31.011: INFO: Wait up to 5m0s for pod "var-expansion-551a7286-5b38-4cea-80bb-dccfea96002a" to be fully deleted
  STEP: Destroying namespace "var-expansion-8919" for this suite. @ 08/05/23 13:14:33.021
• [4.075 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/05/23 13:14:33.028
  Aug  5 13:14:33.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption @ 08/05/23 13:14:33.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:14:33.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:14:33.051
  STEP: Creating a kubernetes client @ 08/05/23 13:14:33.054
  Aug  5 13:14:33.054: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename disruption-2 @ 08/05/23 13:14:33.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:14:33.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:14:33.072
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:14:33.082
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:14:33.09
  STEP: Waiting for the pdb to be processed @ 08/05/23 13:14:35.105
  STEP: listing a collection of PDBs across all namespaces @ 08/05/23 13:14:37.116
  STEP: listing a collection of PDBs in namespace disruption-6148 @ 08/05/23 13:14:37.119
  STEP: deleting a collection of PDBs @ 08/05/23 13:14:37.123
  STEP: Waiting for the PDB collection to be deleted @ 08/05/23 13:14:37.136
  Aug  5 13:14:37.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:14:37.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-709" for this suite. @ 08/05/23 13:14:37.148
  STEP: Destroying namespace "disruption-6148" for this suite. @ 08/05/23 13:14:37.156
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/05/23 13:14:37.165
  Aug  5 13:14:37.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subpath @ 08/05/23 13:14:37.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:14:37.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:14:37.184
  STEP: Setting up data @ 08/05/23 13:14:37.186
  STEP: Creating pod pod-subpath-test-projected-s9fr @ 08/05/23 13:14:37.195
  STEP: Creating a pod to test atomic-volume-subpath @ 08/05/23 13:14:37.195
  STEP: Saw pod success @ 08/05/23 13:15:01.268
  Aug  5 13:15:01.272: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-subpath-test-projected-s9fr container test-container-subpath-projected-s9fr: <nil>
  STEP: delete the pod @ 08/05/23 13:15:01.29
  STEP: Deleting pod pod-subpath-test-projected-s9fr @ 08/05/23 13:15:01.307
  Aug  5 13:15:01.307: INFO: Deleting pod "pod-subpath-test-projected-s9fr" in namespace "subpath-2206"
  Aug  5 13:15:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2206" for this suite. @ 08/05/23 13:15:01.315
• [24.158 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/05/23 13:15:01.324
  Aug  5 13:15:01.324: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 13:15:01.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:01.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:01.354
  STEP: Updating Namespace "namespaces-4242" @ 08/05/23 13:15:01.356
  Aug  5 13:15:01.365: INFO: Namespace "namespaces-4242" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8b103839-1338-4d28-9fa2-89009d2c96d1", "kubernetes.io/metadata.name":"namespaces-4242", "namespaces-4242":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug  5 13:15:01.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4242" for this suite. @ 08/05/23 13:15:01.369
• [0.056 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/05/23 13:15:01.38
  Aug  5 13:15:01.380: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subpath @ 08/05/23 13:15:01.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:01.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:01.402
  STEP: Setting up data @ 08/05/23 13:15:01.404
  STEP: Creating pod pod-subpath-test-configmap-t6d6 @ 08/05/23 13:15:01.417
  STEP: Creating a pod to test atomic-volume-subpath @ 08/05/23 13:15:01.417
  STEP: Saw pod success @ 08/05/23 13:15:25.49
  Aug  5 13:15:25.493: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-subpath-test-configmap-t6d6 container test-container-subpath-configmap-t6d6: <nil>
  STEP: delete the pod @ 08/05/23 13:15:25.501
  STEP: Deleting pod pod-subpath-test-configmap-t6d6 @ 08/05/23 13:15:25.519
  Aug  5 13:15:25.519: INFO: Deleting pod "pod-subpath-test-configmap-t6d6" in namespace "subpath-8885"
  Aug  5 13:15:25.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8885" for this suite. @ 08/05/23 13:15:25.526
• [24.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/05/23 13:15:25.538
  Aug  5 13:15:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubelet-test @ 08/05/23 13:15:25.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:25.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:25.56
  Aug  5 13:15:27.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6850" for this suite. @ 08/05/23 13:15:27.61
• [2.080 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/05/23 13:15:27.618
  Aug  5 13:15:27.618: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-pred @ 08/05/23 13:15:27.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:27.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:27.641
  Aug  5 13:15:27.643: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  5 13:15:27.651: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 13:15:27.655: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-47 before test
  Aug  5 13:15:27.660: INFO: nginx-ingress-controller-kubernetes-worker-87f7r from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: coredns-5c7f76ccb8-8kfns from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container coredns ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: kube-state-metrics-5b95b4459c-p5wzf from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: metrics-server-v0.5.2-6cf8c8b69c-vnlhz from kube-system started at 2023-08-05 11:58:13 +0000 UTC (2 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container metrics-server ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: dashboard-metrics-scraper-6b8586b5c9-ghxkg from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: kubernetes-dashboard-6869f4cd5f-mr7dz from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2 from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:15:27.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 13:15:27.660: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-35-140 before test
  Aug  5 13:15:27.666: INFO: nginx-ingress-controller-kubernetes-worker-49tnn from ingress-nginx-kubernetes-worker started at 2023-08-05 12:02:34 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.666: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: sonobuoy from sonobuoy started at 2023-08-05 12:07:56 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.666: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: sonobuoy-e2e-job-6ea4f22076f544af from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:15:27.666: INFO: 	Container e2e ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-bxrtb from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:15:27.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 13:15:27.666: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-95-133 before test
  Aug  5 13:15:27.671: INFO: default-http-backend-kubernetes-worker-65fc475d49-wwntt from ingress-nginx-kubernetes-worker started at 2023-08-05 12:46:04 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.671: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug  5 13:15:27.671: INFO: nginx-ingress-controller-kubernetes-worker-n56xb from ingress-nginx-kubernetes-worker started at 2023-08-05 12:46:15 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.671: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:15:27.671: INFO: calico-kube-controllers-5b8cb49547-m7sct from kube-system started at 2023-08-05 12:46:04 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.671: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug  5 13:15:27.671: INFO: busybox-readonly-fsa8725724-a5c3-47d0-b397-9058e1434a86 from kubelet-test-6850 started at 2023-08-05 13:15:25 +0000 UTC (1 container statuses recorded)
  Aug  5 13:15:27.671: INFO: 	Container busybox-readonly-fsa8725724-a5c3-47d0-b397-9058e1434a86 ready: true, restart count 0
  Aug  5 13:15:27.671: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-kjhcj from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:15:27.671: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:15:27.671: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/05/23 13:15:27.671
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/05/23 13:15:29.693
  STEP: Trying to apply a random label on the found node. @ 08/05/23 13:15:29.71
  STEP: verifying the node has the label kubernetes.io/e2e-cba47e63-6306-46ab-b91b-2065cb12a3cf 42 @ 08/05/23 13:15:29.719
  STEP: Trying to relaunch the pod, now with labels. @ 08/05/23 13:15:29.723
  STEP: removing the label kubernetes.io/e2e-cba47e63-6306-46ab-b91b-2065cb12a3cf off the node ip-172-31-95-133 @ 08/05/23 13:15:31.745
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-cba47e63-6306-46ab-b91b-2065cb12a3cf @ 08/05/23 13:15:31.759
  Aug  5 13:15:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9568" for this suite. @ 08/05/23 13:15:31.77
• [4.161 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/05/23 13:15:31.779
  Aug  5 13:15:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 13:15:31.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:31.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:31.805
  STEP: Creating ReplicationController "e2e-rc-hk7mn" @ 08/05/23 13:15:31.813
  Aug  5 13:15:31.820: INFO: Get Replication Controller "e2e-rc-hk7mn" to confirm replicas
  Aug  5 13:15:32.825: INFO: Get Replication Controller "e2e-rc-hk7mn" to confirm replicas
  Aug  5 13:15:32.829: INFO: Found 1 replicas for "e2e-rc-hk7mn" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-hk7mn" @ 08/05/23 13:15:32.829
  STEP: Updating a scale subresource @ 08/05/23 13:15:32.833
  STEP: Verifying replicas where modified for replication controller "e2e-rc-hk7mn" @ 08/05/23 13:15:32.838
  Aug  5 13:15:32.838: INFO: Get Replication Controller "e2e-rc-hk7mn" to confirm replicas
  Aug  5 13:15:33.841: INFO: Get Replication Controller "e2e-rc-hk7mn" to confirm replicas
  Aug  5 13:15:33.845: INFO: Found 2 replicas for "e2e-rc-hk7mn" replication controller
  Aug  5 13:15:33.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6994" for this suite. @ 08/05/23 13:15:33.85
• [2.078 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/05/23 13:15:33.858
  Aug  5 13:15:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-runtime @ 08/05/23 13:15:33.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:33.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:33.88
  STEP: create the container @ 08/05/23 13:15:33.882
  W0805 13:15:33.895104      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/05/23 13:15:33.895
  STEP: get the container status @ 08/05/23 13:15:36.913
  STEP: the container should be terminated @ 08/05/23 13:15:36.916
  STEP: the termination message should be set @ 08/05/23 13:15:36.916
  Aug  5 13:15:36.916: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/05/23 13:15:36.916
  Aug  5 13:15:36.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9349" for this suite. @ 08/05/23 13:15:36.938
• [3.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/05/23 13:15:36.947
  Aug  5 13:15:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:15:36.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:36.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:36.966
  STEP: Creating projection with secret that has name projected-secret-test-3ce03b5a-8c61-43fb-878f-fb5a18bab453 @ 08/05/23 13:15:36.968
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:15:36.974
  STEP: Saw pod success @ 08/05/23 13:15:40.998
  Aug  5 13:15:41.001: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-projected-secrets-c494e625-ed4b-4068-b9fd-462d2bb3ded3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:15:41.008
  Aug  5 13:15:41.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4814" for this suite. @ 08/05/23 13:15:41.031
• [4.092 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/05/23 13:15:41.039
  Aug  5 13:15:41.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename events @ 08/05/23 13:15:41.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:41.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:41.06
  STEP: Create set of events @ 08/05/23 13:15:41.062
  STEP: get a list of Events with a label in the current namespace @ 08/05/23 13:15:41.08
  STEP: delete a list of events @ 08/05/23 13:15:41.083
  Aug  5 13:15:41.083: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/05/23 13:15:41.108
  Aug  5 13:15:41.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4087" for this suite. @ 08/05/23 13:15:41.116
• [0.084 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/05/23 13:15:41.123
  Aug  5 13:15:41.123: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:15:41.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:41.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:41.146
  STEP: Setting up server cert @ 08/05/23 13:15:41.169
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:15:41.785
  STEP: Deploying the webhook pod @ 08/05/23 13:15:41.794
  STEP: Wait for the deployment to be ready @ 08/05/23 13:15:41.808
  Aug  5 13:15:41.815: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 13:15:43.827
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:15:43.839
  Aug  5 13:15:44.839: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  5 13:15:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1691-crds.webhook.example.com via the AdmissionRegistration API @ 08/05/23 13:15:45.359
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/05/23 13:15:45.376
  Aug  5 13:15:47.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8184" for this suite. @ 08/05/23 13:15:48.001
  STEP: Destroying namespace "webhook-markers-7509" for this suite. @ 08/05/23 13:15:48.012
• [6.896 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/05/23 13:15:48.019
  Aug  5 13:15:48.019: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-watch @ 08/05/23 13:15:48.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:15:48.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:15:48.041
  Aug  5 13:15:48.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Creating first CR  @ 08/05/23 13:15:50.599
  Aug  5 13:15:50.604: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:15:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:15:50Z]] name:name1 resourceVersion:28826 uid:3c0aafa1-8510-493a-afe7-e8db79bf9310] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 08/05/23 13:16:00.608
  Aug  5 13:16:00.614: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:16:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:16:00Z]] name:name2 resourceVersion:28867 uid:2646a1a0-478b-41c7-b192-75f9d5a83152] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 08/05/23 13:16:10.619
  Aug  5 13:16:10.627: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:15:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:16:10Z]] name:name1 resourceVersion:28898 uid:3c0aafa1-8510-493a-afe7-e8db79bf9310] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 08/05/23 13:16:20.63
  Aug  5 13:16:20.636: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:16:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:16:20Z]] name:name2 resourceVersion:28920 uid:2646a1a0-478b-41c7-b192-75f9d5a83152] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 08/05/23 13:16:30.637
  Aug  5 13:16:30.647: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:15:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:16:10Z]] name:name1 resourceVersion:28939 uid:3c0aafa1-8510-493a-afe7-e8db79bf9310] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 08/05/23 13:16:40.648
  Aug  5 13:16:40.656: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-05T13:16:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-05T13:16:20Z]] name:name2 resourceVersion:28959 uid:2646a1a0-478b-41c7-b192-75f9d5a83152] num:map[num1:9223372036854775807 num2:1000000]]}
  Aug  5 13:16:51.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-5101" for this suite. @ 08/05/23 13:16:51.181
• [63.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/05/23 13:16:51.192
  Aug  5 13:16:51.192: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:16:51.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:16:51.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:16:51.215
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:16:51.218
  STEP: Saw pod success @ 08/05/23 13:16:55.241
  Aug  5 13:16:55.245: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-b01319b0-c863-4528-86ae-a58a6c036060 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:16:55.253
  Aug  5 13:16:55.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5809" for this suite. @ 08/05/23 13:16:55.278
• [4.094 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/05/23 13:16:55.287
  Aug  5 13:16:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:16:55.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:16:55.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:16:55.307
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/05/23 13:16:55.312
  Aug  5 13:16:55.312: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/05/23 13:17:00.916
  Aug  5 13:17:00.916: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:17:02.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:17:07.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4185" for this suite. @ 08/05/23 13:17:07.721
• [12.445 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/05/23 13:17:07.732
  Aug  5 13:17:07.732: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:17:07.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:17:07.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:17:07.76
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/05/23 13:17:07.764
  STEP: Saw pod success @ 08/05/23 13:17:11.804
  Aug  5 13:17:11.807: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-fe68f9ea-c5d7-4c8b-a334-86e009603333 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:17:11.815
  Aug  5 13:17:11.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3961" for this suite. @ 08/05/23 13:17:11.836
• [4.111 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/05/23 13:17:11.844
  Aug  5 13:17:11.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/05/23 13:17:11.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:17:11.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:17:11.864
  STEP: mirroring a new custom Endpoint @ 08/05/23 13:17:11.877
  Aug  5 13:17:11.887: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 08/05/23 13:17:13.897
  Aug  5 13:17:13.906: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 08/05/23 13:17:15.911
  Aug  5 13:17:15.922: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Aug  5 13:17:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-6017" for this suite. @ 08/05/23 13:17:17.931
• [6.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/05/23 13:17:17.94
  Aug  5 13:17:17.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename containers @ 08/05/23 13:17:17.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:17:17.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:17:17.961
  STEP: Creating a pod to test override arguments @ 08/05/23 13:17:17.963
  STEP: Saw pod success @ 08/05/23 13:17:21.988
  Aug  5 13:17:21.991: INFO: Trying to get logs from node ip-172-31-95-133 pod client-containers-94f2dde9-a065-4ce4-b2c4-5ad6297754ab container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:17:21.999
  Aug  5 13:17:22.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-745" for this suite. @ 08/05/23 13:17:22.022
• [4.089 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/05/23 13:17:22.029
  Aug  5 13:17:22.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename init-container @ 08/05/23 13:17:22.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:17:22.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:17:22.052
  STEP: creating the pod @ 08/05/23 13:17:22.054
  Aug  5 13:17:22.054: INFO: PodSpec: initContainers in spec.initContainers
  Aug  5 13:18:01.788: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e3219370-de75-4150-9861-e402df111a34", GenerateName:"", Namespace:"init-container-403", SelfLink:"", UID:"27dedc36-aa13-4b65-883e-9c3c1f7f7dd7", ResourceVersion:"29319", Generation:0, CreationTimestamp:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"54700732"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001312d80), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 5, 13, 18, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001312df8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nvq6n", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004163840), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nvq6n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nvq6n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nvq6n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004806798), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-95-133", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003e3f10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004806a30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004806a60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004806a68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004806a6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012932d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.95.133", PodIP:"192.168.52.181", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.52.181"}}, StartTime:time.Date(2023, time.August, 5, 13, 17, 22, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00025e460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00025e4d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://54f25fba5a74bc29ce7122cdb0cea4d4c3912e4a30275b5f595c9b6b7a9c6ac7", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041638c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041638a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004806ae4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug  5 13:18:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-403" for this suite. @ 08/05/23 13:18:01.794
• [39.773 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/05/23 13:18:01.802
  Aug  5 13:18:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/05/23 13:18:01.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:01.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:01.824
  STEP: creating a target pod @ 08/05/23 13:18:01.827
  STEP: adding an ephemeral container @ 08/05/23 13:18:03.85
  STEP: checking pod container endpoints @ 08/05/23 13:18:05.869
  Aug  5 13:18:05.869: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-974 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:18:05.869: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:18:05.870: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:18:05.870: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-974/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Aug  5 13:18:05.945: INFO: Exec stderr: ""
  Aug  5 13:18:05.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-974" for this suite. @ 08/05/23 13:18:05.965
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/05/23 13:18:05.976
  Aug  5 13:18:05.976: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:18:05.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:05.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:06.001
  Aug  5 13:18:06.003: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: creating the pod @ 08/05/23 13:18:06.004
  STEP: submitting the pod to kubernetes @ 08/05/23 13:18:06.004
  Aug  5 13:18:08.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7150" for this suite. @ 08/05/23 13:18:08.048
• [2.079 seconds]
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/05/23 13:18:08.055
  Aug  5 13:18:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename server-version @ 08/05/23 13:18:08.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:08.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:08.076
  STEP: Request ServerVersion @ 08/05/23 13:18:08.078
  STEP: Confirm major version @ 08/05/23 13:18:08.079
  Aug  5 13:18:08.080: INFO: Major version: 1
  STEP: Confirm minor version @ 08/05/23 13:18:08.08
  Aug  5 13:18:08.080: INFO: cleanMinorVersion: 27
  Aug  5 13:18:08.080: INFO: Minor version: 27
  Aug  5 13:18:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-350" for this suite. @ 08/05/23 13:18:08.084
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/05/23 13:18:08.092
  Aug  5 13:18:08.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:18:08.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:08.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:08.112
  STEP: Setting up server cert @ 08/05/23 13:18:08.137
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:18:08.586
  STEP: Deploying the webhook pod @ 08/05/23 13:18:08.595
  STEP: Wait for the deployment to be ready @ 08/05/23 13:18:08.608
  Aug  5 13:18:08.615: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 13:18:10.627
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:18:10.639
  Aug  5 13:18:11.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/05/23 13:18:11.643
  STEP: create a configmap that should be updated by the webhook @ 08/05/23 13:18:11.659
  Aug  5 13:18:11.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9672" for this suite. @ 08/05/23 13:18:11.725
  STEP: Destroying namespace "webhook-markers-1719" for this suite. @ 08/05/23 13:18:11.733
• [3.648 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/05/23 13:18:11.74
  Aug  5 13:18:11.740: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:18:11.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:11.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:11.761
  STEP: Creating projection with secret that has name projected-secret-test-67de272e-b006-4679-a3ce-53f6be36a588 @ 08/05/23 13:18:11.764
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:18:11.769
  STEP: Saw pod success @ 08/05/23 13:18:15.793
  Aug  5 13:18:15.797: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-secrets-9efe1f08-9a0a-4067-b6ad-d7585dcdd0ac container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:18:15.805
  Aug  5 13:18:15.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4013" for this suite. @ 08/05/23 13:18:15.827
• [4.094 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/05/23 13:18:15.834
  Aug  5 13:18:15.835: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename watch @ 08/05/23 13:18:15.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:15.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:15.856
  STEP: creating a new configmap @ 08/05/23 13:18:15.858
  STEP: modifying the configmap once @ 08/05/23 13:18:15.865
  STEP: modifying the configmap a second time @ 08/05/23 13:18:15.872
  STEP: deleting the configmap @ 08/05/23 13:18:15.881
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/05/23 13:18:15.887
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/05/23 13:18:15.888
  Aug  5 13:18:15.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-265  a820eb81-89da-42e4-9600-ecbe49b71059 29533 0 2023-08-05 13:18:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-05 13:18:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 13:18:15.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-265  a820eb81-89da-42e4-9600-ecbe49b71059 29534 0 2023-08-05 13:18:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-05 13:18:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  5 13:18:15.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-265" for this suite. @ 08/05/23 13:18:15.893
• [0.066 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/05/23 13:18:15.901
  Aug  5 13:18:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:18:15.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:15.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:15.923
  STEP: Creating a pod to test env composition @ 08/05/23 13:18:15.925
  STEP: Saw pod success @ 08/05/23 13:18:19.947
  Aug  5 13:18:19.950: INFO: Trying to get logs from node ip-172-31-95-133 pod var-expansion-3da2bf55-c3dc-406a-8066-4c8dddf21529 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:18:19.959
  Aug  5 13:18:19.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4211" for this suite. @ 08/05/23 13:18:19.982
• [4.087 seconds]
------------------------------
SS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/05/23 13:18:19.989
  Aug  5 13:18:19.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename ingressclass @ 08/05/23 13:18:19.989
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:20.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:20.011
  STEP: getting /apis @ 08/05/23 13:18:20.014
  STEP: getting /apis/networking.k8s.io @ 08/05/23 13:18:20.017
  STEP: getting /apis/networking.k8s.iov1 @ 08/05/23 13:18:20.018
  STEP: creating @ 08/05/23 13:18:20.019
  STEP: getting @ 08/05/23 13:18:20.034
  STEP: listing @ 08/05/23 13:18:20.037
  STEP: watching @ 08/05/23 13:18:20.04
  Aug  5 13:18:20.040: INFO: starting watch
  STEP: patching @ 08/05/23 13:18:20.041
  STEP: updating @ 08/05/23 13:18:20.046
  Aug  5 13:18:20.051: INFO: waiting for watch events with expected annotations
  Aug  5 13:18:20.051: INFO: saw patched and updated annotations
  STEP: deleting @ 08/05/23 13:18:20.051
  STEP: deleting a collection @ 08/05/23 13:18:20.064
  Aug  5 13:18:20.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-2875" for this suite. @ 08/05/23 13:18:20.085
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/05/23 13:18:20.093
  Aug  5 13:18:20.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 13:18:20.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:20.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:20.113
  STEP: create the deployment @ 08/05/23 13:18:20.115
  W0805 13:18:20.122263      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/05/23 13:18:20.122
  STEP: delete the deployment @ 08/05/23 13:18:20.634
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/05/23 13:18:20.642
  STEP: Gathering metrics @ 08/05/23 13:18:21.163
  W0805 13:18:21.167539      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 13:18:21.167: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 13:18:21.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4781" for this suite. @ 08/05/23 13:18:21.172
• [1.087 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/05/23 13:18:21.18
  Aug  5 13:18:21.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context-test @ 08/05/23 13:18:21.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:21.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:21.199
  Aug  5 13:18:25.233: INFO: Got logs for pod "busybox-privileged-false-61656d73-665f-42f5-8cf7-c9be7cfefc92": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug  5 13:18:25.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9961" for this suite. @ 08/05/23 13:18:25.238
• [4.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/05/23 13:18:25.247
  Aug  5 13:18:25.247: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename events @ 08/05/23 13:18:25.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:25.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:25.266
  STEP: creating a test event @ 08/05/23 13:18:25.268
  STEP: listing events in all namespaces @ 08/05/23 13:18:25.276
  STEP: listing events in test namespace @ 08/05/23 13:18:25.28
  STEP: listing events with field selection filtering on source @ 08/05/23 13:18:25.284
  STEP: listing events with field selection filtering on reportingController @ 08/05/23 13:18:25.288
  STEP: getting the test event @ 08/05/23 13:18:25.291
  STEP: patching the test event @ 08/05/23 13:18:25.296
  STEP: getting the test event @ 08/05/23 13:18:25.306
  STEP: updating the test event @ 08/05/23 13:18:25.31
  STEP: getting the test event @ 08/05/23 13:18:25.322
  STEP: deleting the test event @ 08/05/23 13:18:25.326
  STEP: listing events in all namespaces @ 08/05/23 13:18:25.334
  STEP: listing events in test namespace @ 08/05/23 13:18:25.338
  Aug  5 13:18:25.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1654" for this suite. @ 08/05/23 13:18:25.346
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/05/23 13:18:25.354
  Aug  5 13:18:25.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-webhook @ 08/05/23 13:18:25.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:25.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:25.373
  STEP: Setting up server cert @ 08/05/23 13:18:25.376
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/05/23 13:18:25.664
  STEP: Deploying the custom resource conversion webhook pod @ 08/05/23 13:18:25.669
  STEP: Wait for the deployment to be ready @ 08/05/23 13:18:25.682
  Aug  5 13:18:25.691: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/05/23 13:18:27.703
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:18:27.714
  Aug  5 13:18:28.714: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug  5 13:18:28.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Creating a v1 custom resource @ 08/05/23 13:18:31.312
  STEP: Create a v2 custom resource @ 08/05/23 13:18:31.331
  STEP: List CRs in v1 @ 08/05/23 13:18:31.342
  STEP: List CRs in v2 @ 08/05/23 13:18:31.38
  Aug  5 13:18:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6058" for this suite. @ 08/05/23 13:18:31.948
• [6.607 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/05/23 13:18:31.965
  Aug  5 13:18:31.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 13:18:31.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:31.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:31.991
  STEP: creating a ReplicationController @ 08/05/23 13:18:31.997
  STEP: waiting for RC to be added @ 08/05/23 13:18:32.003
  STEP: waiting for available Replicas @ 08/05/23 13:18:32.003
  STEP: patching ReplicationController @ 08/05/23 13:18:32.884
  STEP: waiting for RC to be modified @ 08/05/23 13:18:32.893
  STEP: patching ReplicationController status @ 08/05/23 13:18:32.893
  STEP: waiting for RC to be modified @ 08/05/23 13:18:32.899
  STEP: waiting for available Replicas @ 08/05/23 13:18:32.899
  STEP: fetching ReplicationController status @ 08/05/23 13:18:32.904
  STEP: patching ReplicationController scale @ 08/05/23 13:18:32.908
  STEP: waiting for RC to be modified @ 08/05/23 13:18:32.915
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/05/23 13:18:32.915
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/05/23 13:18:34.488
  STEP: updating ReplicationController status @ 08/05/23 13:18:34.492
  STEP: waiting for RC to be modified @ 08/05/23 13:18:34.498
  STEP: listing all ReplicationControllers @ 08/05/23 13:18:34.498
  STEP: checking that ReplicationController has expected values @ 08/05/23 13:18:34.504
  STEP: deleting ReplicationControllers by collection @ 08/05/23 13:18:34.504
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/05/23 13:18:34.515
  Aug  5 13:18:34.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0805 13:18:34.543625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-6059" for this suite. @ 08/05/23 13:18:34.549
• [2.593 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/05/23 13:18:34.559
  Aug  5 13:18:34.559: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:18:34.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:34.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:34.583
  STEP: Creating secret with name secret-test-ae2a80ae-f637-4236-8321-49c903275f87 @ 08/05/23 13:18:34.585
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:18:34.591
  E0805 13:18:35.543742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:36.544002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:37.544085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:38.544177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:18:38.613
  Aug  5 13:18:38.617: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-064e8dc8-19a7-42a4-b21d-9fd6258bfa46 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:18:38.625
  Aug  5 13:18:38.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7051" for this suite. @ 08/05/23 13:18:38.647
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/05/23 13:18:38.656
  Aug  5 13:18:38.656: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sysctl @ 08/05/23 13:18:38.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:38.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:38.677
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/05/23 13:18:38.68
  STEP: Watching for error events or started pod @ 08/05/23 13:18:38.693
  E0805 13:18:39.545066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:40.545153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 08/05/23 13:18:40.698
  E0805 13:18:41.545241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:42.545450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 08/05/23 13:18:42.71
  STEP: Getting logs from the pod @ 08/05/23 13:18:42.71
  STEP: Checking that the sysctl is actually updated @ 08/05/23 13:18:42.718
  Aug  5 13:18:42.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9881" for this suite. @ 08/05/23 13:18:42.722
• [4.073 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/05/23 13:18:42.73
  Aug  5 13:18:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sysctl @ 08/05/23 13:18:42.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:42.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:42.752
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/05/23 13:18:42.754
  Aug  5 13:18:42.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7997" for this suite. @ 08/05/23 13:18:42.763
• [0.039 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/05/23 13:18:42.77
  Aug  5 13:18:42.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:18:42.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:18:42.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:18:42.79
  STEP: creating the pod with failed condition @ 08/05/23 13:18:42.792
  E0805 13:18:43.545544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:44.545701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:45.546703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:46.546776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:47.547068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:48.547252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:49.547365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:50.547404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:51.547498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:52.547703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:53.547896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:54.547986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:55.548159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:56.548891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:57.548967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:58.549199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:18:59.549282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:00.549381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:01.550081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:02.550718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:03.550793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:04.551093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:05.551213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:06.551291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:07.552302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:08.552440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:09.552527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:10.552710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:11.553049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:12.553139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:13.553736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:14.554672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:15.554766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:16.554863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:17.554948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:18.555413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:19.555504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:20.555656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:21.555763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:22.555860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:23.555962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:24.556169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:25.556501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:26.556903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:27.557392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:28.557483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:29.557593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:30.557627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:31.558453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:32.558770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:33.559769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:34.560798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:35.560882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:36.561693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:37.562255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:38.562703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:39.563488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:40.563579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:41.564235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:42.564319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:43.564740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:44.564849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:45.564929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:46.565001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:47.565111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:48.565303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:49.565636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:50.565745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:51.565927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:52.566005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:53.566560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:54.566747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:55.566839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:56.567025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:57.567348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:58.567443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:19:59.568299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:00.568502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:01.569315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:02.569494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:03.569638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:04.570703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:05.570788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:06.571146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:07.571253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:08.571468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:09.571553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:10.571849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:11.571942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:12.571955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:13.572051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:14.572267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:15.572356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:16.572672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:17.573388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:18.573609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:19.574267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:20.574338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:21.574445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:22.574578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:23.575417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:24.575594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:25.576007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:26.576083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:27.576183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:28.576279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:29.576372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:30.576570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:31.576664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:32.576916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:33.576991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:34.577055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:35.577416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:36.577615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:37.578446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:38.578692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:39.579117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:40.579281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:41.579699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:42.579858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 08/05/23 13:20:42.803
  Aug  5 13:20:43.316: INFO: Successfully updated pod "var-expansion-d2d82226-0e76-4336-ad50-f21e491b79ef"
  STEP: waiting for pod running @ 08/05/23 13:20:43.316
  E0805 13:20:43.580342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:44.580535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/05/23 13:20:45.324
  Aug  5 13:20:45.324: INFO: Deleting pod "var-expansion-d2d82226-0e76-4336-ad50-f21e491b79ef" in namespace "var-expansion-1529"
  Aug  5 13:20:45.333: INFO: Wait up to 5m0s for pod "var-expansion-d2d82226-0e76-4336-ad50-f21e491b79ef" to be fully deleted
  E0805 13:20:45.581380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:46.581619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:47.582028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:48.582121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:49.583060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:50.583768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:51.584227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:52.584784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:53.585475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:54.585623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:55.586705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:56.587688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:57.588686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:58.588865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:20:59.588957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:00.589399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:01.590136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:02.590679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:03.591166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:04.591353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:05.591448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:06.591521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:07.591625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:08.591678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:09.592292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:10.592372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:11.592702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:12.592917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:13.593006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:14.593064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:15.593927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:16.594183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:17.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1529" for this suite. @ 08/05/23 13:21:17.416
• [154.654 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/05/23 13:21:17.426
  Aug  5 13:21:17.426: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename tables @ 08/05/23 13:21:17.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:17.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:17.449
  Aug  5 13:21:17.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-73" for this suite. @ 08/05/23 13:21:17.458
• [0.041 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/05/23 13:21:17.467
  Aug  5 13:21:17.468: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:21:17.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:17.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:17.488
  STEP: creating the pod @ 08/05/23 13:21:17.49
  STEP: submitting the pod to kubernetes @ 08/05/23 13:21:17.49
  W0805 13:21:17.501428      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0805 13:21:17.595155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:18.595334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/05/23 13:21:19.513
  STEP: updating the pod @ 08/05/23 13:21:19.516
  E0805 13:21:19.596133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:20.029: INFO: Successfully updated pod "pod-update-activedeadlineseconds-682e300f-3ee4-41c0-b069-ede878ed2ea8"
  E0805 13:21:20.596305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:21.596746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:22.597745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:23.597838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:24.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9186" for this suite. @ 08/05/23 13:21:24.049
• [6.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 08/05/23 13:21:24.063
  Aug  5 13:21:24.063: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 13:21:24.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:24.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:24.087
  STEP: Creating simple DaemonSet "daemon-set" @ 08/05/23 13:21:24.119
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/05/23 13:21:24.126
  Aug  5 13:21:24.133: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:24.133: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:24.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:21:24.137: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:21:24.598628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:25.143: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:25.143: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:25.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:21:25.147: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:21:25.599344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:26.142: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:26.142: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:26.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 13:21:26.147: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/05/23 13:21:26.151
  Aug  5 13:21:26.170: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:26.170: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:26.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 13:21:26.175: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:21:26.599944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:27.181: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:27.181: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:27.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 13:21:27.186: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:21:27.600329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:28.181: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:28.182: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:28.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  5 13:21:28.186: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:21:28.600900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:29.181: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:29.181: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:21:29.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 13:21:29.186: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 13:21:29.191
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7831, will wait for the garbage collector to delete the pods @ 08/05/23 13:21:29.191
  Aug  5 13:21:29.260: INFO: Deleting DaemonSet.extensions daemon-set took: 13.912401ms
  Aug  5 13:21:29.361: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.859997ms
  E0805 13:21:29.601881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:30.602348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:30.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:21:30.766: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 13:21:30.770: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30595"},"items":null}

  Aug  5 13:21:30.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30595"},"items":null}

  Aug  5 13:21:30.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7831" for this suite. @ 08/05/23 13:21:30.796
• [6.741 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/05/23 13:21:30.807
  Aug  5 13:21:30.807: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename init-container @ 08/05/23 13:21:30.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:30.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:30.831
  STEP: creating the pod @ 08/05/23 13:21:30.836
  Aug  5 13:21:30.836: INFO: PodSpec: initContainers in spec.initContainers
  E0805 13:21:31.602671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:32.603619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:33.603711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:34.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1130" for this suite. @ 08/05/23 13:21:34.573
• [3.772 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/05/23 13:21:34.58
  Aug  5 13:21:34.580: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:21:34.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:34.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:34.601
  E0805 13:21:34.603968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating the pod @ 08/05/23 13:21:34.605
  E0805 13:21:35.603885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:36.604785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:37.177: INFO: Successfully updated pod "annotationupdate9489bf25-8380-491e-8947-19af097b591e"
  E0805 13:21:37.604921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:38.605180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:21:39.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4057" for this suite. @ 08/05/23 13:21:39.207
• [4.634 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/05/23 13:21:39.215
  Aug  5 13:21:39.215: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 13:21:39.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:21:39.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:21:39.237
  STEP: Creating a job @ 08/05/23 13:21:39.239
  STEP: Ensuring active pods == parallelism @ 08/05/23 13:21:39.247
  E0805 13:21:39.605710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:40.606708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 08/05/23 13:21:41.254
  STEP: deleting Job.batch foo in namespace job-3911, will wait for the garbage collector to delete the pods @ 08/05/23 13:21:41.254
  Aug  5 13:21:41.319: INFO: Deleting Job.batch foo took: 8.667293ms
  Aug  5 13:21:41.419: INFO: Terminating Job.batch foo pods took: 100.526204ms
  E0805 13:21:41.607129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:42.607169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:43.607235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:44.607412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:45.607508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:46.607797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:47.607881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:48.607978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:49.608069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:50.608193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:51.608672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:52.608757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:53.608862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:54.608919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:55.609158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:56.609560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:57.609625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:58.609739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:21:59.609863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:00.610921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:01.611395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:02.612434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:03.612490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:04.612723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:05.612973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:06.613369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:07.613435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:08.613623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:09.613837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:10.613920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:11.614894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:12.615387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 08/05/23 13:22:13.02
  Aug  5 13:22:13.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3911" for this suite. @ 08/05/23 13:22:13.028
• [33.821 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/05/23 13:22:13.038
  Aug  5 13:22:13.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 13:22:13.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:13.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:13.062
  STEP: Read namespace status @ 08/05/23 13:22:13.065
  Aug  5 13:22:13.069: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/05/23 13:22:13.069
  Aug  5 13:22:13.074: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/05/23 13:22:13.074
  Aug  5 13:22:13.084: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug  5 13:22:13.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7253" for this suite. @ 08/05/23 13:22:13.088
• [0.059 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/05/23 13:22:13.097
  Aug  5 13:22:13.097: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/05/23 13:22:13.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:13.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:13.118
  Aug  5 13:22:13.120: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:22:13.615464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:13.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-224" for this suite. @ 08/05/23 13:22:13.674
• [0.585 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/05/23 13:22:13.683
  Aug  5 13:22:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl-logs @ 08/05/23 13:22:13.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:13.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:13.704
  STEP: creating an pod @ 08/05/23 13:22:13.706
  Aug  5 13:22:13.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug  5 13:22:13.774: INFO: stderr: ""
  Aug  5 13:22:13.774: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/05/23 13:22:13.774
  Aug  5 13:22:13.774: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0805 13:22:14.615574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:15.615786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:15.784: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/05/23 13:22:15.784
  Aug  5 13:22:15.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator'
  Aug  5 13:22:15.851: INFO: stderr: ""
  Aug  5 13:22:15.851: INFO: stdout: "I0805 13:22:14.479720       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/pjkc 442\nI0805 13:22:14.679810       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/r4x2 435\nI0805 13:22:14.880372       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/n2d 596\nI0805 13:22:15.080662       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/2kvq 507\nI0805 13:22:15.279805       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/nfb5 473\nI0805 13:22:15.480095       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kvs 308\nI0805 13:22:15.680372       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/t24r 264\n"
  STEP: limiting log lines @ 08/05/23 13:22:15.851
  Aug  5 13:22:15.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator --tail=1'
  Aug  5 13:22:15.930: INFO: stderr: ""
  Aug  5 13:22:15.930: INFO: stdout: "I0805 13:22:15.880660       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9lb 547\n"
  Aug  5 13:22:15.930: INFO: got output "I0805 13:22:15.880660       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9lb 547\n"
  STEP: limiting log bytes @ 08/05/23 13:22:15.93
  Aug  5 13:22:15.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator --limit-bytes=1'
  Aug  5 13:22:15.997: INFO: stderr: ""
  Aug  5 13:22:15.997: INFO: stdout: "I"
  Aug  5 13:22:15.997: INFO: got output "I"
  STEP: exposing timestamps @ 08/05/23 13:22:15.997
  Aug  5 13:22:15.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug  5 13:22:16.068: INFO: stderr: ""
  Aug  5 13:22:16.068: INFO: stdout: "2023-08-05T13:22:15.880751756Z I0805 13:22:15.880660       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9lb 547\n"
  Aug  5 13:22:16.068: INFO: got output "2023-08-05T13:22:15.880751756Z I0805 13:22:15.880660       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9lb 547\n"
  STEP: restricting to a time range @ 08/05/23 13:22:16.068
  E0805 13:22:16.616775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:17.616971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:18.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator --since=1s'
  E0805 13:22:18.617408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:18.640: INFO: stderr: ""
  Aug  5 13:22:18.640: INFO: stdout: "I0805 13:22:17.680317       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/6krf 216\nI0805 13:22:17.880578       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/9tbh 455\nI0805 13:22:18.079807       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qlx 426\nI0805 13:22:18.280096       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/rfh 294\nI0805 13:22:18.480374       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6qc 226\n"
  Aug  5 13:22:18.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 logs logs-generator logs-generator --since=24h'
  Aug  5 13:22:18.711: INFO: stderr: ""
  Aug  5 13:22:18.711: INFO: stdout: "I0805 13:22:14.479720       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/pjkc 442\nI0805 13:22:14.679810       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/r4x2 435\nI0805 13:22:14.880372       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/n2d 596\nI0805 13:22:15.080662       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/2kvq 507\nI0805 13:22:15.279805       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/nfb5 473\nI0805 13:22:15.480095       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/kvs 308\nI0805 13:22:15.680372       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/t24r 264\nI0805 13:22:15.880660       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/9lb 547\nI0805 13:22:16.079892       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/2sq 244\nI0805 13:22:16.280182       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/gjf 396\nI0805 13:22:16.480478       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/xgp 567\nI0805 13:22:16.680760       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/tl4j 592\nI0805 13:22:16.880048       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/fld9 240\nI0805 13:22:17.080356       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/pq2 522\nI0805 13:22:17.280658       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/9xz8 549\nI0805 13:22:17.480048       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/dgb8 349\nI0805 13:22:17.680317       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/6krf 216\nI0805 13:22:17.880578       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/9tbh 455\nI0805 13:22:18.079807       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qlx 426\nI0805 13:22:18.280096       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/rfh 294\nI0805 13:22:18.480374       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6qc 226\nI0805 13:22:18.680665       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/hvmq 425\n"
  Aug  5 13:22:18.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-logs-4188 delete pod logs-generator'
  Aug  5 13:22:19.368: INFO: stderr: ""
  Aug  5 13:22:19.368: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug  5 13:22:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4188" for this suite. @ 08/05/23 13:22:19.372
• [5.696 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/05/23 13:22:19.38
  Aug  5 13:22:19.380: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context-test @ 08/05/23 13:22:19.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:19.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:19.404
  E0805 13:22:19.618458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:20.618554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:21.618989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:22.619214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:23.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3416" for this suite. @ 08/05/23 13:22:23.433
• [4.060 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/05/23 13:22:23.441
  Aug  5 13:22:23.441: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:22:23.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:23.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:23.465
  STEP: Creating projection with secret that has name projected-secret-test-map-d52c23d1-f947-4d6a-aacb-5d1e81c6f0e0 @ 08/05/23 13:22:23.467
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:22:23.472
  E0805 13:22:23.620218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:24.620318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:25.620717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:26.620807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:22:27.495
  Aug  5 13:22:27.498: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-secrets-b33a1145-ac4d-464a-9f2e-a87e4ef2f88a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:22:27.506
  Aug  5 13:22:27.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7251" for this suite. @ 08/05/23 13:22:27.527
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/05/23 13:22:27.536
  Aug  5 13:22:27.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/05/23 13:22:27.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:27.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:27.557
  STEP: Creating 50 configmaps @ 08/05/23 13:22:27.559
  E0805 13:22:27.621668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 08/05/23 13:22:27.792
  Aug  5 13:22:27.900: INFO: Pod name wrapped-volume-race-3cc51304-8657-450c-ae8b-8883b2d0296f: Found 3 pods out of 5
  E0805 13:22:28.622402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:29.622640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:30.622854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:31.623044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:32.623467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:32.907: INFO: Pod name wrapped-volume-race-3cc51304-8657-450c-ae8b-8883b2d0296f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/05/23 13:22:32.907
  STEP: Creating RC which spawns configmap-volume pods @ 08/05/23 13:22:32.931
  Aug  5 13:22:32.951: INFO: Pod name wrapped-volume-race-fd995df1-7b9a-4aea-8f65-5167f6e428ee: Found 0 pods out of 5
  E0805 13:22:33.623247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:34.623326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:35.623417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:36.623491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:37.623591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:37.959: INFO: Pod name wrapped-volume-race-fd995df1-7b9a-4aea-8f65-5167f6e428ee: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/05/23 13:22:37.959
  STEP: Creating RC which spawns configmap-volume pods @ 08/05/23 13:22:37.981
  Aug  5 13:22:37.997: INFO: Pod name wrapped-volume-race-e202facf-da4b-48e1-82e1-766c4a669739: Found 0 pods out of 5
  E0805 13:22:38.623716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:39.633384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:40.633654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:41.634693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:42.634779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:43.005: INFO: Pod name wrapped-volume-race-e202facf-da4b-48e1-82e1-766c4a669739: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/05/23 13:22:43.005
  Aug  5 13:22:43.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-e202facf-da4b-48e1-82e1-766c4a669739 in namespace emptydir-wrapper-9529, will wait for the garbage collector to delete the pods @ 08/05/23 13:22:43.027
  Aug  5 13:22:43.088: INFO: Deleting ReplicationController wrapped-volume-race-e202facf-da4b-48e1-82e1-766c4a669739 took: 7.166585ms
  Aug  5 13:22:43.189: INFO: Terminating ReplicationController wrapped-volume-race-e202facf-da4b-48e1-82e1-766c4a669739 pods took: 100.811571ms
  E0805 13:22:43.635365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:44.636285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-fd995df1-7b9a-4aea-8f65-5167f6e428ee in namespace emptydir-wrapper-9529, will wait for the garbage collector to delete the pods @ 08/05/23 13:22:45.19
  Aug  5 13:22:45.253: INFO: Deleting ReplicationController wrapped-volume-race-fd995df1-7b9a-4aea-8f65-5167f6e428ee took: 8.876337ms
  Aug  5 13:22:45.354: INFO: Terminating ReplicationController wrapped-volume-race-fd995df1-7b9a-4aea-8f65-5167f6e428ee pods took: 100.585301ms
  E0805 13:22:45.636872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:46.637222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:47.638281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-3cc51304-8657-450c-ae8b-8883b2d0296f in namespace emptydir-wrapper-9529, will wait for the garbage collector to delete the pods @ 08/05/23 13:22:47.754
  Aug  5 13:22:47.817: INFO: Deleting ReplicationController wrapped-volume-race-3cc51304-8657-450c-ae8b-8883b2d0296f took: 7.701131ms
  Aug  5 13:22:47.917: INFO: Terminating ReplicationController wrapped-volume-race-3cc51304-8657-450c-ae8b-8883b2d0296f pods took: 100.752344ms
  E0805 13:22:48.639322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:49.639494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 08/05/23 13:22:49.918
  STEP: Destroying namespace "emptydir-wrapper-9529" for this suite. @ 08/05/23 13:22:50.233
• [22.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/05/23 13:22:50.243
  Aug  5 13:22:50.243: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 13:22:50.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:50.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:50.263
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/05/23 13:22:50.266
  Aug  5 13:22:50.275: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0805 13:22:50.639902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:51.640064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:52.640873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:53.641036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:54.641217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:22:55.279: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 13:22:55.279
  STEP: getting scale subresource @ 08/05/23 13:22:55.28
  STEP: updating a scale subresource @ 08/05/23 13:22:55.283
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/05/23 13:22:55.288
  STEP: Patch a scale subresource @ 08/05/23 13:22:55.291
  Aug  5 13:22:55.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9512" for this suite. @ 08/05/23 13:22:55.306
• [5.070 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/05/23 13:22:55.313
  Aug  5 13:22:55.314: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename containers @ 08/05/23 13:22:55.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:55.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:55.338
  STEP: Creating a pod to test override command @ 08/05/23 13:22:55.34
  E0805 13:22:55.641623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:56.641848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:57.641909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:22:58.642018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:22:59.36
  Aug  5 13:22:59.364: INFO: Trying to get logs from node ip-172-31-95-133 pod client-containers-ca248a03-f4ac-4f21-b278-9a37991f52d6 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:22:59.372
  Aug  5 13:22:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6729" for this suite. @ 08/05/23 13:22:59.39
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/05/23 13:22:59.399
  Aug  5 13:22:59.399: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:22:59.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:22:59.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:22:59.418
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:22:59.421
  E0805 13:22:59.642568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:00.643393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:01.643504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:02.643642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:23:03.443
  Aug  5 13:23:03.447: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-d63f2eea-09bb-4cc8-8f7e-d9835e52abef container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:23:03.454
  Aug  5 13:23:03.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-815" for this suite. @ 08/05/23 13:23:03.477
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/05/23 13:23:03.485
  Aug  5 13:23:03.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:23:03.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:03.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:03.508
  Aug  5 13:23:03.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 create -f -'
  E0805 13:23:03.643923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:03.760: INFO: stderr: ""
  Aug  5 13:23:03.760: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug  5 13:23:03.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 create -f -'
  Aug  5 13:23:04.009: INFO: stderr: ""
  Aug  5 13:23:04.009: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/05/23 13:23:04.009
  E0805 13:23:04.644538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:05.014: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:05.014: INFO: Found 1 / 1
  Aug  5 13:23:05.014: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug  5 13:23:05.017: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:05.017: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  5 13:23:05.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 describe pod agnhost-primary-dhmgv'
  Aug  5 13:23:05.095: INFO: stderr: ""
  Aug  5 13:23:05.095: INFO: stdout: "Name:             agnhost-primary-dhmgv\nNamespace:        kubectl-890\nPriority:         0\nService Account:  default\nNode:             ip-172-31-95-133/172.31.95.133\nStart Time:       Sat, 05 Aug 2023 13:23:03 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.52.168\nIPs:\n  IP:           192.168.52.168\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3ec8d465bf52608b6ca159b597d0afe00bbaecaca9a4568901f30c791699263c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 05 Aug 2023 13:23:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5bpsk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5bpsk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-890/agnhost-primary-dhmgv to ip-172-31-95-133\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Aug  5 13:23:05.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 describe rc agnhost-primary'
  Aug  5 13:23:05.173: INFO: stderr: ""
  Aug  5 13:23:05.173: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-890\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-dhmgv\n"
  Aug  5 13:23:05.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 describe service agnhost-primary'
  Aug  5 13:23:05.250: INFO: stderr: ""
  Aug  5 13:23:05.250: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-890\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.62\nIPs:               10.152.183.62\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.52.168:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug  5 13:23:05.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 describe node ip-172-31-1-47'
  Aug  5 13:23:05.341: INFO: stderr: ""
  Aug  5 13:23:05.341: INFO: stdout: "Name:               ip-172-31-1-47\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-1-47\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 05 Aug 2023 11:58:10 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-1-47\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 05 Aug 2023 13:23:03 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 05 Aug 2023 13:20:59 +0000   Sat, 05 Aug 2023 11:58:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 05 Aug 2023 13:20:59 +0000   Sat, 05 Aug 2023 11:58:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 05 Aug 2023 13:20:59 +0000   Sat, 05 Aug 2023 11:58:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 05 Aug 2023 13:20:59 +0000   Sat, 05 Aug 2023 12:01:14 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.1.47\n  Hostname:    ip-172-31-1-47\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16070192Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15967792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec224f3e226a65fcf82fa2916146db76\n  System UUID:                     ec224f3e-226a-65fc-f82f-a2916146db76\n  Boot ID:                         65ce9b21-a4da-4e5d-a6d3-3f4a94297ebd\n  Kernel Version:                  5.19.0-1029-aws\n  OS Image:                        Ubuntu 22.04.3 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.4\n  Kube-Proxy Version:              v1.27.4\nNon-terminated Pods:               (7 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-87f7r           0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                      coredns-5c7f76ccb8-8kfns                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (1%)     85m\n  kube-system                      kube-state-metrics-5b95b4459c-p5wzf                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  kube-system                      metrics-server-v0.5.2-6cf8c8b69c-vnlhz                     5m (0%)       100m (5%)   50Mi (0%)        300Mi (1%)     85m\n  kubernetes-dashboard             dashboard-metrics-scraper-6b8586b5c9-ghxkg                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  kubernetes-dashboard             kubernetes-dashboard-6869f4cd5f-mr7dz                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                105m (5%)   100m (5%)\n  memory             120Mi (0%)  470Mi (3%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Aug  5 13:23:05.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-890 describe namespace kubectl-890'
  Aug  5 13:23:05.421: INFO: stderr: ""
  Aug  5 13:23:05.421: INFO: stdout: "Name:         kubectl-890\nLabels:       e2e-framework=kubectl\n              e2e-run=8b103839-1338-4d28-9fa2-89009d2c96d1\n              kubernetes.io/metadata.name=kubectl-890\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug  5 13:23:05.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-890" for this suite. @ 08/05/23 13:23:05.426
• [1.949 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/05/23 13:23:05.435
  Aug  5 13:23:05.435: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:23:05.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:05.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:05.458
  STEP: creating Agnhost RC @ 08/05/23 13:23:05.46
  Aug  5 13:23:05.460: INFO: namespace kubectl-638
  Aug  5 13:23:05.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-638 create -f -'
  E0805 13:23:05.645226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:05.719: INFO: stderr: ""
  Aug  5 13:23:05.719: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/05/23 13:23:05.719
  E0805 13:23:06.645651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:06.723: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:06.723: INFO: Found 0 / 1
  E0805 13:23:07.645739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:07.723: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:07.723: INFO: Found 0 / 1
  E0805 13:23:08.645820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:08.723: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:08.723: INFO: Found 0 / 1
  E0805 13:23:09.645911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:09.724: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:09.724: INFO: Found 1 / 1
  Aug  5 13:23:09.724: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug  5 13:23:09.727: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:23:09.727: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  5 13:23:09.727: INFO: wait on agnhost-primary startup in kubectl-638 
  Aug  5 13:23:09.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-638 logs agnhost-primary-7hdt9 agnhost-primary'
  Aug  5 13:23:09.815: INFO: stderr: ""
  Aug  5 13:23:09.815: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/05/23 13:23:09.815
  Aug  5 13:23:09.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-638 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Aug  5 13:23:09.937: INFO: stderr: ""
  Aug  5 13:23:09.937: INFO: stdout: "service/rm2 exposed\n"
  Aug  5 13:23:09.942: INFO: Service rm2 in namespace kubectl-638 found.
  E0805 13:23:10.646407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:11.646492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 08/05/23 13:23:11.95
  Aug  5 13:23:11.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-638 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Aug  5 13:23:12.026: INFO: stderr: ""
  Aug  5 13:23:12.026: INFO: stdout: "service/rm3 exposed\n"
  Aug  5 13:23:12.031: INFO: Service rm3 in namespace kubectl-638 found.
  E0805 13:23:12.646529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:13.646730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:14.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-638" for this suite. @ 08/05/23 13:23:14.043
• [8.615 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/05/23 13:23:14.051
  Aug  5 13:23:14.051: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-runtime @ 08/05/23 13:23:14.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:14.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:14.072
  STEP: create the container @ 08/05/23 13:23:14.075
  W0805 13:23:14.083511      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/05/23 13:23:14.083
  E0805 13:23:14.647458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:15.648252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:16.648314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/05/23 13:23:17.098
  STEP: the container should be terminated @ 08/05/23 13:23:17.102
  STEP: the termination message should be set @ 08/05/23 13:23:17.102
  Aug  5 13:23:17.102: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/05/23 13:23:17.102
  Aug  5 13:23:17.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-963" for this suite. @ 08/05/23 13:23:17.124
• [3.080 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/05/23 13:23:17.131
  Aug  5 13:23:17.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/05/23 13:23:17.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:17.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:17.15
  STEP: getting /apis @ 08/05/23 13:23:17.152
  STEP: getting /apis/storage.k8s.io @ 08/05/23 13:23:17.156
  STEP: getting /apis/storage.k8s.io/v1 @ 08/05/23 13:23:17.157
  STEP: creating @ 08/05/23 13:23:17.158
  STEP: watching @ 08/05/23 13:23:17.174
  Aug  5 13:23:17.174: INFO: starting watch
  STEP: getting @ 08/05/23 13:23:17.18
  STEP: listing in namespace @ 08/05/23 13:23:17.183
  STEP: listing across namespaces @ 08/05/23 13:23:17.187
  STEP: patching @ 08/05/23 13:23:17.19
  STEP: updating @ 08/05/23 13:23:17.195
  Aug  5 13:23:17.199: INFO: waiting for watch events with expected annotations in namespace
  Aug  5 13:23:17.199: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/05/23 13:23:17.2
  STEP: deleting a collection @ 08/05/23 13:23:17.213
  Aug  5 13:23:17.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5683" for this suite. @ 08/05/23 13:23:17.234
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/05/23 13:23:17.243
  Aug  5 13:23:17.243: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/05/23 13:23:17.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:17.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:17.265
  Aug  5 13:23:17.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:23:17.649236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:18.649732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:19.649809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:20.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2203" for this suite. @ 08/05/23 13:23:20.384
• [3.149 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/05/23 13:23:20.392
  Aug  5 13:23:20.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-pred @ 08/05/23 13:23:20.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:20.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:20.414
  Aug  5 13:23:20.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  5 13:23:20.425: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 13:23:20.428: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-47 before test
  Aug  5 13:23:20.434: INFO: nginx-ingress-controller-kubernetes-worker-87f7r from ingress-nginx-kubernetes-worker started at 2023-08-05 11:58:16 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: coredns-5c7f76ccb8-8kfns from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container coredns ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: kube-state-metrics-5b95b4459c-p5wzf from kube-system started at 2023-08-05 11:58:13 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: metrics-server-v0.5.2-6cf8c8b69c-vnlhz from kube-system started at 2023-08-05 11:58:13 +0000 UTC (2 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container metrics-server ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: dashboard-metrics-scraper-6b8586b5c9-ghxkg from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: kubernetes-dashboard-6869f4cd5f-mr7dz from kubernetes-dashboard started at 2023-08-05 11:58:14 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-qgzn2 from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:23:20.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 13:23:20.434: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-35-140 before test
  Aug  5 13:23:20.439: INFO: nginx-ingress-controller-kubernetes-worker-49tnn from ingress-nginx-kubernetes-worker started at 2023-08-05 12:02:34 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.439: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: sonobuoy from sonobuoy started at 2023-08-05 12:07:56 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.439: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: sonobuoy-e2e-job-6ea4f22076f544af from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:23:20.439: INFO: 	Container e2e ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-bxrtb from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:23:20.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  5 13:23:20.439: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-95-133 before test
  Aug  5 13:23:20.445: INFO: default-http-backend-kubernetes-worker-65fc475d49-wwntt from ingress-nginx-kubernetes-worker started at 2023-08-05 12:46:04 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.445: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Aug  5 13:23:20.445: INFO: nginx-ingress-controller-kubernetes-worker-n56xb from ingress-nginx-kubernetes-worker started at 2023-08-05 12:46:15 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.445: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Aug  5 13:23:20.445: INFO: calico-kube-controllers-5b8cb49547-m7sct from kube-system started at 2023-08-05 12:46:04 +0000 UTC (1 container statuses recorded)
  Aug  5 13:23:20.445: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Aug  5 13:23:20.445: INFO: sonobuoy-systemd-logs-daemon-set-fda9a0aa71af4102-kjhcj from sonobuoy started at 2023-08-05 12:07:58 +0000 UTC (2 container statuses recorded)
  Aug  5 13:23:20.445: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  5 13:23:20.445: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/05/23 13:23:20.445
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17787f74fa3db7e6], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 08/05/23 13:23:20.477
  E0805 13:23:20.650071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:21.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3332" for this suite. @ 08/05/23 13:23:21.477
• [1.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/05/23 13:23:21.49
  Aug  5 13:23:21.490: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 13:23:21.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:21.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:21.51
  STEP: Create a Replicaset @ 08/05/23 13:23:21.517
  STEP: Verify that the required pods have come up. @ 08/05/23 13:23:21.521
  Aug  5 13:23:21.525: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0805 13:23:21.650691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:22.650827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:23.650906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:24.651210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:25.651431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:23:26.529: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 13:23:26.529
  STEP: Getting /status @ 08/05/23 13:23:26.529
  Aug  5 13:23:26.533: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/05/23 13:23:26.533
  Aug  5 13:23:26.543: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/05/23 13:23:26.543
  Aug  5 13:23:26.544: INFO: Observed &ReplicaSet event: ADDED
  Aug  5 13:23:26.545: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.545: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.545: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.545: INFO: Found replicaset test-rs in namespace replicaset-2177 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  5 13:23:26.545: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/05/23 13:23:26.545
  Aug  5 13:23:26.545: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  5 13:23:26.552: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/05/23 13:23:26.552
  Aug  5 13:23:26.554: INFO: Observed &ReplicaSet event: ADDED
  Aug  5 13:23:26.554: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.554: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.554: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.554: INFO: Observed replicaset test-rs in namespace replicaset-2177 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 13:23:26.555: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  5 13:23:26.555: INFO: Found replicaset test-rs in namespace replicaset-2177 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug  5 13:23:26.555: INFO: Replicaset test-rs has a patched status
  Aug  5 13:23:26.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2177" for this suite. @ 08/05/23 13:23:26.559
• [5.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/05/23 13:23:26.567
  Aug  5 13:23:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 13:23:26.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:23:26.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:23:26.587
  E0805 13:23:26.651895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:27.652278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:28.652929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:29.653395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:30.653778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:31.654243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:32.654509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:33.655580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:34.656122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:35.657044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:36.657651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:37.657685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:38.658045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:39.658266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:40.658581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:41.658659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:42.658964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:43.659040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:44.659120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:45.659320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:46.659427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:47.660216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:48.660323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:49.660357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:50.660569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:51.660759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:52.660860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:53.660924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:54.661042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:55.661770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:56.661951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:57.662716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:58.662970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:23:59.663040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:00.663541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:01.663639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:02.663704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:03.663968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:04.664065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:05.664500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:06.664756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:07.665757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:08.665849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:09.666718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:10.667599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:11.668358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:12.668435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:13.668517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:14.668561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:15.668818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:16.668879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:17.668960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:18.669005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:19.669097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:20.669572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:21.669626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:22.669735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:23.670680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:24.671082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:25.671503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:24:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3068" for this suite. @ 08/05/23 13:24:26.607
• [60.047 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/05/23 13:24:26.615
  Aug  5 13:24:26.615: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-runtime @ 08/05/23 13:24:26.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:24:26.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:24:26.638
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/05/23 13:24:26.651
  E0805 13:24:26.672358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:27.673352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:28.673833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:29.674793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:30.675786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:31.675880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:32.676147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:33.676277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:34.676306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:35.676459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:36.676899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:37.677793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:38.678723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:39.678809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:40.679122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:41.680035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:42.680163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:43.680861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/05/23 13:24:43.737
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/05/23 13:24:43.741
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/05/23 13:24:43.748
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/05/23 13:24:43.748
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/05/23 13:24:43.771
  E0805 13:24:44.681641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:45.681799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:46.681899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/05/23 13:24:46.791
  E0805 13:24:47.682708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/05/23 13:24:47.8
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/05/23 13:24:47.807
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/05/23 13:24:47.807
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/05/23 13:24:47.831
  E0805 13:24:48.683716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/05/23 13:24:48.841
  E0805 13:24:49.683761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:50.684592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/05/23 13:24:50.853
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/05/23 13:24:50.861
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/05/23 13:24:50.861
  Aug  5 13:24:50.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6340" for this suite. @ 08/05/23 13:24:50.889
• [24.281 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/05/23 13:24:50.896
  Aug  5 13:24:50.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:24:50.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:24:50.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:24:50.92
  STEP: creating service in namespace services-7909 @ 08/05/23 13:24:50.922
  STEP: creating service affinity-nodeport-transition in namespace services-7909 @ 08/05/23 13:24:50.922
  STEP: creating replication controller affinity-nodeport-transition in namespace services-7909 @ 08/05/23 13:24:50.938
  I0805 13:24:50.947190      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-7909, replica count: 3
  E0805 13:24:51.684931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:52.685039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:53.685131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:24:53.998639      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:24:54.010: INFO: Creating new exec pod
  E0805 13:24:54.685661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:55.685813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:56.685902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:24:57.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug  5 13:24:57.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug  5 13:24:57.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:24:57.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.43 80'
  Aug  5 13:24:57.290: INFO: stderr: "+ nc -v -t -w 2 10.152.183.43 80\n+ echo hostName\nConnection to 10.152.183.43 80 port [tcp/http] succeeded!\n"
  Aug  5 13:24:57.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:24:57.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.95.133 30500'
  Aug  5 13:24:57.419: INFO: stderr: "+ nc -v -t -w 2 172.31.95.133 30500\n+ echo hostName\nConnection to 172.31.95.133 30500 port [tcp/*] succeeded!\n"
  Aug  5 13:24:57.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:24:57.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.140 30500'
  Aug  5 13:24:57.545: INFO: stderr: "+ nc -v -t -w 2 172.31.35.140 30500\n+ echo hostName\nConnection to 172.31.35.140 30500 port [tcp/*] succeeded!\n"
  Aug  5 13:24:57.545: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:24:57.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.47:30500/ ; done'
  E0805 13:24:57.686577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:24:57.765: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n"
  Aug  5 13:24:57.765: INFO: stdout: "\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-h2b2m\naffinity-nodeport-transition-kptlm\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-kptlm\naffinity-nodeport-transition-kptlm\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-kptlm\naffinity-nodeport-transition-h2b2m\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-kptlm\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-h2b2m\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5"
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-h2b2m
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-kptlm
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-kptlm
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-kptlm
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-kptlm
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-h2b2m
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-kptlm
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-h2b2m
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.765: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7909 exec execpod-affinity6mc9n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.47:30500/ ; done'
  Aug  5 13:24:57.990: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:30500/\n"
  Aug  5 13:24:57.990: INFO: stdout: "\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5\naffinity-nodeport-transition-2wbl5"
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Received response from host: affinity-nodeport-transition-2wbl5
  Aug  5 13:24:57.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:24:57.995: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7909, will wait for the garbage collector to delete the pods @ 08/05/23 13:24:58.01
  Aug  5 13:24:58.072: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.675974ms
  Aug  5 13:24:58.172: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.303827ms
  E0805 13:24:58.687625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:24:59.688388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7909" for this suite. @ 08/05/23 13:25:00.398
• [9.508 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/05/23 13:25:00.407
  Aug  5 13:25:00.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename dns @ 08/05/23 13:25:00.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:00.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:00.443
  STEP: Creating a test headless service @ 08/05/23 13:25:00.446
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 110.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.110_udp@PTR;check="$$(dig +tcp +noall +answer +search 110.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.110_tcp@PTR;sleep 1; done
   @ 08/05/23 13:25:00.469
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2638.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2638.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2638.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2638.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 110.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.110_udp@PTR;check="$$(dig +tcp +noall +answer +search 110.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.110_tcp@PTR;sleep 1; done
   @ 08/05/23 13:25:00.469
  STEP: creating a pod to probe DNS @ 08/05/23 13:25:00.469
  STEP: submitting the pod to kubernetes @ 08/05/23 13:25:00.469
  E0805 13:25:00.689341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:01.689429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/05/23 13:25:02.491
  STEP: looking for the results for each expected name from probers @ 08/05/23 13:25:02.495
  Aug  5 13:25:02.500: INFO: Unable to read wheezy_udp@dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.505: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.509: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.513: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.538: INFO: Unable to read jessie_udp@dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.546: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.550: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local from pod dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301: the server could not find the requested resource (get pods dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301)
  Aug  5 13:25:02.566: INFO: Lookups using dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301 failed for: [wheezy_udp@dns-test-service.dns-2638.svc.cluster.local wheezy_tcp@dns-test-service.dns-2638.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local jessie_udp@dns-test-service.dns-2638.svc.cluster.local jessie_tcp@dns-test-service.dns-2638.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2638.svc.cluster.local]

  E0805 13:25:02.690408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:03.690499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:04.690744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:05.691806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:06.691906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:07.634: INFO: DNS probes using dns-2638/dns-test-b36a02aa-4722-4445-ae8d-5b546ebeb301 succeeded

  Aug  5 13:25:07.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 13:25:07.638
  STEP: deleting the test service @ 08/05/23 13:25:07.653
  STEP: deleting the test headless service @ 08/05/23 13:25:07.676
  STEP: Destroying namespace "dns-2638" for this suite. @ 08/05/23 13:25:07.69
  E0805 13:25:07.692643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [7.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/05/23 13:25:07.701
  Aug  5 13:25:07.701: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:25:07.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:07.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:07.724
  STEP: creating Agnhost RC @ 08/05/23 13:25:07.727
  Aug  5 13:25:07.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-4472 create -f -'
  Aug  5 13:25:08.080: INFO: stderr: ""
  Aug  5 13:25:08.080: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/05/23 13:25:08.08
  E0805 13:25:08.692732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:09.085: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:25:09.085: INFO: Found 1 / 1
  Aug  5 13:25:09.085: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/05/23 13:25:09.086
  Aug  5 13:25:09.089: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:25:09.089: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  5 13:25:09.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-4472 patch pod agnhost-primary-mzb4k -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug  5 13:25:09.160: INFO: stderr: ""
  Aug  5 13:25:09.160: INFO: stdout: "pod/agnhost-primary-mzb4k patched\n"
  STEP: checking annotations @ 08/05/23 13:25:09.16
  Aug  5 13:25:09.164: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  5 13:25:09.164: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  5 13:25:09.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4472" for this suite. @ 08/05/23 13:25:09.168
• [1.475 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/05/23 13:25:09.177
  Aug  5 13:25:09.177: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 13:25:09.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:09.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:09.271
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/05/23 13:25:09.274
  E0805 13:25:09.692839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:10.693274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 08/05/23 13:25:11.296
  STEP: Then the orphan pod is adopted @ 08/05/23 13:25:11.301
  E0805 13:25:11.694159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:12.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9655" for this suite. @ 08/05/23 13:25:12.313
• [3.144 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/05/23 13:25:12.321
  Aug  5 13:25:12.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/05/23 13:25:12.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:12.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:12.34
  E0805 13:25:12.694244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:13.694265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:14.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/05/23 13:25:14.378
  STEP: Cleaning up the configmap @ 08/05/23 13:25:14.385
  STEP: Cleaning up the pod @ 08/05/23 13:25:14.391
  STEP: Destroying namespace "emptydir-wrapper-8871" for this suite. @ 08/05/23 13:25:14.404
• [2.092 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/05/23 13:25:14.413
  Aug  5 13:25:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:25:14.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:14.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:14.434
  STEP: creating service in namespace services-4560 @ 08/05/23 13:25:14.437
  STEP: creating service affinity-nodeport in namespace services-4560 @ 08/05/23 13:25:14.437
  STEP: creating replication controller affinity-nodeport in namespace services-4560 @ 08/05/23 13:25:14.452
  I0805 13:25:14.459750      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4560, replica count: 3
  E0805 13:25:14.694784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:15.695674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:16.696741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:25:17.511708      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:25:17.523: INFO: Creating new exec pod
  E0805 13:25:17.697200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:18.697478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:19.698114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:20.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-4560 exec execpod-affinityrdbq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug  5 13:25:20.673: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug  5 13:25:20.673: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:20.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-4560 exec execpod-affinityrdbq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.110 80'
  E0805 13:25:20.698813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:20.800: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.110 80\nConnection to 10.152.183.110 80 port [tcp/http] succeeded!\n"
  Aug  5 13:25:20.800: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:20.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-4560 exec execpod-affinityrdbq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.140 31223'
  Aug  5 13:25:20.935: INFO: stderr: "+ nc -v -t -w 2 172.31.35.140 31223\n+ echo hostName\nConnection to 172.31.35.140 31223 port [tcp/*] succeeded!\n"
  Aug  5 13:25:20.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:20.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-4560 exec execpod-affinityrdbq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.95.133 31223'
  Aug  5 13:25:21.060: INFO: stderr: "+ nc -v -t -w 2 172.31.95.133 31223\n+ echo hostName\nConnection to 172.31.95.133 31223 port [tcp/*] succeeded!\n"
  Aug  5 13:25:21.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:21.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-4560 exec execpod-affinityrdbq8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.47:31223/ ; done'
  Aug  5 13:25:21.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.47:31223/\n"
  Aug  5 13:25:21.267: INFO: stdout: "\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj\naffinity-nodeport-cj2jj"
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.267: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Received response from host: affinity-nodeport-cj2jj
  Aug  5 13:25:21.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:25:21.272: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4560, will wait for the garbage collector to delete the pods @ 08/05/23 13:25:21.287
  Aug  5 13:25:21.348: INFO: Deleting ReplicationController affinity-nodeport took: 7.204683ms
  Aug  5 13:25:21.449: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.139189ms
  E0805 13:25:21.698872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:22.698904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4560" for this suite. @ 08/05/23 13:25:23.375
• [8.969 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/05/23 13:25:23.382
  Aug  5 13:25:23.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename conformance-tests @ 08/05/23 13:25:23.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:23.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:23.403
  STEP: Getting node addresses @ 08/05/23 13:25:23.405
  Aug  5 13:25:23.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug  5 13:25:23.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-9688" for this suite. @ 08/05/23 13:25:23.414
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/05/23 13:25:23.424
  Aug  5 13:25:23.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:25:23.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:23.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:23.458
  STEP: Setting up server cert @ 08/05/23 13:25:23.487
  E0805 13:25:23.699749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:25:23.741
  STEP: Deploying the webhook pod @ 08/05/23 13:25:23.75
  STEP: Wait for the deployment to be ready @ 08/05/23 13:25:23.763
  Aug  5 13:25:23.770: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:25:24.699842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:25.700790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:25:25.782
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:25:25.793
  E0805 13:25:26.701040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:26.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/05/23 13:25:26.797
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/05/23 13:25:26.813
  Aug  5 13:25:26.813: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:25:26.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9891" for this suite. @ 08/05/23 13:25:26.875
  STEP: Destroying namespace "webhook-markers-2950" for this suite. @ 08/05/23 13:25:26.883
• [3.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/05/23 13:25:26.891
  Aug  5 13:25:26.891: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 13:25:26.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:26.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:26.91
  STEP: Creating service test in namespace statefulset-1400 @ 08/05/23 13:25:26.912
  STEP: Looking for a node to schedule stateful set and pod @ 08/05/23 13:25:26.918
  STEP: Creating pod with conflicting port in namespace statefulset-1400 @ 08/05/23 13:25:26.923
  STEP: Waiting until pod test-pod will start running in namespace statefulset-1400 @ 08/05/23 13:25:26.932
  E0805 13:25:27.701362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:28.701649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-1400 @ 08/05/23 13:25:28.94
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1400 @ 08/05/23 13:25:28.947
  Aug  5 13:25:28.977: INFO: Observed stateful pod in namespace: statefulset-1400, name: ss-0, uid: 00bdb328-7031-4f8c-b270-e63b6c281503, status phase: Pending. Waiting for statefulset controller to delete.
  Aug  5 13:25:28.993: INFO: Observed stateful pod in namespace: statefulset-1400, name: ss-0, uid: 00bdb328-7031-4f8c-b270-e63b6c281503, status phase: Failed. Waiting for statefulset controller to delete.
  Aug  5 13:25:29.009: INFO: Observed stateful pod in namespace: statefulset-1400, name: ss-0, uid: 00bdb328-7031-4f8c-b270-e63b6c281503, status phase: Failed. Waiting for statefulset controller to delete.
  Aug  5 13:25:29.012: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1400
  STEP: Removing pod with conflicting port in namespace statefulset-1400 @ 08/05/23 13:25:29.012
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1400 and will be in running state @ 08/05/23 13:25:29.031
  E0805 13:25:29.701738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:30.702651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:31.039: INFO: Deleting all statefulset in ns statefulset-1400
  Aug  5 13:25:31.043: INFO: Scaling statefulset ss to 0
  E0805 13:25:31.703598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:32.703774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:33.703871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:34.703953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:35.704786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:36.704884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:37.705103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:38.705184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:39.705373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:40.705723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:41.060: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:25:41.064: INFO: Deleting statefulset ss
  Aug  5 13:25:41.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1400" for this suite. @ 08/05/23 13:25:41.082
• [14.198 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/05/23 13:25:41.089
  Aug  5 13:25:41.089: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:25:41.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:41.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:41.108
  STEP: creating service in namespace services-2379 @ 08/05/23 13:25:41.11
  STEP: creating service affinity-clusterip-transition in namespace services-2379 @ 08/05/23 13:25:41.11
  STEP: creating replication controller affinity-clusterip-transition in namespace services-2379 @ 08/05/23 13:25:41.12
  I0805 13:25:41.128803      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-2379, replica count: 3
  E0805 13:25:41.706240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:42.707196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:43.707291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:25:44.180974      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:25:44.188: INFO: Creating new exec pod
  E0805 13:25:44.707974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:45.708423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:46.708535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:47.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2379 exec execpod-affinity9xqhw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Aug  5 13:25:47.337: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug  5 13:25:47.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:47.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2379 exec execpod-affinity9xqhw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.195 80'
  Aug  5 13:25:47.464: INFO: stderr: "+ nc -v -t -w 2 10.152.183.195 80\n+ echo hostName\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
  Aug  5 13:25:47.464: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:25:47.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2379 exec execpod-affinity9xqhw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.195:80/ ; done'
  Aug  5 13:25:47.675: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n"
  Aug  5 13:25:47.675: INFO: stdout: "\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-j5pvv\naffinity-clusterip-transition-j5pvv\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-j5pvv\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-j5pvv\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-j5pvv\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-6k494\naffinity-clusterip-transition-6k494"
  Aug  5 13:25:47.675: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.675: INFO: Received response from host: affinity-clusterip-transition-j5pvv
  Aug  5 13:25:47.675: INFO: Received response from host: affinity-clusterip-transition-j5pvv
  Aug  5 13:25:47.675: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.675: INFO: Received response from host: affinity-clusterip-transition-j5pvv
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-j5pvv
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-j5pvv
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.676: INFO: Received response from host: affinity-clusterip-transition-6k494
  Aug  5 13:25:47.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2379 exec execpod-affinity9xqhw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.195:80/ ; done'
  E0805 13:25:47.708992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:25:47.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.195:80/\n"
  Aug  5 13:25:47.882: INFO: stdout: "\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9\naffinity-clusterip-transition-xx2j9"
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Received response from host: affinity-clusterip-transition-xx2j9
  Aug  5 13:25:47.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:25:47.887: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2379, will wait for the garbage collector to delete the pods @ 08/05/23 13:25:47.902
  Aug  5 13:25:47.963: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.871379ms
  Aug  5 13:25:48.063: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.137362ms
  E0805 13:25:48.709377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:49.709771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2379" for this suite. @ 08/05/23 13:25:50.482
• [9.401 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/05/23 13:25:50.492
  Aug  5 13:25:50.493: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename namespaces @ 08/05/23 13:25:50.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:50.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:25:50.513
  STEP: Creating a test namespace @ 08/05/23 13:25:50.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:25:50.532
  STEP: Creating a pod in the namespace @ 08/05/23 13:25:50.534
  STEP: Waiting for the pod to have running status @ 08/05/23 13:25:50.542
  E0805 13:25:50.710324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:51.710708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 08/05/23 13:25:52.552
  STEP: Waiting for the namespace to be removed. @ 08/05/23 13:25:52.559
  E0805 13:25:52.711477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:53.712414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:54.712682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:55.713488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:56.714241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:57.714282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:58.714331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:25:59.715316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:00.715787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:01.716412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:02.717185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/05/23 13:26:03.563
  STEP: Verifying there are no pods in the namespace @ 08/05/23 13:26:03.584
  Aug  5 13:26:03.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5132" for this suite. @ 08/05/23 13:26:03.592
  STEP: Destroying namespace "nsdeletetest-3790" for this suite. @ 08/05/23 13:26:03.599
  Aug  5 13:26:03.602: INFO: Namespace nsdeletetest-3790 was already deleted
  STEP: Destroying namespace "nsdeletetest-6146" for this suite. @ 08/05/23 13:26:03.602
• [13.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/05/23 13:26:03.611
  Aug  5 13:26:03.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 13:26:03.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:03.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:03.629
  STEP: Creating service test in namespace statefulset-1958 @ 08/05/23 13:26:03.631
  Aug  5 13:26:03.646: INFO: Found 0 stateful pods, waiting for 1
  E0805 13:26:03.717760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:04.718727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:05.718744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:06.719094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:07.719185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:08.719366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:09.719461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:10.720266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:11.720896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:12.720972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:13.651: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/05/23 13:26:13.658
  W0805 13:26:13.668037      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  5 13:26:13.676: INFO: Found 1 stateful pods, waiting for 2
  E0805 13:26:13.721560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:14.721651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:15.721809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:16.722657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:17.722917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:18.722987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:19.723753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:20.724025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:21.724153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:22.724871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:23.683: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:26:23.683: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/05/23 13:26:23.693
  STEP: Delete all of the StatefulSets @ 08/05/23 13:26:23.698
  STEP: Verify that StatefulSets have been deleted @ 08/05/23 13:26:23.71
  Aug  5 13:26:23.715: INFO: Deleting all statefulset in ns statefulset-1958
  E0805 13:26:23.725230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:23.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1958" for this suite. @ 08/05/23 13:26:23.746
• [20.145 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/05/23 13:26:23.757
  Aug  5 13:26:23.757: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:26:23.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:23.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:23.784
  STEP: Creating configMap configmap-8618/configmap-test-661d7125-833f-4923-a941-c3a1e3710ca6 @ 08/05/23 13:26:23.787
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:26:23.793
  E0805 13:26:24.725370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:25.725462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:26.725661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:27.725766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:26:27.819
  Aug  5 13:26:27.823: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-25ae29cf-75e3-486c-98fc-17095d6cc35d container env-test: <nil>
  STEP: delete the pod @ 08/05/23 13:26:27.845
  Aug  5 13:26:27.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8618" for this suite. @ 08/05/23 13:26:27.871
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/05/23 13:26:27.881
  Aug  5 13:26:27.881: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubelet-test @ 08/05/23 13:26:27.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:27.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:27.905
  STEP: Waiting for pod completion @ 08/05/23 13:26:27.92
  E0805 13:26:28.726669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:29.726870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:30.727634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:31.727732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4416" for this suite. @ 08/05/23 13:26:31.949
• [4.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/05/23 13:26:31.959
  Aug  5 13:26:31.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename discovery @ 08/05/23 13:26:31.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:31.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:31.98
  STEP: Setting up server cert @ 08/05/23 13:26:31.985
  Aug  5 13:26:32.173: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug  5 13:26:32.174: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug  5 13:26:32.174: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug  5 13:26:32.174: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug  5 13:26:32.174: INFO: Checking APIGroup: apps
  Aug  5 13:26:32.175: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug  5 13:26:32.175: INFO: Versions found [{apps/v1 v1}]
  Aug  5 13:26:32.175: INFO: apps/v1 matches apps/v1
  Aug  5 13:26:32.175: INFO: Checking APIGroup: events.k8s.io
  Aug  5 13:26:32.176: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug  5 13:26:32.176: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug  5 13:26:32.176: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug  5 13:26:32.176: INFO: Checking APIGroup: authentication.k8s.io
  Aug  5 13:26:32.177: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug  5 13:26:32.177: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug  5 13:26:32.177: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug  5 13:26:32.177: INFO: Checking APIGroup: authorization.k8s.io
  Aug  5 13:26:32.179: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug  5 13:26:32.179: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug  5 13:26:32.179: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug  5 13:26:32.179: INFO: Checking APIGroup: autoscaling
  Aug  5 13:26:32.180: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug  5 13:26:32.180: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug  5 13:26:32.180: INFO: autoscaling/v2 matches autoscaling/v2
  Aug  5 13:26:32.180: INFO: Checking APIGroup: batch
  Aug  5 13:26:32.181: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug  5 13:26:32.181: INFO: Versions found [{batch/v1 v1}]
  Aug  5 13:26:32.181: INFO: batch/v1 matches batch/v1
  Aug  5 13:26:32.181: INFO: Checking APIGroup: certificates.k8s.io
  Aug  5 13:26:32.183: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug  5 13:26:32.183: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug  5 13:26:32.183: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug  5 13:26:32.183: INFO: Checking APIGroup: networking.k8s.io
  Aug  5 13:26:32.184: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug  5 13:26:32.184: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug  5 13:26:32.184: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug  5 13:26:32.184: INFO: Checking APIGroup: policy
  Aug  5 13:26:32.186: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug  5 13:26:32.186: INFO: Versions found [{policy/v1 v1}]
  Aug  5 13:26:32.186: INFO: policy/v1 matches policy/v1
  Aug  5 13:26:32.186: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug  5 13:26:32.187: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug  5 13:26:32.187: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug  5 13:26:32.187: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug  5 13:26:32.187: INFO: Checking APIGroup: storage.k8s.io
  Aug  5 13:26:32.188: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug  5 13:26:32.188: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug  5 13:26:32.188: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug  5 13:26:32.188: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug  5 13:26:32.190: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug  5 13:26:32.190: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug  5 13:26:32.190: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug  5 13:26:32.190: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug  5 13:26:32.191: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug  5 13:26:32.191: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug  5 13:26:32.191: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug  5 13:26:32.191: INFO: Checking APIGroup: scheduling.k8s.io
  Aug  5 13:26:32.192: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug  5 13:26:32.192: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug  5 13:26:32.192: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug  5 13:26:32.192: INFO: Checking APIGroup: coordination.k8s.io
  Aug  5 13:26:32.194: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug  5 13:26:32.194: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug  5 13:26:32.194: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug  5 13:26:32.194: INFO: Checking APIGroup: node.k8s.io
  Aug  5 13:26:32.195: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug  5 13:26:32.195: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug  5 13:26:32.195: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug  5 13:26:32.195: INFO: Checking APIGroup: discovery.k8s.io
  Aug  5 13:26:32.197: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug  5 13:26:32.197: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug  5 13:26:32.197: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug  5 13:26:32.197: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug  5 13:26:32.200: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug  5 13:26:32.200: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug  5 13:26:32.200: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug  5 13:26:32.200: INFO: Checking APIGroup: metrics.k8s.io
  Aug  5 13:26:32.201: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Aug  5 13:26:32.201: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Aug  5 13:26:32.201: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Aug  5 13:26:32.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-7217" for this suite. @ 08/05/23 13:26:32.206
• [0.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/05/23 13:26:32.216
  Aug  5 13:26:32.216: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:26:32.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:32.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:32.238
  STEP: creating service nodeport-test with type=NodePort in namespace services-8713 @ 08/05/23 13:26:32.241
  STEP: creating replication controller nodeport-test in namespace services-8713 @ 08/05/23 13:26:32.258
  I0805 13:26:32.267127      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-8713, replica count: 2
  E0805 13:26:32.728253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:33.729229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:34.729479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:26:35.317872      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:26:35.317: INFO: Creating new exec pod
  E0805 13:26:35.730531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:36.731558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:37.732558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:38.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8713 exec execpodz54hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug  5 13:26:38.469: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug  5 13:26:38.469: INFO: stdout: "nodeport-test-c5qst"
  Aug  5 13:26:38.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8713 exec execpodz54hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.53 80'
  Aug  5 13:26:38.590: INFO: stderr: "+ nc -v -t -w 2 10.152.183.53 80\n+ echo hostName\nConnection to 10.152.183.53 80 port [tcp/http] succeeded!\n"
  Aug  5 13:26:38.590: INFO: stdout: ""
  E0805 13:26:38.732632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:39.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8713 exec execpodz54hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.53 80'
  Aug  5 13:26:39.723: INFO: stderr: "+ nc -v -t -w 2 10.152.183.53 80\n+ echo hostName\nConnection to 10.152.183.53 80 port [tcp/http] succeeded!\n"
  Aug  5 13:26:39.723: INFO: stdout: "nodeport-test-c5qst"
  Aug  5 13:26:39.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8713 exec execpodz54hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.140 31522'
  E0805 13:26:39.733265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:26:39.848: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.35.140 31522\nConnection to 172.31.35.140 31522 port [tcp/*] succeeded!\n"
  Aug  5 13:26:39.848: INFO: stdout: "nodeport-test-4lfdq"
  Aug  5 13:26:39.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-8713 exec execpodz54hk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.95.133 31522'
  Aug  5 13:26:39.984: INFO: stderr: "+ nc -v -t -w 2 172.31.95.133 31522\n+ echo hostName\nConnection to 172.31.95.133 31522 port [tcp/*] succeeded!\n"
  Aug  5 13:26:39.984: INFO: stdout: "nodeport-test-c5qst"
  Aug  5 13:26:39.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8713" for this suite. @ 08/05/23 13:26:39.988
• [7.780 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/05/23 13:26:39.996
  Aug  5 13:26:39.996: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:26:39.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:40.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:40.018
  STEP: Creating configMap with name configmap-test-volume-map-7e952793-ae45-4daa-8fc8-67c92ca04738 @ 08/05/23 13:26:40.02
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:26:40.026
  E0805 13:26:40.733956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:41.734711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:42.734800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:43.735014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:26:44.049
  Aug  5 13:26:44.053: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-ab3dab9d-5d4f-43d7-acba-876405f2ad90 container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:26:44.061
  Aug  5 13:26:44.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3276" for this suite. @ 08/05/23 13:26:44.086
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/05/23 13:26:44.094
  Aug  5 13:26:44.094: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename endpointslice @ 08/05/23 13:26:44.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:26:44.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:26:44.114
  E0805 13:26:44.735127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:45.735271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:46.735358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:47.735585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:48.735791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/05/23 13:26:49.185
  E0805 13:26:49.735857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:50.736255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:51.736436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:52.736532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:53.736660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/05/23 13:26:54.193
  E0805 13:26:54.736861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:55.737841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:56.737942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:57.738713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:26:58.738796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/05/23 13:26:59.202
  E0805 13:26:59.739804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:00.740236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:01.740321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:02.740532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:03.740711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/05/23 13:27:04.21
  Aug  5 13:27:04.232: INFO: EndpointSlice for Service endpointslice-4824/example-named-port not found
  E0805 13:27:04.741399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:05.741486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:06.741646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:07.742705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:08.743628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:09.744313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:10.745000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:11.745092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:12.745207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:13.745331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:14.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4824" for this suite. @ 08/05/23 13:27:14.246
• [30.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/05/23 13:27:14.255
  Aug  5 13:27:14.255: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:27:14.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:14.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:14.277
  STEP: creating a secret @ 08/05/23 13:27:14.279
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/05/23 13:27:14.284
  STEP: patching the secret @ 08/05/23 13:27:14.287
  STEP: deleting the secret using a LabelSelector @ 08/05/23 13:27:14.296
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/05/23 13:27:14.306
  Aug  5 13:27:14.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1990" for this suite. @ 08/05/23 13:27:14.314
• [0.066 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/05/23 13:27:14.321
  Aug  5 13:27:14.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:27:14.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:14.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:14.342
  Aug  5 13:27:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4407" for this suite. @ 08/05/23 13:27:14.353
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/05/23 13:27:14.363
  Aug  5 13:27:14.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:27:14.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:14.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:14.381
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:27:14.383
  E0805 13:27:14.745407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:15.746239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:16.747288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:17.747390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:27:18.408
  Aug  5 13:27:18.412: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-b9e81921-c724-43c3-8f5c-a0ce17880ab9 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:27:18.419
  Aug  5 13:27:18.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1557" for this suite. @ 08/05/23 13:27:18.443
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/05/23 13:27:18.452
  Aug  5 13:27:18.452: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pod-network-test @ 08/05/23 13:27:18.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:18.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:18.475
  STEP: Performing setup for networking test in namespace pod-network-test-8791 @ 08/05/23 13:27:18.477
  STEP: creating a selector @ 08/05/23 13:27:18.477
  STEP: Creating the service pods in kubernetes @ 08/05/23 13:27:18.477
  Aug  5 13:27:18.477: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0805 13:27:18.747907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:19.748263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:20.748554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:21.749591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:22.749648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:23.750706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:24.751236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:25.751467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:26.751531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:27.751702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:28.751807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:29.752014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:30.752095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:31.752213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:32.752560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:33.753062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:34.753612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:35.753965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:36.754456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:37.754541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:38.754583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:39.754724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/05/23 13:27:40.578
  E0805 13:27:40.755537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:41.755642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:42.608: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug  5 13:27:42.608: INFO: Going to poll 192.168.122.109 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug  5 13:27:42.611: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.122.109 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:27:42.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:27:42.612: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:27:42.612: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.122.109+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0805 13:27:42.756172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:43.671: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug  5 13:27:43.671: INFO: Going to poll 192.168.166.224 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug  5 13:27:43.675: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.166.224 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:27:43.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:27:43.676: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:27:43.676: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.166.224+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0805 13:27:43.756541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:44.753: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug  5 13:27:44.753: INFO: Going to poll 192.168.52.180 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  E0805 13:27:44.756662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:44.757: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.52.180 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8791 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:27:44.757: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:27:44.757: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:27:44.757: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8791/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.52.180+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0805 13:27:45.757574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:45.845: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug  5 13:27:45.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8791" for this suite. @ 08/05/23 13:27:45.85
• [27.405 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 08/05/23 13:27:45.857
  Aug  5 13:27:45.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 13:27:45.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:45.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:45.879
  Aug  5 13:27:45.881: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:27:46.758018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:47.758107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0805 13:27:48.431712      19 warnings.go:70] unknown field "alpha"
  W0805 13:27:48.431732      19 warnings.go:70] unknown field "beta"
  W0805 13:27:48.431739      19 warnings.go:70] unknown field "delta"
  W0805 13:27:48.431744      19 warnings.go:70] unknown field "epsilon"
  W0805 13:27:48.431750      19 warnings.go:70] unknown field "gamma"
  E0805 13:27:48.758600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:48.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2903" for this suite. @ 08/05/23 13:27:48.982
• [3.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/05/23 13:27:48.991
  Aug  5 13:27:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:27:48.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:49.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:49.012
  STEP: Creating the pod @ 08/05/23 13:27:49.014
  E0805 13:27:49.758671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:50.759682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:51.565: INFO: Successfully updated pod "labelsupdate4673b348-376f-4836-8b85-ddd8edb32831"
  E0805 13:27:51.759699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:52.759797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:53.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4753" for this suite. @ 08/05/23 13:27:53.585
• [4.602 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/05/23 13:27:53.596
  Aug  5 13:27:53.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 13:27:53.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:53.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:53.62
  Aug  5 13:27:53.622: INFO: Creating simple deployment test-new-deployment
  Aug  5 13:27:53.635: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0805 13:27:53.760770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:54.760860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 08/05/23 13:27:55.651
  STEP: updating a scale subresource @ 08/05/23 13:27:55.654
  STEP: verifying the deployment Spec.Replicas was modified @ 08/05/23 13:27:55.661
  STEP: Patch a scale subresource @ 08/05/23 13:27:55.664
  Aug  5 13:27:55.683: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4610  55e635cd-7f26-4300-bdee-e3964043ba85 34302 3 2023-08-05 13:27:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-05 13:27:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:27:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047dcef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-05 13:27:54 +0000 UTC,LastTransitionTime:2023-08-05 13:27:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-05 13:27:54 +0000 UTC,LastTransitionTime:2023-08-05 13:27:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  5 13:27:55.689: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4610  56ce8dda-90f5-4b7f-a47d-54245f049808 34307 2 2023-08-05 13:27:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 55e635cd-7f26-4300-bdee-e3964043ba85 0xc0047dd347 0xc0047dd348}] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:27:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55e635cd-7f26-4300-bdee-e3964043ba85\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:27:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047dd3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 13:27:55.693: INFO: Pod "test-new-deployment-67bd4bf6dc-h5wjj" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-h5wjj test-new-deployment-67bd4bf6dc- deployment-4610  7189f312-ff87-4b82-8b1f-1d2a3094acf6 34305 0 2023-08-05 13:27:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 56ce8dda-90f5-4b7f-a47d-54245f049808 0xc0033de7d7 0xc0033de7d8}] [] [{kube-controller-manager Update v1 2023-08-05 13:27:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ce8dda-90f5-4b7f-a47d-54245f049808\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjjd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjjd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:27:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:27:55.693: INFO: Pod "test-new-deployment-67bd4bf6dc-j42rt" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-j42rt test-new-deployment-67bd4bf6dc- deployment-4610  2596a540-3af3-49c9-8c69-6b1b8a88a54f 34297 0 2023-08-05 13:27:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 56ce8dda-90f5-4b7f-a47d-54245f049808 0xc0033deae0 0xc0033deae1}] [] [{kube-controller-manager Update v1 2023-08-05 13:27:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ce8dda-90f5-4b7f-a47d-54245f049808\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:27:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckdl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckdl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:27:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:27:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:27:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:27:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.184,StartTime:2023-08-05 13:27:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:27:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1f9f431bf7941abd2eecdd4f56c128f6be582225915c9ca3260b1d25ae2b899f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:27:55.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4610" for this suite. @ 08/05/23 13:27:55.699
• [2.110 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/05/23 13:27:55.708
  Aug  5 13:27:55.708: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubelet-test @ 08/05/23 13:27:55.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:55.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:55.74
  E0805 13:27:55.761070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:56.761180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:57.761373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:57.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9916" for this suite. @ 08/05/23 13:27:57.776
• [2.075 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/05/23 13:27:57.783
  Aug  5 13:27:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:27:57.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:57.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:57.803
  STEP: creating pod @ 08/05/23 13:27:57.805
  E0805 13:27:58.761651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:27:59.762458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:27:59.830: INFO: Pod pod-hostip-82559be8-d6c1-43e1-b084-10bd9b73d513 has hostIP: 172.31.95.133
  Aug  5 13:27:59.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1137" for this suite. @ 08/05/23 13:27:59.834
• [2.059 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/05/23 13:27:59.844
  Aug  5 13:27:59.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 13:27:59.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:27:59.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:27:59.867
  STEP: Creating pod busybox-da0af6d1-9f2e-4a2d-92ec-7c5cd1a3513f in namespace container-probe-2864 @ 08/05/23 13:27:59.869
  E0805 13:28:00.763080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:01.764094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:28:01.887: INFO: Started pod busybox-da0af6d1-9f2e-4a2d-92ec-7c5cd1a3513f in namespace container-probe-2864
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 13:28:01.887
  Aug  5 13:28:01.891: INFO: Initial restart count of pod busybox-da0af6d1-9f2e-4a2d-92ec-7c5cd1a3513f is 0
  E0805 13:28:02.764889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:03.764983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:04.765690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:05.765781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:06.765884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:07.766691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:08.766779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:09.766883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:10.767385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:11.767482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:12.767516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:13.768478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:14.768576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:15.768864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:16.768970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:17.769181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:18.769243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:19.769456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:20.769859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:21.770695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:22.771632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:23.771813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:24.771919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:25.772849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:26.772951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:27.773045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:28.773134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:29.773332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:30.774015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:31.774105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:32.774461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:33.774833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:34.774827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:35.775876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:36.776877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:37.777041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:38.777142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:39.777299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:40.777792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:41.778849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:42.778934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:43.779092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:44.779517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:45.779826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:46.780281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:47.780342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:48.780388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:49.780464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:50.780563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:51.780734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:52.781726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:53.782682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:54.782912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:55.783827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:56.783893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:57.783930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:58.784309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:28:59.784366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:00.784660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:01.785274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:02.785310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:03.785391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:04.785640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:05.785867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:06.786681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:07.786865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:08.786983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:09.787694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:10.788154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:11.788337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:12.788803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:13.788981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:14.789020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:15.789864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:16.790006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:17.790690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:18.791578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:19.791853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:20.791974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:21.792159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:22.792413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:23.792508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:24.792799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:25.793867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:26.794903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:27.794975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:28.795064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:29.795529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:30.795598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:31.795874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:32.796699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:33.796869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:34.797839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:35.797927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:36.798682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:37.798841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:38.799207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:39.799460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:40.799934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:41.800130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:42.800222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:43.801064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:44.801420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:45.801746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:46.802470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:47.802739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:48.802837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:49.802922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:50.802995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:51.804023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:52.804478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:53.804955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:54.805455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:55.806134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:56.807037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:57.807162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:58.807693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:29:59.808176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:00.808875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:01.808967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:02.809413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:03.809613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:04.810688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:05.810829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:06.810884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:07.810986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:08.811069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:09.812034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:10.812532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:11.813031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:12.813137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:13.813362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:14.814086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:15.814536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:16.814885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:17.814982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:18.816051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:19.816865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:20.817615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:21.818680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:22.818890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:23.819000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:24.819603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:25.819887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:26.820931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:27.821198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:28.821857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:29.821937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:30.822029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:31.822693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:32.823477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:33.823888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:34.823692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:35.823767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:36.823857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:37.824056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:38.824458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:39.824555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:40.825118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:41.825856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:42.826194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:43.826685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:44.826774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:45.826860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:46.827015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:47.827403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:48.827532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:49.827625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:50.828239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:51.828340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:52.828863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:53.829045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:54.829178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:55.829903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:56.829992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:57.830681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:58.831710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:30:59.831911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:00.832049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:01.832161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:02.832647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:03.832860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:04.833603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:05.834632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:06.835010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:07.835169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:08.835308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:09.835401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:10.835515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:11.835739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:12.836593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:13.836750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:14.837433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:15.837484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:16.838128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:17.838688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:18.839540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:19.839722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:20.840140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:21.840591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:22.841481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:23.841747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:24.842112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:25.842151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:26.842239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:27.842326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:28.842415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:29.842510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:30.843453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:31.843557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:32.844530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:33.845489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:34.845622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:35.846157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:36.846424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:37.846686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:38.847597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:39.847688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:40.848584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:41.848773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:42.849605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:43.849712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:44.850679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:45.850768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:46.850857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:47.851956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:48.852025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:49.852190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:50.852906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:51.853052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:52.853985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:53.854079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:54.854682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:55.854869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:56.855223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:57.855383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:58.855516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:31:59.855615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:00.856145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:01.856237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:02.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 13:32:02.465
  STEP: Destroying namespace "container-probe-2864" for this suite. @ 08/05/23 13:32:02.482
• [242.647 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/05/23 13:32:02.491
  Aug  5 13:32:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 13:32:02.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:02.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:02.514
  STEP: create the rc @ 08/05/23 13:32:02.521
  W0805 13:32:02.526754      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0805 13:32:02.857185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:03.857718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:04.858404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:05.858754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:06.859659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:07.859728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/05/23 13:32:08.531
  STEP: wait for the rc to be deleted @ 08/05/23 13:32:08.545
  E0805 13:32:08.859973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:09.562: INFO: 80 pods remaining
  Aug  5 13:32:09.562: INFO: 80 pods has nil DeletionTimestamp
  Aug  5 13:32:09.562: INFO: 
  E0805 13:32:09.860434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:10.564: INFO: 71 pods remaining
  Aug  5 13:32:10.564: INFO: 70 pods has nil DeletionTimestamp
  Aug  5 13:32:10.564: INFO: 
  E0805 13:32:10.862888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:11.563: INFO: 60 pods remaining
  Aug  5 13:32:11.563: INFO: 60 pods has nil DeletionTimestamp
  Aug  5 13:32:11.563: INFO: 
  E0805 13:32:11.862095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:12.575: INFO: 40 pods remaining
  Aug  5 13:32:12.575: INFO: 40 pods has nil DeletionTimestamp
  Aug  5 13:32:12.576: INFO: 
  E0805 13:32:12.862537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:13.558: INFO: 31 pods remaining
  Aug  5 13:32:13.558: INFO: 31 pods has nil DeletionTimestamp
  Aug  5 13:32:13.559: INFO: 
  E0805 13:32:13.863497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:14.558: INFO: 20 pods remaining
  Aug  5 13:32:14.558: INFO: 20 pods has nil DeletionTimestamp
  Aug  5 13:32:14.558: INFO: 
  E0805 13:32:14.864052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/05/23 13:32:15.572
  W0805 13:32:15.576466      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 13:32:15.576: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 13:32:15.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2681" for this suite. @ 08/05/23 13:32:15.587
• [13.104 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/05/23 13:32:15.596
  Aug  5 13:32:15.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-webhook @ 08/05/23 13:32:15.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:15.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:15.618
  STEP: Setting up server cert @ 08/05/23 13:32:15.621
  E0805 13:32:15.864030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/05/23 13:32:16.18
  STEP: Deploying the custom resource conversion webhook pod @ 08/05/23 13:32:16.193
  STEP: Wait for the deployment to be ready @ 08/05/23 13:32:16.206
  Aug  5 13:32:16.213: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0805 13:32:16.864897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:17.865703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:18.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:32:18.866188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:19.866758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:20.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:32:20.867260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:21.867553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:22.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:32:22.867763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:23.868699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:24.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 32, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:32:24.868810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:25.868916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:32:26.23
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:32:26.244
  E0805 13:32:26.869000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:27.245: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug  5 13:32:27.249: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:32:27.870051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:28.870318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/05/23 13:32:29.827
  STEP: v2 custom resource should be converted @ 08/05/23 13:32:29.832
  Aug  5 13:32:29.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0805 13:32:29.871336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-2470" for this suite. @ 08/05/23 13:32:30.408
• [14.823 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/05/23 13:32:30.421
  Aug  5 13:32:30.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:32:30.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:30.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:30.442
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:32:30.445
  E0805 13:32:30.871695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:31.872559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:32.872607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:33.872847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:32:34.472
  Aug  5 13:32:34.476: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-203f93d9-c1e4-44b7-b2a0-8db2f0c85dc1 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:32:34.495
  Aug  5 13:32:34.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3631" for this suite. @ 08/05/23 13:32:34.518
• [4.103 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/05/23 13:32:34.526
  Aug  5 13:32:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:32:34.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:34.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:34.552
  STEP: fetching services @ 08/05/23 13:32:34.554
  Aug  5 13:32:34.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8878" for this suite. @ 08/05/23 13:32:34.561
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/05/23 13:32:34.569
  Aug  5 13:32:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:32:34.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:34.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:34.59
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/05/23 13:32:34.592
  E0805 13:32:34.873743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:35.873900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:36.874022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:37.874109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:32:38.615
  Aug  5 13:32:38.618: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-92d7e81b-6879-4b24-947c-130652657801 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:32:38.626
  Aug  5 13:32:38.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6877" for this suite. @ 08/05/23 13:32:38.649
• [4.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/05/23 13:32:38.657
  Aug  5 13:32:38.657: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:32:38.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:38.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:38.676
  Aug  5 13:32:38.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: creating the pod @ 08/05/23 13:32:38.679
  STEP: submitting the pod to kubernetes @ 08/05/23 13:32:38.679
  E0805 13:32:38.874432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:39.874519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:40.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3767" for this suite. @ 08/05/23 13:32:40.786
• [2.136 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/05/23 13:32:40.794
  Aug  5 13:32:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:32:40.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:40.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:40.816
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/05/23 13:32:40.818
  E0805 13:32:40.875450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:41.875652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:42.876578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:43.876843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:32:44.843
  Aug  5 13:32:44.846: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-82f58c8e-6b4b-439d-824d-7d131df04ac4 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:32:44.854
  Aug  5 13:32:44.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0805 13:32:44.877400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-8404" for this suite. @ 08/05/23 13:32:44.879
• [4.092 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/05/23 13:32:44.886
  Aug  5 13:32:44.886: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 13:32:44.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:44.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:44.906
  Aug  5 13:32:44.908: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug  5 13:32:44.919: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0805 13:32:45.877897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:46.878694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:47.878909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:48.879110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:49.879376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:49.925: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/05/23 13:32:49.925
  Aug  5 13:32:49.925: INFO: Creating deployment "test-rolling-update-deployment"
  Aug  5 13:32:49.932: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug  5 13:32:49.939: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0805 13:32:50.880227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:51.880345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:51.947: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug  5 13:32:51.950: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug  5 13:32:51.962: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8217  7c3e7d9f-f8fe-40c1-afcf-9b0c7dc80e54 37506 1 2023-08-05 13:32:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-05 13:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002549e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-05 13:32:49 +0000 UTC,LastTransitionTime:2023-08-05 13:32:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-05 13:32:51 +0000 UTC,LastTransitionTime:2023-08-05 13:32:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  5 13:32:51.967: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8217  752a5d4e-b34a-4594-9532-eebf488d52ea 37496 1 2023-08-05 13:32:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7c3e7d9f-f8fe-40c1-afcf-9b0c7dc80e54 0xc00378d6c7 0xc00378d6c8}] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c3e7d9f-f8fe-40c1-afcf-9b0c7dc80e54\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:32:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00378d778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 13:32:51.967: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug  5 13:32:51.967: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8217  44055f34-5f70-454f-9014-07222c0daaa4 37505 2 2023-08-05 13:32:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7c3e7d9f-f8fe-40c1-afcf-9b0c7dc80e54 0xc00378d597 0xc00378d598}] [] [{e2e.test Update apps/v1 2023-08-05 13:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c3e7d9f-f8fe-40c1-afcf-9b0c7dc80e54\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-05 13:32:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00378d658 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 13:32:51.971: INFO: Pod "test-rolling-update-deployment-656d657cd8-6xmgt" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-6xmgt test-rolling-update-deployment-656d657cd8- deployment-8217  899758a6-15ac-49f8-a376-d438c6d2dae5 37495 0 2023-08-05 13:32:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 752a5d4e-b34a-4594-9532-eebf488d52ea 0xc00378dbe7 0xc00378dbe8}] [] [{kube-controller-manager Update v1 2023-08-05 13:32:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"752a5d4e-b34a-4594-9532-eebf488d52ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:32:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m99dm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m99dm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:32:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:32:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.244,StartTime:2023-08-05 13:32:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:32:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://eb7e685d91d8b6acc23ada369bfa29def41a6adfddecf9a947acfbf8e6e4b632,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.244,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:32:51.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8217" for this suite. @ 08/05/23 13:32:51.975
• [7.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/05/23 13:32:51.985
  Aug  5 13:32:51.985: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:32:51.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:52.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:52.007
  STEP: creating all guestbook components @ 08/05/23 13:32:52.009
  Aug  5 13:32:52.009: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug  5 13:32:52.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  Aug  5 13:32:52.341: INFO: stderr: ""
  Aug  5 13:32:52.341: INFO: stdout: "service/agnhost-replica created\n"
  Aug  5 13:32:52.341: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug  5 13:32:52.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  Aug  5 13:32:52.682: INFO: stderr: ""
  Aug  5 13:32:52.682: INFO: stdout: "service/agnhost-primary created\n"
  Aug  5 13:32:52.682: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug  5 13:32:52.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  E0805 13:32:52.880864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:52.951: INFO: stderr: ""
  Aug  5 13:32:52.951: INFO: stdout: "service/frontend created\n"
  Aug  5 13:32:52.951: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug  5 13:32:52.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  Aug  5 13:32:53.190: INFO: stderr: ""
  Aug  5 13:32:53.190: INFO: stdout: "deployment.apps/frontend created\n"
  Aug  5 13:32:53.190: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug  5 13:32:53.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  Aug  5 13:32:53.448: INFO: stderr: ""
  Aug  5 13:32:53.448: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug  5 13:32:53.448: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug  5 13:32:53.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 create -f -'
  Aug  5 13:32:53.745: INFO: stderr: ""
  Aug  5 13:32:53.745: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/05/23 13:32:53.745
  Aug  5 13:32:53.745: INFO: Waiting for all frontend pods to be Running.
  E0805 13:32:53.881889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:54.882271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:55.882717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:56.882797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:32:57.883101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:58.797: INFO: Waiting for frontend to serve content.
  Aug  5 13:32:58.809: INFO: Trying to add a new entry to the guestbook.
  Aug  5 13:32:58.821: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/05/23 13:32:58.83
  Aug  5 13:32:58.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  E0805 13:32:58.883459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:32:58.913: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:58.913: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/05/23 13:32:58.913
  Aug  5 13:32:58.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  Aug  5 13:32:59.020: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:59.020: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/05/23 13:32:59.02
  Aug  5 13:32:59.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  Aug  5 13:32:59.100: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:59.100: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/05/23 13:32:59.1
  Aug  5 13:32:59.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  Aug  5 13:32:59.166: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:59.166: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/05/23 13:32:59.166
  Aug  5 13:32:59.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  Aug  5 13:32:59.251: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:59.251: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/05/23 13:32:59.252
  Aug  5 13:32:59.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-2120 delete --grace-period=0 --force -f -'
  Aug  5 13:32:59.354: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  5 13:32:59.354: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug  5 13:32:59.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2120" for this suite. @ 08/05/23 13:32:59.359
• [7.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/05/23 13:32:59.366
  Aug  5 13:32:59.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pods @ 08/05/23 13:32:59.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:32:59.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:32:59.388
  STEP: Create a pod @ 08/05/23 13:32:59.39
  E0805 13:32:59.884352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:00.884779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/05/23 13:33:01.412
  Aug  5 13:33:01.420: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug  5 13:33:01.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6109" for this suite. @ 08/05/23 13:33:01.424
• [2.065 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/05/23 13:33:01.431
  Aug  5 13:33:01.431: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:33:01.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:01.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:01.457
  STEP: Setting up server cert @ 08/05/23 13:33:01.483
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:33:01.767
  STEP: Deploying the webhook pod @ 08/05/23 13:33:01.777
  STEP: Wait for the deployment to be ready @ 08/05/23 13:33:01.792
  Aug  5 13:33:01.800: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:33:01.885097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:02.885285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:33:03.811
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:33:03.823
  E0805 13:33:03.885882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:04.823: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/05/23 13:33:04.827
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/05/23 13:33:04.847
  STEP: Creating a configMap that should not be mutated @ 08/05/23 13:33:04.854
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/05/23 13:33:04.866
  STEP: Creating a configMap that should be mutated @ 08/05/23 13:33:04.873
  E0805 13:33:04.886580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:04.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9818" for this suite. @ 08/05/23 13:33:04.945
  STEP: Destroying namespace "webhook-markers-7594" for this suite. @ 08/05/23 13:33:04.954
• [3.531 seconds]
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/05/23 13:33:04.962
  Aug  5 13:33:04.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:33:04.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:04.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:04.984
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-7198 @ 08/05/23 13:33:04.986
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/05/23 13:33:05.004
  STEP: creating service externalsvc in namespace services-7198 @ 08/05/23 13:33:05.004
  STEP: creating replication controller externalsvc in namespace services-7198 @ 08/05/23 13:33:05.015
  I0805 13:33:05.023378      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7198, replica count: 2
  E0805 13:33:05.886993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:06.887150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:07.887357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:33:08.074633      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/05/23 13:33:08.079
  Aug  5 13:33:08.099: INFO: Creating new exec pod
  E0805 13:33:08.889766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:09.889802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:10.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7198 exec execpodd82kd -- /bin/sh -x -c nslookup nodeport-service.services-7198.svc.cluster.local'
  Aug  5 13:33:10.307: INFO: stderr: "+ nslookup nodeport-service.services-7198.svc.cluster.local\n"
  Aug  5 13:33:10.307: INFO: stdout: "Server:\t\t10.152.183.228\nAddress:\t10.152.183.228#53\n\nnodeport-service.services-7198.svc.cluster.local\tcanonical name = externalsvc.services-7198.svc.cluster.local.\nName:\texternalsvc.services-7198.svc.cluster.local\nAddress: 10.152.183.183\n\n"
  Aug  5 13:33:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7198, will wait for the garbage collector to delete the pods @ 08/05/23 13:33:10.312
  Aug  5 13:33:10.374: INFO: Deleting ReplicationController externalsvc took: 7.269877ms
  Aug  5 13:33:10.475: INFO: Terminating ReplicationController externalsvc pods took: 100.942457ms
  E0805 13:33:10.890331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:11.891108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:12.793: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-7198" for this suite. @ 08/05/23 13:33:12.813
• [7.860 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/05/23 13:33:12.824
  Aug  5 13:33:12.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:33:12.825
  E0805 13:33:12.891504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:12.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:12.894
  STEP: Creating secret with name secret-test-map-17c4c528-889c-4cfb-a2a8-205d058e4e46 @ 08/05/23 13:33:12.896
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:33:12.902
  E0805 13:33:13.891610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:14.891702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:15.891884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:16.891948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:33:16.925
  Aug  5 13:33:16.929: INFO: Trying to get logs from node ip-172-31-35-140 pod pod-secrets-a0d1fc00-79cb-4194-a2de-ff173b35e33b container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:33:16.952
  Aug  5 13:33:16.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2214" for this suite. @ 08/05/23 13:33:16.976
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/05/23 13:33:16.984
  Aug  5 13:33:16.984: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename pod-network-test @ 08/05/23 13:33:16.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:17.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:17.053
  STEP: Performing setup for networking test in namespace pod-network-test-6622 @ 08/05/23 13:33:17.055
  STEP: creating a selector @ 08/05/23 13:33:17.055
  STEP: Creating the service pods in kubernetes @ 08/05/23 13:33:17.055
  Aug  5 13:33:17.055: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0805 13:33:17.892106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:18.892261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:19.892287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:20.893205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:21.893561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:22.893660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:23.893757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:24.893825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:25.894506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:26.894604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:27.894794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:28.894892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/05/23 13:33:29.136
  E0805 13:33:29.895317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:30.896173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:31.170: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug  5 13:33:31.170: INFO: Going to poll 192.168.122.86 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug  5 13:33:31.173: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.122.86:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6622 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:33:31.173: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:33:31.174: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:33:31.174: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6622/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.122.86%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  5 13:33:31.237: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug  5 13:33:31.237: INFO: Going to poll 192.168.166.206 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug  5 13:33:31.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.166.206:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6622 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:33:31.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:33:31.242: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:33:31.242: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6622/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.166.206%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  5 13:33:31.305: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug  5 13:33:31.305: INFO: Going to poll 192.168.52.169 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug  5 13:33:31.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.52.169:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6622 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:33:31.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:33:31.318: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:33:31.318: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6622/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.52.169%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  5 13:33:31.379: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug  5 13:33:31.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6622" for this suite. @ 08/05/23 13:33:31.384
• [14.409 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/05/23 13:33:31.394
  Aug  5 13:33:31.394: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:33:31.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:31.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:31.415
  STEP: Creating configMap that has name configmap-test-emptyKey-7ffff134-4e03-42b8-ab18-6f1105d81b31 @ 08/05/23 13:33:31.417
  Aug  5 13:33:31.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8875" for this suite. @ 08/05/23 13:33:31.423
• [0.037 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/05/23 13:33:31.432
  Aug  5 13:33:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:33:31.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:31.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:31.454
  Aug  5 13:33:31.457: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:33:31.896287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/05/23 13:33:32.765
  Aug  5 13:33:32.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3536 --namespace=crd-publish-openapi-3536 create -f -'
  E0805 13:33:32.897075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:33.380: INFO: stderr: ""
  Aug  5 13:33:33.380: INFO: stdout: "e2e-test-crd-publish-openapi-7029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug  5 13:33:33.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3536 --namespace=crd-publish-openapi-3536 delete e2e-test-crd-publish-openapi-7029-crds test-cr'
  Aug  5 13:33:33.449: INFO: stderr: ""
  Aug  5 13:33:33.449: INFO: stdout: "e2e-test-crd-publish-openapi-7029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug  5 13:33:33.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3536 --namespace=crd-publish-openapi-3536 apply -f -'
  Aug  5 13:33:33.649: INFO: stderr: ""
  Aug  5 13:33:33.649: INFO: stdout: "e2e-test-crd-publish-openapi-7029-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug  5 13:33:33.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3536 --namespace=crd-publish-openapi-3536 delete e2e-test-crd-publish-openapi-7029-crds test-cr'
  Aug  5 13:33:33.734: INFO: stderr: ""
  Aug  5 13:33:33.734: INFO: stdout: "e2e-test-crd-publish-openapi-7029-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/05/23 13:33:33.734
  Aug  5 13:33:33.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3536 explain e2e-test-crd-publish-openapi-7029-crds'
  E0805 13:33:33.897990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:33.923: INFO: stderr: ""
  Aug  5 13:33:33.923: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-7029-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0805 13:33:34.898126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3536" for this suite. @ 08/05/23 13:33:35.305
• [3.882 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/05/23 13:33:35.314
  Aug  5 13:33:35.314: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename proxy @ 08/05/23 13:33:35.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:35.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:35.334
  STEP: starting an echo server on multiple ports @ 08/05/23 13:33:35.349
  STEP: creating replication controller proxy-service-xdlrn in namespace proxy-8026 @ 08/05/23 13:33:35.349
  I0805 13:33:35.359584      19 runners.go:194] Created replication controller with name: proxy-service-xdlrn, namespace: proxy-8026, replica count: 1
  E0805 13:33:35.898488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:33:36.410383      19 runners.go:194] proxy-service-xdlrn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0805 13:33:36.898468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:33:37.411283      19 runners.go:194] proxy-service-xdlrn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0805 13:33:37.899450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:33:38.412104      19 runners.go:194] proxy-service-xdlrn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:33:38.416: INFO: setup took 3.07938933s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/05/23 13:33:38.416
  Aug  5 13:33:38.430: INFO: (0) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 13.438927ms)
  Aug  5 13:33:38.430: INFO: (0) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 13.223244ms)
  Aug  5 13:33:38.430: INFO: (0) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 13.585432ms)
  Aug  5 13:33:38.432: INFO: (0) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 15.780993ms)
  Aug  5 13:33:38.433: INFO: (0) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 16.164003ms)
  Aug  5 13:33:38.433: INFO: (0) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 15.747539ms)
  Aug  5 13:33:38.433: INFO: (0) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 15.970777ms)
  Aug  5 13:33:38.433: INFO: (0) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 16.523607ms)
  Aug  5 13:33:38.433: INFO: (0) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 16.960804ms)
  Aug  5 13:33:38.434: INFO: (0) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 17.134415ms)
  Aug  5 13:33:38.434: INFO: (0) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 16.741274ms)
  Aug  5 13:33:38.435: INFO: (0) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 18.15669ms)
  Aug  5 13:33:38.435: INFO: (0) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 17.921062ms)
  Aug  5 13:33:38.435: INFO: (0) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 18.771434ms)
  Aug  5 13:33:38.435: INFO: (0) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 18.278609ms)
  Aug  5 13:33:38.435: INFO: (0) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 18.044205ms)
  Aug  5 13:33:38.440: INFO: (1) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 4.548363ms)
  Aug  5 13:33:38.440: INFO: (1) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 4.811178ms)
  Aug  5 13:33:38.443: INFO: (1) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 6.826129ms)
  Aug  5 13:33:38.443: INFO: (1) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 7.122475ms)
  Aug  5 13:33:38.443: INFO: (1) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 7.871943ms)
  Aug  5 13:33:38.444: INFO: (1) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 7.70434ms)
  Aug  5 13:33:38.444: INFO: (1) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.585303ms)
  Aug  5 13:33:38.445: INFO: (1) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 9.114385ms)
  Aug  5 13:33:38.445: INFO: (1) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 9.736154ms)
  Aug  5 13:33:38.445: INFO: (1) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.552028ms)
  Aug  5 13:33:38.446: INFO: (1) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.128143ms)
  Aug  5 13:33:38.446: INFO: (1) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 10.329426ms)
  Aug  5 13:33:38.446: INFO: (1) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.77844ms)
  Aug  5 13:33:38.446: INFO: (1) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 10.734525ms)
  Aug  5 13:33:38.447: INFO: (1) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 10.809446ms)
  Aug  5 13:33:38.447: INFO: (1) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.892887ms)
  Aug  5 13:33:38.451: INFO: (2) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 4.186341ms)
  Aug  5 13:33:38.453: INFO: (2) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 6.304638ms)
  Aug  5 13:33:38.455: INFO: (2) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.299945ms)
  Aug  5 13:33:38.455: INFO: (2) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 8.120304ms)
  Aug  5 13:33:38.456: INFO: (2) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 8.293395ms)
  Aug  5 13:33:38.456: INFO: (2) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 8.824415ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 10.194922ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.581816ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.628274ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 10.569881ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 10.530788ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 10.633821ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 11.126768ms)
  Aug  5 13:33:38.458: INFO: (2) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 11.058563ms)
  Aug  5 13:33:38.459: INFO: (2) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 11.590826ms)
  Aug  5 13:33:38.459: INFO: (2) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 11.853518ms)
  Aug  5 13:33:38.464: INFO: (3) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 4.984094ms)
  Aug  5 13:33:38.464: INFO: (3) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 4.853316ms)
  Aug  5 13:33:38.464: INFO: (3) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.210291ms)
  Aug  5 13:33:38.466: INFO: (3) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 6.505183ms)
  Aug  5 13:33:38.466: INFO: (3) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 6.776424ms)
  Aug  5 13:33:38.468: INFO: (3) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.990388ms)
  Aug  5 13:33:38.468: INFO: (3) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.245554ms)
  Aug  5 13:33:38.468: INFO: (3) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 9.009735ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.258521ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 9.992473ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 10.443692ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.518367ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.526768ms)
  Aug  5 13:33:38.470: INFO: (3) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.83244ms)
  Aug  5 13:33:38.472: INFO: (3) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 12.139351ms)
  Aug  5 13:33:38.472: INFO: (3) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 12.277526ms)
  Aug  5 13:33:38.479: INFO: (4) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 6.821111ms)
  Aug  5 13:33:38.480: INFO: (4) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 7.775587ms)
  Aug  5 13:33:38.481: INFO: (4) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 8.695833ms)
  Aug  5 13:33:38.482: INFO: (4) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 9.78745ms)
  Aug  5 13:33:38.482: INFO: (4) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 9.724235ms)
  Aug  5 13:33:38.482: INFO: (4) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 9.915286ms)
  Aug  5 13:33:38.482: INFO: (4) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 10.179766ms)
  Aug  5 13:33:38.482: INFO: (4) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 10.322977ms)
  Aug  5 13:33:38.483: INFO: (4) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.196897ms)
  Aug  5 13:33:38.483: INFO: (4) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 10.215717ms)
  Aug  5 13:33:38.483: INFO: (4) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.843828ms)
  Aug  5 13:33:38.483: INFO: (4) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.994049ms)
  Aug  5 13:33:38.484: INFO: (4) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 11.417377ms)
  Aug  5 13:33:38.484: INFO: (4) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 11.45269ms)
  Aug  5 13:33:38.484: INFO: (4) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 11.591709ms)
  Aug  5 13:33:38.485: INFO: (4) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 12.569775ms)
  Aug  5 13:33:38.490: INFO: (5) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 4.838432ms)
  Aug  5 13:33:38.490: INFO: (5) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 4.96606ms)
  Aug  5 13:33:38.491: INFO: (5) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.295229ms)
  Aug  5 13:33:38.491: INFO: (5) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 6.097424ms)
  Aug  5 13:33:38.492: INFO: (5) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 6.566323ms)
  Aug  5 13:33:38.492: INFO: (5) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 6.715129ms)
  Aug  5 13:33:38.493: INFO: (5) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 7.721988ms)
  Aug  5 13:33:38.493: INFO: (5) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 7.857074ms)
  Aug  5 13:33:38.494: INFO: (5) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 8.180447ms)
  Aug  5 13:33:38.494: INFO: (5) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 8.953378ms)
  Aug  5 13:33:38.494: INFO: (5) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 8.728594ms)
  Aug  5 13:33:38.495: INFO: (5) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 9.112612ms)
  Aug  5 13:33:38.495: INFO: (5) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 9.224851ms)
  Aug  5 13:33:38.495: INFO: (5) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 9.953789ms)
  Aug  5 13:33:38.496: INFO: (5) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 10.387429ms)
  Aug  5 13:33:38.496: INFO: (5) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.616293ms)
  Aug  5 13:33:38.501: INFO: (6) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 4.288467ms)
  Aug  5 13:33:38.501: INFO: (6) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 4.362695ms)
  Aug  5 13:33:38.502: INFO: (6) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.800169ms)
  Aug  5 13:33:38.503: INFO: (6) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 5.818904ms)
  Aug  5 13:33:38.503: INFO: (6) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.887476ms)
  Aug  5 13:33:38.504: INFO: (6) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.489194ms)
  Aug  5 13:33:38.504: INFO: (6) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 7.823409ms)
  Aug  5 13:33:38.505: INFO: (6) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.633305ms)
  Aug  5 13:33:38.506: INFO: (6) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 8.987754ms)
  Aug  5 13:33:38.506: INFO: (6) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 9.364111ms)
  Aug  5 13:33:38.506: INFO: (6) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.621894ms)
  Aug  5 13:33:38.507: INFO: (6) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 9.954295ms)
  Aug  5 13:33:38.507: INFO: (6) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.125567ms)
  Aug  5 13:33:38.507: INFO: (6) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.035346ms)
  Aug  5 13:33:38.507: INFO: (6) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.420566ms)
  Aug  5 13:33:38.507: INFO: (6) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 10.304129ms)
  Aug  5 13:33:38.513: INFO: (7) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 5.349105ms)
  Aug  5 13:33:38.513: INFO: (7) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 5.224667ms)
  Aug  5 13:33:38.514: INFO: (7) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.981907ms)
  Aug  5 13:33:38.515: INFO: (7) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 7.58756ms)
  Aug  5 13:33:38.515: INFO: (7) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.31982ms)
  Aug  5 13:33:38.516: INFO: (7) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 8.604779ms)
  Aug  5 13:33:38.516: INFO: (7) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.348927ms)
  Aug  5 13:33:38.516: INFO: (7) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.433164ms)
  Aug  5 13:33:38.517: INFO: (7) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 9.277332ms)
  Aug  5 13:33:38.517: INFO: (7) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 9.168218ms)
  Aug  5 13:33:38.518: INFO: (7) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.925284ms)
  Aug  5 13:33:38.518: INFO: (7) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 9.885933ms)
  Aug  5 13:33:38.518: INFO: (7) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 10.589576ms)
  Aug  5 13:33:38.518: INFO: (7) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.769437ms)
  Aug  5 13:33:38.518: INFO: (7) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.713197ms)
  Aug  5 13:33:38.519: INFO: (7) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.841518ms)
  Aug  5 13:33:38.523: INFO: (8) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 4.51586ms)
  Aug  5 13:33:38.525: INFO: (8) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 6.251141ms)
  Aug  5 13:33:38.526: INFO: (8) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.042114ms)
  Aug  5 13:33:38.526: INFO: (8) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 7.450224ms)
  Aug  5 13:33:38.527: INFO: (8) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 8.094427ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 9.987391ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 10.179391ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 10.38919ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 10.131674ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 9.970528ms)
  Aug  5 13:33:38.529: INFO: (8) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.607287ms)
  Aug  5 13:33:38.530: INFO: (8) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 11.13402ms)
  Aug  5 13:33:38.530: INFO: (8) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 11.03152ms)
  Aug  5 13:33:38.531: INFO: (8) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 11.399839ms)
  Aug  5 13:33:38.531: INFO: (8) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 11.54152ms)
  Aug  5 13:33:38.531: INFO: (8) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 12.046116ms)
  Aug  5 13:33:38.535: INFO: (9) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 3.957341ms)
  Aug  5 13:33:38.536: INFO: (9) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 4.8121ms)
  Aug  5 13:33:38.537: INFO: (9) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 5.161689ms)
  Aug  5 13:33:38.537: INFO: (9) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.741781ms)
  Aug  5 13:33:38.539: INFO: (9) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.070323ms)
  Aug  5 13:33:38.539: INFO: (9) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 7.619886ms)
  Aug  5 13:33:38.540: INFO: (9) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.496557ms)
  Aug  5 13:33:38.541: INFO: (9) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.467069ms)
  Aug  5 13:33:38.541: INFO: (9) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 8.975852ms)
  Aug  5 13:33:38.541: INFO: (9) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.624661ms)
  Aug  5 13:33:38.541: INFO: (9) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 9.554459ms)
  Aug  5 13:33:38.542: INFO: (9) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 9.801983ms)
  Aug  5 13:33:38.542: INFO: (9) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 9.507408ms)
  Aug  5 13:33:38.542: INFO: (9) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 9.981936ms)
  Aug  5 13:33:38.542: INFO: (9) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.026434ms)
  Aug  5 13:33:38.542: INFO: (9) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.325219ms)
  Aug  5 13:33:38.547: INFO: (10) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 4.262305ms)
  Aug  5 13:33:38.547: INFO: (10) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 4.635275ms)
  Aug  5 13:33:38.548: INFO: (10) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 5.122394ms)
  Aug  5 13:33:38.548: INFO: (10) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 5.785425ms)
  Aug  5 13:33:38.549: INFO: (10) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 6.34467ms)
  Aug  5 13:33:38.549: INFO: (10) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 6.449084ms)
  Aug  5 13:33:38.550: INFO: (10) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 6.826314ms)
  Aug  5 13:33:38.550: INFO: (10) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.690522ms)
  Aug  5 13:33:38.551: INFO: (10) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 7.856338ms)
  Aug  5 13:33:38.552: INFO: (10) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 8.957096ms)
  Aug  5 13:33:38.552: INFO: (10) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.463207ms)
  Aug  5 13:33:38.552: INFO: (10) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 9.687163ms)
  Aug  5 13:33:38.552: INFO: (10) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 9.465108ms)
  Aug  5 13:33:38.552: INFO: (10) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 9.731394ms)
  Aug  5 13:33:38.553: INFO: (10) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.306661ms)
  Aug  5 13:33:38.554: INFO: (10) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 11.151457ms)
  Aug  5 13:33:38.559: INFO: (11) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.102844ms)
  Aug  5 13:33:38.561: INFO: (11) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 7.007686ms)
  Aug  5 13:33:38.561: INFO: (11) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 6.839196ms)
  Aug  5 13:33:38.561: INFO: (11) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.370027ms)
  Aug  5 13:33:38.561: INFO: (11) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 7.577106ms)
  Aug  5 13:33:38.564: INFO: (11) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 9.788016ms)
  Aug  5 13:33:38.564: INFO: (11) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 10.295782ms)
  Aug  5 13:33:38.564: INFO: (11) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 10.166795ms)
  Aug  5 13:33:38.564: INFO: (11) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 10.109718ms)
  Aug  5 13:33:38.564: INFO: (11) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.658777ms)
  Aug  5 13:33:38.565: INFO: (11) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 11.079776ms)
  Aug  5 13:33:38.565: INFO: (11) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 11.41869ms)
  Aug  5 13:33:38.565: INFO: (11) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 11.516306ms)
  Aug  5 13:33:38.566: INFO: (11) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 11.668323ms)
  Aug  5 13:33:38.566: INFO: (11) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 12.10435ms)
  Aug  5 13:33:38.566: INFO: (11) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 12.133391ms)
  Aug  5 13:33:38.570: INFO: (12) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 4.305485ms)
  Aug  5 13:33:38.573: INFO: (12) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 6.654897ms)
  Aug  5 13:33:38.573: INFO: (12) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 6.524883ms)
  Aug  5 13:33:38.575: INFO: (12) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 8.002077ms)
  Aug  5 13:33:38.575: INFO: (12) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.077749ms)
  Aug  5 13:33:38.576: INFO: (12) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 8.799379ms)
  Aug  5 13:33:38.576: INFO: (12) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 9.297628ms)
  Aug  5 13:33:38.577: INFO: (12) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 9.985766ms)
  Aug  5 13:33:38.577: INFO: (12) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 10.188621ms)
  Aug  5 13:33:38.577: INFO: (12) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.804204ms)
  Aug  5 13:33:38.578: INFO: (12) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 11.523678ms)
  Aug  5 13:33:38.578: INFO: (12) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 11.484684ms)
  Aug  5 13:33:38.578: INFO: (12) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 11.583076ms)
  Aug  5 13:33:38.578: INFO: (12) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 11.634954ms)
  Aug  5 13:33:38.579: INFO: (12) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 13.01637ms)
  Aug  5 13:33:38.579: INFO: (12) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 12.641437ms)
  Aug  5 13:33:38.586: INFO: (13) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 5.803626ms)
  Aug  5 13:33:38.587: INFO: (13) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 6.58782ms)
  Aug  5 13:33:38.587: INFO: (13) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 6.941078ms)
  Aug  5 13:33:38.587: INFO: (13) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.153815ms)
  Aug  5 13:33:38.588: INFO: (13) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 8.217784ms)
  Aug  5 13:33:38.589: INFO: (13) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.995284ms)
  Aug  5 13:33:38.590: INFO: (13) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 9.776383ms)
  Aug  5 13:33:38.590: INFO: (13) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 10.101629ms)
  Aug  5 13:33:38.590: INFO: (13) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.288046ms)
  Aug  5 13:33:38.591: INFO: (13) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 11.134429ms)
  Aug  5 13:33:38.591: INFO: (13) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 11.808377ms)
  Aug  5 13:33:38.592: INFO: (13) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 11.993748ms)
  Aug  5 13:33:38.592: INFO: (13) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 11.781477ms)
  Aug  5 13:33:38.591: INFO: (13) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 11.909783ms)
  Aug  5 13:33:38.592: INFO: (13) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 12.759043ms)
  Aug  5 13:33:38.593: INFO: (13) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 13.073921ms)
  Aug  5 13:33:38.598: INFO: (14) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.005968ms)
  Aug  5 13:33:38.598: INFO: (14) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 5.055498ms)
  Aug  5 13:33:38.599: INFO: (14) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.514447ms)
  Aug  5 13:33:38.600: INFO: (14) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 6.526366ms)
  Aug  5 13:33:38.600: INFO: (14) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 6.465632ms)
  Aug  5 13:33:38.600: INFO: (14) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.030072ms)
  Aug  5 13:33:38.601: INFO: (14) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 7.888862ms)
  Aug  5 13:33:38.602: INFO: (14) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 8.090004ms)
  Aug  5 13:33:38.602: INFO: (14) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 9.070074ms)
  Aug  5 13:33:38.602: INFO: (14) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 8.789882ms)
  Aug  5 13:33:38.603: INFO: (14) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 9.603013ms)
  Aug  5 13:33:38.603: INFO: (14) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 9.627161ms)
  Aug  5 13:33:38.603: INFO: (14) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.196256ms)
  Aug  5 13:33:38.603: INFO: (14) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.522263ms)
  Aug  5 13:33:38.604: INFO: (14) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.080235ms)
  Aug  5 13:33:38.605: INFO: (14) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 11.336547ms)
  Aug  5 13:33:38.609: INFO: (15) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 3.804842ms)
  Aug  5 13:33:38.609: INFO: (15) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 4.098781ms)
  Aug  5 13:33:38.611: INFO: (15) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 6.322155ms)
  Aug  5 13:33:38.611: INFO: (15) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 6.239176ms)
  Aug  5 13:33:38.612: INFO: (15) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 7.161951ms)
  Aug  5 13:33:38.612: INFO: (15) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 7.470022ms)
  Aug  5 13:33:38.613: INFO: (15) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 7.680134ms)
  Aug  5 13:33:38.614: INFO: (15) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 9.040894ms)
  Aug  5 13:33:38.614: INFO: (15) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 9.164255ms)
  Aug  5 13:33:38.614: INFO: (15) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 9.168996ms)
  Aug  5 13:33:38.615: INFO: (15) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 9.558023ms)
  Aug  5 13:33:38.616: INFO: (15) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 10.756781ms)
  Aug  5 13:33:38.616: INFO: (15) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.581818ms)
  Aug  5 13:33:38.616: INFO: (15) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.688135ms)
  Aug  5 13:33:38.616: INFO: (15) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 10.954299ms)
  Aug  5 13:33:38.616: INFO: (15) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 11.215191ms)
  Aug  5 13:33:38.621: INFO: (16) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 4.44207ms)
  Aug  5 13:33:38.622: INFO: (16) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.17725ms)
  Aug  5 13:33:38.622: INFO: (16) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 5.153491ms)
  Aug  5 13:33:38.623: INFO: (16) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 6.288967ms)
  Aug  5 13:33:38.623: INFO: (16) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 6.316207ms)
  Aug  5 13:33:38.624: INFO: (16) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.177259ms)
  Aug  5 13:33:38.624: INFO: (16) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 7.40876ms)
  Aug  5 13:33:38.624: INFO: (16) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 7.26766ms)
  Aug  5 13:33:38.625: INFO: (16) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 8.337148ms)
  Aug  5 13:33:38.625: INFO: (16) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 8.292923ms)
  Aug  5 13:33:38.626: INFO: (16) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 9.461467ms)
  Aug  5 13:33:38.626: INFO: (16) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 10.039436ms)
  Aug  5 13:33:38.626: INFO: (16) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 9.692661ms)
  Aug  5 13:33:38.627: INFO: (16) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.236811ms)
  Aug  5 13:33:38.627: INFO: (16) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 10.029087ms)
  Aug  5 13:33:38.627: INFO: (16) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 10.383456ms)
  Aug  5 13:33:38.631: INFO: (17) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 4.270872ms)
  Aug  5 13:33:38.632: INFO: (17) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 4.468265ms)
  Aug  5 13:33:38.632: INFO: (17) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 5.14553ms)
  Aug  5 13:33:38.633: INFO: (17) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 5.449484ms)
  Aug  5 13:33:38.634: INFO: (17) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 6.254334ms)
  Aug  5 13:33:38.634: INFO: (17) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 6.915452ms)
  Aug  5 13:33:38.635: INFO: (17) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 7.229134ms)
  Aug  5 13:33:38.636: INFO: (17) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.075582ms)
  Aug  5 13:33:38.636: INFO: (17) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.1407ms)
  Aug  5 13:33:38.636: INFO: (17) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.597763ms)
  Aug  5 13:33:38.637: INFO: (17) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 9.334582ms)
  Aug  5 13:33:38.637: INFO: (17) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 9.320536ms)
  Aug  5 13:33:38.637: INFO: (17) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 9.770227ms)
  Aug  5 13:33:38.638: INFO: (17) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 10.184615ms)
  Aug  5 13:33:38.638: INFO: (17) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.564997ms)
  Aug  5 13:33:38.638: INFO: (17) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.735654ms)
  Aug  5 13:33:38.642: INFO: (18) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 4.070685ms)
  Aug  5 13:33:38.643: INFO: (18) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 4.207611ms)
  Aug  5 13:33:38.645: INFO: (18) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 5.962899ms)
  Aug  5 13:33:38.645: INFO: (18) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 6.623331ms)
  Aug  5 13:33:38.646: INFO: (18) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 7.592637ms)
  Aug  5 13:33:38.647: INFO: (18) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 7.911317ms)
  Aug  5 13:33:38.647: INFO: (18) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 8.762867ms)
  Aug  5 13:33:38.649: INFO: (18) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.140574ms)
  Aug  5 13:33:38.649: INFO: (18) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 10.306361ms)
  Aug  5 13:33:38.649: INFO: (18) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 10.352392ms)
  Aug  5 13:33:38.649: INFO: (18) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 10.462098ms)
  Aug  5 13:33:38.650: INFO: (18) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 11.75098ms)
  Aug  5 13:33:38.650: INFO: (18) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 11.459303ms)
  Aug  5 13:33:38.651: INFO: (18) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 12.519984ms)
  Aug  5 13:33:38.653: INFO: (18) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 13.822352ms)
  Aug  5 13:33:38.653: INFO: (18) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 14.192289ms)
  Aug  5 13:33:38.658: INFO: (19) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:462/proxy/: tls qux (200; 4.994085ms)
  Aug  5 13:33:38.659: INFO: (19) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">... (200; 6.065786ms)
  Aug  5 13:33:38.659: INFO: (19) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:443/proxy/tlsrewritem... (200; 5.638553ms)
  Aug  5 13:33:38.659: INFO: (19) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 5.791672ms)
  Aug  5 13:33:38.659: INFO: (19) /api/v1/namespaces/proxy-8026/pods/https:proxy-service-xdlrn-g28xv:460/proxy/: tls baz (200; 5.152929ms)
  Aug  5 13:33:38.661: INFO: (19) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv/proxy/rewriteme">test</a> (200; 6.414738ms)
  Aug  5 13:33:38.661: INFO: (19) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 6.697877ms)
  Aug  5 13:33:38.661: INFO: (19) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:160/proxy/: foo (200; 7.872112ms)
  Aug  5 13:33:38.662: INFO: (19) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname2/proxy/: bar (200; 8.58596ms)
  Aug  5 13:33:38.662: INFO: (19) /api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/: <a href="/api/v1/namespaces/proxy-8026/pods/proxy-service-xdlrn-g28xv:1080/proxy/rewriteme">test<... (200; 8.404179ms)
  Aug  5 13:33:38.662: INFO: (19) /api/v1/namespaces/proxy-8026/pods/http:proxy-service-xdlrn-g28xv:162/proxy/: bar (200; 8.140418ms)
  Aug  5 13:33:38.663: INFO: (19) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname1/proxy/: tls baz (200; 10.130179ms)
  Aug  5 13:33:38.663: INFO: (19) /api/v1/namespaces/proxy-8026/services/https:proxy-service-xdlrn:tlsportname2/proxy/: tls qux (200; 9.286017ms)
  Aug  5 13:33:38.663: INFO: (19) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname1/proxy/: foo (200; 8.997545ms)
  Aug  5 13:33:38.664: INFO: (19) /api/v1/namespaces/proxy-8026/services/proxy-service-xdlrn:portname1/proxy/: foo (200; 9.938417ms)
  Aug  5 13:33:38.664: INFO: (19) /api/v1/namespaces/proxy-8026/services/http:proxy-service-xdlrn:portname2/proxy/: bar (200; 10.549759ms)
  Aug  5 13:33:38.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-xdlrn in namespace proxy-8026, will wait for the garbage collector to delete the pods @ 08/05/23 13:33:38.669
  Aug  5 13:33:38.730: INFO: Deleting ReplicationController proxy-service-xdlrn took: 7.835154ms
  Aug  5 13:33:38.831: INFO: Terminating ReplicationController proxy-service-xdlrn pods took: 100.749084ms
  E0805 13:33:38.899532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:39.899913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-8026" for this suite. @ 08/05/23 13:33:40.731
• [5.426 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/05/23 13:33:40.741
  Aug  5 13:33:40.741: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:33:40.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:40.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:40.761
  STEP: Setting up server cert @ 08/05/23 13:33:40.788
  E0805 13:33:40.900044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:33:41.21
  STEP: Deploying the webhook pod @ 08/05/23 13:33:41.219
  STEP: Wait for the deployment to be ready @ 08/05/23 13:33:41.232
  Aug  5 13:33:41.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:33:41.900906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:42.901064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:33:43.252
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:33:43.267
  E0805 13:33:43.901179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:44.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/05/23 13:33:44.34
  STEP: Creating a configMap that should be mutated @ 08/05/23 13:33:44.353
  STEP: Deleting the collection of validation webhooks @ 08/05/23 13:33:44.387
  STEP: Creating a configMap that should not be mutated @ 08/05/23 13:33:44.446
  Aug  5 13:33:44.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2522" for this suite. @ 08/05/23 13:33:44.504
  STEP: Destroying namespace "webhook-markers-3373" for this suite. @ 08/05/23 13:33:44.514
• [3.780 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/05/23 13:33:44.522
  Aug  5 13:33:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replicaset @ 08/05/23 13:33:44.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:44.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:44.546
  STEP: Create a ReplicaSet @ 08/05/23 13:33:44.548
  STEP: Verify that the required pods have come up @ 08/05/23 13:33:44.556
  Aug  5 13:33:44.559: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0805 13:33:44.902137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:45.902523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:46.902715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:47.902790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:48.902971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:49.564: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/05/23 13:33:49.564
  Aug  5 13:33:49.567: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/05/23 13:33:49.567
  STEP: DeleteCollection of the ReplicaSets @ 08/05/23 13:33:49.571
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/05/23 13:33:49.581
  Aug  5 13:33:49.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2583" for this suite. @ 08/05/23 13:33:49.59
• [5.084 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 08/05/23 13:33:49.607
  Aug  5 13:33:49.607: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename aggregator @ 08/05/23 13:33:49.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:33:49.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:33:49.634
  Aug  5 13:33:49.639: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Registering the sample API server. @ 08/05/23 13:33:49.64
  E0805 13:33:49.903900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:50.297: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug  5 13:33:50.325: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0805 13:33:50.904660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:51.904742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:52.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:33:52.904803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:53.905024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:54.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:33:54.905693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:55.905787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:56.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:33:56.906286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:57.906376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:33:58.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:33:58.906468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:33:59.906541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:00.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:00.907202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:01.908195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:02.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:02.909129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:03.909314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:04.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:04.909573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:05.909616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:06.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:06.910241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:07.910342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:08.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:08.910822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:09.910969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:10.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:10.911057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:11.912058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:12.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 5, 13, 33, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0805 13:34:12.912414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:13.912597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:14.497: INFO: Waited 112.821574ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/05/23 13:34:14.533
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/05/23 13:34:14.538
  STEP: List APIServices @ 08/05/23 13:34:14.545
  Aug  5 13:34:14.551: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/05/23 13:34:14.551
  Aug  5 13:34:14.565: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/05/23 13:34:14.565
  Aug  5 13:34:14.576: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 5, 13, 34, 14, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/05/23 13:34:14.576
  Aug  5 13:34:14.580: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-05 13:34:14 +0000 UTC Passed all checks passed}
  Aug  5 13:34:14.580: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 13:34:14.580: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/05/23 13:34:14.58
  Aug  5 13:34:14.592: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-344791311" @ 08/05/23 13:34:14.592
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/05/23 13:34:14.601
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/05/23 13:34:14.608
  STEP: Patch APIService Status @ 08/05/23 13:34:14.612
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/05/23 13:34:14.618
  Aug  5 13:34:14.622: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-05 13:34:14 +0000 UTC Passed all checks passed}
  Aug  5 13:34:14.622: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 13:34:14.622: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug  5 13:34:14.622: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/05/23 13:34:14.622
  STEP: Confirm that the generated APIService has been deleted @ 08/05/23 13:34:14.627
  Aug  5 13:34:14.627: INFO: Requesting list of APIServices to confirm quantity
  Aug  5 13:34:14.631: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug  5 13:34:14.632: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug  5 13:34:14.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8701" for this suite. @ 08/05/23 13:34:14.761
• [25.161 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/05/23 13:34:14.769
  Aug  5 13:34:14.769: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename deployment @ 08/05/23 13:34:14.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:34:14.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:34:14.792
  Aug  5 13:34:14.794: INFO: Creating deployment "webserver-deployment"
  Aug  5 13:34:14.801: INFO: Waiting for observed generation 1
  E0805 13:34:14.913575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:15.914106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:16.810: INFO: Waiting for all required pods to come up
  Aug  5 13:34:16.816: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/05/23 13:34:16.816
  E0805 13:34:16.914912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:17.915107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:18.826: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug  5 13:34:18.833: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug  5 13:34:18.844: INFO: Updating deployment webserver-deployment
  Aug  5 13:34:18.844: INFO: Waiting for observed generation 2
  E0805 13:34:18.915719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:19.915823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:20.853: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug  5 13:34:20.861: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug  5 13:34:20.865: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug  5 13:34:20.875: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug  5 13:34:20.875: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug  5 13:34:20.878: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug  5 13:34:20.885: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug  5 13:34:20.885: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug  5 13:34:20.895: INFO: Updating deployment webserver-deployment
  Aug  5 13:34:20.895: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug  5 13:34:20.902: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Aug  5 13:34:20.909: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0805 13:34:20.916586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:34:20.919: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2015  31406502-fe39-4cbf-ae4a-90bb44c6e0b2 38999 3 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003366098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-05 13:34:16 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-05 13:34:19 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug  5 13:34:20.931: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2015  46657ab0-9968-45ef-9059-bc908cb7887b 39003 3 2023-08-05 13:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 31406502-fe39-4cbf-ae4a-90bb44c6e0b2 0xc0033665a7 0xc0033665a8}] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:34:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31406502-fe39-4cbf-ae4a-90bb44c6e0b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003366648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 13:34:20.931: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug  5 13:34:20.931: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2015  a6ee04f2-554b-42ae-9c68-a31862accab2 39000 3 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 31406502-fe39-4cbf-ae4a-90bb44c6e0b2 0xc0033664b7 0xc0033664b8}] [] [{kube-controller-manager Update apps/v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31406502-fe39-4cbf-ae4a-90bb44c6e0b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003366548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug  5 13:34:20.944: INFO: Pod "webserver-deployment-67bd4bf6dc-2kvmm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2kvmm webserver-deployment-67bd4bf6dc- deployment-2015  cff42d92-f731-4a6e-bd4d-77baa9b31584 38834 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90287 0xc004b90288}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vl474,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vl474,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.47,PodIP:192.168.122.88,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c291ca866ff1f2e6ecd826e5f80b9ec470804dcf13ba381ecd9e6bc0757cfd42,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.88,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.944: INFO: Pod "webserver-deployment-67bd4bf6dc-2vq6g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2vq6g webserver-deployment-67bd4bf6dc- deployment-2015  c0e4e53d-814e-4a58-96d6-1e88c791560c 39020 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90477 0xc004b90478}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hfc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hfc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.945: INFO: Pod "webserver-deployment-67bd4bf6dc-5k94b" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5k94b webserver-deployment-67bd4bf6dc- deployment-2015  ff26555d-4c7a-427b-912d-0d378d3c5d63 38853 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b905e0 0xc004b905e1}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjjr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjjr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.249,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://edc5a9a5db0798be98e38c83534bfb747dc452d18d5ef3fd6683b4529f7d36ca,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.249,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.945: INFO: Pod "webserver-deployment-67bd4bf6dc-7d2g2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7d2g2 webserver-deployment-67bd4bf6dc- deployment-2015  4d76c251-1f67-4ee6-80c4-5793d047db3d 38864 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b907c7 0xc004b907c8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sc7h9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sc7h9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.174,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b35f60bfa815d0b1c841d8bd092ca43ef4fa9c3f9661b83a1d4d8fe875c0965c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.174,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.945: INFO: Pod "webserver-deployment-67bd4bf6dc-clmnv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-clmnv webserver-deployment-67bd4bf6dc- deployment-2015  0d9342a2-ff00-4732-bcfa-5fb4bdbd9303 39019 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b909b7 0xc004b909b8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdx6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdx6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.945: INFO: Pod "webserver-deployment-67bd4bf6dc-g25xs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g25xs webserver-deployment-67bd4bf6dc- deployment-2015  dc75ff65-0fda-46d6-b0d6-a942ad7bfb88 39013 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90b20 0xc004b90b21}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v84r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v84r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.946: INFO: Pod "webserver-deployment-67bd4bf6dc-h4l2l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h4l2l webserver-deployment-67bd4bf6dc- deployment-2015  8a2c0b68-5469-45d9-8b52-c7d39935d117 39012 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90c57 0xc004b90c58}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgtff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgtff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.946: INFO: Pod "webserver-deployment-67bd4bf6dc-nk9mk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nk9mk webserver-deployment-67bd4bf6dc- deployment-2015  8870a1af-fbd1-435e-8c35-cabca70bf827 38858 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90d97 0xc004b90d98}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.52.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jns2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jns2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:192.168.52.175,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2c7197d32686a4ab71ccead9851a225676b0407c935f52e52e133a7d3809cf3e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.52.175,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.946: INFO: Pod "webserver-deployment-67bd4bf6dc-qsrtf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qsrtf webserver-deployment-67bd4bf6dc- deployment-2015  0669890e-0471-46d7-9be7-6c45a38e6317 39015 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b90f87 0xc004b90f88}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lqj9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lqj9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.947: INFO: Pod "webserver-deployment-67bd4bf6dc-rzmz2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rzmz2 webserver-deployment-67bd4bf6dc- deployment-2015  288540c9-e59c-4ac4-9a48-094dac4faf25 38846 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b910c7 0xc004b910c8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgxkj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgxkj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.212,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://391a810bd26c1d5ebd2d83e5f36a4de6809d71c5b21086f9e60dc71c5b18d6ce,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.212,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.947: INFO: Pod "webserver-deployment-67bd4bf6dc-v86bl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-v86bl webserver-deployment-67bd4bf6dc- deployment-2015  f11cc081-7298-4bd9-9f7e-d402dba95fad 39006 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b912b7 0xc004b912b8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjd2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjd2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.947: INFO: Pod "webserver-deployment-67bd4bf6dc-wds9q" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wds9q webserver-deployment-67bd4bf6dc- deployment-2015  111ed922-0f7b-4892-a335-22c070813a3b 38828 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b91420 0xc004b91421}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzkr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzkr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.47,PodIP:192.168.122.101,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://602e8c67d38e187d5b283111d965270303a1bf068b6e4200617a2cffac34579b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.101,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.947: INFO: Pod "webserver-deployment-67bd4bf6dc-wqm8z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wqm8z webserver-deployment-67bd4bf6dc- deployment-2015  da51147c-8048-4879-af23-ca7e5617869f 39011 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b91607 0xc004b91608}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbdzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbdzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.948: INFO: Pod "webserver-deployment-67bd4bf6dc-x85hn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x85hn webserver-deployment-67bd4bf6dc- deployment-2015  1eee8615-998d-4fcb-a99a-2bea7c760e03 38831 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b91770 0xc004b91771}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2d6nz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2d6nz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.47,PodIP:192.168.122.70,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50b1915aece39f613966a9d45fc286b49e70da55698d863997f17425f242c0c3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.70,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.948: INFO: Pod "webserver-deployment-67bd4bf6dc-xhd79" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xhd79 webserver-deployment-67bd4bf6dc- deployment-2015  2aa44a4d-1f28-4f38-b388-e35a97c2cce1 38850 0 2023-08-05 13:34:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a6ee04f2-554b-42ae-9c68-a31862accab2 0xc004b91957 0xc004b91958}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6ee04f2-554b-42ae-9c68-a31862accab2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2t2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2t2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.214,StartTime:2023-08-05 13:34:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-05 13:34:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c79a766536f922ccc4b24c7ac52db16b1029db586ada8c4094498d00f9d6ba13,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.214,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.948: INFO: Pod "webserver-deployment-7b75d79cf5-c5wft" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c5wft webserver-deployment-7b75d79cf5- deployment-2015  27f4f92f-c75d-4474-bdf9-53edb334fe32 39021 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc004b91b47 0xc004b91b48}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzctd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzctd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.948: INFO: Pod "webserver-deployment-7b75d79cf5-df6q8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-df6q8 webserver-deployment-7b75d79cf5- deployment-2015  2aac679c-642d-4a34-b7e4-d83c40ab7ca5 38957 0 2023-08-05 13:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc004b91ca7 0xc004b91ca8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8fsm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8fsm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.216,StartTime:2023-08-05 13:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.216,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.949: INFO: Pod "webserver-deployment-7b75d79cf5-f7ln2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-f7ln2 webserver-deployment-7b75d79cf5- deployment-2015  6f220771-7660-456c-9db9-628394f90ebc 38921 0 2023-08-05 13:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc004b91ec7 0xc004b91ec8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzhxg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzhxg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:,StartTime:2023-08-05 13:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.949: INFO: Pod "webserver-deployment-7b75d79cf5-fhln2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fhln2 webserver-deployment-7b75d79cf5- deployment-2015  c054da0d-0358-4c4d-a342-f62a6417d7a8 38896 0 2023-08-05 13:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc005374297 0xc005374298}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps6qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps6qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-95-133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.95.133,PodIP:,StartTime:2023-08-05 13:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.951: INFO: Pod "webserver-deployment-7b75d79cf5-lxwrj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lxwrj webserver-deployment-7b75d79cf5- deployment-2015  3b4134c8-39a8-4349-84b3-060edf49a8aa 38995 0 2023-08-05 13:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc0053748b7 0xc0053748b8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.122.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6fkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6fkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.47,PodIP:192.168.122.75,StartTime:2023-08-05 13:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.122.75,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.951: INFO: Pod "webserver-deployment-7b75d79cf5-mzc4q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mzc4q webserver-deployment-7b75d79cf5- deployment-2015  36277bfc-6eb1-46d1-8699-4c1ba4b73b9c 38998 0 2023-08-05 13:34:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc005375020 0xc005375021}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7kdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7kdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-35-140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.35.140,PodIP:192.168.166.228,StartTime:2023-08-05 13:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.228,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.951: INFO: Pod "webserver-deployment-7b75d79cf5-trm6g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-trm6g webserver-deployment-7b75d79cf5- deployment-2015  0078c041-2b47-45df-a60f-b1e0fe15a2ee 39017 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc005375bf7 0xc005375bf8}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcqfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcqfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.952: INFO: Pod "webserver-deployment-7b75d79cf5-w2ksm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w2ksm webserver-deployment-7b75d79cf5- deployment-2015  a0ab6012-caf0-4ed0-ae33-dd6bee8647d2 39018 0 2023-08-05 13:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 46657ab0-9968-45ef-9059-bc908cb7887b 0xc001494027 0xc001494028}] [] [{kube-controller-manager Update v1 2023-08-05 13:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46657ab0-9968-45ef-9059-bc908cb7887b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px62v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px62v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-47,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-05 13:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  5 13:34:20.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2015" for this suite. @ 08/05/23 13:34:20.961
• [6.207 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/05/23 13:34:20.982
  Aug  5 13:34:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/05/23 13:34:20.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:34:21.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:34:21.07
  STEP: creating @ 08/05/23 13:34:21.073
  STEP: getting @ 08/05/23 13:34:21.097
  STEP: listing in namespace @ 08/05/23 13:34:21.1
  STEP: patching @ 08/05/23 13:34:21.103
  STEP: deleting @ 08/05/23 13:34:21.11
  Aug  5 13:34:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9731" for this suite. @ 08/05/23 13:34:21.129
• [0.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/05/23 13:34:21.137
  Aug  5 13:34:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:34:21.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:34:21.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:34:21.162
  STEP: Creating secret with name s-test-opt-del-8d739d29-adb5-40eb-9b45-5ca1d72535fc @ 08/05/23 13:34:21.169
  STEP: Creating secret with name s-test-opt-upd-349a0a5a-8830-4400-b030-93cd25c5018c @ 08/05/23 13:34:21.173
  STEP: Creating the pod @ 08/05/23 13:34:21.179
  E0805 13:34:21.917354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:22.917651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-8d739d29-adb5-40eb-9b45-5ca1d72535fc @ 08/05/23 13:34:23.223
  STEP: Updating secret s-test-opt-upd-349a0a5a-8830-4400-b030-93cd25c5018c @ 08/05/23 13:34:23.23
  STEP: Creating secret with name s-test-opt-create-524f1cd4-0299-4375-a6d0-77de15bafd11 @ 08/05/23 13:34:23.235
  STEP: waiting to observe update in volume @ 08/05/23 13:34:23.24
  E0805 13:34:23.917727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:24.918705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:25.919448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:26.920485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:27.921378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:28.921586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:29.922072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:30.922179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:31.922675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:32.922765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:33.923632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:34.923699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:35.924464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:36.924996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:37.925069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:38.925257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:39.925915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:40.926688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:41.926787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:42.927809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:43.928646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:44.928847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:45.929695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:46.929774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:47.929868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:48.930068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:49.930205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:50.930584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:51.931301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:52.931388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:53.931447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:54.931613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:55.932097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:56.932316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:57.932947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:58.933835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:34:59.934715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:00.934777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:01.935580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:02.935666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:03.936328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:04.936572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:05.937116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:06.937297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:07.937924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:08.938018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:09.938113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:10.939169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:11.939283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:12.939488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:13.940034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:14.940426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:15.940673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:16.940870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:17.941305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:18.941535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:19.941647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:20.942705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:21.943060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:22.943525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:23.943536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:24.943640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:25.944451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:26.944510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:27.944645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:28.944908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:29.945724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:30.946018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:35:31.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4981" for this suite. @ 08/05/23 13:35:31.58
• [70.450 seconds]
------------------------------
SSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/05/23 13:35:31.587
  Aug  5 13:35:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename ingress @ 08/05/23 13:35:31.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:31.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:31.621
  STEP: getting /apis @ 08/05/23 13:35:31.624
  STEP: getting /apis/networking.k8s.io @ 08/05/23 13:35:31.627
  STEP: getting /apis/networking.k8s.iov1 @ 08/05/23 13:35:31.628
  STEP: creating @ 08/05/23 13:35:31.63
  STEP: getting @ 08/05/23 13:35:31.65
  STEP: listing @ 08/05/23 13:35:31.656
  STEP: watching @ 08/05/23 13:35:31.66
  Aug  5 13:35:31.660: INFO: starting watch
  STEP: cluster-wide listing @ 08/05/23 13:35:31.662
  STEP: cluster-wide watching @ 08/05/23 13:35:31.666
  Aug  5 13:35:31.667: INFO: starting watch
  STEP: patching @ 08/05/23 13:35:31.668
  STEP: updating @ 08/05/23 13:35:31.676
  Aug  5 13:35:31.686: INFO: waiting for watch events with expected annotations
  Aug  5 13:35:31.687: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/05/23 13:35:31.687
  STEP: updating /status @ 08/05/23 13:35:31.702
  STEP: get /status @ 08/05/23 13:35:31.723
  STEP: deleting @ 08/05/23 13:35:31.728
  STEP: deleting a collection @ 08/05/23 13:35:31.753
  Aug  5 13:35:31.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-1587" for this suite. @ 08/05/23 13:35:31.79
• [0.214 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/05/23 13:35:31.804
  Aug  5 13:35:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:35:31.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:31.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:31.826
  STEP: Creating a pod to test downward api env vars @ 08/05/23 13:35:31.831
  E0805 13:35:31.947156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:32.947169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:33.948025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:34.948275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:35:35.857
  Aug  5 13:35:35.861: INFO: Trying to get logs from node ip-172-31-95-133 pod downward-api-7b7eca45-dff5-4075-a310-52965c1c11cc container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:35:35.879
  Aug  5 13:35:35.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1907" for this suite. @ 08/05/23 13:35:35.9
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/05/23 13:35:35.908
  Aug  5 13:35:35.908: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 13:35:35.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:35.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:35.931
  STEP: Creating service test in namespace statefulset-8893 @ 08/05/23 13:35:35.934
  STEP: Creating statefulset ss in namespace statefulset-8893 @ 08/05/23 13:35:35.947
  E0805 13:35:35.948888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:35:35.958: INFO: Found 0 stateful pods, waiting for 1
  E0805 13:35:36.949736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:37.949773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:38.949967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:39.950712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:40.951090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:41.951361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:42.951579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:43.951660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:44.952580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:45.952673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:35:45.963: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/05/23 13:35:45.971
  STEP: Getting /status @ 08/05/23 13:35:45.978
  Aug  5 13:35:45.982: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/05/23 13:35:45.982
  Aug  5 13:35:45.993: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/05/23 13:35:45.993
  Aug  5 13:35:45.995: INFO: Observed &StatefulSet event: ADDED
  Aug  5 13:35:45.995: INFO: Found Statefulset ss in namespace statefulset-8893 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  5 13:35:45.995: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/05/23 13:35:45.995
  Aug  5 13:35:45.995: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  5 13:35:46.003: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/05/23 13:35:46.003
  Aug  5 13:35:46.005: INFO: Observed &StatefulSet event: ADDED
  Aug  5 13:35:46.005: INFO: Deleting all statefulset in ns statefulset-8893
  Aug  5 13:35:46.008: INFO: Scaling statefulset ss to 0
  E0805 13:35:46.952782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:47.953872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:48.953933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:49.954696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:50.954806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:51.954978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:52.955121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:53.955182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:54.955262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:55.955330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:35:56.028: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:35:56.031: INFO: Deleting statefulset ss
  Aug  5 13:35:56.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8893" for this suite. @ 08/05/23 13:35:56.05
• [20.150 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 08/05/23 13:35:56.058
  Aug  5 13:35:56.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 13:35:56.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:56.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:56.078
  Aug  5 13:35:56.080: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:35:56.955441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:57.955552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:35:58.956026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:35:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5369" for this suite. @ 08/05/23 13:35:59.187
• [3.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/05/23 13:35:59.198
  Aug  5 13:35:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:35:59.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:59.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:59.217
  STEP: starting the proxy server @ 08/05/23 13:35:59.22
  Aug  5 13:35:59.220: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-9389 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/05/23 13:35:59.263
  Aug  5 13:35:59.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9389" for this suite. @ 08/05/23 13:35:59.277
• [0.085 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/05/23 13:35:59.284
  Aug  5 13:35:59.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename init-container @ 08/05/23 13:35:59.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:35:59.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:35:59.305
  STEP: creating the pod @ 08/05/23 13:35:59.308
  Aug  5 13:35:59.308: INFO: PodSpec: initContainers in spec.initContainers
  E0805 13:35:59.956113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:00.956204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:01.957163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:02.957216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:03.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7346" for this suite. @ 08/05/23 13:36:03.529
• [4.254 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/05/23 13:36:03.538
  Aug  5 13:36:03.538: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subpath @ 08/05/23 13:36:03.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:36:03.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:36:03.557
  STEP: Setting up data @ 08/05/23 13:36:03.559
  STEP: Creating pod pod-subpath-test-secret-xqvp @ 08/05/23 13:36:03.569
  STEP: Creating a pod to test atomic-volume-subpath @ 08/05/23 13:36:03.569
  E0805 13:36:03.957756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:04.958706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:05.958927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:06.959313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:07.959838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:08.959916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:09.960007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:10.960454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:11.960540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:12.960841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:13.960948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:14.961041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:15.961131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:16.961323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:17.962098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:18.962699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:19.963714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:20.964705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:21.964756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:22.964844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:23.964934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:24.965029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:25.965320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:26.965531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:36:27.64
  Aug  5 13:36:27.644: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-subpath-test-secret-xqvp container test-container-subpath-secret-xqvp: <nil>
  STEP: delete the pod @ 08/05/23 13:36:27.653
  STEP: Deleting pod pod-subpath-test-secret-xqvp @ 08/05/23 13:36:27.669
  Aug  5 13:36:27.669: INFO: Deleting pod "pod-subpath-test-secret-xqvp" in namespace "subpath-4498"
  Aug  5 13:36:27.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4498" for this suite. @ 08/05/23 13:36:27.677
• [24.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 08/05/23 13:36:27.685
  Aug  5 13:36:27.685: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename daemonsets @ 08/05/23 13:36:27.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:36:27.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:36:27.704
  Aug  5 13:36:27.728: INFO: Create a RollingUpdate DaemonSet
  Aug  5 13:36:27.733: INFO: Check that daemon pods launch on every node of the cluster
  Aug  5 13:36:27.737: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:27.737: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:27.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:36:27.740: INFO: Node ip-172-31-1-47 is running 0 daemon pod, expected 1
  E0805 13:36:27.966043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:28.745: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:28.745: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:28.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  5 13:36:28.749: INFO: Node ip-172-31-35-140 is running 0 daemon pod, expected 1
  E0805 13:36:28.966410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:29.745: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:29.745: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:29.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug  5 13:36:29.749: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Aug  5 13:36:29.749: INFO: Update the DaemonSet to trigger a rollout
  Aug  5 13:36:29.759: INFO: Updating DaemonSet daemon-set
  E0805 13:36:29.967352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:30.777: INFO: Roll back the DaemonSet before rollout is complete
  Aug  5 13:36:30.787: INFO: Updating DaemonSet daemon-set
  Aug  5 13:36:30.787: INFO: Make sure DaemonSet rollback is complete
  Aug  5 13:36:30.790: INFO: Wrong image for pod: daemon-set-z2bkc. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug  5 13:36:30.790: INFO: Pod daemon-set-z2bkc is not available
  Aug  5 13:36:30.794: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:30.795: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0805 13:36:30.967974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:31.803: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:31.803: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0805 13:36:31.969001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:32.804: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:32.804: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0805 13:36:32.969460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:33.800: INFO: Pod daemon-set-47c4w is not available
  Aug  5 13:36:33.805: INFO: DaemonSet pods can't tolerate node ip-172-31-11-29 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Aug  5 13:36:33.805: INFO: DaemonSet pods can't tolerate node ip-172-31-47-149 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 08/05/23 13:36:33.813
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3825, will wait for the garbage collector to delete the pods @ 08/05/23 13:36:33.813
  Aug  5 13:36:33.875: INFO: Deleting DaemonSet.extensions daemon-set took: 7.553569ms
  E0805 13:36:33.969831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:33.976: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.455402ms
  E0805 13:36:34.970553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:36:35.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  5 13:36:35.381: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  5 13:36:35.385: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40057"},"items":null}

  Aug  5 13:36:35.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40057"},"items":null}

  Aug  5 13:36:35.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3825" for this suite. @ 08/05/23 13:36:35.412
• [7.738 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/05/23 13:36:35.424
  Aug  5 13:36:35.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption @ 08/05/23 13:36:35.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:36:35.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:36:35.446
  Aug  5 13:36:35.472: INFO: Waiting up to 1m0s for all nodes to be ready
  E0805 13:36:35.970607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:36.970889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:37.971610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:38.971676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:39.972311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:40.973201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:41.974054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:42.974137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:43.975080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:44.975245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:45.975349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:46.975458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:47.976389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:48.976481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:49.976583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:50.976777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:51.976822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:52.977003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:53.977301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:54.977490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:55.978414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:56.978699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:57.979471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:58.979562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:36:59.980578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:00.980788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:01.980894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:02.980990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:03.981801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:04.982704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:05.983344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:06.983585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:07.984004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:08.984090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:09.984183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:10.984222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:11.985002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:12.985064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:13.985229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:14.985435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:15.986458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:16.987161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:17.988038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:18.988105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:19.988198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:20.989201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:21.989600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:22.989625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:23.990637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:24.990740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:25.991145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:26.991259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:27.991348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:28.991526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:29.992553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:30.993213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:31.993490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:32.993628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:33.993755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:34.993817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:37:35.495: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/05/23 13:37:35.499
  Aug  5 13:37:35.499: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/05/23 13:37:35.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:37:35.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:37:35.529
  Aug  5 13:37:35.557: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug  5 13:37:35.563: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug  5 13:37:35.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:37:35.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1755" for this suite. @ 08/05/23 13:37:35.666
  STEP: Destroying namespace "sched-preemption-9057" for this suite. @ 08/05/23 13:37:35.673
• [60.259 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/05/23 13:37:35.684
  Aug  5 13:37:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 13:37:35.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:37:35.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:37:35.712
  Aug  5 13:37:35.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:37:35.994404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:36.995167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:37.995248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0805 13:37:38.275988      19 warnings.go:70] unknown field "alpha"
  W0805 13:37:38.276007      19 warnings.go:70] unknown field "beta"
  W0805 13:37:38.276014      19 warnings.go:70] unknown field "delta"
  W0805 13:37:38.276020      19 warnings.go:70] unknown field "epsilon"
  W0805 13:37:38.276026      19 warnings.go:70] unknown field "gamma"
  Aug  5 13:37:38.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5926" for this suite. @ 08/05/23 13:37:38.828
• [3.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/05/23 13:37:38.836
  Aug  5 13:37:38.836: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 13:37:38.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:37:38.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:37:38.857
  STEP: Creating Indexed job @ 08/05/23 13:37:38.859
  STEP: Ensuring job reaches completions @ 08/05/23 13:37:38.865
  E0805 13:37:38.996054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:39.996134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:40.996405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:41.996883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:42.997188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:43.997586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:44.997877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:45.998017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:46.998882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:47.999053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 08/05/23 13:37:48.869
  Aug  5 13:37:48.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8153" for this suite. @ 08/05/23 13:37:48.878
• [10.048 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/05/23 13:37:48.885
  Aug  5 13:37:48.885: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename sched-preemption @ 08/05/23 13:37:48.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:37:48.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:37:48.906
  Aug  5 13:37:48.922: INFO: Waiting up to 1m0s for all nodes to be ready
  E0805 13:37:48.999564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:49.999687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:51.000654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:52.000777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:53.001054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:54.002020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:55.002944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:56.002979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:57.003783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:58.004507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:37:59.005272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:00.005363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:01.005545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:02.005649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:03.005692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:04.006699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:05.007553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:06.007719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:07.008165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:08.008276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:09.009122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:10.009840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:11.010840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:12.010943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:13.011019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:14.011240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:15.011418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:16.011516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:17.012288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:18.012565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:19.012736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:20.013140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:21.013786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:22.013914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:23.014683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:24.015215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:25.015677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:26.015729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:27.016485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:28.016579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:29.017150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:30.017641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:31.018538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:32.019544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:33.020516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:34.020625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:35.020983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:36.021119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:37.021173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:38.021266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:39.022218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:40.022340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:41.022435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:42.022683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:43.023338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:44.023415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:45.024180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:46.024450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:47.025124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:48.025167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:38:48.939: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/05/23 13:38:48.943
  Aug  5 13:38:48.966: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug  5 13:38:48.974: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug  5 13:38:48.992: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug  5 13:38:49.001: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug  5 13:38:49.017: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug  5 13:38:49.024: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/05/23 13:38:49.024
  E0805 13:38:49.025270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:50.025647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:51.026569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/05/23 13:38:51.057
  E0805 13:38:52.026694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:53.026878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:54.026966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:55.027165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:38:55.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1247" for this suite. @ 08/05/23 13:38:55.173
• [66.297 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/05/23 13:38:55.182
  Aug  5 13:38:55.182: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:38:55.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:38:55.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:38:55.205
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7182 @ 08/05/23 13:38:55.207
  STEP: changing the ExternalName service to type=NodePort @ 08/05/23 13:38:55.217
  STEP: creating replication controller externalname-service in namespace services-7182 @ 08/05/23 13:38:55.239
  I0805 13:38:55.248267      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7182, replica count: 2
  E0805 13:38:56.027683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:57.028681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:38:58.028776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0805 13:38:58.299315      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  5 13:38:58.299: INFO: Creating new exec pod
  E0805 13:38:59.028871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:00.028947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:01.029118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:01.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:39:01.468: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:01.468: INFO: stdout: ""
  E0805 13:39:02.029173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:02.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  5 13:39:02.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:02.599: INFO: stdout: "externalname-service-ppdnq"
  Aug  5 13:39:02.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:02.734: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\n+ echo hostName\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:02.734: INFO: stdout: ""
  E0805 13:39:03.030120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:03.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:03.858: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug  5 13:39:03.858: INFO: stdout: ""
  E0805 13:39:04.030927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:04.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:04.857: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\n+ echo hostName\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:04.857: INFO: stdout: ""
  E0805 13:39:05.031020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:05.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:05.854: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Aug  5 13:39:05.854: INFO: stdout: ""
  E0805 13:39:06.031061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:06.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:06.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.109 80\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:06.854: INFO: stdout: ""
  E0805 13:39:07.031809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:07.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:07.862: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\n+ echo hostName\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:07.862: INFO: stdout: ""
  E0805 13:39:08.032712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:08.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.109 80'
  Aug  5 13:39:08.855: INFO: stderr: "+ nc -v -t -w 2 10.152.183.109 80\n+ echo hostName\nConnection to 10.152.183.109 80 port [tcp/http] succeeded!\n"
  Aug  5 13:39:08.855: INFO: stdout: "externalname-service-ppdnq"
  Aug  5 13:39:08.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.35.140 30144'
  Aug  5 13:39:08.980: INFO: stderr: "+ nc -v -t -w 2 172.31.35.140 30144\n+ echo hostName\nConnection to 172.31.35.140 30144 port [tcp/*] succeeded!\n"
  Aug  5 13:39:08.980: INFO: stdout: "externalname-service-64bxq"
  Aug  5 13:39:08.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-7182 exec execpodnn4ws -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.95.133 30144'
  E0805 13:39:09.033028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:09.112: INFO: stderr: "+ nc -v -t -w 2 172.31.95.133 30144\n+ echo hostName\nConnection to 172.31.95.133 30144 port [tcp/*] succeeded!\n"
  Aug  5 13:39:09.112: INFO: stdout: "externalname-service-ppdnq"
  Aug  5 13:39:09.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:39:09.117: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7182" for this suite. @ 08/05/23 13:39:09.14
• [13.967 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/05/23 13:39:09.149
  Aug  5 13:39:09.149: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:39:09.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:09.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:09.171
  STEP: creating the pod @ 08/05/23 13:39:09.175
  STEP: waiting for pod running @ 08/05/23 13:39:09.184
  E0805 13:39:10.033211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:11.033272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 08/05/23 13:39:11.194
  Aug  5 13:39:11.198: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-451 PodName:var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:39:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:39:11.199: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:39:11.199: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-451/pods/var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/05/23 13:39:11.255
  Aug  5 13:39:11.260: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-451 PodName:var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:39:11.260: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:39:11.260: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:39:11.260: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-451/pods/var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 08/05/23 13:39:11.328
  Aug  5 13:39:11.840: INFO: Successfully updated pod "var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a"
  STEP: waiting for annotated pod running @ 08/05/23 13:39:11.84
  STEP: deleting the pod gracefully @ 08/05/23 13:39:11.844
  Aug  5 13:39:11.844: INFO: Deleting pod "var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a" in namespace "var-expansion-451"
  Aug  5 13:39:11.852: INFO: Wait up to 5m0s for pod "var-expansion-9d56e345-2a6d-4783-ad1a-5042035dbd3a" to be fully deleted
  E0805 13:39:12.033733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:13.033818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:14.034611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:15.034718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:16.035434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:17.035788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:18.036753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:19.036841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:20.037698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:21.038709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:22.038959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:23.039059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:24.039489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:25.039584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:26.039666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:27.039847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:28.040474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:29.040644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:30.041490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:31.041638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:32.042338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:33.042419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:34.042505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:35.043535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:36.044534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:37.044571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:38.044578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:39.045123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:40.046116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:41.046216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:42.046967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:43.047062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:43.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-451" for this suite. @ 08/05/23 13:39:43.933
• [34.790 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/05/23 13:39:43.942
  Aug  5 13:39:43.942: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:39:43.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:43.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:43.968
  Aug  5 13:39:43.970: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:39:44.048005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:45.048097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/05/23 13:39:45.346
  Aug  5 13:39:45.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3824 --namespace=crd-publish-openapi-3824 create -f -'
  Aug  5 13:39:45.901: INFO: stderr: ""
  Aug  5 13:39:45.901: INFO: stdout: "e2e-test-crd-publish-openapi-6627-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug  5 13:39:45.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3824 --namespace=crd-publish-openapi-3824 delete e2e-test-crd-publish-openapi-6627-crds test-cr'
  Aug  5 13:39:45.984: INFO: stderr: ""
  Aug  5 13:39:45.984: INFO: stdout: "e2e-test-crd-publish-openapi-6627-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug  5 13:39:45.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3824 --namespace=crd-publish-openapi-3824 apply -f -'
  E0805 13:39:46.048395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:46.191: INFO: stderr: ""
  Aug  5 13:39:46.191: INFO: stdout: "e2e-test-crd-publish-openapi-6627-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug  5 13:39:46.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3824 --namespace=crd-publish-openapi-3824 delete e2e-test-crd-publish-openapi-6627-crds test-cr'
  Aug  5 13:39:46.257: INFO: stderr: ""
  Aug  5 13:39:46.257: INFO: stdout: "e2e-test-crd-publish-openapi-6627-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/05/23 13:39:46.257
  Aug  5 13:39:46.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-3824 explain e2e-test-crd-publish-openapi-6627-crds'
  Aug  5 13:39:46.443: INFO: stderr: ""
  Aug  5 13:39:46.443: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6627-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0805 13:39:47.049316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:47.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3824" for this suite. @ 08/05/23 13:39:47.737
• [3.805 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/05/23 13:39:47.747
  Aug  5 13:39:47.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:39:47.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:47.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:47.765
  STEP: Creating configMap with name projected-configmap-test-volume-map-0c4c065a-72dd-4f6c-b7af-049e0ab939d7 @ 08/05/23 13:39:47.768
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:39:47.774
  E0805 13:39:48.049806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:49.049890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:50.049967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:51.050229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:39:51.799
  Aug  5 13:39:51.802: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-056cecd5-18e6-4ace-a827-0d899c5258fc container agnhost-container: <nil>
  STEP: delete the pod @ 08/05/23 13:39:51.82
  Aug  5 13:39:51.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8167" for this suite. @ 08/05/23 13:39:51.838
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/05/23 13:39:51.846
  Aug  5 13:39:51.846: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:39:51.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:51.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:51.865
  STEP: Creating Pod @ 08/05/23 13:39:51.868
  E0805 13:39:52.050684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:53.051047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 08/05/23 13:39:53.888
  Aug  5 13:39:53.888: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4292 PodName:pod-sharedvolume-797d7f13-2fff-404f-b9ae-9a0930d02d11 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  5 13:39:53.888: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  Aug  5 13:39:53.888: INFO: ExecWithOptions: Clientset creation
  Aug  5 13:39:53.888: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-4292/pods/pod-sharedvolume-797d7f13-2fff-404f-b9ae-9a0930d02d11/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Aug  5 13:39:53.949: INFO: Exec stderr: ""
  Aug  5 13:39:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4292" for this suite. @ 08/05/23 13:39:53.953
• [2.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/05/23 13:39:53.963
  Aug  5 13:39:53.963: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubectl @ 08/05/23 13:39:53.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:53.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:53.982
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/05/23 13:39:53.985
  Aug  5 13:39:53.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-641 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug  5 13:39:54.049: INFO: stderr: ""
  Aug  5 13:39:54.049: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/05/23 13:39:54.049
  E0805 13:39:54.051193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:54.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=kubectl-641 delete pods e2e-test-httpd-pod'
  E0805 13:39:55.051312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:56.052248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:39:56.472: INFO: stderr: ""
  Aug  5 13:39:56.473: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  5 13:39:56.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-641" for this suite. @ 08/05/23 13:39:56.477
• [2.520 seconds]
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/05/23 13:39:56.484
  Aug  5 13:39:56.484: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/05/23 13:39:56.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:39:56.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:39:56.502
  Aug  5 13:39:56.505: INFO: Waiting up to 1m0s for all nodes to be ready
  E0805 13:39:57.052347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:58.052890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:39:59.052932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:00.053031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:01.053167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:02.053204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:03.053430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:04.053617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:05.054423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:06.054723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:07.055046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:08.055229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:09.056235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:10.056323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:11.056532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:12.057565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:13.058490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:14.058668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:15.058781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:16.058871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:17.059433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:18.060097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:19.060185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:20.060382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:21.061443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:22.061635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:23.062560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:24.062963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:25.063056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:26.064097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:27.064667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:28.064801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:29.064886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:30.065067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:31.065255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:32.065286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:33.065373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:34.065590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:35.066419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:36.066694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:37.067538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:38.067810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:39.068803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:40.068897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:41.069631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:42.070705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:43.070786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:44.070880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:45.070978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:46.071082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:47.071999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:48.072169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:49.072508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:50.072805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:51.073528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:52.073677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:53.073970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:54.074068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:55.074697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:56.074774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:40:56.520: INFO: Waiting for terminating namespaces to be deleted...
  Aug  5 13:40:56.524: INFO: Starting informer...
  STEP: Starting pods... @ 08/05/23 13:40:56.524
  Aug  5 13:40:56.744: INFO: Pod1 is running on ip-172-31-95-133. Tainting Node
  E0805 13:40:57.075494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:40:58.075507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:40:58.966: INFO: Pod2 is running on ip-172-31-95-133. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/05/23 13:40:58.966
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/05/23 13:40:58.976
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/05/23 13:40:58.981
  E0805 13:40:59.076366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:00.076462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:01.076558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:02.076635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:03.077046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:04.077793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:41:04.852: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0805 13:41:05.078499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:06.078593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:07.078785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:08.079046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:09.079255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:10.079842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:11.079908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:12.080276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:13.080373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:14.080485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:15.080558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:16.080665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:17.080807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:18.081048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:19.081124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:20.081976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:21.082682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:22.082804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:23.082887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:24.083171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:41:24.887: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug  5 13:41:24.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/05/23 13:41:24.903
  STEP: Destroying namespace "taint-multiple-pods-3221" for this suite. @ 08/05/23 13:41:24.907
• [88.433 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/05/23 13:41:24.918
  Aug  5 13:41:24.918: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:41:24.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:41:24.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:41:24.948
  STEP: Creating a pod to test downward api env vars @ 08/05/23 13:41:24.955
  E0805 13:41:25.084145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:26.084957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:27.086025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:28.086124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:41:28.979
  Aug  5 13:41:28.982: INFO: Trying to get logs from node ip-172-31-95-133 pod downward-api-73aafc54-f4de-4532-94d5-de34545f940d container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:41:28.997
  Aug  5 13:41:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2833" for this suite. @ 08/05/23 13:41:29.018
• [4.107 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/05/23 13:41:29.025
  Aug  5 13:41:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 13:41:29.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:41:29.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:41:29.042
  STEP: Creating pod test-webserver-f163486a-9fe7-4528-8fbd-d742ed3cc951 in namespace container-probe-7532 @ 08/05/23 13:41:29.046
  E0805 13:41:29.086535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:30.086672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:41:31.065: INFO: Started pod test-webserver-f163486a-9fe7-4528-8fbd-d742ed3cc951 in namespace container-probe-7532
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 13:41:31.065
  Aug  5 13:41:31.069: INFO: Initial restart count of pod test-webserver-f163486a-9fe7-4528-8fbd-d742ed3cc951 is 0
  E0805 13:41:31.086840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:32.086999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:33.087852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:34.087973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:35.088676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:36.088838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:37.089095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:38.089234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:39.089519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:40.089893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:41.090697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:42.090786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:43.090935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:44.091040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:45.091229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:46.091383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:47.091583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:48.091735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:49.091920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:50.092043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:51.092159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:52.092262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:53.092461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:54.092509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:55.092534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:56.093173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:57.093272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:58.093626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:41:59.093723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:00.093832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:01.093907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:02.094014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:03.094428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:04.094825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:05.094899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:06.095878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:07.095969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:08.096257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:09.096441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:10.096545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:11.097617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:12.098678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:13.098858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:14.099599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:15.099761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:16.099856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:17.099956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:18.100627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:19.101309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:20.101614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:21.102644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:22.102720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:23.102822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:24.103417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:25.103515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:26.103715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:27.103770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:28.104100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:29.104193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:30.104330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:31.105385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:32.106384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:33.106480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:34.106561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:35.106740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:36.107438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:37.107996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:38.108056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:39.108223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:40.108465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:41.108737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:42.108828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:43.109011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:44.110067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:45.110827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:46.111395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:47.111566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:48.112576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:49.112829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:50.112922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:51.113246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:52.113411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:53.113534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:54.113615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:55.113736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:56.114490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:57.114563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:58.114595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:42:59.114919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:00.115680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:01.115776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:02.115872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:03.116050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:04.116131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:05.116230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:06.117152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:07.117248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:08.117685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:09.117786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:10.118682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:11.119002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:12.119688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:13.120014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:14.120103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:15.120290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:16.121133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:17.121815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:18.121909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:19.122683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:20.123576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:21.123620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:22.123696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:23.123791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:24.124497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:25.124577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:26.125169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:27.125269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:28.125620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:29.126681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:30.127244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:31.127299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:32.128212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:33.128314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:34.129258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:35.129446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:36.129627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:37.129694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:38.130596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:39.130668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:40.130690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:41.131052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:42.131170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:43.131326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:44.131420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:45.131614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:46.132459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:47.132632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:48.132725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:49.132759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:50.133710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:51.134534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:52.134891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:53.135078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:54.136147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:55.136340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:56.136835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:57.137012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:58.137101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:43:59.137201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:00.138164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:01.138258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:02.138683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:03.139084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:04.139286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:05.139475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:06.140298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:07.140389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:08.141451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:09.141615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:10.141706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:11.142664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:12.142755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:13.142899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:14.143832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:15.143917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:16.144533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:17.144659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:18.144888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:19.145048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:20.145477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:21.145613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:22.146582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:23.146746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:24.146988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:25.147098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:26.147412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:27.147504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:28.148490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:29.148739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:30.148849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:31.149204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:32.149297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:33.149475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:34.149593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:35.149751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:36.150572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:37.151172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:38.151262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:39.151462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:40.151957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:41.152638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:42.152726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:43.152917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:44.153605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:45.153691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:46.153792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:47.154687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:48.154775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:49.154868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:50.155167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:51.155384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:52.155895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:53.155997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:54.156082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:55.156190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:56.156546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:57.156707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:58.156981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:44:59.157074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:00.157876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:01.158734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:02.159024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:03.159160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:04.159955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:05.160060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:06.160900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:07.161380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:08.161619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:09.161714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:10.162680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:11.163007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:12.163811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:13.164652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:14.164747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:15.164847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:16.165818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:17.165918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:18.166189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:19.166687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:20.166770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:21.166855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:22.167096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:23.167258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:24.168242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:25.168412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:26.169231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:27.169431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:28.169610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:29.169711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:30.170685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:31.170775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:31.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 13:45:31.64
  STEP: Destroying namespace "container-probe-7532" for this suite. @ 08/05/23 13:45:31.658
• [242.641 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/05/23 13:45:31.668
  Aug  5 13:45:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:45:31.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:31.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:31.69
  STEP: Creating secret with name secret-test-bd7a5d11-4308-4d1c-9e2b-ed133b8511c6 @ 08/05/23 13:45:31.693
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:45:31.698
  E0805 13:45:32.171693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:33.171796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:34.171887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:35.172011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:45:35.72
  Aug  5 13:45:35.723: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-bfc0b7b4-d267-452b-80de-11bdf829beb4 container secret-env-test: <nil>
  STEP: delete the pod @ 08/05/23 13:45:35.741
  Aug  5 13:45:35.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3058" for this suite. @ 08/05/23 13:45:35.762
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/05/23 13:45:35.771
  Aug  5 13:45:35.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:45:35.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:35.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:35.792
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:45:35.796
  E0805 13:45:36.172466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:37.172615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:38.172711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:39.172908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:45:39.82
  Aug  5 13:45:39.824: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-02bf3309-94be-427a-8cb3-734de57e7213 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:45:39.831
  Aug  5 13:45:39.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4949" for this suite. @ 08/05/23 13:45:39.854
• [4.090 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/05/23 13:45:39.862
  Aug  5 13:45:39.862: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:45:39.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:39.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:39.88
  Aug  5 13:45:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:45:40.173968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:41.174032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/05/23 13:45:41.218
  Aug  5 13:45:41.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 create -f -'
  Aug  5 13:45:41.810: INFO: stderr: ""
  Aug  5 13:45:41.811: INFO: stdout: "e2e-test-crd-publish-openapi-606-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug  5 13:45:41.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 delete e2e-test-crd-publish-openapi-606-crds test-foo'
  Aug  5 13:45:41.877: INFO: stderr: ""
  Aug  5 13:45:41.877: INFO: stdout: "e2e-test-crd-publish-openapi-606-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug  5 13:45:41.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 apply -f -'
  Aug  5 13:45:42.060: INFO: stderr: ""
  Aug  5 13:45:42.060: INFO: stdout: "e2e-test-crd-publish-openapi-606-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug  5 13:45:42.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 delete e2e-test-crd-publish-openapi-606-crds test-foo'
  Aug  5 13:45:42.148: INFO: stderr: ""
  Aug  5 13:45:42.148: INFO: stdout: "e2e-test-crd-publish-openapi-606-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/05/23 13:45:42.148
  Aug  5 13:45:42.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 create -f -'
  E0805 13:45:42.174523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:42.347: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/05/23 13:45:42.347
  Aug  5 13:45:42.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 create -f -'
  Aug  5 13:45:42.549: INFO: rc: 1
  Aug  5 13:45:42.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 apply -f -'
  Aug  5 13:45:42.751: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/05/23 13:45:42.751
  Aug  5 13:45:42.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 create -f -'
  E0805 13:45:43.174655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:43.342: INFO: rc: 1
  Aug  5 13:45:43.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 --namespace=crd-publish-openapi-9217 apply -f -'
  Aug  5 13:45:43.538: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/05/23 13:45:43.539
  Aug  5 13:45:43.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 explain e2e-test-crd-publish-openapi-606-crds'
  Aug  5 13:45:43.732: INFO: stderr: ""
  Aug  5 13:45:43.732: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-606-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/05/23 13:45:43.732
  Aug  5 13:45:43.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 explain e2e-test-crd-publish-openapi-606-crds.metadata'
  Aug  5 13:45:43.924: INFO: stderr: ""
  Aug  5 13:45:43.925: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-606-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug  5 13:45:43.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 explain e2e-test-crd-publish-openapi-606-crds.spec'
  Aug  5 13:45:44.122: INFO: stderr: ""
  Aug  5 13:45:44.122: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-606-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug  5 13:45:44.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 explain e2e-test-crd-publish-openapi-606-crds.spec.bars'
  E0805 13:45:44.175689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:44.309: INFO: stderr: ""
  Aug  5 13:45:44.309: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-606-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/05/23 13:45:44.309
  Aug  5 13:45:44.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=crd-publish-openapi-9217 explain e2e-test-crd-publish-openapi-606-crds.spec.bars2'
  Aug  5 13:45:44.520: INFO: rc: 1
  E0805 13:45:45.176326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:45.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9217" for this suite. @ 08/05/23 13:45:45.826
• [5.972 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/05/23 13:45:45.838
  Aug  5 13:45:45.838: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:45:45.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:45.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:45.858
  Aug  5 13:45:45.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8315" for this suite. @ 08/05/23 13:45:45.904
• [0.073 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/05/23 13:45:45.911
  Aug  5 13:45:45.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename init-container @ 08/05/23 13:45:45.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:45.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:45.931
  STEP: creating the pod @ 08/05/23 13:45:45.933
  Aug  5 13:45:45.933: INFO: PodSpec: initContainers in spec.initContainers
  E0805 13:45:46.177090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:47.178110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:48.178649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:49.179245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2523" for this suite. @ 08/05/23 13:45:49.33
• [3.426 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/05/23 13:45:49.338
  Aug  5 13:45:49.338: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename field-validation @ 08/05/23 13:45:49.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:49.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:49.358
  STEP: apply creating a deployment @ 08/05/23 13:45:49.36
  Aug  5 13:45:49.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2548" for this suite. @ 08/05/23 13:45:49.379
• [0.048 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/05/23 13:45:49.386
  Aug  5 13:45:49.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:45:49.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:49.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:49.406
  STEP: set up a multi version CRD @ 08/05/23 13:45:49.408
  Aug  5 13:45:49.409: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:45:50.179275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:51.179963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:52.181035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 08/05/23 13:45:52.775
  STEP: check the unserved version gets removed @ 08/05/23 13:45:52.799
  E0805 13:45:53.181216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 08/05/23 13:45:53.674
  E0805 13:45:54.182186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:55.182810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:56.183402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:45:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3801" for this suite. @ 08/05/23 13:45:56.357
• [6.979 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/05/23 13:45:56.371
  Aug  5 13:45:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:45:56.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:56.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:56.393
  STEP: Creating a ResourceQuota @ 08/05/23 13:45:56.395
  STEP: Getting a ResourceQuota @ 08/05/23 13:45:56.4
  STEP: Updating a ResourceQuota @ 08/05/23 13:45:56.404
  STEP: Verifying a ResourceQuota was modified @ 08/05/23 13:45:56.408
  STEP: Deleting a ResourceQuota @ 08/05/23 13:45:56.414
  STEP: Verifying the deleted ResourceQuota @ 08/05/23 13:45:56.42
  Aug  5 13:45:56.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5773" for this suite. @ 08/05/23 13:45:56.428
• [0.063 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/05/23 13:45:56.435
  Aug  5 13:45:56.435: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:45:56.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:45:56.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:45:56.458
  STEP: Counting existing ResourceQuota @ 08/05/23 13:45:56.461
  E0805 13:45:57.183574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:58.183796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:45:59.183864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:00.184138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:01.184893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/05/23 13:46:01.465
  STEP: Ensuring resource quota status is calculated @ 08/05/23 13:46:01.47
  E0805 13:46:02.185361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:03.185642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/05/23 13:46:03.475
  STEP: Ensuring resource quota status captures replication controller creation @ 08/05/23 13:46:03.487
  E0805 13:46:04.185731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:05.186699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/05/23 13:46:05.492
  STEP: Ensuring resource quota status released usage @ 08/05/23 13:46:05.499
  E0805 13:46:06.187641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:07.187842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:07.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1012" for this suite. @ 08/05/23 13:46:07.509
• [11.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/05/23 13:46:07.517
  Aug  5 13:46:07.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:46:07.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:07.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:07.541
  E0805 13:46:08.187880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:09.187954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:09.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  5 13:46:09.568: INFO: Deleting pod "var-expansion-1888bf62-a6fc-4b4e-a5d5-8db43ae2d040" in namespace "var-expansion-1330"
  Aug  5 13:46:09.576: INFO: Wait up to 5m0s for pod "var-expansion-1888bf62-a6fc-4b4e-a5d5-8db43ae2d040" to be fully deleted
  E0805 13:46:10.188070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:11.188437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-1330" for this suite. @ 08/05/23 13:46:11.585
• [4.075 seconds]
------------------------------
SS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/05/23 13:46:11.593
  Aug  5 13:46:11.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename subjectreview @ 08/05/23 13:46:11.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:11.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:11.613
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-5023" @ 08/05/23 13:46:11.615
  Aug  5 13:46:11.620: INFO: saUsername: "system:serviceaccount:subjectreview-5023:e2e"
  Aug  5 13:46:11.620: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-5023"}
  Aug  5 13:46:11.620: INFO: saUID: "3ae3d142-4645-42bb-8409-61bbb63cf534"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-5023:e2e" @ 08/05/23 13:46:11.62
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-5023:e2e" @ 08/05/23 13:46:11.62
  Aug  5 13:46:11.622: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-5023:e2e" api 'list' configmaps in "subjectreview-5023" namespace @ 08/05/23 13:46:11.622
  Aug  5 13:46:11.623: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-5023:e2e" @ 08/05/23 13:46:11.623
  Aug  5 13:46:11.625: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug  5 13:46:11.625: INFO: LocalSubjectAccessReview has been verified
  Aug  5 13:46:11.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-5023" for this suite. @ 08/05/23 13:46:11.63
• [0.044 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/05/23 13:46:11.637
  Aug  5 13:46:11.637: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/05/23 13:46:11.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:11.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:11.657
  STEP: create the container to handle the HTTPGet hook request. @ 08/05/23 13:46:11.664
  E0805 13:46:12.188601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:13.188633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/05/23 13:46:13.686
  E0805 13:46:14.188745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:15.189063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/05/23 13:46:15.706
  E0805 13:46:16.190128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:17.190171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/05/23 13:46:17.721
  Aug  5 13:46:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-812" for this suite. @ 08/05/23 13:46:17.746
• [6.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/05/23 13:46:17.756
  Aug  5 13:46:17.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:46:17.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:17.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:17.779
  STEP: Setting up server cert @ 08/05/23 13:46:17.804
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:46:18.179
  STEP: Deploying the webhook pod @ 08/05/23 13:46:18.188
  E0805 13:46:18.190285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 08/05/23 13:46:18.2
  Aug  5 13:46:18.208: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:46:19.191360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:20.191450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:46:20.22
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:46:20.23
  E0805 13:46:21.192221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:21.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  5 13:46:21.235: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3685-crds.webhook.example.com via the AdmissionRegistration API @ 08/05/23 13:46:21.747
  STEP: Creating a custom resource while v1 is storage version @ 08/05/23 13:46:21.762
  E0805 13:46:22.193253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:23.194265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/05/23 13:46:23.811
  STEP: Patching the custom resource while v2 is storage version @ 08/05/23 13:46:23.828
  Aug  5 13:46:23.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0805 13:46:24.194969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-5553" for this suite. @ 08/05/23 13:46:24.414
  STEP: Destroying namespace "webhook-markers-8092" for this suite. @ 08/05/23 13:46:24.422
• [6.674 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/05/23 13:46:24.433
  Aug  5 13:46:24.433: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:46:24.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:24.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:24.453
  STEP: creating a collection of services @ 08/05/23 13:46:24.456
  Aug  5 13:46:24.456: INFO: Creating e2e-svc-a-dwj9k
  Aug  5 13:46:24.468: INFO: Creating e2e-svc-b-2xl7r
  Aug  5 13:46:24.479: INFO: Creating e2e-svc-c-m9nmn
  STEP: deleting service collection @ 08/05/23 13:46:24.494
  Aug  5 13:46:24.528: INFO: Collection of services has been deleted
  Aug  5 13:46:24.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7355" for this suite. @ 08/05/23 13:46:24.532
• [0.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/05/23 13:46:24.542
  Aug  5 13:46:24.542: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename certificates @ 08/05/23 13:46:24.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:24.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:24.563
  STEP: getting /apis @ 08/05/23 13:46:24.969
  STEP: getting /apis/certificates.k8s.io @ 08/05/23 13:46:24.972
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/05/23 13:46:24.973
  STEP: creating @ 08/05/23 13:46:24.974
  STEP: getting @ 08/05/23 13:46:24.992
  STEP: listing @ 08/05/23 13:46:24.995
  STEP: watching @ 08/05/23 13:46:24.999
  Aug  5 13:46:24.999: INFO: starting watch
  STEP: patching @ 08/05/23 13:46:25
  STEP: updating @ 08/05/23 13:46:25.005
  Aug  5 13:46:25.012: INFO: waiting for watch events with expected annotations
  Aug  5 13:46:25.012: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/05/23 13:46:25.012
  STEP: patching /approval @ 08/05/23 13:46:25.015
  STEP: updating /approval @ 08/05/23 13:46:25.02
  STEP: getting /status @ 08/05/23 13:46:25.027
  STEP: patching /status @ 08/05/23 13:46:25.03
  STEP: updating /status @ 08/05/23 13:46:25.038
  STEP: deleting @ 08/05/23 13:46:25.045
  STEP: deleting a collection @ 08/05/23 13:46:25.058
  Aug  5 13:46:25.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3758" for this suite. @ 08/05/23 13:46:25.08
• [0.545 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/05/23 13:46:25.09
  Aug  5 13:46:25.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:46:25.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:25.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:25.11
  STEP: Setting up server cert @ 08/05/23 13:46:25.137
  E0805 13:46:25.195745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:46:25.356
  STEP: Deploying the webhook pod @ 08/05/23 13:46:25.363
  STEP: Wait for the deployment to be ready @ 08/05/23 13:46:25.374
  Aug  5 13:46:25.383: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:46:26.196695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:27.196801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:46:27.395
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:46:27.405
  E0805 13:46:28.197380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:28.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/05/23 13:46:28.41
  STEP: create a namespace for the webhook @ 08/05/23 13:46:28.426
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/05/23 13:46:28.443
  Aug  5 13:46:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9081" for this suite. @ 08/05/23 13:46:28.546
  STEP: Destroying namespace "webhook-markers-3067" for this suite. @ 08/05/23 13:46:28.556
  STEP: Destroying namespace "fail-closed-namespace-505" for this suite. @ 08/05/23 13:46:28.563
• [3.479 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/05/23 13:46:28.57
  Aug  5 13:46:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename endpointslice @ 08/05/23 13:46:28.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:28.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:28.587
  Aug  5 13:46:28.605: INFO: Endpoints addresses: [172.31.11.29 172.31.47.149] , ports: [6443]
  Aug  5 13:46:28.605: INFO: EndpointSlices addresses: [172.31.11.29 172.31.47.149] , ports: [6443]
  Aug  5 13:46:28.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6348" for this suite. @ 08/05/23 13:46:28.609
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/05/23 13:46:28.619
  Aug  5 13:46:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-probe @ 08/05/23 13:46:28.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:28.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:28.639
  STEP: Creating pod liveness-ef61b544-1508-4f26-bf08-b2c1783bfb5f in namespace container-probe-4931 @ 08/05/23 13:46:28.646
  E0805 13:46:29.197873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:30.198741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:30.664: INFO: Started pod liveness-ef61b544-1508-4f26-bf08-b2c1783bfb5f in namespace container-probe-4931
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/05/23 13:46:30.664
  Aug  5 13:46:30.667: INFO: Initial restart count of pod liveness-ef61b544-1508-4f26-bf08-b2c1783bfb5f is 0
  E0805 13:46:31.199452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:32.199639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:33.200313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:34.200530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:35.201328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:36.202367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:37.202470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:38.202622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:39.203355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:40.203538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:41.203597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:42.203717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:43.203750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:44.204656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:45.204774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:46.205002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:47.206022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:48.206180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:49.206703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:50.207689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:50.716: INFO: Restart count of pod container-probe-4931/liveness-ef61b544-1508-4f26-bf08-b2c1783bfb5f is now 1 (20.048782717s elapsed)
  Aug  5 13:46:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/05/23 13:46:50.721
  STEP: Destroying namespace "container-probe-4931" for this suite. @ 08/05/23 13:46:50.735
• [22.123 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/05/23 13:46:50.743
  Aug  5 13:46:50.743: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/05/23 13:46:50.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:50.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:50.768
  STEP: create the container to handle the HTTPGet hook request. @ 08/05/23 13:46:50.775
  E0805 13:46:51.207813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:52.208650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/05/23 13:46:52.796
  E0805 13:46:53.208925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:54.209020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/05/23 13:46:54.816
  STEP: delete the pod with lifecycle hook @ 08/05/23 13:46:54.823
  E0805 13:46:55.209224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:56.209295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:46:56.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3108" for this suite. @ 08/05/23 13:46:56.844
• [6.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/05/23 13:46:56.854
  Aug  5 13:46:56.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename emptydir @ 08/05/23 13:46:56.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:46:56.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:46:56.885
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/05/23 13:46:56.893
  E0805 13:46:57.209869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:58.210700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:46:59.211756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:00.212332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:47:00.921
  Aug  5 13:47:00.925: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-53711773-6fc8-46e9-b372-bcec8298a789 container test-container: <nil>
  STEP: delete the pod @ 08/05/23 13:47:00.948
  Aug  5 13:47:00.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1090" for this suite. @ 08/05/23 13:47:00.973
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/05/23 13:47:00.986
  Aug  5 13:47:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename cronjob @ 08/05/23 13:47:00.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:47:01.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:47:01.008
  STEP: Creating a cronjob @ 08/05/23 13:47:01.012
  STEP: Ensuring more than one job is running at a time @ 08/05/23 13:47:01.018
  E0805 13:47:01.212870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:02.213087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:03.213078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:04.213726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:05.214057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:06.214271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:07.215133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:08.215316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:09.216290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:10.216365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:11.216901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:12.217001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:13.218018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:14.218119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:15.218219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:16.218330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:17.219031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:18.219225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:19.219699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:20.219770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:21.220266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:22.220483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:23.220546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:24.220658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:25.221644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:26.221722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:27.222188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:28.222305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:29.222976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:30.223213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:31.223626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:32.223670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:33.224460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:34.224542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:35.224628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:36.224708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:37.225685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:38.226738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:39.227044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:40.227134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:41.227235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:42.227681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:43.228542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:44.228659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:45.229053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:46.229254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:47.229312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:48.229396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:49.230202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:50.230264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:51.230689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:52.230765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:53.231536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:54.231638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:55.232489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:56.232871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:57.233078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:58.233066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:47:59.234140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:00.234216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:01.235228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:02.235310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:03.235416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:04.235502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:05.235995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:06.237045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:07.237435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:08.237618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:09.238056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:10.238691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:11.239731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:12.239916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:13.240033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:14.240810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:15.241014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:16.241253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:17.242205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:18.242757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:19.243194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:20.243286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:21.243372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:22.243586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:23.244524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:24.244700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:25.245073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:26.245167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:27.245852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:28.246001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:29.246675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:30.246833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:31.247399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:32.247592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:33.247677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:34.247867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:35.248481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:36.248856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:37.249218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:38.249450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:39.250174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:40.250448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:41.250533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:42.250619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:43.250698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:44.251033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:45.251064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:46.251266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:47.252210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:48.252411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:49.252935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:50.253030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:51.253395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:52.253494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:53.254254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:54.254353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:55.254770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:56.254989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:57.255077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:58.255738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:48:59.256378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:00.256494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/05/23 13:49:01.024
  STEP: Removing cronjob @ 08/05/23 13:49:01.028
  Aug  5 13:49:01.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3764" for this suite. @ 08/05/23 13:49:01.039
• [120.060 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/05/23 13:49:01.047
  Aug  5 13:49:01.047: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:49:01.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:01.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:01.176
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:49:01.178
  E0805 13:49:01.257051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:02.257152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:03.258063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:04.258194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:49:05.202
  Aug  5 13:49:05.206: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-fc1cdae8-ba6c-4ae9-993e-982bfa2f7268 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:49:05.222
  Aug  5 13:49:05.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5329" for this suite. @ 08/05/23 13:49:05.245
• [4.205 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/05/23 13:49:05.252
  Aug  5 13:49:05.252: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:49:05.253
  E0805 13:49:05.258559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:05.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:05.277
  STEP: Setting up server cert @ 08/05/23 13:49:05.311
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:49:06.205
  STEP: Deploying the webhook pod @ 08/05/23 13:49:06.214
  STEP: Wait for the deployment to be ready @ 08/05/23 13:49:06.225
  Aug  5 13:49:06.232: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0805 13:49:06.259342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:07.259470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:49:08.243
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:49:08.257
  E0805 13:49:08.260349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:49:09.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0805 13:49:09.260381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/05/23 13:49:09.262
  STEP: create a pod that should be denied by the webhook @ 08/05/23 13:49:09.278
  STEP: create a pod that causes the webhook to hang @ 08/05/23 13:49:09.292
  E0805 13:49:10.260508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:11.260731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:12.260943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:13.261025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:14.261107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:15.261188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:16.261570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:17.261632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:18.262679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:19.262844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 08/05/23 13:49:19.298
  STEP: create a configmap that should be admitted by the webhook @ 08/05/23 13:49:19.308
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/05/23 13:49:19.319
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/05/23 13:49:19.328
  STEP: create a namespace that bypass the webhook @ 08/05/23 13:49:19.333
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/05/23 13:49:19.35
  Aug  5 13:49:19.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7771" for this suite. @ 08/05/23 13:49:19.416
  STEP: Destroying namespace "webhook-markers-4946" for this suite. @ 08/05/23 13:49:19.424
  STEP: Destroying namespace "exempted-namespace-5191" for this suite. @ 08/05/23 13:49:19.431
• [14.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/05/23 13:49:19.439
  Aug  5 13:49:19.439: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:49:19.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:19.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:19.458
  STEP: Creating configMap with name configmap-test-upd-0ea58e84-3e4e-4914-bff3-80744e0c3cd9 @ 08/05/23 13:49:19.465
  STEP: Creating the pod @ 08/05/23 13:49:19.469
  E0805 13:49:20.262958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:21.263517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 08/05/23 13:49:21.489
  STEP: Waiting for pod with binary data @ 08/05/23 13:49:21.497
  Aug  5 13:49:21.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6059" for this suite. @ 08/05/23 13:49:21.508
• [2.077 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/05/23 13:49:21.517
  Aug  5 13:49:21.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 13:49:21.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:21.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:21.537
  Aug  5 13:49:21.566: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"88b127a5-7bb6-4950-99c6-ac0e36343dff", Controller:(*bool)(0xc005106b96), BlockOwnerDeletion:(*bool)(0xc005106b97)}}
  Aug  5 13:49:21.574: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"82a48819-1feb-4018-80b4-46fb3f53b702", Controller:(*bool)(0xc005106dc6), BlockOwnerDeletion:(*bool)(0xc005106dc7)}}
  Aug  5 13:49:21.582: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"55f221ca-a604-462a-af0d-99853d98a264", Controller:(*bool)(0xc005107006), BlockOwnerDeletion:(*bool)(0xc005107007)}}
  E0805 13:49:22.264421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:23.264460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:24.265548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:25.265645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:26.265726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:49:26.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6366" for this suite. @ 08/05/23 13:49:26.599
• [5.091 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/05/23 13:49:26.608
  Aug  5 13:49:26.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:49:26.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:26.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:26.628
  STEP: Creating secret with name secret-test-a3bb94e2-4c92-4388-b07d-660adac0daa6 @ 08/05/23 13:49:26.63
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:49:26.635
  E0805 13:49:27.266667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:28.266905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:29.267981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:30.268198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:49:30.656
  Aug  5 13:49:30.660: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-c2f75d49-6e23-48ad-9146-730c9871c8e8 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:49:30.667
  Aug  5 13:49:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6960" for this suite. @ 08/05/23 13:49:30.691
• [4.091 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/05/23 13:49:30.699
  Aug  5 13:49:30.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:49:30.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:30.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:30.721
  STEP: Counting existing ResourceQuota @ 08/05/23 13:49:30.723
  E0805 13:49:31.268751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:32.269316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:33.269397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:34.270321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:35.270415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/05/23 13:49:35.727
  STEP: Ensuring resource quota status is calculated @ 08/05/23 13:49:35.732
  E0805 13:49:36.270522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:37.270618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 08/05/23 13:49:37.738
  STEP: Creating a NodePort Service @ 08/05/23 13:49:37.765
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/05/23 13:49:37.792
  STEP: Ensuring resource quota status captures service creation @ 08/05/23 13:49:37.826
  E0805 13:49:38.271393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:39.271571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 08/05/23 13:49:39.83
  STEP: Ensuring resource quota status released usage @ 08/05/23 13:49:39.865
  E0805 13:49:40.272250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:41.272367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:49:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7473" for this suite. @ 08/05/23 13:49:41.875
• [11.182 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/05/23 13:49:41.883
  Aug  5 13:49:41.883: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename podtemplate @ 08/05/23 13:49:41.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:41.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:41.903
  STEP: Create set of pod templates @ 08/05/23 13:49:41.906
  Aug  5 13:49:41.911: INFO: created test-podtemplate-1
  Aug  5 13:49:41.916: INFO: created test-podtemplate-2
  Aug  5 13:49:41.921: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/05/23 13:49:41.921
  STEP: delete collection of pod templates @ 08/05/23 13:49:41.925
  Aug  5 13:49:41.925: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/05/23 13:49:41.944
  Aug  5 13:49:41.944: INFO: requesting list of pod templates to confirm quantity
  Aug  5 13:49:41.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1550" for this suite. @ 08/05/23 13:49:41.951
• [0.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/05/23 13:49:41.959
  Aug  5 13:49:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename webhook @ 08/05/23 13:49:41.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:41.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:41.978
  STEP: Setting up server cert @ 08/05/23 13:49:42.001
  E0805 13:49:42.272527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/05/23 13:49:42.492
  STEP: Deploying the webhook pod @ 08/05/23 13:49:42.5
  STEP: Wait for the deployment to be ready @ 08/05/23 13:49:42.513
  Aug  5 13:49:42.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0805 13:49:43.272632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:44.272725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/05/23 13:49:44.531
  STEP: Verifying the service has paired with the endpoint @ 08/05/23 13:49:44.541
  E0805 13:49:45.272839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:49:45.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/05/23 13:49:45.546
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/05/23 13:49:45.562
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/05/23 13:49:45.571
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/05/23 13:49:45.582
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/05/23 13:49:45.594
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/05/23 13:49:45.601
  Aug  5 13:49:45.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9243" for this suite. @ 08/05/23 13:49:45.764
  STEP: Destroying namespace "webhook-markers-6727" for this suite. @ 08/05/23 13:49:45.772
• [3.821 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/05/23 13:49:45.78
  Aug  5 13:49:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename statefulset @ 08/05/23 13:49:45.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:49:45.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:49:45.809
  STEP: Creating service test in namespace statefulset-8138 @ 08/05/23 13:49:45.811
  STEP: Creating stateful set ss in namespace statefulset-8138 @ 08/05/23 13:49:45.817
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8138 @ 08/05/23 13:49:45.824
  Aug  5 13:49:45.830: INFO: Found 0 stateful pods, waiting for 1
  E0805 13:49:46.273788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:47.273854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:48.274130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:49.274282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:50.274366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:51.275424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:52.275520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:53.275717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:54.276206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:55.276296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:49:55.835: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/05/23 13:49:55.835
  Aug  5 13:49:55.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:49:55.972: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:49:55.972: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:49:55.972: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:49:55.977: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0805 13:49:56.276385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:57.276795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:58.276613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:49:59.276874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:00.277753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:01.278232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:02.279250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:03.279678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:04.279772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:05.279935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:05.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:50:05.982: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:50:05.999: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug  5 13:50:05.999: INFO: ss-0  ip-172-31-95-133  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC  }]
  Aug  5 13:50:05.999: INFO: 
  Aug  5 13:50:05.999: INFO: StatefulSet ss has not reached scale 3, at 1
  E0805 13:50:06.280413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:07.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996671561s
  E0805 13:50:07.280979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:08.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992145438s
  E0805 13:50:08.281194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:09.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986761502s
  E0805 13:50:09.281781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:10.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982247654s
  E0805 13:50:10.281865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:11.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977866852s
  E0805 13:50:11.282695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:12.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973171779s
  E0805 13:50:12.283248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:13.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968784596s
  E0805 13:50:13.283860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:14.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964140828s
  E0805 13:50:14.284481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:15.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.497163ms
  E0805 13:50:15.284534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8138 @ 08/05/23 13:50:16.04
  Aug  5 13:50:16.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:50:16.168: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  5 13:50:16.168: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:50:16.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:50:16.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0805 13:50:16.284792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:16.295: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug  5 13:50:16.295: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:50:16.295: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:50:16.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  5 13:50:16.420: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug  5 13:50:16.420: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  5 13:50:16.420: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  5 13:50:16.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:50:16.425: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  5 13:50:16.425: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/05/23 13:50:16.425
  Aug  5 13:50:16.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:50:16.546: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:50:16.547: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:50:16.547: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:50:16.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:50:16.685: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:50:16.685: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:50:16.685: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:50:16.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=statefulset-8138 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  5 13:50:16.806: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  5 13:50:16.806: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  5 13:50:16.806: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  5 13:50:16.806: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:50:16.810: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0805 13:50:17.284996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:18.285111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:19.285182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:20.285298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:21.285385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:22.285631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:23.285722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:24.285841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:25.285929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:26.286001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:26.819: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:50:26.819: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:50:26.819: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug  5 13:50:26.832: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Aug  5 13:50:26.832: INFO: ss-0  ip-172-31-95-133  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC  }]
  Aug  5 13:50:26.832: INFO: ss-1  ip-172-31-35-140  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC  }]
  Aug  5 13:50:26.832: INFO: ss-2  ip-172-31-1-47    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC  }]
  Aug  5 13:50:26.832: INFO: 
  Aug  5 13:50:26.832: INFO: StatefulSet ss has not reached scale 0, at 3
  E0805 13:50:27.286713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:27.837: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Aug  5 13:50:27.837: INFO: ss-0  ip-172-31-95-133  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:49:45 +0000 UTC  }]
  Aug  5 13:50:27.837: INFO: ss-2  ip-172-31-1-47    Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:17 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-05 13:50:06 +0000 UTC  }]
  Aug  5 13:50:27.837: INFO: 
  Aug  5 13:50:27.837: INFO: StatefulSet ss has not reached scale 0, at 2
  E0805 13:50:28.287262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:28.842: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990124663s
  E0805 13:50:29.288228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:29.847: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.985227233s
  E0805 13:50:30.288597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:30.851: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98136521s
  E0805 13:50:31.289343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:31.856: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976523299s
  E0805 13:50:32.289588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:32.860: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972501041s
  E0805 13:50:33.289667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:33.865: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967373322s
  E0805 13:50:34.289757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:34.869: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963314257s
  E0805 13:50:35.290701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:35.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.103063ms
  E0805 13:50:36.291311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8138 @ 08/05/23 13:50:36.873
  Aug  5 13:50:36.878: INFO: Scaling statefulset ss to 0
  Aug  5 13:50:36.890: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:50:36.894: INFO: Deleting all statefulset in ns statefulset-8138
  Aug  5 13:50:36.897: INFO: Scaling statefulset ss to 0
  Aug  5 13:50:36.908: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  5 13:50:36.911: INFO: Deleting statefulset ss
  Aug  5 13:50:36.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8138" for this suite. @ 08/05/23 13:50:36.928
• [51.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/05/23 13:50:36.938
  Aug  5 13:50:36.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:50:36.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:50:36.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:50:36.958
  STEP: Creating configMap with name projected-configmap-test-volume-6ccfa581-71b3-4aad-bed2-4519dfcc6396 @ 08/05/23 13:50:36.96
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:50:36.965
  E0805 13:50:37.291405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:38.291596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:39.292421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:40.292521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:50:40.988
  Aug  5 13:50:40.992: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-projected-configmaps-d9b5552e-7714-4793-9932-b2cd81ff8e11 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:50:41.003
  Aug  5 13:50:41.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3245" for this suite. @ 08/05/23 13:50:41.028
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/05/23 13:50:41.039
  Aug  5 13:50:41.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename services @ 08/05/23 13:50:41.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:50:41.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:50:41.057
  STEP: creating service multi-endpoint-test in namespace services-2824 @ 08/05/23 13:50:41.06
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2824 to expose endpoints map[] @ 08/05/23 13:50:41.069
  Aug  5 13:50:41.074: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0805 13:50:41.293045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:42.083: INFO: successfully validated that service multi-endpoint-test in namespace services-2824 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2824 @ 08/05/23 13:50:42.083
  E0805 13:50:42.293680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:43.293764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2824 to expose endpoints map[pod1:[100]] @ 08/05/23 13:50:44.106
  Aug  5 13:50:44.116: INFO: successfully validated that service multi-endpoint-test in namespace services-2824 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-2824 @ 08/05/23 13:50:44.116
  E0805 13:50:44.294137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:45.294214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2824 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/05/23 13:50:46.136
  Aug  5 13:50:46.151: INFO: successfully validated that service multi-endpoint-test in namespace services-2824 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/05/23 13:50:46.151
  Aug  5 13:50:46.151: INFO: Creating new exec pod
  E0805 13:50:46.295149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:47.295352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:48.296180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:49.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2824 exec execpodj6sq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Aug  5 13:50:49.288: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug  5 13:50:49.288: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:50:49.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2824 exec execpodj6sq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.17 80'
  E0805 13:50:49.296883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:49.419: INFO: stderr: "+ nc -v -t -w 2 10.152.183.17 80\n+ echo hostName\nConnection to 10.152.183.17 80 port [tcp/http] succeeded!\n"
  Aug  5 13:50:49.419: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:50:49.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2824 exec execpodj6sq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Aug  5 13:50:49.541: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug  5 13:50:49.541: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  5 13:50:49.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4235439149 --namespace=services-2824 exec execpodj6sq2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.17 81'
  Aug  5 13:50:49.664: INFO: stderr: "+ nc -v -t -w 2 10.152.183.17 81\n+ echo hostName\nConnection to 10.152.183.17 81 port [tcp/*] succeeded!\n"
  Aug  5 13:50:49.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2824 @ 08/05/23 13:50:49.664
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2824 to expose endpoints map[pod2:[101]] @ 08/05/23 13:50:49.683
  E0805 13:50:50.297239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:50.706: INFO: successfully validated that service multi-endpoint-test in namespace services-2824 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-2824 @ 08/05/23 13:50:50.706
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2824 to expose endpoints map[] @ 08/05/23 13:50:50.718
  Aug  5 13:50:50.731: INFO: successfully validated that service multi-endpoint-test in namespace services-2824 exposes endpoints map[]
  Aug  5 13:50:50.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2824" for this suite. @ 08/05/23 13:50:50.754
• [9.723 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/05/23 13:50:50.764
  Aug  5 13:50:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename configmap @ 08/05/23 13:50:50.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:50:50.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:50:50.788
  STEP: Creating configMap configmap-9660/configmap-test-f57771cc-28b5-4e78-b92a-6b8515c05a97 @ 08/05/23 13:50:50.791
  STEP: Creating a pod to test consume configMaps @ 08/05/23 13:50:50.795
  E0805 13:50:51.297550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:52.297876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:53.297952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:54.298053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:50:54.819
  Aug  5 13:50:54.823: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-configmaps-e3a7e0db-c283-465c-b871-6375d2a45bb5 container env-test: <nil>
  STEP: delete the pod @ 08/05/23 13:50:54.831
  Aug  5 13:50:54.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9660" for this suite. @ 08/05/23 13:50:54.85
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/05/23 13:50:54.859
  Aug  5 13:50:54.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename proxy @ 08/05/23 13:50:54.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:50:54.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:50:54.878
  Aug  5 13:50:54.881: INFO: Creating pod...
  E0805 13:50:55.298264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:56.298358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:50:56.899: INFO: Creating service...
  Aug  5 13:50:56.910: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=DELETE
  Aug  5 13:50:56.917: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  5 13:50:56.917: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=OPTIONS
  Aug  5 13:50:56.922: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  5 13:50:56.922: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=PATCH
  Aug  5 13:50:56.927: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  5 13:50:56.927: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=POST
  Aug  5 13:50:56.931: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  5 13:50:56.931: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=PUT
  Aug  5 13:50:56.935: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  5 13:50:56.935: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug  5 13:50:56.942: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  5 13:50:56.942: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug  5 13:50:56.950: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  5 13:50:56.950: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug  5 13:50:56.956: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  5 13:50:56.956: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=POST
  Aug  5 13:50:56.962: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  5 13:50:56.962: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=PUT
  Aug  5 13:50:56.969: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  5 13:50:56.969: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=GET
  Aug  5 13:50:56.972: INFO: http.Client request:GET StatusCode:301
  Aug  5 13:50:56.972: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=GET
  Aug  5 13:50:56.978: INFO: http.Client request:GET StatusCode:301
  Aug  5 13:50:56.978: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/pods/agnhost/proxy?method=HEAD
  Aug  5 13:50:56.981: INFO: http.Client request:HEAD StatusCode:301
  Aug  5 13:50:56.981: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1347/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug  5 13:50:56.987: INFO: http.Client request:HEAD StatusCode:301
  Aug  5 13:50:56.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1347" for this suite. @ 08/05/23 13:50:56.991
• [2.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/05/23 13:50:57
  Aug  5 13:50:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename secrets @ 08/05/23 13:50:57.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:50:57.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:50:57.021
  STEP: Creating secret with name secret-test-c7537d96-dff2-4662-a0be-c7e73f2fdc22 @ 08/05/23 13:50:57.024
  STEP: Creating a pod to test consume secrets @ 08/05/23 13:50:57.028
  E0805 13:50:57.298700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:58.299563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:50:59.300367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:00.300455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:51:01.051
  Aug  5 13:51:01.055: INFO: Trying to get logs from node ip-172-31-95-133 pod pod-secrets-2ff413d5-463d-4961-bd5d-a78eadb46904 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/05/23 13:51:01.062
  Aug  5 13:51:01.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2528" for this suite. @ 08/05/23 13:51:01.087
• [4.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/05/23 13:51:01.095
  Aug  5 13:51:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename security-context-test @ 08/05/23 13:51:01.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:01.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:01.116
  E0805 13:51:01.301459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:02.301647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:03.301709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:04.302679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:05.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4934" for this suite. @ 08/05/23 13:51:05.151
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/05/23 13:51:05.16
  Aug  5 13:51:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename resourcequota @ 08/05/23 13:51:05.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:05.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:05.179
  STEP: Counting existing ResourceQuota @ 08/05/23 13:51:05.181
  E0805 13:51:05.303616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:06.304000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:07.304700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:08.305698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:09.306320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/05/23 13:51:10.185
  STEP: Ensuring resource quota status is calculated @ 08/05/23 13:51:10.193
  E0805 13:51:10.306506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:11.307475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:12.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6781" for this suite. @ 08/05/23 13:51:12.203
• [7.050 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/05/23 13:51:12.21
  Aug  5 13:51:12.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename var-expansion @ 08/05/23 13:51:12.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:12.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:12.231
  STEP: Creating a pod to test substitution in container's command @ 08/05/23 13:51:12.233
  E0805 13:51:12.308537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:13.308648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:14.309070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:15.309115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:51:16.257
  Aug  5 13:51:16.261: INFO: Trying to get logs from node ip-172-31-95-133 pod var-expansion-85146fa2-1cfd-4b66-b1ce-f51e9aab0433 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:51:16.269
  Aug  5 13:51:16.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4247" for this suite. @ 08/05/23 13:51:16.291
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/05/23 13:51:16.301
  Aug  5 13:51:16.301: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/05/23 13:51:16.302
  E0805 13:51:16.309897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:16.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:16.322
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/05/23 13:51:16.324
  Aug  5 13:51:16.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:51:17.310101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:17.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  E0805 13:51:18.311097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:19.311396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:20.312052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:21.312507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:22.312630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:23.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-922" for this suite. @ 08/05/23 13:51:23.188
• [6.893 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/05/23 13:51:23.196
  Aug  5 13:51:23.196: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename downward-api @ 08/05/23 13:51:23.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:23.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:23.214
  STEP: Creating a pod to test downward api env vars @ 08/05/23 13:51:23.218
  E0805 13:51:23.313159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:24.313482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:25.314242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:26.314667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:51:27.237
  Aug  5 13:51:27.241: INFO: Trying to get logs from node ip-172-31-95-133 pod downward-api-9ae635be-2c41-4bc3-9b99-54e6c19b42e8 container dapi-container: <nil>
  STEP: delete the pod @ 08/05/23 13:51:27.256
  Aug  5 13:51:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7600" for this suite. @ 08/05/23 13:51:27.275
• [4.086 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/05/23 13:51:27.282
  Aug  5 13:51:27.282: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename projected @ 08/05/23 13:51:27.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:27.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:27.301
  STEP: Creating a pod to test downward API volume plugin @ 08/05/23 13:51:27.304
  E0805 13:51:27.314932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:28.315237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:29.315533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:30.315720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:31.316102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/05/23 13:51:31.327
  Aug  5 13:51:31.331: INFO: Trying to get logs from node ip-172-31-95-133 pod downwardapi-volume-ec2c7307-ed8d-406d-be5d-2449474a2085 container client-container: <nil>
  STEP: delete the pod @ 08/05/23 13:51:31.338
  Aug  5 13:51:31.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8106" for this suite. @ 08/05/23 13:51:31.359
• [4.083 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/05/23 13:51:31.366
  Aug  5 13:51:31.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename job @ 08/05/23 13:51:31.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:31.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:31.382
  STEP: Creating a suspended job @ 08/05/23 13:51:31.394
  STEP: Patching the Job @ 08/05/23 13:51:31.399
  STEP: Watching for Job to be patched @ 08/05/23 13:51:31.418
  Aug  5 13:51:31.420: INFO: Event ADDED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug  5 13:51:31.420: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug  5 13:51:31.420: INFO: Event MODIFIED found for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/05/23 13:51:31.42
  STEP: Watching for Job to be updated @ 08/05/23 13:51:31.43
  Aug  5 13:51:31.432: INFO: Event MODIFIED found for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:31.432: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/05/23 13:51:31.432
  Aug  5 13:51:31.436: INFO: Job: e2e-glqbw as labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw]
  STEP: Waiting for job to complete @ 08/05/23 13:51:31.436
  E0805 13:51:32.316300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:33.316460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:34.317492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:35.317643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:36.317756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:37.317850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:38.317960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:39.318046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:40.318166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:41.318404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 08/05/23 13:51:41.441
  STEP: Watching for Job to be deleted @ 08/05/23 13:51:41.449
  Aug  5 13:51:41.452: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:41.452: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:41.452: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:41.453: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:41.453: INFO: Event MODIFIED observed for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  5 13:51:41.453: INFO: Event DELETED found for Job e2e-glqbw in namespace job-2342 with labels: map[e2e-glqbw:patched e2e-job-label:e2e-glqbw] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/05/23 13:51:41.453
  Aug  5 13:51:41.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2342" for this suite. @ 08/05/23 13:51:41.464
• [10.123 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/05/23 13:51:41.49
  Aug  5 13:51:41.490: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename replication-controller @ 08/05/23 13:51:41.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:41.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:41.508
  STEP: Creating replication controller my-hostname-basic-8c9fbdae-b32a-4c6a-85ad-75311740d3d9 @ 08/05/23 13:51:41.511
  Aug  5 13:51:41.523: INFO: Pod name my-hostname-basic-8c9fbdae-b32a-4c6a-85ad-75311740d3d9: Found 0 pods out of 1
  E0805 13:51:42.318783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:43.319016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:44.319112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:45.319733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:46.319809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:46.528: INFO: Pod name my-hostname-basic-8c9fbdae-b32a-4c6a-85ad-75311740d3d9: Found 1 pods out of 1
  Aug  5 13:51:46.528: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8c9fbdae-b32a-4c6a-85ad-75311740d3d9" are running
  Aug  5 13:51:46.531: INFO: Pod "my-hostname-basic-8c9fbdae-b32a-4c6a-85ad-75311740d3d9-fwpzw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 13:51:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 13:51:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 13:51:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-05 13:51:41 +0000 UTC Reason: Message:}])
  Aug  5 13:51:46.531: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/05/23 13:51:46.531
  Aug  5 13:51:46.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-681" for this suite. @ 08/05/23 13:51:46.547
• [5.064 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/05/23 13:51:46.555
  Aug  5 13:51:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename kubelet-test @ 08/05/23 13:51:46.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:46.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:46.58
  E0805 13:51:47.319903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:48.320169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:49.320277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:50.320419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  5 13:51:50.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5028" for this suite. @ 08/05/23 13:51:50.606
• [4.062 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/05/23 13:51:50.618
  Aug  5 13:51:50.618: INFO: >>> kubeConfig: /tmp/kubeconfig-4235439149
  STEP: Building a namespace api object, basename gc @ 08/05/23 13:51:50.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/05/23 13:51:50.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/05/23 13:51:50.635
  STEP: create the rc @ 08/05/23 13:51:50.639
  W0805 13:51:50.645746      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0805 13:51:51.320844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:52.320923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:53.321231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:54.321331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:55.321427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/05/23 13:51:55.653
  STEP: wait for all pods to be garbage collected @ 08/05/23 13:51:55.66
  E0805 13:51:56.321649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:57.321961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:58.322044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:51:59.322702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0805 13:52:00.322840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/05/23 13:52:00.668
  W0805 13:52:00.672400      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Aug  5 13:52:00.672: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  5 13:52:00.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1121" for this suite. @ 08/05/23 13:52:00.677
• [10.065 seconds]
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug  5 13:52:00.684: INFO: Running AfterSuite actions on node 1
  Aug  5 13:52:00.684: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.043 seconds]
------------------------------

Ran 378 of 7207 Specs in 6229.812 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h43m50.195711071s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

