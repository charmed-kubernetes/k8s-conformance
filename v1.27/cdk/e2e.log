  I0624 12:09:06.868930      19 e2e.go:117] Starting e2e run "f5149a8b-d2eb-4506-8cd6-d2dd259ec86e" on Ginkgo node 1
  Jun 24 12:09:06.911: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1687608546 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jun 24 12:09:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:09:07.263: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jun 24 12:09:07.308: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jun 24 12:09:07.313: INFO: e2e test version: v1.27.3
  Jun 24 12:09:07.314: INFO: kube-apiserver version: v1.27.3
  Jun 24 12:09:07.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:09:07.321: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.059 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 06/24/23 12:09:07.896
  Jun 24 12:09:07.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-runtime @ 06/24/23 12:09:07.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:07.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:07.921
  STEP: create the container @ 06/24/23 12:09:07.928
  W0624 12:09:07.939218      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/24/23 12:09:07.939
  STEP: get the container status @ 06/24/23 12:09:26.037
  STEP: the container should be terminated @ 06/24/23 12:09:26.041
  STEP: the termination message should be set @ 06/24/23 12:09:26.041
  Jun 24 12:09:26.041: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/24/23 12:09:26.041
  Jun 24 12:09:26.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3440" for this suite. @ 06/24/23 12:09:26.062
• [18.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 06/24/23 12:09:26.072
  Jun 24 12:09:26.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:09:26.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:26.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:26.096
  STEP: Creating configMap configmap-5711/configmap-test-a2f58ae4-71ea-4393-84fc-1ddd909c0a2a @ 06/24/23 12:09:26.101
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:09:26.106
  STEP: Saw pod success @ 06/24/23 12:09:30.139
  Jun 24 12:09:30.144: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-6d1c4df2-36db-4e8f-b21f-c3fc152b59de container env-test: <nil>
  STEP: delete the pod @ 06/24/23 12:09:30.166
  Jun 24 12:09:30.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5711" for this suite. @ 06/24/23 12:09:30.188
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 06/24/23 12:09:30.2
  Jun 24 12:09:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:09:30.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:30.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:30.273
  STEP: Creating projection with secret that has name projected-secret-test-3ff45bd2-f4e8-4731-a542-11939b001808 @ 06/24/23 12:09:30.278
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:09:30.284
  STEP: Saw pod success @ 06/24/23 12:09:40.326
  Jun 24 12:09:40.331: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-5d31475e-e5e0-4464-9048-cf2e92c655f0 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:09:40.339
  Jun 24 12:09:40.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5719" for this suite. @ 06/24/23 12:09:40.369
• [10.177 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 06/24/23 12:09:40.377
  Jun 24 12:09:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 12:09:40.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:40.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:40.401
  Jun 24 12:09:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  W0624 12:09:42.982238      19 warnings.go:70] unknown field "alpha"
  W0624 12:09:42.982271      19 warnings.go:70] unknown field "beta"
  W0624 12:09:42.982279      19 warnings.go:70] unknown field "delta"
  W0624 12:09:42.982286      19 warnings.go:70] unknown field "epsilon"
  W0624 12:09:42.982292      19 warnings.go:70] unknown field "gamma"
  Jun 24 12:09:43.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8426" for this suite. @ 06/24/23 12:09:43.024
• [2.654 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 06/24/23 12:09:43.033
  Jun 24 12:09:43.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename proxy @ 06/24/23 12:09:43.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:43.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:43.058
  Jun 24 12:09:43.062: INFO: Creating pod...
  Jun 24 12:09:45.087: INFO: Creating service...
  Jun 24 12:09:45.107: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=DELETE
  Jun 24 12:09:45.117: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 24 12:09:45.117: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=OPTIONS
  Jun 24 12:09:45.121: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 24 12:09:45.121: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=PATCH
  Jun 24 12:09:45.125: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 24 12:09:45.126: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=POST
  Jun 24 12:09:45.130: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 24 12:09:45.130: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=PUT
  Jun 24 12:09:45.135: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 24 12:09:45.135: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=DELETE
  Jun 24 12:09:45.142: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 24 12:09:45.142: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jun 24 12:09:45.149: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 24 12:09:45.150: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=PATCH
  Jun 24 12:09:45.156: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 24 12:09:45.156: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=POST
  Jun 24 12:09:45.163: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 24 12:09:45.163: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=PUT
  Jun 24 12:09:45.169: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 24 12:09:45.169: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=GET
  Jun 24 12:09:45.172: INFO: http.Client request:GET StatusCode:301
  Jun 24 12:09:45.173: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=GET
  Jun 24 12:09:45.178: INFO: http.Client request:GET StatusCode:301
  Jun 24 12:09:45.178: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/pods/agnhost/proxy?method=HEAD
  Jun 24 12:09:45.182: INFO: http.Client request:HEAD StatusCode:301
  Jun 24 12:09:45.182: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-1124/services/e2e-proxy-test-service/proxy?method=HEAD
  Jun 24 12:09:45.186: INFO: http.Client request:HEAD StatusCode:301
  Jun 24 12:09:45.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1124" for this suite. @ 06/24/23 12:09:45.192
• [2.168 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 06/24/23 12:09:45.201
  Jun 24 12:09:45.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename podtemplate @ 06/24/23 12:09:45.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:45.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:45.223
  Jun 24 12:09:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1388" for this suite. @ 06/24/23 12:09:45.269
• [0.076 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 06/24/23 12:09:45.279
  Jun 24 12:09:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename runtimeclass @ 06/24/23 12:09:45.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:45.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:45.304
  Jun 24 12:09:47.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-719" for this suite. @ 06/24/23 12:09:47.35
• [2.080 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 06/24/23 12:09:47.361
  Jun 24 12:09:47.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:09:47.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:47.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:47.382
  STEP: Setting up server cert @ 06/24/23 12:09:47.415
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:09:47.969
  STEP: Deploying the webhook pod @ 06/24/23 12:09:47.979
  STEP: Wait for the deployment to be ready @ 06/24/23 12:09:47.993
  Jun 24 12:09:48.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jun 24 12:09:50.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:09:52.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 9, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 06/24/23 12:09:54.026
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:09:54.038
  Jun 24 12:09:55.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 24 12:09:55.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 06/24/23 12:09:55.556
  STEP: Creating a custom resource that should be denied by the webhook @ 06/24/23 12:09:55.577
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 06/24/23 12:09:57.614
  STEP: Updating the custom resource with disallowed data should be denied @ 06/24/23 12:09:57.621
  STEP: Deleting the custom resource should be denied @ 06/24/23 12:09:57.634
  STEP: Remove the offending key and value from the custom resource data @ 06/24/23 12:09:57.644
  STEP: Deleting the updated custom resource should be successful @ 06/24/23 12:09:57.657
  Jun 24 12:09:57.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6814" for this suite. @ 06/24/23 12:09:58.311
  STEP: Destroying namespace "webhook-markers-782" for this suite. @ 06/24/23 12:09:58.318
• [10.967 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 06/24/23 12:09:58.329
  Jun 24 12:09:58.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 12:09:58.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:09:58.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:09:58.354
  Jun 24 12:09:58.383: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 06/24/23 12:09:58.391
  Jun 24 12:09:58.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:09:58.398: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 06/24/23 12:09:58.398
  Jun 24 12:09:58.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:09:58.424: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:09:59.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:09:59.430: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:00.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:00.430: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:01.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:01.429: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:02.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:02.429: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:03.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 12:10:03.431: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 06/24/23 12:10:03.435
  Jun 24 12:10:03.458: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 12:10:03.458: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jun 24 12:10:04.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:04.463: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 06/24/23 12:10:04.463
  Jun 24 12:10:04.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:04.478: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:05.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:05.485: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:06.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:06.483: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:07.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:07.482: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:08.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:08.482: INFO: Node ip-172-31-89-202 is running 0 daemon pod, expected 1
  Jun 24 12:10:09.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 12:10:09.484: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 12:10:09.492
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6075, will wait for the garbage collector to delete the pods @ 06/24/23 12:10:09.492
  Jun 24 12:10:09.556: INFO: Deleting DaemonSet.extensions daemon-set took: 9.102696ms
  Jun 24 12:10:09.656: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.633828ms
  Jun 24 12:10:11.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:10:11.462: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 12:10:11.470: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3585"},"items":null}

  Jun 24 12:10:11.475: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3585"},"items":null}

  Jun 24 12:10:11.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6075" for this suite. @ 06/24/23 12:10:11.508
• [13.188 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 06/24/23 12:10:11.519
  Jun 24 12:10:11.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:10:11.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:11.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:11.55
  STEP: Creating a pod to test substitution in volume subpath @ 06/24/23 12:10:11.554
  STEP: Saw pod success @ 06/24/23 12:10:15.58
  Jun 24 12:10:15.585: INFO: Trying to get logs from node ip-172-31-19-205 pod var-expansion-ee95c5af-6212-4f98-a0bf-8d07bda0b9f7 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:10:15.593
  Jun 24 12:10:15.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-627" for this suite. @ 06/24/23 12:10:15.621
• [4.111 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 06/24/23 12:10:15.631
  Jun 24 12:10:15.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:10:15.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:15.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:15.657
  STEP: Creating a pod to test downward api env vars @ 06/24/23 12:10:15.663
  STEP: Saw pod success @ 06/24/23 12:10:19.698
  Jun 24 12:10:19.703: INFO: Trying to get logs from node ip-172-31-19-205 pod downward-api-8e643ebb-813b-4e0c-b887-2b79dabca860 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:10:19.712
  Jun 24 12:10:19.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6456" for this suite. @ 06/24/23 12:10:19.739
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 06/24/23 12:10:19.749
  Jun 24 12:10:19.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:10:19.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:19.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:19.773
  STEP: set up a multi version CRD @ 06/24/23 12:10:19.778
  Jun 24 12:10:19.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: mark a version not serverd @ 06/24/23 12:10:23.656
  STEP: check the unserved version gets removed @ 06/24/23 12:10:23.681
  STEP: check the other version is not changed @ 06/24/23 12:10:25.12
  Jun 24 12:10:28.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5914" for this suite. @ 06/24/23 12:10:28.148
• [8.408 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 06/24/23 12:10:28.158
  Jun 24 12:10:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:10:28.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:28.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:28.183
  STEP: Creating configMap configmap-6759/configmap-test-c4e32a9d-3e37-407f-af4f-f8dab3f7e942 @ 06/24/23 12:10:28.187
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:10:28.193
  STEP: Saw pod success @ 06/24/23 12:10:32.218
  Jun 24 12:10:32.222: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-c6cfa26b-2fda-49c5-b8d1-e154e58f309e container env-test: <nil>
  STEP: delete the pod @ 06/24/23 12:10:32.242
  Jun 24 12:10:32.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6759" for this suite. @ 06/24/23 12:10:32.263
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 06/24/23 12:10:32.274
  Jun 24 12:10:32.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:10:32.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:32.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:32.297
  STEP: set up a multi version CRD @ 06/24/23 12:10:32.301
  Jun 24 12:10:32.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: rename a version @ 06/24/23 12:10:35.975
  STEP: check the new version name is served @ 06/24/23 12:10:35.995
  STEP: check the old version name is removed @ 06/24/23 12:10:36.888
  STEP: check the other version is not changed @ 06/24/23 12:10:37.645
  Jun 24 12:10:40.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5374" for this suite. @ 06/24/23 12:10:40.71
• [8.445 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 06/24/23 12:10:40.719
  Jun 24 12:10:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:10:40.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:40.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:40.745
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/24/23 12:10:40.749
  Jun 24 12:10:40.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1541 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun 24 12:10:40.852: INFO: stderr: ""
  Jun 24 12:10:40.852: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 06/24/23 12:10:40.852
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/24/23 12:10:45.904
  Jun 24 12:10:45.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1541 get pod e2e-test-httpd-pod -o json'
  Jun 24 12:10:45.990: INFO: stderr: ""
  Jun 24 12:10:45.990: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-24T12:10:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1541\",\n        \"resourceVersion\": \"3837\",\n        \"uid\": \"84a33f50-9cab-424a-b323-ec628622d0d1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tmnlw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-19-205\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tmnlw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-24T12:10:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-24T12:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-24T12:10:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-24T12:10:40Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c1498105a283fc2f54e466013abc6291d84c18b0cd62b76d25f12f50653df515\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-24T12:10:45Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.19.205\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.150.201\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.150.201\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-24T12:10:40Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 06/24/23 12:10:45.99
  Jun 24 12:10:45.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1541 replace -f -'
  Jun 24 12:10:46.804: INFO: stderr: ""
  Jun 24 12:10:46.804: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 06/24/23 12:10:46.804
  Jun 24 12:10:46.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1541 delete pods e2e-test-httpd-pod'
  Jun 24 12:10:52.900: INFO: stderr: ""
  Jun 24 12:10:52.901: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 24 12:10:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1541" for this suite. @ 06/24/23 12:10:52.905
• [12.196 seconds]
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 06/24/23 12:10:52.915
  Jun 24 12:10:52.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 12:10:52.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:52.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:52.941
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 06/24/23 12:10:52.948
  STEP: When a replication controller with a matching selector is created @ 06/24/23 12:10:54.974
  STEP: Then the orphan pod is adopted @ 06/24/23 12:10:54.981
  Jun 24 12:10:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3328" for this suite. @ 06/24/23 12:10:55.997
• [3.089 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 06/24/23 12:10:56.006
  Jun 24 12:10:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:10:56.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:56.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:56.035
  Jun 24 12:10:56.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5215" for this suite. @ 06/24/23 12:10:56.049
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 06/24/23 12:10:56.058
  Jun 24 12:10:56.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:10:56.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:10:56.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:10:56.081
  STEP: Creating the pod @ 06/24/23 12:10:56.09
  Jun 24 12:10:58.638: INFO: Successfully updated pod "labelsupdateb55534f4-fffc-41d7-8738-8acef104fe01"
  Jun 24 12:11:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1121" for this suite. @ 06/24/23 12:11:02.672
• [6.622 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 06/24/23 12:11:02.684
  Jun 24 12:11:02.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:11:02.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:02.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:02.706
  STEP: creating a Deployment @ 06/24/23 12:11:02.714
  Jun 24 12:11:02.715: INFO: Creating simple deployment test-deployment-cw7kg
  Jun 24 12:11:02.738: INFO: deployment "test-deployment-cw7kg" doesn't have the required revision set
  STEP: Getting /status @ 06/24/23 12:11:04.755
  Jun 24 12:11:04.760: INFO: Deployment test-deployment-cw7kg has Conditions: [{Available True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cw7kg-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 06/24/23 12:11:04.76
  Jun 24 12:11:04.772: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 11, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 11, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 11, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 11, 2, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-cw7kg-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 06/24/23 12:11:04.772
  Jun 24 12:11:04.775: INFO: Observed &Deployment event: ADDED
  Jun 24 12:11:04.775: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cw7kg-5994cf9475"}
  Jun 24 12:11:04.775: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.775: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cw7kg-5994cf9475"}
  Jun 24 12:11:04.775: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 24 12:11:04.776: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.776: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 24 12:11:04.776: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cw7kg-5994cf9475" is progressing.}
  Jun 24 12:11:04.777: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.777: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 24 12:11:04.777: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cw7kg-5994cf9475" has successfully progressed.}
  Jun 24 12:11:04.777: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.777: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 24 12:11:04.777: INFO: Observed Deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cw7kg-5994cf9475" has successfully progressed.}
  Jun 24 12:11:04.777: INFO: Found Deployment test-deployment-cw7kg in namespace deployment-8086 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 12:11:04.777: INFO: Deployment test-deployment-cw7kg has an updated status
  STEP: patching the Statefulset Status @ 06/24/23 12:11:04.777
  Jun 24 12:11:04.777: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 24 12:11:04.789: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 06/24/23 12:11:04.789
  Jun 24 12:11:04.792: INFO: Observed &Deployment event: ADDED
  Jun 24 12:11:04.792: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cw7kg-5994cf9475"}
  Jun 24 12:11:04.793: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.793: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-cw7kg-5994cf9475"}
  Jun 24 12:11:04.793: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 24 12:11:04.793: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.793: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun 24 12:11:04.793: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:02 +0000 UTC 2023-06-24 12:11:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-cw7kg-5994cf9475" is progressing.}
  Jun 24 12:11:04.793: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.793: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 24 12:11:04.794: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cw7kg-5994cf9475" has successfully progressed.}
  Jun 24 12:11:04.794: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.794: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun 24 12:11:04.794: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-24 12:11:03 +0000 UTC 2023-06-24 12:11:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-cw7kg-5994cf9475" has successfully progressed.}
  Jun 24 12:11:04.794: INFO: Observed deployment test-deployment-cw7kg in namespace deployment-8086 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 12:11:04.794: INFO: Observed &Deployment event: MODIFIED
  Jun 24 12:11:04.794: INFO: Found deployment test-deployment-cw7kg in namespace deployment-8086 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jun 24 12:11:04.794: INFO: Deployment test-deployment-cw7kg has a patched status
  Jun 24 12:11:04.802: INFO: Deployment "test-deployment-cw7kg":
  &Deployment{ObjectMeta:{test-deployment-cw7kg  deployment-8086  4ba28c5a-0af2-4122-b23b-19801b130006 4005 1 2023-06-24 12:11:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-24 12:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-24 12:11:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-24 12:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029409a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-cw7kg-5994cf9475",LastUpdateTime:2023-06-24 12:11:04 +0000 UTC,LastTransitionTime:2023-06-24 12:11:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 24 12:11:04.806: INFO: New ReplicaSet "test-deployment-cw7kg-5994cf9475" of Deployment "test-deployment-cw7kg":
  &ReplicaSet{ObjectMeta:{test-deployment-cw7kg-5994cf9475  deployment-8086  e18ff739-5b89-4cc5-a06e-1d86c6ea4a23 4001 1 2023-06-24 12:11:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-cw7kg 4ba28c5a-0af2-4122-b23b-19801b130006 0xc004287e37 0xc004287e38}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ba28c5a-0af2-4122-b23b-19801b130006\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:11:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004287ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:11:04.812: INFO: Pod "test-deployment-cw7kg-5994cf9475-dxqhc" is available:
  &Pod{ObjectMeta:{test-deployment-cw7kg-5994cf9475-dxqhc test-deployment-cw7kg-5994cf9475- deployment-8086  1ad99785-9778-4c2a-8e14-271cfa1174c4 4000 0 2023-06-24 12:11:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-cw7kg-5994cf9475 e18ff739-5b89-4cc5-a06e-1d86c6ea4a23 0xc002a5a297 0xc002a5a298}] [] [{kube-controller-manager Update v1 2023-06-24 12:11:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e18ff739-5b89-4cc5-a06e-1d86c6ea4a23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:11:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76pc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76pc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.204,StartTime:2023-06-24 12:11:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:11:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://57ae125bc1af87ecb8f24abc152c3f70e1f140b425bcee1bb0f2f242106713a7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.204,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:11:04.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8086" for this suite. @ 06/24/23 12:11:04.817
• [2.142 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 06/24/23 12:11:04.826
  Jun 24 12:11:04.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:11:04.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:04.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:04.857
  STEP: Creating a pod to test substitution in container's command @ 06/24/23 12:11:04.862
  STEP: Saw pod success @ 06/24/23 12:11:08.893
  Jun 24 12:11:08.897: INFO: Trying to get logs from node ip-172-31-19-205 pod var-expansion-2d758d92-5c7d-4ac2-817a-cfe7f546d662 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:11:08.905
  Jun 24 12:11:08.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5235" for this suite. @ 06/24/23 12:11:08.929
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 06/24/23 12:11:08.95
  Jun 24 12:11:08.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:11:08.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:08.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:08.977
  STEP: Creating secret with name secret-test-map-fccb9e70-7c8b-4ab9-9f7b-7b84c9b32adb @ 06/24/23 12:11:08.981
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:11:08.986
  STEP: Saw pod success @ 06/24/23 12:11:13.016
  Jun 24 12:11:13.020: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-d64a24d8-9f6c-407c-be6a-cff554b081d6 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:11:13.027
  Jun 24 12:11:13.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8040" for this suite. @ 06/24/23 12:11:13.056
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 06/24/23 12:11:13.068
  Jun 24 12:11:13.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:11:13.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:13.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:13.093
  STEP: Creating secret with name projected-secret-test-8e7e0a4d-1a3a-4d8f-9901-f75a1ddd78c8 @ 06/24/23 12:11:13.098
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:11:13.104
  STEP: Saw pod success @ 06/24/23 12:11:17.133
  Jun 24 12:11:17.138: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-306455f0-eeba-4c15-949c-ce80c7b91c9f container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:11:17.146
  Jun 24 12:11:17.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-109" for this suite. @ 06/24/23 12:11:17.171
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 06/24/23 12:11:17.186
  Jun 24 12:11:17.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:11:17.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:17.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:17.212
  STEP: Creating configMap with name projected-configmap-test-volume-a57cc317-1533-4c6d-8f49-27a6263e7e87 @ 06/24/23 12:11:17.216
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:11:17.221
  STEP: Saw pod success @ 06/24/23 12:11:21.246
  Jun 24 12:11:21.250: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-c07c313f-944a-427b-bbc6-8fa4436a0de6 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 12:11:21.258
  Jun 24 12:11:21.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6438" for this suite. @ 06/24/23 12:11:21.286
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 06/24/23 12:11:21.297
  Jun 24 12:11:21.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename events @ 06/24/23 12:11:21.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:21.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:21.325
  STEP: creating a test event @ 06/24/23 12:11:21.33
  STEP: listing events in all namespaces @ 06/24/23 12:11:21.341
  STEP: listing events in test namespace @ 06/24/23 12:11:21.351
  STEP: listing events with field selection filtering on source @ 06/24/23 12:11:21.356
  STEP: listing events with field selection filtering on reportingController @ 06/24/23 12:11:21.36
  STEP: getting the test event @ 06/24/23 12:11:21.364
  STEP: patching the test event @ 06/24/23 12:11:21.368
  STEP: getting the test event @ 06/24/23 12:11:21.381
  STEP: updating the test event @ 06/24/23 12:11:21.385
  STEP: getting the test event @ 06/24/23 12:11:21.392
  STEP: deleting the test event @ 06/24/23 12:11:21.396
  STEP: listing events in all namespaces @ 06/24/23 12:11:21.406
  STEP: listing events in test namespace @ 06/24/23 12:11:21.416
  Jun 24 12:11:21.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9373" for this suite. @ 06/24/23 12:11:21.425
• [0.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 06/24/23 12:11:21.437
  Jun 24 12:11:21.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:11:21.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:21.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:21.461
  STEP: creating a Deployment @ 06/24/23 12:11:21.471
  STEP: waiting for Deployment to be created @ 06/24/23 12:11:21.478
  STEP: waiting for all Replicas to be Ready @ 06/24/23 12:11:21.481
  Jun 24 12:11:21.483: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.483: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.494: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.494: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.520: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.520: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.546: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:21.546: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun 24 12:11:22.397: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 24 12:11:22.398: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun 24 12:11:23.009: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 06/24/23 12:11:23.009
  W0624 12:11:23.021554      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 24 12:11:23.024: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 06/24/23 12:11:23.024
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.027: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 0
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.028: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.041: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.041: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.067: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.067: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:23.081: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:23.081: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:23.097: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:23.097: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:25.061: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:25.061: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:25.089: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  STEP: listing Deployments @ 06/24/23 12:11:25.089
  Jun 24 12:11:25.095: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 06/24/23 12:11:25.095
  Jun 24 12:11:25.112: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 06/24/23 12:11:25.112
  Jun 24 12:11:25.122: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:25.131: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:25.156: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:25.177: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:25.190: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:26.037: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:26.070: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:26.086: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:26.099: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun 24 12:11:31.541: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 06/24/23 12:11:31.565
  STEP: fetching the DeploymentStatus @ 06/24/23 12:11:31.574
  Jun 24 12:11:31.581: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:31.582: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:31.582: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:31.582: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:31.583: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 1
  Jun 24 12:11:31.583: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:31.583: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:31.583: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:31.583: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 2
  Jun 24 12:11:31.584: INFO: observed Deployment test-deployment in namespace deployment-8886 with ReadyReplicas 3
  STEP: deleting the Deployment @ 06/24/23 12:11:31.584
  Jun 24 12:11:31.596: INFO: observed event type MODIFIED
  Jun 24 12:11:31.597: INFO: observed event type MODIFIED
  Jun 24 12:11:31.597: INFO: observed event type MODIFIED
  Jun 24 12:11:31.597: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.598: INFO: observed event type MODIFIED
  Jun 24 12:11:31.599: INFO: observed event type MODIFIED
  Jun 24 12:11:31.604: INFO: Log out all the ReplicaSets if there is no deployment created
  Jun 24 12:11:31.610: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-8886  4b57e3eb-e2f0-4af9-9a7c-74dd4e251caa 4314 3 2023-06-24 12:11:21 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 83443428-36ca-45d4-8aaa-5c79e607ad50 0xc004a124d7 0xc004a124d8}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:11:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83443428-36ca-45d4-8aaa-5c79e607ad50\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:11:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a12560 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 24 12:11:31.616: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-8886  d11318fa-4da6-438e-97d0-28277c19e8d6 4435 4 2023-06-24 12:11:23 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 83443428-36ca-45d4-8aaa-5c79e607ad50 0xc004a125c7 0xc004a125c8}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:11:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83443428-36ca-45d4-8aaa-5c79e607ad50\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:11:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a12650 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun 24 12:11:31.625: INFO: pod: "test-deployment-5b5dcbcd95-4nh6c":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-4nh6c test-deployment-5b5dcbcd95- deployment-8886  1613df5f-e304-4866-8718-097ae6bcdf0a 4429 0 2023-06-24 12:11:23 +0000 UTC 2023-06-24 12:11:32 +0000 UTC 0xc004a12c50 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 d11318fa-4da6-438e-97d0-28277c19e8d6 0xc004a12c87 0xc004a12c88}] [] [{kube-controller-manager Update v1 2023-06-24 12:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d11318fa-4da6-438e-97d0-28277c19e8d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:11:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pn4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pn4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.210,StartTime:2023-06-24 12:11:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:11:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://12c5d17aaba015d6454737cd012d9ba227fcc7090d91dccf2ee8c0a929a5cc0a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.210,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 24 12:11:31.626: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-8886  31ae704e-51c1-4c53-b249-8ea34a440fe3 4427 2 2023-06-24 12:11:25 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 83443428-36ca-45d4-8aaa-5c79e607ad50 0xc004a126b7 0xc004a126b8}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83443428-36ca-45d4-8aaa-5c79e607ad50\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:11:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a12740 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jun 24 12:11:31.630: INFO: pod: "test-deployment-6fc78d85c6-d25c8":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-d25c8 test-deployment-6fc78d85c6- deployment-8886  57ec3a96-575c-4bb7-ab12-7381846a0817 4426 0 2023-06-24 12:11:26 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 31ae704e-51c1-4c53-b249-8ea34a440fe3 0xc0047e7de7 0xc0047e7de8}] [] [{kube-controller-manager Update v1 2023-06-24 12:11:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31ae704e-51c1-4c53-b249-8ea34a440fe3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:11:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6smjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6smjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.198,StartTime:2023-06-24 12:11:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:11:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://11563f1c46e00a6ee8b189c26fe8eeb8e4b25b7fd57cccaac59ab9725b51ee3d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 24 12:11:31.631: INFO: pod: "test-deployment-6fc78d85c6-mgg8b":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-mgg8b test-deployment-6fc78d85c6- deployment-8886  1ba0ab86-d93a-418d-95c6-b84ba77d01a4 4356 0 2023-06-24 12:11:25 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 31ae704e-51c1-4c53-b249-8ea34a440fe3 0xc0047e7fd7 0xc0047e7fd8}] [] [{kube-controller-manager Update v1 2023-06-24 12:11:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31ae704e-51c1-4c53-b249-8ea34a440fe3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:11:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfg5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfg5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:11:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.211,StartTime:2023-06-24 12:11:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:11:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2420636a9119cadccbfd00bb5afce3f645343b170ce65cd9f6a4dc488abf0f92,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.211,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun 24 12:11:31.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8886" for this suite. @ 06/24/23 12:11:31.64
• [10.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 06/24/23 12:11:31.653
  Jun 24 12:11:31.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:11:31.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:31.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:31.678
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:11:31.683
  STEP: Saw pod success @ 06/24/23 12:11:35.715
  Jun 24 12:11:35.718: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-f78af528-e31e-4bc1-bd23-3ae4c6d90341 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:11:35.727
  Jun 24 12:11:35.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8211" for this suite. @ 06/24/23 12:11:35.752
• [4.107 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 06/24/23 12:11:35.761
  Jun 24 12:11:35.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 12:11:35.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:35.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:35.785
  Jun 24 12:11:35.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  W0624 12:11:38.354549      19 warnings.go:70] unknown field "alpha"
  W0624 12:11:38.354825      19 warnings.go:70] unknown field "beta"
  W0624 12:11:38.354870      19 warnings.go:70] unknown field "delta"
  W0624 12:11:38.354940      19 warnings.go:70] unknown field "epsilon"
  W0624 12:11:38.354948      19 warnings.go:70] unknown field "gamma"
  Jun 24 12:11:38.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6114" for this suite. @ 06/24/23 12:11:38.392
• [2.639 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 06/24/23 12:11:38.406
  Jun 24 12:11:38.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:11:38.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:38.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:38.436
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8424 @ 06/24/23 12:11:38.44
  STEP: changing the ExternalName service to type=NodePort @ 06/24/23 12:11:38.447
  STEP: creating replication controller externalname-service in namespace services-8424 @ 06/24/23 12:11:38.469
  I0624 12:11:38.480865      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8424, replica count: 2
  I0624 12:11:41.532380      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:11:41.532: INFO: Creating new exec pod
  Jun 24 12:11:44.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 24 12:11:44.730: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 24 12:11:44.730: INFO: stdout: "externalname-service-hsks4"
  Jun 24 12:11:44.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.123 80'
  Jun 24 12:11:44.888: INFO: stderr: "+ nc -v -t -w 2 10.152.183.123 80\nConnection to 10.152.183.123 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 24 12:11:44.888: INFO: stdout: ""
  Jun 24 12:11:45.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.123 80'
  Jun 24 12:11:46.059: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.123 80\nConnection to 10.152.183.123 80 port [tcp/http] succeeded!\n"
  Jun 24 12:11:46.059: INFO: stdout: ""
  Jun 24 12:11:46.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.123 80'
  Jun 24 12:11:47.039: INFO: stderr: "+ nc -v -t -w 2 10.152.183.123 80\n+ echo hostName\nConnection to 10.152.183.123 80 port [tcp/http] succeeded!\n"
  Jun 24 12:11:47.039: INFO: stdout: "externalname-service-hsks4"
  Jun 24 12:11:47.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.205 31590'
  Jun 24 12:11:47.199: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.19.205 31590\nConnection to 172.31.19.205 31590 port [tcp/*] succeeded!\n"
  Jun 24 12:11:47.199: INFO: stdout: ""
  Jun 24 12:11:48.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.205 31590'
  Jun 24 12:11:48.360: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.19.205 31590\nConnection to 172.31.19.205 31590 port [tcp/*] succeeded!\n"
  Jun 24 12:11:48.360: INFO: stdout: "externalname-service-hsks4"
  Jun 24 12:11:48.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-8424 exec execpodlb4t5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.202 31590'
  Jun 24 12:11:48.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.89.202 31590\nConnection to 172.31.89.202 31590 port [tcp/*] succeeded!\n"
  Jun 24 12:11:48.521: INFO: stdout: "externalname-service-hsks4"
  Jun 24 12:11:48.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:11:48.526: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-8424" for this suite. @ 06/24/23 12:11:48.557
• [10.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 06/24/23 12:11:48.571
  Jun 24 12:11:48.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-pred @ 06/24/23 12:11:48.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:48.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:48.6
  Jun 24 12:11:48.605: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 24 12:11:48.614: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 12:11:48.618: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-136 before test
  Jun 24 12:11:48.625: INFO: nginx-ingress-controller-kubernetes-worker-9295m from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:39 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.625: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: calico-kube-controllers-69b5565d84-5wz9f from kube-system started at 2023-06-24 12:02:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.625: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: externalname-service-hsks4 from services-8424 started at 2023-06-24 12:11:38 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.625: INFO: 	Container externalname-service ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: sonobuoy-e2e-job-a6e2d2ee4f8b45eb from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:11:48.625: INFO: 	Container e2e ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57 from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:11:48.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:11:48.625: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-205 before test
  Jun 24 12:11:48.631: INFO: nginx-ingress-controller-kubernetes-worker-6vrl7 from ingress-nginx-kubernetes-worker started at 2023-06-24 12:03:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.631: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:11:48.631: INFO: execpodlb4t5 from services-8424 started at 2023-06-24 12:11:41 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.631: INFO: 	Container agnhost-container ready: true, restart count 0
  Jun 24 12:11:48.631: INFO: externalname-service-pz9gz from services-8424 started at 2023-06-24 12:11:38 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.631: INFO: 	Container externalname-service ready: true, restart count 0
  Jun 24 12:11:48.631: INFO: sonobuoy from sonobuoy started at 2023-06-24 12:08:47 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.631: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 24 12:11:48.631: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-2vcbs from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:11:48.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:11:48.631: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:11:48.632: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-202 before test
  Jun 24 12:11:48.638: INFO: default-http-backend-kubernetes-worker-65fc475d49-cs8f9 from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.638: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 24 12:11:48.638: INFO: nginx-ingress-controller-kubernetes-worker-bvt8s from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: coredns-5c7f76ccb8-d25lj from kube-system started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container coredns ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: kube-state-metrics-5b95b4459c-pcn82 from kube-system started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: metrics-server-v0.5.2-6cf8c8b69c-m28bd from kube-system started at 2023-06-24 11:54:03 +0000 UTC (2 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: dashboard-metrics-scraper-6b8586b5c9-c7tbk from kubernetes-dashboard started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: kubernetes-dashboard-6869f4cd5f-6rqxs from kubernetes-dashboard started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-ldg4s from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:11:48.639: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:11:48.639: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-15-136 @ 06/24/23 12:11:48.657
  STEP: verifying the node has the label node ip-172-31-19-205 @ 06/24/23 12:11:48.673
  STEP: verifying the node has the label node ip-172-31-89-202 @ 06/24/23 12:11:48.689
  Jun 24 12:11:48.702: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-cs8f9 requesting resource cpu=10m on Node ip-172-31-89-202
  Jun 24 12:11:48.702: INFO: Pod nginx-ingress-controller-kubernetes-worker-6vrl7 requesting resource cpu=0m on Node ip-172-31-19-205
  Jun 24 12:11:48.702: INFO: Pod nginx-ingress-controller-kubernetes-worker-9295m requesting resource cpu=0m on Node ip-172-31-15-136
  Jun 24 12:11:48.702: INFO: Pod nginx-ingress-controller-kubernetes-worker-bvt8s requesting resource cpu=0m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod calico-kube-controllers-69b5565d84-5wz9f requesting resource cpu=0m on Node ip-172-31-15-136
  Jun 24 12:11:48.703: INFO: Pod coredns-5c7f76ccb8-d25lj requesting resource cpu=100m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod kube-state-metrics-5b95b4459c-pcn82 requesting resource cpu=0m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-m28bd requesting resource cpu=5m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-c7tbk requesting resource cpu=0m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod kubernetes-dashboard-6869f4cd5f-6rqxs requesting resource cpu=0m on Node ip-172-31-89-202
  Jun 24 12:11:48.703: INFO: Pod execpodlb4t5 requesting resource cpu=0m on Node ip-172-31-19-205
  Jun 24 12:11:48.704: INFO: Pod externalname-service-hsks4 requesting resource cpu=0m on Node ip-172-31-15-136
  Jun 24 12:11:48.704: INFO: Pod externalname-service-pz9gz requesting resource cpu=0m on Node ip-172-31-19-205
  Jun 24 12:11:48.704: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-19-205
  Jun 24 12:11:48.704: INFO: Pod sonobuoy-e2e-job-a6e2d2ee4f8b45eb requesting resource cpu=0m on Node ip-172-31-15-136
  Jun 24 12:11:48.704: INFO: Pod sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-2vcbs requesting resource cpu=0m on Node ip-172-31-19-205
  Jun 24 12:11:48.704: INFO: Pod sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-ldg4s requesting resource cpu=0m on Node ip-172-31-89-202
  Jun 24 12:11:48.705: INFO: Pod sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57 requesting resource cpu=0m on Node ip-172-31-15-136
  STEP: Starting Pods to consume most of the cluster CPU. @ 06/24/23 12:11:48.705
  Jun 24 12:11:48.705: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-15-136
  Jun 24 12:11:48.716: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-19-205
  Jun 24 12:11:48.728: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-89-202
  STEP: Creating another pod that requires unavailable amount of CPU. @ 06/24/23 12:11:50.768
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d.176b972de3faab60], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3174/filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d to ip-172-31-19-205] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d.176b972e0c690b93], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d.176b972e0db566c1], Reason = [Created], Message = [Created container filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d.176b972e12219721], Reason = [Started], Message = [Started container filler-pod-c2fe90ca-4030-435d-ad70-622214c0247d] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd.176b972de53ff612], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3174/filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd to ip-172-31-89-202] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd.176b972e119a3e9b], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd.176b972e25071da4], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 325.862008ms (325.876628ms including waiting)] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd.176b972e266119f0], Reason = [Created], Message = [Created container filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd.176b972e2af66a46], Reason = [Started], Message = [Started container filler-pod-cbcd7392-9f56-42ac-a28a-420a0f9cc6cd] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d.176b972de36a0a0a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3174/filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d to ip-172-31-15-136] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d.176b972e0dd7eca6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d.176b972e0f53dcf2], Reason = [Created], Message = [Created container filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d.176b972e13edbaef], Reason = [Started], Message = [Started container filler-pod-cfeaf7c0-0ade-4adc-9c69-e5253a1ef38d] @ 06/24/23 12:11:50.775
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176b972e5e3cdf00], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 06/24/23 12:11:50.793
  STEP: removing the label node off the node ip-172-31-15-136 @ 06/24/23 12:11:51.791
  STEP: verifying the node doesn't have the label node @ 06/24/23 12:11:51.807
  STEP: removing the label node off the node ip-172-31-19-205 @ 06/24/23 12:11:51.815
  STEP: verifying the node doesn't have the label node @ 06/24/23 12:11:51.847
  STEP: removing the label node off the node ip-172-31-89-202 @ 06/24/23 12:11:51.853
  STEP: verifying the node doesn't have the label node @ 06/24/23 12:11:51.87
  Jun 24 12:11:51.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3174" for this suite. @ 06/24/23 12:11:51.88
• [3.318 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 06/24/23 12:11:51.89
  Jun 24 12:11:51.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:11:51.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:51.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:51.912
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/24/23 12:11:51.916
  STEP: Saw pod success @ 06/24/23 12:11:57.95
  Jun 24 12:11:57.954: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-31c94078-292b-4d9b-92cf-15b09f141837 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:11:57.963
  Jun 24 12:11:57.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2627" for this suite. @ 06/24/23 12:11:57.99
• [6.108 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 06/24/23 12:11:58.001
  Jun 24 12:11:58.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:11:58.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:11:58.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:11:58.028
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:11:58.033
  STEP: Saw pod success @ 06/24/23 12:12:02.068
  Jun 24 12:12:02.073: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-a55e2275-7a1e-43e9-af67-7f25631aca98 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:12:02.08
  Jun 24 12:12:02.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2249" for this suite. @ 06/24/23 12:12:02.102
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 06/24/23 12:12:02.114
  Jun 24 12:12:02.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:12:02.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:12:02.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:12:02.136
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7981 @ 06/24/23 12:12:02.142
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/24/23 12:12:02.153
  STEP: creating service externalsvc in namespace services-7981 @ 06/24/23 12:12:02.153
  STEP: creating replication controller externalsvc in namespace services-7981 @ 06/24/23 12:12:02.181
  I0624 12:12:02.198783      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7981, replica count: 2
  I0624 12:12:05.249399      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 06/24/23 12:12:05.253
  Jun 24 12:12:05.275: INFO: Creating new exec pod
  Jun 24 12:12:07.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-7981 exec execpodrfv9g -- /bin/sh -x -c nslookup clusterip-service.services-7981.svc.cluster.local'
  Jun 24 12:12:07.493: INFO: stderr: "+ nslookup clusterip-service.services-7981.svc.cluster.local\n"
  Jun 24 12:12:07.493: INFO: stdout: "Server:\t\t10.152.183.180\nAddress:\t10.152.183.180#53\n\nclusterip-service.services-7981.svc.cluster.local\tcanonical name = externalsvc.services-7981.svc.cluster.local.\nName:\texternalsvc.services-7981.svc.cluster.local\nAddress: 10.152.183.112\n\n"
  Jun 24 12:12:07.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7981, will wait for the garbage collector to delete the pods @ 06/24/23 12:12:07.498
  Jun 24 12:12:07.561: INFO: Deleting ReplicationController externalsvc took: 8.441108ms
  Jun 24 12:12:07.662: INFO: Terminating ReplicationController externalsvc pods took: 101.003359ms
  Jun 24 12:12:09.701: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7981" for this suite. @ 06/24/23 12:12:09.722
• [7.621 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 06/24/23 12:12:09.738
  Jun 24 12:12:09.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:12:09.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:12:09.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:12:09.765
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:12:09.77
  STEP: Saw pod success @ 06/24/23 12:12:13.804
  Jun 24 12:12:13.809: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-6d93f371-3687-4ac1-b5d0-62b80d55f633 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:12:13.816
  Jun 24 12:12:13.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3623" for this suite. @ 06/24/23 12:12:13.838
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 06/24/23 12:12:13.849
  Jun 24 12:12:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subpath @ 06/24/23 12:12:13.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:12:13.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:12:13.875
  STEP: Setting up data @ 06/24/23 12:12:13.88
  STEP: Creating pod pod-subpath-test-configmap-s249 @ 06/24/23 12:12:13.892
  STEP: Creating a pod to test atomic-volume-subpath @ 06/24/23 12:12:13.892
  STEP: Saw pod success @ 06/24/23 12:12:37.975
  Jun 24 12:12:37.980: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-subpath-test-configmap-s249 container test-container-subpath-configmap-s249: <nil>
  STEP: delete the pod @ 06/24/23 12:12:37.99
  STEP: Deleting pod pod-subpath-test-configmap-s249 @ 06/24/23 12:12:38.017
  Jun 24 12:12:38.017: INFO: Deleting pod "pod-subpath-test-configmap-s249" in namespace "subpath-5140"
  Jun 24 12:12:38.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5140" for this suite. @ 06/24/23 12:12:38.028
• [24.195 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 06/24/23 12:12:38.044
  Jun 24 12:12:38.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:12:38.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:12:38.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:12:38.093
  STEP: Creating pod busybox-62c4acc4-8423-475e-a764-5ca27c6c9df8 in namespace container-probe-289 @ 06/24/23 12:12:38.103
  Jun 24 12:12:40.133: INFO: Started pod busybox-62c4acc4-8423-475e-a764-5ca27c6c9df8 in namespace container-probe-289
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 12:12:40.133
  Jun 24 12:12:40.138: INFO: Initial restart count of pod busybox-62c4acc4-8423-475e-a764-5ca27c6c9df8 is 0
  Jun 24 12:13:30.279: INFO: Restart count of pod container-probe-289/busybox-62c4acc4-8423-475e-a764-5ca27c6c9df8 is now 1 (50.140601464s elapsed)
  Jun 24 12:13:30.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:13:30.283
  STEP: Destroying namespace "container-probe-289" for this suite. @ 06/24/23 12:13:30.298
• [52.262 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 06/24/23 12:13:30.308
  Jun 24 12:13:30.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption @ 06/24/23 12:13:30.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:13:30.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:13:30.333
  STEP: Creating a kubernetes client @ 06/24/23 12:13:30.338
  Jun 24 12:13:30.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption-2 @ 06/24/23 12:13:30.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:13:30.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:13:30.366
  STEP: Waiting for the pdb to be processed @ 06/24/23 12:13:30.376
  STEP: Waiting for the pdb to be processed @ 06/24/23 12:13:32.401
  STEP: Waiting for the pdb to be processed @ 06/24/23 12:13:34.421
  STEP: listing a collection of PDBs across all namespaces @ 06/24/23 12:13:36.429
  STEP: listing a collection of PDBs in namespace disruption-409 @ 06/24/23 12:13:36.433
  STEP: deleting a collection of PDBs @ 06/24/23 12:13:36.437
  STEP: Waiting for the PDB collection to be deleted @ 06/24/23 12:13:36.451
  Jun 24 12:13:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:13:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-6841" for this suite. @ 06/24/23 12:13:36.465
  STEP: Destroying namespace "disruption-409" for this suite. @ 06/24/23 12:13:36.474
• [6.173 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 06/24/23 12:13:36.481
  Jun 24 12:13:36.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:13:36.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:13:36.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:13:36.502
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:13:36.508
  STEP: Saw pod success @ 06/24/23 12:13:40.542
  Jun 24 12:13:40.546: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-2d3b0dff-59bd-40fd-8ea5-44c5febc6ca7 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:13:40.556
  Jun 24 12:13:40.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-555" for this suite. @ 06/24/23 12:13:40.58
• [4.106 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 06/24/23 12:13:40.587
  Jun 24 12:13:40.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pod-network-test @ 06/24/23 12:13:40.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:13:40.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:13:40.61
  STEP: Performing setup for networking test in namespace pod-network-test-9034 @ 06/24/23 12:13:40.614
  STEP: creating a selector @ 06/24/23 12:13:40.614
  STEP: Creating the service pods in kubernetes @ 06/24/23 12:13:40.614
  Jun 24 12:13:40.614: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/24/23 12:14:02.749
  Jun 24 12:14:04.772: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 24 12:14:04.772: INFO: Breadth first check of 192.168.116.202 on host 172.31.15.136...
  Jun 24 12:14:04.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.225:9080/dial?request=hostname&protocol=http&host=192.168.116.202&port=8083&tries=1'] Namespace:pod-network-test-9034 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:14:04.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:14:04.777: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:14:04.777: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9034/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.116.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:14:04.877: INFO: Waiting for responses: map[]
  Jun 24 12:14:04.877: INFO: reached 192.168.116.202 after 0/1 tries
  Jun 24 12:14:04.877: INFO: Breadth first check of 192.168.150.224 on host 172.31.19.205...
  Jun 24 12:14:04.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.225:9080/dial?request=hostname&protocol=http&host=192.168.150.224&port=8083&tries=1'] Namespace:pod-network-test-9034 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:14:04.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:14:04.885: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:14:04.885: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9034/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.150.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:14:04.970: INFO: Waiting for responses: map[]
  Jun 24 12:14:04.970: INFO: reached 192.168.150.224 after 0/1 tries
  Jun 24 12:14:04.971: INFO: Breadth first check of 192.168.144.139 on host 172.31.89.202...
  Jun 24 12:14:04.975: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.225:9080/dial?request=hostname&protocol=http&host=192.168.144.139&port=8083&tries=1'] Namespace:pod-network-test-9034 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:14:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:14:04.976: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:14:04.976: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9034/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.144.139%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:14:05.073: INFO: Waiting for responses: map[]
  Jun 24 12:14:05.073: INFO: reached 192.168.144.139 after 0/1 tries
  Jun 24 12:14:05.073: INFO: Going to retry 0 out of 3 pods....
  Jun 24 12:14:05.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9034" for this suite. @ 06/24/23 12:14:05.079
• [24.499 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 06/24/23 12:14:05.089
  Jun 24 12:14:05.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:14:05.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:14:05.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:14:05.117
  STEP: creating a collection of services @ 06/24/23 12:14:05.122
  Jun 24 12:14:05.122: INFO: Creating e2e-svc-a-pmxr8
  Jun 24 12:14:05.135: INFO: Creating e2e-svc-b-79bqk
  Jun 24 12:14:05.151: INFO: Creating e2e-svc-c-9zsnk
  STEP: deleting service collection @ 06/24/23 12:14:05.17
  Jun 24 12:14:05.211: INFO: Collection of services has been deleted
  Jun 24 12:14:05.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7236" for this suite. @ 06/24/23 12:14:05.217
• [0.136 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 06/24/23 12:14:05.227
  Jun 24 12:14:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:14:05.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:14:05.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:14:05.25
  STEP: Creating configMap with name configmap-projected-all-test-volume-b2c4e2c8-9149-4d71-9915-d0ce11da8c76 @ 06/24/23 12:14:05.256
  STEP: Creating secret with name secret-projected-all-test-volume-1902fc04-3e99-431b-91c7-96aed21f8479 @ 06/24/23 12:14:05.262
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 06/24/23 12:14:05.268
  STEP: Saw pod success @ 06/24/23 12:14:09.292
  Jun 24 12:14:09.296: INFO: Trying to get logs from node ip-172-31-19-205 pod projected-volume-9c5c2e95-8577-4d4c-b7b4-b20d389b8605 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:14:09.304
  Jun 24 12:14:09.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-160" for this suite. @ 06/24/23 12:14:09.339
• [4.120 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 06/24/23 12:14:09.347
  Jun 24 12:14:09.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption @ 06/24/23 12:14:09.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:14:09.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:14:09.369
  Jun 24 12:14:09.391: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 24 12:15:09.421: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/24/23 12:15:09.426
  Jun 24 12:15:09.450: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 24 12:15:09.463: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 24 12:15:09.486: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 24 12:15:09.494: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 24 12:15:09.518: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 24 12:15:09.532: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/24/23 12:15:09.532
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 06/24/23 12:15:11.565
  Jun 24 12:15:15.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6591" for this suite. @ 06/24/23 12:15:15.665
• [66.326 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 06/24/23 12:15:15.674
  Jun 24 12:15:15.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:15:15.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:15:15.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:15:15.697
  STEP: Setting up server cert @ 06/24/23 12:15:15.725
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:15:16.18
  STEP: Deploying the webhook pod @ 06/24/23 12:15:16.189
  STEP: Wait for the deployment to be ready @ 06/24/23 12:15:16.202
  Jun 24 12:15:16.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/24/23 12:15:18.226
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:15:18.238
  Jun 24 12:15:19.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 06/24/23 12:15:19.244
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/24/23 12:15:19.262
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 06/24/23 12:15:19.273
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/24/23 12:15:19.286
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 06/24/23 12:15:19.3
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/24/23 12:15:19.314
  Jun 24 12:15:19.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3588" for this suite. @ 06/24/23 12:15:19.392
  STEP: Destroying namespace "webhook-markers-7690" for this suite. @ 06/24/23 12:15:19.401
• [3.737 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 06/24/23 12:15:19.411
  Jun 24 12:15:19.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:15:19.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:15:19.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:15:19.443
  STEP: creating service nodeport-test with type=NodePort in namespace services-9460 @ 06/24/23 12:15:19.446
  STEP: creating replication controller nodeport-test in namespace services-9460 @ 06/24/23 12:15:19.464
  I0624 12:15:19.472952      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-9460, replica count: 2
  I0624 12:15:22.524701      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0624 12:15:25.524898      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:15:25.525: INFO: Creating new exec pod
  Jun 24 12:15:28.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 24 12:15:28.708: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 24 12:15:28.708: INFO: stdout: ""
  Jun 24 12:15:29.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun 24 12:15:29.872: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun 24 12:15:29.872: INFO: stdout: "nodeport-test-6tccm"
  Jun 24 12:15:29.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.242 80'
  Jun 24 12:15:30.032: INFO: stderr: "+ nc -v -t -w 2 10.152.183.242 80\n+ echo hostName\nConnection to 10.152.183.242 80 port [tcp/http] succeeded!\n"
  Jun 24 12:15:30.032: INFO: stdout: "nodeport-test-2fdjt"
  Jun 24 12:15:30.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.136 32580'
  Jun 24 12:15:30.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.136 32580\nConnection to 172.31.15.136 32580 port [tcp/*] succeeded!\n"
  Jun 24 12:15:30.232: INFO: stdout: "nodeport-test-2fdjt"
  Jun 24 12:15:30.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.205 32580'
  Jun 24 12:15:30.388: INFO: stderr: "+ nc -v -t -w 2 172.31.19.205 32580\nConnection to 172.31.19.205 32580 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jun 24 12:15:30.388: INFO: stdout: ""
  Jun 24 12:15:31.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9460 exec execpod59jh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.205 32580'
  Jun 24 12:15:31.548: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.19.205 32580\nConnection to 172.31.19.205 32580 port [tcp/*] succeeded!\n"
  Jun 24 12:15:31.548: INFO: stdout: "nodeport-test-2fdjt"
  Jun 24 12:15:31.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9460" for this suite. @ 06/24/23 12:15:31.552
• [12.148 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 06/24/23 12:15:31.56
  Jun 24 12:15:31.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:15:31.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:15:31.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:15:31.584
  Jun 24 12:15:31.589: INFO: Creating deployment "webserver-deployment"
  Jun 24 12:15:31.594: INFO: Waiting for observed generation 1
  Jun 24 12:15:33.604: INFO: Waiting for all required pods to come up
  Jun 24 12:15:33.609: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 06/24/23 12:15:33.609
  Jun 24 12:15:35.626: INFO: Waiting for deployment "webserver-deployment" to complete
  Jun 24 12:15:35.633: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jun 24 12:15:35.647: INFO: Updating deployment webserver-deployment
  Jun 24 12:15:35.648: INFO: Waiting for observed generation 2
  Jun 24 12:15:37.658: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jun 24 12:15:37.662: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jun 24 12:15:37.666: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 24 12:15:37.680: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jun 24 12:15:37.680: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jun 24 12:15:37.685: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun 24 12:15:37.692: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jun 24 12:15:37.692: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jun 24 12:15:37.708: INFO: Updating deployment webserver-deployment
  Jun 24 12:15:37.708: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jun 24 12:15:37.721: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jun 24 12:15:37.727: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jun 24 12:15:37.759: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-8454  a5bd6025-7c24-459b-b020-3ba7117d76c7 6427 3 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042e6178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-06-24 12:15:35 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-24 12:15:37 +0000 UTC,LastTransitionTime:2023-06-24 12:15:37 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jun 24 12:15:37.773: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-8454  a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 6425 3 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a5bd6025-7c24-459b-b020-3ba7117d76c7 0xc0042e6637 0xc0042e6638}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5bd6025-7c24-459b-b020-3ba7117d76c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042e66d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:15:37.773: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jun 24 12:15:37.774: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-8454  ea7e9cd5-07a4-457e-9c38-b6b5060419c5 6422 3 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a5bd6025-7c24-459b-b020-3ba7117d76c7 0xc0042e6547 0xc0042e6548}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5bd6025-7c24-459b-b020-3ba7117d76c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042e65d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:15:37.788: INFO: Pod "webserver-deployment-67bd4bf6dc-5l2dq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5l2dq webserver-deployment-67bd4bf6dc- deployment-8454  72912842-1d5c-4a21-9ecf-9d630b708dc9 6283 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e6bd0 0xc0042e6bd1}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.144.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jnvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jnvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.202,PodIP:192.168.144.144,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2796603de18903f5d48980ad1b35b1676d103ba51c94033a074827878235ee92,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.144.144,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.788: INFO: Pod "webserver-deployment-67bd4bf6dc-bx4ln" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bx4ln webserver-deployment-67bd4bf6dc- deployment-8454  ecc5e70e-efaa-418b-94d6-c9278bb9ac2d 6253 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e6db7 0xc0042e6db8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xt9pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xt9pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.234,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f0a98c7faf4a1d86a14a714e20ea3dd9600772bcd438052d9abcf496f44e1d87,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.234,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.789: INFO: Pod "webserver-deployment-67bd4bf6dc-d4n5p" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d4n5p webserver-deployment-67bd4bf6dc- deployment-8454  eac1ef82-5744-4b94-9d6f-783339b80a82 6277 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e6fa7 0xc0042e6fa8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.144.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqhl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqhl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.202,PodIP:192.168.144.143,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8776cc1c25be1cc6645d3a1da2aaa9ad85f3e55c75c93368465a91c7371d643b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.144.143,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.789: INFO: Pod "webserver-deployment-67bd4bf6dc-h4nj9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h4nj9 webserver-deployment-67bd4bf6dc- deployment-8454  325e4d4e-1fee-4983-8e79-b69bb9131ebd 6434 0 2023-06-24 12:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e7197 0xc0042e7198}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltx77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltx77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.789: INFO: Pod "webserver-deployment-67bd4bf6dc-hl2g4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hl2g4 webserver-deployment-67bd4bf6dc- deployment-8454  2fb0efe7-df4f-49ff-9030-6a007df44d47 6251 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e7300 0xc0042e7301}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjsjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjsjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.233,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://960eccaaba0cd2fa486575f0f229919e977b2d733abed72d2c2e6ca21d3b4523,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.233,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.790: INFO: Pod "webserver-deployment-67bd4bf6dc-j98cx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-j98cx webserver-deployment-67bd4bf6dc- deployment-8454  aab64d10-8313-4acc-b3a7-4ff4fbe5d184 6280 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e74e7 0xc0042e74e8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.144.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgrjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgrjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.202,PodIP:192.168.144.142,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ffff3263a78778d59f06530ec69d1fc9e6a160a4bdd655c0002816ee7901060f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.144.142,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.790: INFO: Pod "webserver-deployment-67bd4bf6dc-jrfw5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jrfw5 webserver-deployment-67bd4bf6dc- deployment-8454  00290291-1c8d-4ed5-9a1d-02e1713b7e13 6228 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e76d7 0xc0042e76d8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvh5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvh5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.207,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ecc9feb9f8a16411dabd491e47bb06e5198297779983f4ff974e8c714859cbd1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.207,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.790: INFO: Pod "webserver-deployment-67bd4bf6dc-m2q6k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m2q6k webserver-deployment-67bd4bf6dc- deployment-8454  5dc5a799-a17f-4b95-a4fe-3c2cccc63f41 6231 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e78c7 0xc0042e78c8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6vkn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6vkn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.209,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://672f810e98aeb0c6ce75dd75b03225ef0f3077dc64cfa8e40c047d72aa7760b6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.209,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.791: INFO: Pod "webserver-deployment-67bd4bf6dc-wmrh6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wmrh6 webserver-deployment-67bd4bf6dc- deployment-8454  a1f45f7a-234d-4132-84fe-4968818ddc15 6426 0 2023-06-24 12:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e7ab7 0xc0042e7ab8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcr2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcr2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.791: INFO: Pod "webserver-deployment-67bd4bf6dc-wnhph" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wnhph webserver-deployment-67bd4bf6dc- deployment-8454  4112ed1f-f88d-4d51-8b8c-5af8f06a8848 6435 0 2023-06-24 12:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e7c20 0xc0042e7c21}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvffc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvffc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.791: INFO: Pod "webserver-deployment-67bd4bf6dc-xhd95" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xhd95 webserver-deployment-67bd4bf6dc- deployment-8454  16f0b32e-eeb2-4841-8b98-2d15668c258b 6224 0 2023-06-24 12:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ea7e9cd5-07a4-457e-9c38-b6b5060419c5 0xc0042e7d80 0xc0042e7d81}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea7e9cd5-07a4-457e-9c38-b6b5060419c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jkb6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jkb6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.208,StartTime:2023-06-24 12:15:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:15:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3994485dc66a1032804885e245a83ec7791d0e1164de451c436b65a9491496a5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.208,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.792: INFO: Pod "webserver-deployment-7b75d79cf5-5zfgq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5zfgq webserver-deployment-7b75d79cf5- deployment-8454  eb3e4c3e-af0e-4b7d-86f5-ae5658df8125 6406 0 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0042e7f67 0xc0042e7f68}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsmcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsmcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.210,StartTime:2023-06-24 12:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.210,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.793: INFO: Pod "webserver-deployment-7b75d79cf5-7548p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7548p webserver-deployment-7b75d79cf5- deployment-8454  549b0a57-e86b-4861-ac08-e222071206ab 6309 0 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0043a2187 0xc0043a2188}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzztr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzztr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:,StartTime:2023-06-24 12:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.794: INFO: Pod "webserver-deployment-7b75d79cf5-b85x7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-b85x7 webserver-deployment-7b75d79cf5- deployment-8454  89cfbbb5-e419-48f6-83f2-be1904d9ad8f 6334 0 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0043a2377 0xc0043a2378}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42fgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42fgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:,StartTime:2023-06-24 12:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.794: INFO: Pod "webserver-deployment-7b75d79cf5-cqst7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cqst7 webserver-deployment-7b75d79cf5- deployment-8454  a5b6f65e-a002-4ae3-b21f-c8f20b69e3d9 6432 0 2023-06-24 12:15:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0043a2567 0xc0043a2568}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fsm84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fsm84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.795: INFO: Pod "webserver-deployment-7b75d79cf5-l8mvq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-l8mvq webserver-deployment-7b75d79cf5- deployment-8454  e1c5011a-8624-40b5-b651-924ab74c5aeb 6408 0 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0043a26b7 0xc0043a26b8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.144.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h52h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h52h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-89-202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.89.202,PodIP:192.168.144.145,StartTime:2023-06-24 12:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.144.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.796: INFO: Pod "webserver-deployment-7b75d79cf5-t8hkg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-t8hkg webserver-deployment-7b75d79cf5- deployment-8454  a7c7531e-f4a5-418b-8f7c-0c35e23405d4 6411 0 2023-06-24 12:15:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a5f7b538-f6e3-4767-abe8-0fbf82cc8a41 0xc0043a28d7 0xc0043a28d8}] [] [{kube-controller-manager Update v1 2023-06-24 12:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5f7b538-f6e3-4767-abe8-0fbf82cc8a41\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:15:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qw85h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qw85h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:15:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.136,PodIP:192.168.116.211,StartTime:2023-06-24 12:15:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.116.211,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:15:37.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8454" for this suite. @ 06/24/23 12:15:37.814
• [6.321 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 06/24/23 12:15:37.883
  Jun 24 12:15:37.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 12:15:37.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:15:37.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:15:37.995
  Jun 24 12:15:38.052: INFO: Create a RollingUpdate DaemonSet
  Jun 24 12:15:38.063: INFO: Check that daemon pods launch on every node of the cluster
  Jun 24 12:15:38.069: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:38.069: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:38.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:15:38.076: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:15:39.089: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:39.089: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:39.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 12:15:39.095: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:15:40.083: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:40.083: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:40.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 12:15:40.087: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jun 24 12:15:40.087: INFO: Update the DaemonSet to trigger a rollout
  Jun 24 12:15:40.100: INFO: Updating DaemonSet daemon-set
  Jun 24 12:15:42.126: INFO: Roll back the DaemonSet before rollout is complete
  Jun 24 12:15:42.140: INFO: Updating DaemonSet daemon-set
  Jun 24 12:15:42.140: INFO: Make sure DaemonSet rollback is complete
  Jun 24 12:15:42.148: INFO: Wrong image for pod: daemon-set-d8lgg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jun 24 12:15:42.148: INFO: Pod daemon-set-d8lgg is not available
  Jun 24 12:15:42.153: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:42.153: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:43.183: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:43.184: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:44.166: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:44.167: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:45.159: INFO: Pod daemon-set-ht7mm is not available
  Jun 24 12:15:45.165: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:15:45.165: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 12:15:45.174
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1419, will wait for the garbage collector to delete the pods @ 06/24/23 12:15:45.174
  Jun 24 12:15:45.239: INFO: Deleting DaemonSet.extensions daemon-set took: 9.296925ms
  Jun 24 12:15:45.340: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.809056ms
  Jun 24 12:15:47.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:15:47.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 12:15:47.348: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6959"},"items":null}

  Jun 24 12:15:47.352: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6959"},"items":null}

  Jun 24 12:15:47.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1419" for this suite. @ 06/24/23 12:15:47.373
• [9.498 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 06/24/23 12:15:47.382
  Jun 24 12:15:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:15:47.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:15:47.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:15:47.409
  STEP: Creating pod test-webserver-95a66624-b4c5-4a94-953b-33cc4e01bf60 in namespace container-probe-6139 @ 06/24/23 12:15:47.413
  Jun 24 12:15:49.436: INFO: Started pod test-webserver-95a66624-b4c5-4a94-953b-33cc4e01bf60 in namespace container-probe-6139
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 12:15:49.436
  Jun 24 12:15:49.440: INFO: Initial restart count of pod test-webserver-95a66624-b4c5-4a94-953b-33cc4e01bf60 is 0
  Jun 24 12:19:50.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:19:50.092
  STEP: Destroying namespace "container-probe-6139" for this suite. @ 06/24/23 12:19:50.108
• [242.739 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 06/24/23 12:19:50.124
  Jun 24 12:19:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pod-network-test @ 06/24/23 12:19:50.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:19:50.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:19:50.151
  STEP: Performing setup for networking test in namespace pod-network-test-5011 @ 06/24/23 12:19:50.157
  STEP: creating a selector @ 06/24/23 12:19:50.157
  STEP: Creating the service pods in kubernetes @ 06/24/23 12:19:50.158
  Jun 24 12:19:50.158: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/24/23 12:20:12.3
  Jun 24 12:20:14.339: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 24 12:20:14.339: INFO: Going to poll 192.168.116.215 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 12:20:14.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.116.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5011 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:20:14.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:20:14.343: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:20:14.343: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5011/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.116.215%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 24 12:20:14.438: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 24 12:20:14.438: INFO: Going to poll 192.168.150.240 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 12:20:14.444: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.150.240:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5011 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:20:14.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:20:14.445: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:20:14.446: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5011/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.150.240%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 24 12:20:14.524: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 24 12:20:14.525: INFO: Going to poll 192.168.144.147 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 12:20:14.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.144.147:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5011 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:20:14.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:20:14.530: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:20:14.530: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5011/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.144.147%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 24 12:20:14.604: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 24 12:20:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5011" for this suite. @ 06/24/23 12:20:14.61
• [24.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 06/24/23 12:20:14.622
  Jun 24 12:20:14.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:20:14.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:20:14.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:20:14.649
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:20:14.653
  STEP: Saw pod success @ 06/24/23 12:20:18.682
  Jun 24 12:20:18.686: INFO: Trying to get logs from node ip-172-31-15-136 pod downwardapi-volume-e314abe2-ba88-4e3c-b0ef-0e4ac104fb6c container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:20:18.709
  Jun 24 12:20:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9275" for this suite. @ 06/24/23 12:20:18.732
• [4.119 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 06/24/23 12:20:18.742
  Jun 24 12:20:18.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 12:20:18.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:20:18.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:20:18.767
  STEP: Creating a test externalName service @ 06/24/23 12:20:18.776
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:18.781
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:18.782
  STEP: creating a pod to probe DNS @ 06/24/23 12:20:18.782
  STEP: submitting the pod to kubernetes @ 06/24/23 12:20:18.782
  STEP: retrieving the pod @ 06/24/23 12:20:30.838
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:20:30.843
  Jun 24 12:20:30.853: INFO: DNS probes using dns-test-c95182f0-da32-45cb-aa3f-8042a7fc61d0 succeeded

  STEP: changing the externalName to bar.example.com @ 06/24/23 12:20:30.853
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:30.862
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:30.862
  STEP: creating a second pod to probe DNS @ 06/24/23 12:20:30.862
  STEP: submitting the pod to kubernetes @ 06/24/23 12:20:30.862
  STEP: retrieving the pod @ 06/24/23 12:20:40.901
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:20:40.906
  Jun 24 12:20:40.912: INFO: File wheezy_udp@dns-test-service-3.dns-9.svc.cluster.local from pod  dns-9/dns-test-755bd44a-4c98-4efb-ae5c-deced6dd3a59 contains '' instead of 'bar.example.com.'
  Jun 24 12:20:40.917: INFO: Lookups using dns-9/dns-test-755bd44a-4c98-4efb-ae5c-deced6dd3a59 failed for: [wheezy_udp@dns-test-service-3.dns-9.svc.cluster.local]

  Jun 24 12:20:45.928: INFO: DNS probes using dns-test-755bd44a-4c98-4efb-ae5c-deced6dd3a59 succeeded

  STEP: changing the service to type=ClusterIP @ 06/24/23 12:20:45.928
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:45.943
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9.svc.cluster.local; sleep 1; done
   @ 06/24/23 12:20:45.943
  STEP: creating a third pod to probe DNS @ 06/24/23 12:20:45.943
  STEP: submitting the pod to kubernetes @ 06/24/23 12:20:45.948
  STEP: retrieving the pod @ 06/24/23 12:20:47.968
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:20:47.972
  Jun 24 12:20:47.981: INFO: DNS probes using dns-test-ef8bf18a-601a-4258-bc4d-ec0d2bd4e66d succeeded

  Jun 24 12:20:47.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:20:47.985
  STEP: deleting the pod @ 06/24/23 12:20:47.997
  STEP: deleting the pod @ 06/24/23 12:20:48.019
  STEP: deleting the test externalName service @ 06/24/23 12:20:48.035
  STEP: Destroying namespace "dns-9" for this suite. @ 06/24/23 12:20:48.064
• [29.330 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 06/24/23 12:20:48.074
  Jun 24 12:20:48.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 12:20:48.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:20:48.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:20:48.122
  STEP: Creating ServiceAccount "e2e-sa-h9nc5"  @ 06/24/23 12:20:48.129
  Jun 24 12:20:48.141: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-h9nc5"  @ 06/24/23 12:20:48.141
  Jun 24 12:20:48.156: INFO: AutomountServiceAccountToken: true
  Jun 24 12:20:48.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3979" for this suite. @ 06/24/23 12:20:48.168
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 06/24/23 12:20:48.18
  Jun 24 12:20:48.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:20:48.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:20:48.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:20:48.217
  Jun 24 12:20:48.238: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jun 24 12:20:53.244: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 12:20:53.244
  Jun 24 12:20:57.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jun 24 12:20:59.267: INFO: Creating deployment "test-rollover-deployment"
  Jun 24 12:20:59.277: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jun 24 12:21:01.287: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jun 24 12:21:01.295: INFO: Ensure that both replica sets have 1 created replica
  Jun 24 12:21:01.304: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jun 24 12:21:01.317: INFO: Updating deployment test-rollover-deployment
  Jun 24 12:21:01.317: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jun 24 12:21:03.332: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jun 24 12:21:03.340: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jun 24 12:21:03.348: INFO: all replica sets need to contain the pod-template-hash label
  Jun 24 12:21:03.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 21, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:21:05.359: INFO: all replica sets need to contain the pod-template-hash label
  Jun 24 12:21:05.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 21, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:21:07.359: INFO: all replica sets need to contain the pod-template-hash label
  Jun 24 12:21:07.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 21, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:21:09.358: INFO: all replica sets need to contain the pod-template-hash label
  Jun 24 12:21:09.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 21, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:21:11.360: INFO: all replica sets need to contain the pod-template-hash label
  Jun 24 12:21:11.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 21, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 20, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun 24 12:21:13.359: INFO: 
  Jun 24 12:21:13.359: INFO: Ensure that both old replica sets have no replicas
  Jun 24 12:21:13.371: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3183  3a83e25d-41a9-47d1-ab29-d6adc93f06e7 8042 2 2023-06-24 12:20:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-24 12:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ab488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-24 12:20:59 +0000 UTC,LastTransitionTime:2023-06-24 12:20:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-06-24 12:21:12 +0000 UTC,LastTransitionTime:2023-06-24 12:20:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 24 12:21:13.375: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-3183  4e320c13-aca8-4bac-8bda-a225ec3861d0 8031 2 2023-06-24 12:21:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3a83e25d-41a9-47d1-ab29-d6adc93f06e7 0xc0043b1c57 0xc0043b1c58}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a83e25d-41a9-47d1-ab29-d6adc93f06e7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:21:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043b1d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:21:13.375: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jun 24 12:21:13.376: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3183  329fadd7-7f4f-49a9-bc75-1a7d519a5592 8040 2 2023-06-24 12:20:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3a83e25d-41a9-47d1-ab29-d6adc93f06e7 0xc0043b18bf 0xc0043b1900}] [] [{e2e.test Update apps/v1 2023-06-24 12:20:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a83e25d-41a9-47d1-ab29-d6adc93f06e7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:21:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043b1bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:21:13.376: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-3183  24999aa6-0eb8-46a0-8099-fbffa36499f5 7988 2 2023-06-24 12:20:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3a83e25d-41a9-47d1-ab29-d6adc93f06e7 0xc0043b1db7 0xc0043b1db8}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a83e25d-41a9-47d1-ab29-d6adc93f06e7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:21:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043b1f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:21:13.380: INFO: Pod "test-rollover-deployment-57777854c9-dg52n" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-dg52n test-rollover-deployment-57777854c9- deployment-3183  c66415e6-55cd-4bef-b410-05d66c7e93b1 8010 0 2023-06-24 12:21:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 4e320c13-aca8-4bac-8bda-a225ec3861d0 0xc0041ab7e7 0xc0041ab7e8}] [] [{kube-controller-manager Update v1 2023-06-24 12:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e320c13-aca8-4bac-8bda-a225ec3861d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:21:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7rchp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7rchp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:21:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:21:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.246,StartTime:2023-06-24 12:21:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:21:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1880c693e70cd24520bac21c5a765b22fe236079eda8a65ad3d6d8e100291b08,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.246,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:21:13.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3183" for this suite. @ 06/24/23 12:21:13.386
• [25.214 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 06/24/23 12:21:13.395
  Jun 24 12:21:13.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:21:13.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:13.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:13.422
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/24/23 12:21:13.427
  STEP: Saw pod success @ 06/24/23 12:21:17.454
  Jun 24 12:21:17.458: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-c8163dc9-17df-40ea-bbce-30f5096dcc18 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:21:17.481
  Jun 24 12:21:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9710" for this suite. @ 06/24/23 12:21:17.506
• [4.120 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 06/24/23 12:21:17.517
  Jun 24 12:21:17.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubelet-test @ 06/24/23 12:21:17.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:17.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:17.538
  Jun 24 12:21:17.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8668" for this suite. @ 06/24/23 12:21:17.575
• [0.065 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 06/24/23 12:21:17.584
  Jun 24 12:21:17.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:21:17.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:17.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:17.611
  STEP: creating Agnhost RC @ 06/24/23 12:21:17.615
  Jun 24 12:21:17.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8004 create -f -'
  Jun 24 12:21:18.082: INFO: stderr: ""
  Jun 24 12:21:18.082: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/24/23 12:21:18.082
  Jun 24 12:21:19.087: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:21:19.087: INFO: Found 0 / 1
  Jun 24 12:21:20.087: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:21:20.087: INFO: Found 1 / 1
  Jun 24 12:21:20.087: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 06/24/23 12:21:20.087
  Jun 24 12:21:20.091: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:21:20.091: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 24 12:21:20.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8004 patch pod agnhost-primary-m8qmr -p {"metadata":{"annotations":{"x":"y"}}}'
  Jun 24 12:21:20.214: INFO: stderr: ""
  Jun 24 12:21:20.214: INFO: stdout: "pod/agnhost-primary-m8qmr patched\n"
  STEP: checking annotations @ 06/24/23 12:21:20.214
  Jun 24 12:21:20.219: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:21:20.219: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 24 12:21:20.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8004" for this suite. @ 06/24/23 12:21:20.225
• [2.648 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 06/24/23 12:21:20.233
  Jun 24 12:21:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:21:20.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:20.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:20.258
  STEP: Creating a pod to test env composition @ 06/24/23 12:21:20.262
  STEP: Saw pod success @ 06/24/23 12:21:24.29
  Jun 24 12:21:24.294: INFO: Trying to get logs from node ip-172-31-19-205 pod var-expansion-7e8af0e0-f5c3-4ca9-912f-e2caa1373def container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:21:24.302
  Jun 24 12:21:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3171" for this suite. @ 06/24/23 12:21:24.324
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 06/24/23 12:21:24.334
  Jun 24 12:21:24.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename events @ 06/24/23 12:21:24.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:24.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:24.358
  STEP: Create set of events @ 06/24/23 12:21:24.364
  STEP: get a list of Events with a label in the current namespace @ 06/24/23 12:21:24.382
  STEP: delete a list of events @ 06/24/23 12:21:24.386
  Jun 24 12:21:24.386: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/24/23 12:21:24.411
  Jun 24 12:21:24.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2611" for this suite. @ 06/24/23 12:21:24.421
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 06/24/23 12:21:24.43
  Jun 24 12:21:24.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:21:24.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:24.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:24.454
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/24/23 12:21:24.459
  STEP: Saw pod success @ 06/24/23 12:21:28.484
  Jun 24 12:21:28.487: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-3ff6a3c5-105a-46e4-9f34-0b726f5be0fb container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:21:28.495
  Jun 24 12:21:28.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3085" for this suite. @ 06/24/23 12:21:28.52
• [4.098 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 06/24/23 12:21:28.529
  Jun 24 12:21:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 12:21:28.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:28.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:28.556
  Jun 24 12:21:28.583: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 12:21:28.592
  Jun 24 12:21:28.597: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:28.597: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:28.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:21:28.603: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:21:29.608: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:29.609: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:29.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:21:29.615: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:21:30.608: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:30.608: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:30.613: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 12:21:30.613: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 06/24/23 12:21:30.631
  STEP: Check that daemon pods images are updated. @ 06/24/23 12:21:30.648
  Jun 24 12:21:30.654: INFO: Wrong image for pod: daemon-set-p4gd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:30.654: INFO: Wrong image for pod: daemon-set-r2bn7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:30.654: INFO: Wrong image for pod: daemon-set-t8v68. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:30.662: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:30.662: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:31.668: INFO: Wrong image for pod: daemon-set-p4gd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:31.668: INFO: Wrong image for pod: daemon-set-t8v68. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:31.673: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:31.673: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:32.668: INFO: Wrong image for pod: daemon-set-p4gd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:32.668: INFO: Wrong image for pod: daemon-set-t8v68. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:32.668: INFO: Pod daemon-set-w4qnq is not available
  Jun 24 12:21:32.672: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:32.672: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:33.668: INFO: Wrong image for pod: daemon-set-p4gd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:33.674: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:33.674: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:34.668: INFO: Pod daemon-set-mlpvc is not available
  Jun 24 12:21:34.668: INFO: Wrong image for pod: daemon-set-p4gd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun 24 12:21:34.673: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:34.673: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:35.671: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:35.671: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:36.668: INFO: Pod daemon-set-vz9pr is not available
  Jun 24 12:21:36.672: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:36.672: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 06/24/23 12:21:36.672
  Jun 24 12:21:36.677: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:36.677: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:36.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 12:21:36.682: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:21:37.687: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:37.688: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:21:37.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 12:21:37.693: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 12:21:37.713
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-543, will wait for the garbage collector to delete the pods @ 06/24/23 12:21:37.713
  Jun 24 12:21:37.778: INFO: Deleting DaemonSet.extensions daemon-set took: 9.859539ms
  Jun 24 12:21:37.978: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.107985ms
  Jun 24 12:21:38.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:21:38.983: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 12:21:38.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8475"},"items":null}

  Jun 24 12:21:38.991: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8475"},"items":null}

  Jun 24 12:21:39.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-543" for this suite. @ 06/24/23 12:21:39.013
• [10.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 06/24/23 12:21:39.024
  Jun 24 12:21:39.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:21:39.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:39.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:39.048
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:21:39.052
  STEP: Saw pod success @ 06/24/23 12:21:43.078
  Jun 24 12:21:43.083: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-d5ab7ab8-697a-4b1b-946c-14b02d61caff container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:21:43.091
  Jun 24 12:21:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3908" for this suite. @ 06/24/23 12:21:43.111
• [4.096 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 06/24/23 12:21:43.121
  Jun 24 12:21:43.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:21:43.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:43.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:43.144
  STEP: creating service multi-endpoint-test in namespace services-9193 @ 06/24/23 12:21:43.149
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9193 to expose endpoints map[] @ 06/24/23 12:21:43.159
  Jun 24 12:21:43.180: INFO: successfully validated that service multi-endpoint-test in namespace services-9193 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-9193 @ 06/24/23 12:21:43.181
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9193 to expose endpoints map[pod1:[100]] @ 06/24/23 12:21:45.209
  Jun 24 12:21:45.223: INFO: successfully validated that service multi-endpoint-test in namespace services-9193 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-9193 @ 06/24/23 12:21:45.224
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9193 to expose endpoints map[pod1:[100] pod2:[101]] @ 06/24/23 12:21:47.254
  Jun 24 12:21:47.271: INFO: successfully validated that service multi-endpoint-test in namespace services-9193 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 06/24/23 12:21:47.272
  Jun 24 12:21:47.272: INFO: Creating new exec pod
  Jun 24 12:21:50.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9193 exec execpod5pbfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jun 24 12:21:50.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jun 24 12:21:50.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:21:50.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9193 exec execpod5pbfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.159 80'
  Jun 24 12:21:50.607: INFO: stderr: "+ nc -v -t -w 2 10.152.183.159 80\nConnection to 10.152.183.159 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 24 12:21:50.607: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:21:50.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9193 exec execpod5pbfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jun 24 12:21:50.764: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jun 24 12:21:50.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:21:50.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-9193 exec execpod5pbfx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.159 81'
  Jun 24 12:21:50.916: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.159 81\nConnection to 10.152.183.159 81 port [tcp/*] succeeded!\n"
  Jun 24 12:21:50.916: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-9193 @ 06/24/23 12:21:50.916
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9193 to expose endpoints map[pod2:[101]] @ 06/24/23 12:21:50.939
  Jun 24 12:21:50.958: INFO: successfully validated that service multi-endpoint-test in namespace services-9193 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-9193 @ 06/24/23 12:21:50.958
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9193 to expose endpoints map[] @ 06/24/23 12:21:50.976
  Jun 24 12:21:52.001: INFO: successfully validated that service multi-endpoint-test in namespace services-9193 exposes endpoints map[]
  Jun 24 12:21:52.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9193" for this suite. @ 06/24/23 12:21:52.024
• [8.913 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 06/24/23 12:21:52.035
  Jun 24 12:21:52.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context-test @ 06/24/23 12:21:52.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:52.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:52.062
  Jun 24 12:21:56.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4958" for this suite. @ 06/24/23 12:21:56.1
• [4.074 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 06/24/23 12:21:56.11
  Jun 24 12:21:56.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:21:56.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:21:56.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:21:56.135
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/24/23 12:21:56.139
  STEP: Saw pod success @ 06/24/23 12:22:00.172
  Jun 24 12:22:00.176: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-fd5cb8ee-75d5-4854-8f7a-4d13930ae7b0 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:22:00.184
  Jun 24 12:22:00.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8040" for this suite. @ 06/24/23 12:22:00.21
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 06/24/23 12:22:00.218
  Jun 24 12:22:00.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:22:00.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:00.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:00.246
  STEP: Creating a test namespace @ 06/24/23 12:22:00.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:00.269
  STEP: Creating a service in the namespace @ 06/24/23 12:22:00.274
  STEP: Deleting the namespace @ 06/24/23 12:22:00.286
  STEP: Waiting for the namespace to be removed. @ 06/24/23 12:22:00.303
  STEP: Recreating the namespace @ 06/24/23 12:22:06.309
  STEP: Verifying there is no service in the namespace @ 06/24/23 12:22:06.327
  Jun 24 12:22:06.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3598" for this suite. @ 06/24/23 12:22:06.339
  STEP: Destroying namespace "nsdeletetest-6434" for this suite. @ 06/24/23 12:22:06.346
  Jun 24 12:22:06.350: INFO: Namespace nsdeletetest-6434 was already deleted
  STEP: Destroying namespace "nsdeletetest-1721" for this suite. @ 06/24/23 12:22:06.35
• [6.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 06/24/23 12:22:06.362
  Jun 24 12:22:06.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 12:22:06.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:06.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:06.39
  Jun 24 12:22:06.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  W0624 12:22:08.966840      19 warnings.go:70] unknown field "alpha"
  W0624 12:22:08.967067      19 warnings.go:70] unknown field "beta"
  W0624 12:22:08.967237      19 warnings.go:70] unknown field "delta"
  W0624 12:22:08.967409      19 warnings.go:70] unknown field "epsilon"
  W0624 12:22:08.967597      19 warnings.go:70] unknown field "gamma"
  Jun 24 12:22:08.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6844" for this suite. @ 06/24/23 12:22:09.011
• [2.656 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 06/24/23 12:22:09.02
  Jun 24 12:22:09.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename limitrange @ 06/24/23 12:22:09.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:09.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:09.045
  STEP: Creating a LimitRange @ 06/24/23 12:22:09.05
  STEP: Setting up watch @ 06/24/23 12:22:09.05
  STEP: Submitting a LimitRange @ 06/24/23 12:22:09.155
  STEP: Verifying LimitRange creation was observed @ 06/24/23 12:22:09.163
  STEP: Fetching the LimitRange to ensure it has proper values @ 06/24/23 12:22:09.163
  Jun 24 12:22:09.167: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 24 12:22:09.167: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 06/24/23 12:22:09.167
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 06/24/23 12:22:09.174
  Jun 24 12:22:09.178: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun 24 12:22:09.178: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 06/24/23 12:22:09.178
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 06/24/23 12:22:09.186
  Jun 24 12:22:09.194: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jun 24 12:22:09.194: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 06/24/23 12:22:09.194
  STEP: Failing to create a Pod with more than max resources @ 06/24/23 12:22:09.197
  STEP: Updating a LimitRange @ 06/24/23 12:22:09.2
  STEP: Verifying LimitRange updating is effective @ 06/24/23 12:22:09.207
  STEP: Creating a Pod with less than former min resources @ 06/24/23 12:22:11.212
  STEP: Failing to create a Pod with more than max resources @ 06/24/23 12:22:11.22
  STEP: Deleting a LimitRange @ 06/24/23 12:22:11.224
  STEP: Verifying the LimitRange was deleted @ 06/24/23 12:22:11.235
  Jun 24 12:22:16.240: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 06/24/23 12:22:16.24
  Jun 24 12:22:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4526" for this suite. @ 06/24/23 12:22:16.26
• [7.248 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 06/24/23 12:22:16.269
  Jun 24 12:22:16.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename init-container @ 06/24/23 12:22:16.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:16.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:16.293
  STEP: creating the pod @ 06/24/23 12:22:16.297
  Jun 24 12:22:16.297: INFO: PodSpec: initContainers in spec.initContainers
  Jun 24 12:22:19.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2625" for this suite. @ 06/24/23 12:22:19.861
• [3.600 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 06/24/23 12:22:19.869
  Jun 24 12:22:19.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:22:19.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:19.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:19.899
  STEP: Setting up server cert @ 06/24/23 12:22:19.926
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:22:20.464
  STEP: Deploying the webhook pod @ 06/24/23 12:22:20.475
  STEP: Wait for the deployment to be ready @ 06/24/23 12:22:20.491
  Jun 24 12:22:20.501: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/24/23 12:22:22.513
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:22:22.523
  Jun 24 12:22:23.523: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/24/23 12:22:23.527
  STEP: create a pod that should be denied by the webhook @ 06/24/23 12:22:23.547
  STEP: create a pod that causes the webhook to hang @ 06/24/23 12:22:23.56
  STEP: create a configmap that should be denied by the webhook @ 06/24/23 12:22:33.572
  STEP: create a configmap that should be admitted by the webhook @ 06/24/23 12:22:33.634
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/24/23 12:22:33.645
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/24/23 12:22:33.656
  STEP: create a namespace that bypass the webhook @ 06/24/23 12:22:33.664
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 06/24/23 12:22:33.682
  Jun 24 12:22:33.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3093" for this suite. @ 06/24/23 12:22:33.78
  STEP: Destroying namespace "webhook-markers-9549" for this suite. @ 06/24/23 12:22:33.791
  STEP: Destroying namespace "exempted-namespace-9248" for this suite. @ 06/24/23 12:22:33.798
• [13.936 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 06/24/23 12:22:33.806
  Jun 24 12:22:33.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename tables @ 06/24/23 12:22:33.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:33.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:33.832
  Jun 24 12:22:33.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5840" for this suite. @ 06/24/23 12:22:33.852
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 06/24/23 12:22:33.867
  Jun 24 12:22:33.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:22:33.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:33.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:33.894
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:22:33.898
  STEP: Saw pod success @ 06/24/23 12:22:37.923
  Jun 24 12:22:37.927: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-c0ba3643-452e-4c68-95ba-2eb480e3ea9c container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:22:37.934
  Jun 24 12:22:37.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8733" for this suite. @ 06/24/23 12:22:37.962
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 06/24/23 12:22:37.974
  Jun 24 12:22:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption @ 06/24/23 12:22:37.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:22:37.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:22:37.997
  Jun 24 12:22:38.019: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 24 12:23:38.040: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/24/23 12:23:38.045
  Jun 24 12:23:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/24/23 12:23:38.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:23:38.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:23:38.074
  STEP: Finding an available node @ 06/24/23 12:23:38.079
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/24/23 12:23:38.079
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/24/23 12:23:40.104
  Jun 24 12:23:40.125: INFO: found a healthy node: ip-172-31-19-205
  Jun 24 12:23:46.220: INFO: pods created so far: [1 1 1]
  Jun 24 12:23:46.220: INFO: length of pods created so far: 3
  Jun 24 12:23:48.234: INFO: pods created so far: [2 2 1]
  Jun 24 12:23:55.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:23:55.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1126" for this suite. @ 06/24/23 12:23:55.346
  STEP: Destroying namespace "sched-preemption-2467" for this suite. @ 06/24/23 12:23:55.36
• [77.393 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 06/24/23 12:23:55.369
  Jun 24 12:23:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:23:55.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:23:55.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:23:55.397
  STEP: Setting up server cert @ 06/24/23 12:23:55.424
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:23:55.998
  STEP: Deploying the webhook pod @ 06/24/23 12:23:56.007
  STEP: Wait for the deployment to be ready @ 06/24/23 12:23:56.022
  Jun 24 12:23:56.032: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jun 24 12:23:58.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 12, 23, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 23, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 12, 23, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 12, 23, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 06/24/23 12:24:00.049
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:24:00.066
  Jun 24 12:24:01.066: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 06/24/23 12:24:01.071
  STEP: create a namespace for the webhook @ 06/24/23 12:24:01.09
  STEP: create a configmap should be unconditionally rejected by the webhook @ 06/24/23 12:24:01.158
  Jun 24 12:24:01.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7488" for this suite. @ 06/24/23 12:24:01.287
  STEP: Destroying namespace "webhook-markers-1548" for this suite. @ 06/24/23 12:24:01.299
  STEP: Destroying namespace "fail-closed-namespace-1758" for this suite. @ 06/24/23 12:24:01.309
• [5.950 seconds]
------------------------------
SS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 06/24/23 12:24:01.319
  Jun 24 12:24:01.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename lease-test @ 06/24/23 12:24:01.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:01.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:01.35
  Jun 24 12:24:01.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8307" for this suite. @ 06/24/23 12:24:01.446
• [0.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 06/24/23 12:24:01.47
  Jun 24 12:24:01.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename controllerrevisions @ 06/24/23 12:24:01.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:01.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:01.499
  STEP: Creating DaemonSet "e2e-qb5xw-daemon-set" @ 06/24/23 12:24:01.536
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 12:24:01.544
  Jun 24 12:24:01.550: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:01.551: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:01.559: INFO: Number of nodes with available pods controlled by daemonset e2e-qb5xw-daemon-set: 0
  Jun 24 12:24:01.560: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:24:02.565: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:02.565: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:02.570: INFO: Number of nodes with available pods controlled by daemonset e2e-qb5xw-daemon-set: 0
  Jun 24 12:24:02.570: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  Jun 24 12:24:03.565: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:03.566: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:24:03.570: INFO: Number of nodes with available pods controlled by daemonset e2e-qb5xw-daemon-set: 3
  Jun 24 12:24:03.570: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-qb5xw-daemon-set
  STEP: Confirm DaemonSet "e2e-qb5xw-daemon-set" successfully created with "daemonset-name=e2e-qb5xw-daemon-set" label @ 06/24/23 12:24:03.574
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-qb5xw-daemon-set" @ 06/24/23 12:24:03.582
  Jun 24 12:24:03.587: INFO: Located ControllerRevision: "e2e-qb5xw-daemon-set-6866bb848b"
  STEP: Patching ControllerRevision "e2e-qb5xw-daemon-set-6866bb848b" @ 06/24/23 12:24:03.591
  Jun 24 12:24:03.600: INFO: e2e-qb5xw-daemon-set-6866bb848b has been patched
  STEP: Create a new ControllerRevision @ 06/24/23 12:24:03.6
  Jun 24 12:24:03.606: INFO: Created ControllerRevision: e2e-qb5xw-daemon-set-66dbc74496
  STEP: Confirm that there are two ControllerRevisions @ 06/24/23 12:24:03.606
  Jun 24 12:24:03.606: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 24 12:24:03.610: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-qb5xw-daemon-set-6866bb848b" @ 06/24/23 12:24:03.61
  STEP: Confirm that there is only one ControllerRevision @ 06/24/23 12:24:03.618
  Jun 24 12:24:03.618: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 24 12:24:03.623: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-qb5xw-daemon-set-66dbc74496" @ 06/24/23 12:24:03.627
  Jun 24 12:24:03.640: INFO: e2e-qb5xw-daemon-set-66dbc74496 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 06/24/23 12:24:03.64
  W0624 12:24:03.648745      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 06/24/23 12:24:03.648
  Jun 24 12:24:03.649: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 24 12:24:04.657: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 24 12:24:04.662: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-qb5xw-daemon-set-66dbc74496=updated" @ 06/24/23 12:24:04.662
  STEP: Confirm that there is only one ControllerRevision @ 06/24/23 12:24:04.674
  Jun 24 12:24:04.674: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun 24 12:24:04.681: INFO: Found 1 ControllerRevisions
  Jun 24 12:24:04.686: INFO: ControllerRevision "e2e-qb5xw-daemon-set-6f854df45" has revision 3
  STEP: Deleting DaemonSet "e2e-qb5xw-daemon-set" @ 06/24/23 12:24:04.692
  STEP: deleting DaemonSet.extensions e2e-qb5xw-daemon-set in namespace controllerrevisions-1885, will wait for the garbage collector to delete the pods @ 06/24/23 12:24:04.692
  Jun 24 12:24:04.756: INFO: Deleting DaemonSet.extensions e2e-qb5xw-daemon-set took: 10.010388ms
  Jun 24 12:24:04.857: INFO: Terminating DaemonSet.extensions e2e-qb5xw-daemon-set pods took: 100.536498ms
  Jun 24 12:24:06.362: INFO: Number of nodes with available pods controlled by daemonset e2e-qb5xw-daemon-set: 0
  Jun 24 12:24:06.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-qb5xw-daemon-set
  Jun 24 12:24:06.368: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9667"},"items":null}

  Jun 24 12:24:06.373: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9667"},"items":null}

  Jun 24 12:24:06.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-1885" for this suite. @ 06/24/23 12:24:06.404
• [4.944 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 06/24/23 12:24:06.414
  Jun 24 12:24:06.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:24:06.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:06.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:06.454
  STEP: creating a Pod with a static label @ 06/24/23 12:24:06.47
  STEP: watching for Pod to be ready @ 06/24/23 12:24:06.487
  Jun 24 12:24:06.492: INFO: observed Pod pod-test in namespace pods-4551 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jun 24 12:24:06.496: INFO: observed Pod pod-test in namespace pods-4551 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC  }]
  Jun 24 12:24:06.512: INFO: observed Pod pod-test in namespace pods-4551 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC  }]
  Jun 24 12:24:08.123: INFO: Found Pod pod-test in namespace pods-4551 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:24:06 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 06/24/23 12:24:08.128
  STEP: getting the Pod and ensuring that it's patched @ 06/24/23 12:24:08.14
  STEP: replacing the Pod's status Ready condition to False @ 06/24/23 12:24:08.147
  STEP: check the Pod again to ensure its Ready conditions are False @ 06/24/23 12:24:08.162
  STEP: deleting the Pod via a Collection with a LabelSelector @ 06/24/23 12:24:08.162
  STEP: watching for the Pod to be deleted @ 06/24/23 12:24:08.173
  Jun 24 12:24:08.176: INFO: observed event type MODIFIED
  Jun 24 12:24:10.132: INFO: observed event type MODIFIED
  Jun 24 12:24:10.487: INFO: observed event type MODIFIED
  Jun 24 12:24:11.134: INFO: observed event type MODIFIED
  Jun 24 12:24:11.145: INFO: observed event type MODIFIED
  Jun 24 12:24:11.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4551" for this suite. @ 06/24/23 12:24:11.157
• [4.752 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 06/24/23 12:24:11.167
  Jun 24 12:24:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:24:11.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:11.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:11.194
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:24:11.199
  STEP: Saw pod success @ 06/24/23 12:24:15.225
  Jun 24 12:24:15.230: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-af0a3a40-fa19-421f-944a-cb3b42f82188 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:24:15.254
  Jun 24 12:24:15.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9462" for this suite. @ 06/24/23 12:24:15.28
• [4.125 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 06/24/23 12:24:15.293
  Jun 24 12:24:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubelet-test @ 06/24/23 12:24:15.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:15.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:15.323
  Jun 24 12:24:17.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1129" for this suite. @ 06/24/23 12:24:17.367
• [2.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 06/24/23 12:24:17.376
  Jun 24 12:24:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:24:17.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:17.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:17.398
  STEP: creating the pod @ 06/24/23 12:24:17.406
  STEP: submitting the pod to kubernetes @ 06/24/23 12:24:17.406
  STEP: verifying QOS class is set on the pod @ 06/24/23 12:24:17.416
  Jun 24 12:24:17.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5002" for this suite. @ 06/24/23 12:24:17.432
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 06/24/23 12:24:17.45
  Jun 24 12:24:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 12:24:17.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:17.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:17.474
  STEP: create the rc1 @ 06/24/23 12:24:17.483
  STEP: create the rc2 @ 06/24/23 12:24:17.491
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 06/24/23 12:24:23.502
  STEP: delete the rc simpletest-rc-to-be-deleted @ 06/24/23 12:24:24.161
  STEP: wait for the rc to be deleted @ 06/24/23 12:24:24.171
  Jun 24 12:24:29.185: INFO: 67 pods remaining
  Jun 24 12:24:29.185: INFO: 67 pods has nil DeletionTimestamp
  Jun 24 12:24:29.185: INFO: 
  STEP: Gathering metrics @ 06/24/23 12:24:34.19
  W0624 12:24:34.204500      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 12:24:34.204: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 12:24:34.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-22j22" in namespace "gc-8390"
  Jun 24 12:24:34.220: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j5n4" in namespace "gc-8390"
  Jun 24 12:24:34.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kzzx" in namespace "gc-8390"
  Jun 24 12:24:34.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-45qpv" in namespace "gc-8390"
  Jun 24 12:24:34.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-46jxn" in namespace "gc-8390"
  Jun 24 12:24:34.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-487mz" in namespace "gc-8390"
  Jun 24 12:24:34.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b762" in namespace "gc-8390"
  Jun 24 12:24:34.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fflp" in namespace "gc-8390"
  Jun 24 12:24:34.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-67h9f" in namespace "gc-8390"
  Jun 24 12:24:34.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-67rzz" in namespace "gc-8390"
  Jun 24 12:24:34.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lbtt" in namespace "gc-8390"
  Jun 24 12:24:34.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lgbj" in namespace "gc-8390"
  Jun 24 12:24:34.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lzds" in namespace "gc-8390"
  Jun 24 12:24:34.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-74vqw" in namespace "gc-8390"
  Jun 24 12:24:34.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-7btz7" in namespace "gc-8390"
  Jun 24 12:24:34.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jlsd" in namespace "gc-8390"
  Jun 24 12:24:34.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-7np7f" in namespace "gc-8390"
  Jun 24 12:24:34.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zjt4" in namespace "gc-8390"
  Jun 24 12:24:34.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-84t4h" in namespace "gc-8390"
  Jun 24 12:24:34.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-954tx" in namespace "gc-8390"
  Jun 24 12:24:34.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-95dhh" in namespace "gc-8390"
  Jun 24 12:24:34.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-9745w" in namespace "gc-8390"
  Jun 24 12:24:34.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bgx9" in namespace "gc-8390"
  Jun 24 12:24:34.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dhnk" in namespace "gc-8390"
  Jun 24 12:24:34.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fjvm" in namespace "gc-8390"
  Jun 24 12:24:34.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gvnv" in namespace "gc-8390"
  Jun 24 12:24:34.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j9g2" in namespace "gc-8390"
  Jun 24 12:24:34.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k55c" in namespace "gc-8390"
  Jun 24 12:24:34.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mrtw" in namespace "gc-8390"
  Jun 24 12:24:34.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qh7l" in namespace "gc-8390"
  Jun 24 12:24:34.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8t5x" in namespace "gc-8390"
  Jun 24 12:24:34.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpkpm" in namespace "gc-8390"
  Jun 24 12:24:34.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-btcsf" in namespace "gc-8390"
  Jun 24 12:24:34.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx77g" in namespace "gc-8390"
  Jun 24 12:24:34.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz8hk" in namespace "gc-8390"
  Jun 24 12:24:34.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwhbn" in namespace "gc-8390"
  Jun 24 12:24:34.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-czx65" in namespace "gc-8390"
  Jun 24 12:24:34.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-dds5l" in namespace "gc-8390"
  Jun 24 12:24:34.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj22n" in namespace "gc-8390"
  Jun 24 12:24:34.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-dm7tv" in namespace "gc-8390"
  Jun 24 12:24:34.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9n9f" in namespace "gc-8390"
  Jun 24 12:24:34.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffdwk" in namespace "gc-8390"
  Jun 24 12:24:34.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwn6v" in namespace "gc-8390"
  Jun 24 12:24:34.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6qld" in namespace "gc-8390"
  Jun 24 12:24:34.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9lkh" in namespace "gc-8390"
  Jun 24 12:24:34.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbfwl" in namespace "gc-8390"
  Jun 24 12:24:34.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfzqf" in namespace "gc-8390"
  Jun 24 12:24:35.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggzzx" in namespace "gc-8390"
  Jun 24 12:24:35.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-gj5j6" in namespace "gc-8390"
  Jun 24 12:24:35.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9ssc" in namespace "gc-8390"
  Jun 24 12:24:35.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8390" for this suite. @ 06/24/23 12:24:35.076
• [17.635 seconds]
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 06/24/23 12:24:35.085
  Jun 24 12:24:35.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:24:35.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:35.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:35.113
  STEP: Creating projection with secret that has name secret-emptykey-test-e4f9f6c7-9f19-4d7a-9edc-b8ec05f24a59 @ 06/24/23 12:24:35.118
  Jun 24 12:24:35.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7605" for this suite. @ 06/24/23 12:24:35.126
• [0.052 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 06/24/23 12:24:35.137
  Jun 24 12:24:35.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename watch @ 06/24/23 12:24:35.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:35.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:35.165
  STEP: creating a watch on configmaps with label A @ 06/24/23 12:24:35.173
  STEP: creating a watch on configmaps with label B @ 06/24/23 12:24:35.175
  STEP: creating a watch on configmaps with label A or B @ 06/24/23 12:24:35.177
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 06/24/23 12:24:35.178
  Jun 24 12:24:35.184: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11420 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:35.185: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11420 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 06/24/23 12:24:35.185
  Jun 24 12:24:35.196: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11421 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:35.197: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11421 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 06/24/23 12:24:35.197
  Jun 24 12:24:35.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11422 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:35.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11422 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 06/24/23 12:24:35.208
  Jun 24 12:24:35.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11423 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:35.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3416  e68e5de8-f9aa-472f-94bb-c158109186c9 11423 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 06/24/23 12:24:35.218
  Jun 24 12:24:35.224: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3416  894eef06-e9e7-449c-b13c-e8488cd0e95a 11424 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:35.224: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3416  894eef06-e9e7-449c-b13c-e8488cd0e95a 11424 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 06/24/23 12:24:45.226
  Jun 24 12:24:45.239: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3416  894eef06-e9e7-449c-b13c-e8488cd0e95a 12095 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:45.239: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3416  894eef06-e9e7-449c-b13c-e8488cd0e95a 12095 0 2023-06-24 12:24:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-24 12:24:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:24:55.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3416" for this suite. @ 06/24/23 12:24:55.248
• [20.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 06/24/23 12:24:55.26
  Jun 24 12:24:55.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:24:55.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:24:55.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:24:55.284
  STEP: Creating pod liveness-e37f9329-01fd-4a10-8085-0ab1ad7f07f0 in namespace container-probe-7160 @ 06/24/23 12:24:55.289
  Jun 24 12:24:57.315: INFO: Started pod liveness-e37f9329-01fd-4a10-8085-0ab1ad7f07f0 in namespace container-probe-7160
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 12:24:57.315
  Jun 24 12:24:57.319: INFO: Initial restart count of pod liveness-e37f9329-01fd-4a10-8085-0ab1ad7f07f0 is 0
  Jun 24 12:28:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:28:57.972
  STEP: Destroying namespace "container-probe-7160" for this suite. @ 06/24/23 12:28:57.987
• [242.736 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 06/24/23 12:28:57.998
  Jun 24 12:28:57.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename runtimeclass @ 06/24/23 12:28:58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:28:58.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:28:58.024
  Jun 24 12:28:58.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9477" for this suite. @ 06/24/23 12:28:58.072
• [0.084 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 06/24/23 12:28:58.084
  Jun 24 12:28:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 12:28:58.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:28:58.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:28:58.11
  STEP: Creating service test in namespace statefulset-6102 @ 06/24/23 12:28:58.114
  STEP: Looking for a node to schedule stateful set and pod @ 06/24/23 12:28:58.127
  STEP: Creating pod with conflicting port in namespace statefulset-6102 @ 06/24/23 12:28:58.137
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6102 @ 06/24/23 12:28:58.146
  STEP: Creating statefulset with conflicting port in namespace statefulset-6102 @ 06/24/23 12:29:00.156
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6102 @ 06/24/23 12:29:00.166
  Jun 24 12:29:00.192: INFO: Observed stateful pod in namespace: statefulset-6102, name: ss-0, uid: 84ebfbb9-7fa7-44f8-a8f6-551444c100b0, status phase: Pending. Waiting for statefulset controller to delete.
  Jun 24 12:29:00.216: INFO: Observed stateful pod in namespace: statefulset-6102, name: ss-0, uid: 84ebfbb9-7fa7-44f8-a8f6-551444c100b0, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 24 12:29:00.224: INFO: Observed stateful pod in namespace: statefulset-6102, name: ss-0, uid: 84ebfbb9-7fa7-44f8-a8f6-551444c100b0, status phase: Failed. Waiting for statefulset controller to delete.
  Jun 24 12:29:00.230: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6102
  STEP: Removing pod with conflicting port in namespace statefulset-6102 @ 06/24/23 12:29:00.231
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6102 and will be in running state @ 06/24/23 12:29:00.251
  Jun 24 12:29:02.262: INFO: Deleting all statefulset in ns statefulset-6102
  Jun 24 12:29:02.267: INFO: Scaling statefulset ss to 0
  Jun 24 12:29:12.289: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:29:12.292: INFO: Deleting statefulset ss
  Jun 24 12:29:12.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6102" for this suite. @ 06/24/23 12:29:12.317
• [14.242 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 06/24/23 12:29:12.327
  Jun 24 12:29:12.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 12:29:12.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:29:12.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:29:12.353
  STEP: Creating a job @ 06/24/23 12:29:12.357
  STEP: Ensuring active pods == parallelism @ 06/24/23 12:29:12.364
  STEP: delete a job @ 06/24/23 12:29:14.37
  STEP: deleting Job.batch foo in namespace job-4788, will wait for the garbage collector to delete the pods @ 06/24/23 12:29:14.37
  Jun 24 12:29:14.434: INFO: Deleting Job.batch foo took: 9.418305ms
  Jun 24 12:29:14.535: INFO: Terminating Job.batch foo pods took: 100.886824ms
  STEP: Ensuring job was deleted @ 06/24/23 12:29:45.935
  Jun 24 12:29:45.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4788" for this suite. @ 06/24/23 12:29:45.945
• [33.625 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 06/24/23 12:29:45.952
  Jun 24 12:29:45.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption @ 06/24/23 12:29:45.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:29:45.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:29:45.976
  Jun 24 12:29:45.996: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun 24 12:30:46.020: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/24/23 12:30:46.024
  Jun 24 12:30:46.054: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun 24 12:30:46.062: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun 24 12:30:46.088: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun 24 12:30:46.096: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun 24 12:30:46.120: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun 24 12:30:46.130: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/24/23 12:30:46.13
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 06/24/23 12:30:48.162
  Jun 24 12:30:50.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6637" for this suite. @ 06/24/23 12:30:50.332
• [64.390 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 06/24/23 12:30:50.344
  Jun 24 12:30:50.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:30:50.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:30:50.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:30:50.384
  STEP: Setting up server cert @ 06/24/23 12:30:50.413
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:30:50.714
  STEP: Deploying the webhook pod @ 06/24/23 12:30:50.72
  STEP: Wait for the deployment to be ready @ 06/24/23 12:30:50.736
  Jun 24 12:30:50.744: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 06/24/23 12:30:52.756
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:30:52.767
  Jun 24 12:30:53.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 06/24/23 12:30:53.773
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/24/23 12:30:53.773
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 06/24/23 12:30:53.791
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 06/24/23 12:30:54.807
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/24/23 12:30:54.807
  STEP: Having no error when timeout is longer than webhook latency @ 06/24/23 12:30:55.849
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/24/23 12:30:55.849
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 06/24/23 12:31:00.893
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/24/23 12:31:00.893
  Jun 24 12:31:05.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3774" for this suite. @ 06/24/23 12:31:06.011
  STEP: Destroying namespace "webhook-markers-2116" for this suite. @ 06/24/23 12:31:06.021
• [15.683 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 06/24/23 12:31:06.031
  Jun 24 12:31:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:31:06.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:06.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:06.062
  STEP: creating a ConfigMap @ 06/24/23 12:31:06.066
  STEP: fetching the ConfigMap @ 06/24/23 12:31:06.072
  STEP: patching the ConfigMap @ 06/24/23 12:31:06.076
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 06/24/23 12:31:06.085
  STEP: deleting the ConfigMap by collection with a label selector @ 06/24/23 12:31:06.09
  STEP: listing all ConfigMaps in test namespace @ 06/24/23 12:31:06.099
  Jun 24 12:31:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1501" for this suite. @ 06/24/23 12:31:06.108
• [0.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 06/24/23 12:31:06.118
  Jun 24 12:31:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:31:06.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:06.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:06.142
  STEP: Setting up server cert @ 06/24/23 12:31:06.176
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:31:06.536
  STEP: Deploying the webhook pod @ 06/24/23 12:31:06.542
  STEP: Wait for the deployment to be ready @ 06/24/23 12:31:06.556
  Jun 24 12:31:06.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/24/23 12:31:08.578
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:31:08.589
  Jun 24 12:31:09.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/24/23 12:31:09.674
  STEP: Creating a configMap that should be mutated @ 06/24/23 12:31:09.69
  STEP: Deleting the collection of validation webhooks @ 06/24/23 12:31:09.723
  STEP: Creating a configMap that should not be mutated @ 06/24/23 12:31:09.778
  Jun 24 12:31:09.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3584" for this suite. @ 06/24/23 12:31:09.862
  STEP: Destroying namespace "webhook-markers-3751" for this suite. @ 06/24/23 12:31:09.873
• [3.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 06/24/23 12:31:09.885
  Jun 24 12:31:09.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-runtime @ 06/24/23 12:31:09.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:09.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:09.907
  STEP: create the container @ 06/24/23 12:31:09.911
  W0624 12:31:09.923043      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/24/23 12:31:09.923
  STEP: get the container status @ 06/24/23 12:31:12.947
  STEP: the container should be terminated @ 06/24/23 12:31:12.952
  STEP: the termination message should be set @ 06/24/23 12:31:12.952
  Jun 24 12:31:12.952: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 06/24/23 12:31:12.952
  Jun 24 12:31:12.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8077" for this suite. @ 06/24/23 12:31:12.978
• [3.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 06/24/23 12:31:12.989
  Jun 24 12:31:12.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svc-latency @ 06/24/23 12:31:12.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:13.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:13.018
  Jun 24 12:31:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7475 @ 06/24/23 12:31:13.028
  I0624 12:31:13.037433      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7475, replica count: 1
  I0624 12:31:14.088840      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:31:14.203: INFO: Created: latency-svc-v2gdw
  Jun 24 12:31:14.215: INFO: Got endpoints: latency-svc-v2gdw [25.464626ms]
  Jun 24 12:31:14.246: INFO: Created: latency-svc-6dqrm
  Jun 24 12:31:14.255: INFO: Got endpoints: latency-svc-6dqrm [39.569994ms]
  Jun 24 12:31:14.265: INFO: Created: latency-svc-4s8nm
  Jun 24 12:31:14.272: INFO: Created: latency-svc-4mlqp
  Jun 24 12:31:14.279: INFO: Got endpoints: latency-svc-4s8nm [63.076689ms]
  Jun 24 12:31:14.280: INFO: Got endpoints: latency-svc-4mlqp [63.505823ms]
  Jun 24 12:31:14.296: INFO: Created: latency-svc-qgfxz
  Jun 24 12:31:14.296: INFO: Created: latency-svc-6x6ts
  Jun 24 12:31:14.308: INFO: Created: latency-svc-5g529
  Jun 24 12:31:14.320: INFO: Got endpoints: latency-svc-5g529 [103.667084ms]
  Jun 24 12:31:14.333: INFO: Got endpoints: latency-svc-6x6ts [116.329056ms]
  Jun 24 12:31:14.333: INFO: Got endpoints: latency-svc-qgfxz [116.571639ms]
  Jun 24 12:31:14.335: INFO: Created: latency-svc-z9mrj
  Jun 24 12:31:14.340: INFO: Created: latency-svc-8clwg
  Jun 24 12:31:14.345: INFO: Got endpoints: latency-svc-z9mrj [128.324062ms]
  Jun 24 12:31:14.352: INFO: Got endpoints: latency-svc-8clwg [135.102632ms]
  Jun 24 12:31:14.354: INFO: Created: latency-svc-6cp4f
  Jun 24 12:31:14.364: INFO: Got endpoints: latency-svc-6cp4f [146.802405ms]
  Jun 24 12:31:14.365: INFO: Created: latency-svc-qsxl4
  Jun 24 12:31:14.382: INFO: Created: latency-svc-dwkzc
  Jun 24 12:31:14.382: INFO: Got endpoints: latency-svc-dwkzc [164.993315ms]
  Jun 24 12:31:14.385: INFO: Got endpoints: latency-svc-qsxl4 [167.506471ms]
  Jun 24 12:31:14.393: INFO: Created: latency-svc-khpkz
  Jun 24 12:31:14.403: INFO: Got endpoints: latency-svc-khpkz [185.440618ms]
  Jun 24 12:31:14.472: INFO: Created: latency-svc-f8zlr
  Jun 24 12:31:14.472: INFO: Created: latency-svc-m5ld7
  Jun 24 12:31:14.477: INFO: Created: latency-svc-hjb7q
  Jun 24 12:31:14.484: INFO: Created: latency-svc-lpjh9
  Jun 24 12:31:14.489: INFO: Created: latency-svc-fzm8d
  Jun 24 12:31:14.504: INFO: Got endpoints: latency-svc-f8zlr [286.447785ms]
  Jun 24 12:31:14.505: INFO: Got endpoints: latency-svc-hjb7q [102.070537ms]
  Jun 24 12:31:14.509: INFO: Got endpoints: latency-svc-m5ld7 [124.117858ms]
  Jun 24 12:31:14.516: INFO: Created: latency-svc-62nx5
  Jun 24 12:31:14.516: INFO: Created: latency-svc-858mn
  Jun 24 12:31:14.517: INFO: Created: latency-svc-pt982
  Jun 24 12:31:14.517: INFO: Created: latency-svc-pzx8h
  Jun 24 12:31:14.517: INFO: Created: latency-svc-sms4h
  Jun 24 12:31:14.517: INFO: Created: latency-svc-jp6tc
  Jun 24 12:31:14.517: INFO: Created: latency-svc-7k7mp
  Jun 24 12:31:14.517: INFO: Created: latency-svc-8kqgz
  Jun 24 12:31:14.518: INFO: Created: latency-svc-q8vsm
  Jun 24 12:31:14.518: INFO: Created: latency-svc-4pxf5
  Jun 24 12:31:14.522: INFO: Got endpoints: latency-svc-lpjh9 [304.737606ms]
  Jun 24 12:31:14.526: INFO: Created: latency-svc-fwvpw
  Jun 24 12:31:14.530: INFO: Got endpoints: latency-svc-fzm8d [184.502739ms]
  Jun 24 12:31:14.535: INFO: Got endpoints: latency-svc-858mn [152.714237ms]
  Jun 24 12:31:14.548: INFO: Got endpoints: latency-svc-pzx8h [330.408954ms]
  Jun 24 12:31:14.551: INFO: Got endpoints: latency-svc-8kqgz [295.789472ms]
  Jun 24 12:31:14.557: INFO: Created: latency-svc-2dv47
  Jun 24 12:31:14.561: INFO: Got endpoints: latency-svc-jp6tc [240.482324ms]
  Jun 24 12:31:14.561: INFO: Got endpoints: latency-svc-q8vsm [281.035318ms]
  Jun 24 12:31:14.562: INFO: Got endpoints: latency-svc-pt982 [229.011634ms]
  Jun 24 12:31:14.577: INFO: Got endpoints: latency-svc-62nx5 [297.420519ms]
  Jun 24 12:31:14.577: INFO: Got endpoints: latency-svc-4pxf5 [244.014301ms]
  Jun 24 12:31:14.586: INFO: Got endpoints: latency-svc-7k7mp [222.672738ms]
  Jun 24 12:31:14.587: INFO: Got endpoints: latency-svc-sms4h [235.418371ms]
  Jun 24 12:31:14.587: INFO: Got endpoints: latency-svc-fwvpw [83.28637ms]
  Jun 24 12:31:14.595: INFO: Got endpoints: latency-svc-2dv47 [89.320244ms]
  Jun 24 12:31:14.654: INFO: Created: latency-svc-8zkpm
  Jun 24 12:31:14.654: INFO: Created: latency-svc-l8sts
  Jun 24 12:31:14.655: INFO: Created: latency-svc-lqd2q
  Jun 24 12:31:14.655: INFO: Created: latency-svc-2qwbq
  Jun 24 12:31:14.670: INFO: Got endpoints: latency-svc-8zkpm [147.820985ms]
  Jun 24 12:31:14.692: INFO: Got endpoints: latency-svc-lqd2q [104.586633ms]
  Jun 24 12:31:14.696: INFO: Got endpoints: latency-svc-l8sts [144.42812ms]
  Jun 24 12:31:14.701: INFO: Created: latency-svc-659ck
  Jun 24 12:31:14.715: INFO: Created: latency-svc-pmmv8
  Jun 24 12:31:14.715: INFO: Created: latency-svc-wxxv6
  Jun 24 12:31:14.715: INFO: Created: latency-svc-pzjfc
  Jun 24 12:31:14.715: INFO: Created: latency-svc-bdxxn
  Jun 24 12:31:14.715: INFO: Created: latency-svc-jhtng
  Jun 24 12:31:14.715: INFO: Created: latency-svc-jqcd6
  Jun 24 12:31:14.716: INFO: Created: latency-svc-wgmh6
  Jun 24 12:31:14.716: INFO: Created: latency-svc-qgjr5
  Jun 24 12:31:14.716: INFO: Got endpoints: latency-svc-qgjr5 [153.719557ms]
  Jun 24 12:31:14.716: INFO: Created: latency-svc-cvgxn
  Jun 24 12:31:14.717: INFO: Got endpoints: latency-svc-cvgxn [186.638031ms]
  Jun 24 12:31:14.717: INFO: Created: latency-svc-rd2hl
  Jun 24 12:31:14.718: INFO: Got endpoints: latency-svc-rd2hl [182.589739ms]
  Jun 24 12:31:14.729: INFO: Got endpoints: latency-svc-2qwbq [220.366834ms]
  Jun 24 12:31:14.731: INFO: Created: latency-svc-8swr5
  Jun 24 12:31:14.733: INFO: Got endpoints: latency-svc-bdxxn [171.075379ms]
  Jun 24 12:31:14.738: INFO: Got endpoints: latency-svc-pmmv8 [161.035754ms]
  Jun 24 12:31:14.738: INFO: Got endpoints: latency-svc-jhtng [177.134292ms]
  Jun 24 12:31:14.764: INFO: Got endpoints: latency-svc-pzjfc [175.907489ms]
  Jun 24 12:31:14.790: INFO: Created: latency-svc-q5ms7
  Jun 24 12:31:14.792: INFO: Created: latency-svc-z7r7g
  Jun 24 12:31:14.806: INFO: Created: latency-svc-d65vn
  Jun 24 12:31:14.808: INFO: Created: latency-svc-v47dl
  Jun 24 12:31:14.811: INFO: Created: latency-svc-hdd5q
  Jun 24 12:31:14.811: INFO: Created: latency-svc-gq7w7
  Jun 24 12:31:14.811: INFO: Created: latency-svc-ds9ct
  Jun 24 12:31:14.812: INFO: Created: latency-svc-jg7wc
  Jun 24 12:31:14.812: INFO: Created: latency-svc-9m8hd
  Jun 24 12:31:14.812: INFO: Created: latency-svc-2hpbw
  Jun 24 12:31:14.814: INFO: Got endpoints: latency-svc-wxxv6 [219.881769ms]
  Jun 24 12:31:14.829: INFO: Created: latency-svc-mdbsr
  Jun 24 12:31:14.861: INFO: Got endpoints: latency-svc-wgmh6 [313.141364ms]
  Jun 24 12:31:14.875: INFO: Created: latency-svc-4b7s4
  Jun 24 12:31:14.912: INFO: Got endpoints: latency-svc-jqcd6 [334.997042ms]
  Jun 24 12:31:14.932: INFO: Created: latency-svc-pzzr7
  Jun 24 12:31:14.965: INFO: Got endpoints: latency-svc-659ck [378.037473ms]
  Jun 24 12:31:14.982: INFO: Created: latency-svc-dh949
  Jun 24 12:31:15.015: INFO: Got endpoints: latency-svc-8swr5 [345.154238ms]
  Jun 24 12:31:15.037: INFO: Created: latency-svc-d6ft5
  Jun 24 12:31:15.074: INFO: Got endpoints: latency-svc-q5ms7 [309.89874ms]
  Jun 24 12:31:15.103: INFO: Created: latency-svc-rwfzw
  Jun 24 12:31:15.113: INFO: Got endpoints: latency-svc-2hpbw [380.287805ms]
  Jun 24 12:31:15.136: INFO: Created: latency-svc-dgw7v
  Jun 24 12:31:15.167: INFO: Got endpoints: latency-svc-d65vn [428.49031ms]
  Jun 24 12:31:15.183: INFO: Created: latency-svc-zkgkz
  Jun 24 12:31:15.220: INFO: Got endpoints: latency-svc-jg7wc [523.653514ms]
  Jun 24 12:31:15.234: INFO: Created: latency-svc-bs8lm
  Jun 24 12:31:15.262: INFO: Got endpoints: latency-svc-z7r7g [544.428072ms]
  Jun 24 12:31:15.284: INFO: Created: latency-svc-gs6tv
  Jun 24 12:31:15.312: INFO: Got endpoints: latency-svc-v47dl [619.82585ms]
  Jun 24 12:31:15.329: INFO: Created: latency-svc-hhknn
  Jun 24 12:31:15.364: INFO: Got endpoints: latency-svc-hdd5q [634.797687ms]
  Jun 24 12:31:15.379: INFO: Created: latency-svc-vsj4k
  Jun 24 12:31:15.410: INFO: Got endpoints: latency-svc-9m8hd [671.487851ms]
  Jun 24 12:31:15.431: INFO: Created: latency-svc-849hj
  Jun 24 12:31:15.464: INFO: Got endpoints: latency-svc-gq7w7 [748.341583ms]
  Jun 24 12:31:15.477: INFO: Created: latency-svc-tvhl7
  Jun 24 12:31:15.512: INFO: Got endpoints: latency-svc-ds9ct [794.667288ms]
  Jun 24 12:31:15.526: INFO: Created: latency-svc-474vl
  Jun 24 12:31:15.561: INFO: Got endpoints: latency-svc-mdbsr [746.190091ms]
  Jun 24 12:31:15.574: INFO: Created: latency-svc-sqp4x
  Jun 24 12:31:15.613: INFO: Got endpoints: latency-svc-4b7s4 [751.542507ms]
  Jun 24 12:31:15.629: INFO: Created: latency-svc-r6lhk
  Jun 24 12:31:15.661: INFO: Got endpoints: latency-svc-pzzr7 [748.97598ms]
  Jun 24 12:31:15.674: INFO: Created: latency-svc-2hfr8
  Jun 24 12:31:15.711: INFO: Got endpoints: latency-svc-dh949 [746.158331ms]
  Jun 24 12:31:15.723: INFO: Created: latency-svc-tqxl7
  Jun 24 12:31:15.765: INFO: Got endpoints: latency-svc-d6ft5 [749.175943ms]
  Jun 24 12:31:15.778: INFO: Created: latency-svc-k9mv9
  Jun 24 12:31:15.811: INFO: Got endpoints: latency-svc-rwfzw [737.45882ms]
  Jun 24 12:31:15.825: INFO: Created: latency-svc-chtnw
  Jun 24 12:31:15.861: INFO: Got endpoints: latency-svc-dgw7v [747.584096ms]
  Jun 24 12:31:15.874: INFO: Created: latency-svc-xpkct
  Jun 24 12:31:15.912: INFO: Got endpoints: latency-svc-zkgkz [745.173891ms]
  Jun 24 12:31:15.932: INFO: Created: latency-svc-gdrmr
  Jun 24 12:31:15.962: INFO: Got endpoints: latency-svc-bs8lm [742.031628ms]
  Jun 24 12:31:15.976: INFO: Created: latency-svc-k24jj
  Jun 24 12:31:16.012: INFO: Got endpoints: latency-svc-gs6tv [749.149972ms]
  Jun 24 12:31:16.025: INFO: Created: latency-svc-56f77
  Jun 24 12:31:16.064: INFO: Got endpoints: latency-svc-hhknn [751.580488ms]
  Jun 24 12:31:16.075: INFO: Created: latency-svc-b6npx
  Jun 24 12:31:16.113: INFO: Got endpoints: latency-svc-vsj4k [748.731567ms]
  Jun 24 12:31:16.128: INFO: Created: latency-svc-57z4t
  Jun 24 12:31:16.161: INFO: Got endpoints: latency-svc-849hj [750.87592ms]
  Jun 24 12:31:16.176: INFO: Created: latency-svc-gwgvw
  Jun 24 12:31:16.216: INFO: Got endpoints: latency-svc-tvhl7 [751.377286ms]
  Jun 24 12:31:16.232: INFO: Created: latency-svc-rb2gd
  Jun 24 12:31:16.262: INFO: Got endpoints: latency-svc-474vl [750.282774ms]
  Jun 24 12:31:16.275: INFO: Created: latency-svc-x6lwx
  Jun 24 12:31:16.312: INFO: Got endpoints: latency-svc-sqp4x [750.82193ms]
  Jun 24 12:31:16.324: INFO: Created: latency-svc-9h7wc
  Jun 24 12:31:16.365: INFO: Got endpoints: latency-svc-r6lhk [751.607427ms]
  Jun 24 12:31:16.377: INFO: Created: latency-svc-kmldn
  Jun 24 12:31:16.411: INFO: Got endpoints: latency-svc-2hfr8 [749.581647ms]
  Jun 24 12:31:16.424: INFO: Created: latency-svc-vlb5m
  Jun 24 12:31:16.461: INFO: Got endpoints: latency-svc-tqxl7 [750.203663ms]
  Jun 24 12:31:16.475: INFO: Created: latency-svc-7ljdw
  Jun 24 12:31:16.510: INFO: Got endpoints: latency-svc-k9mv9 [745.10785ms]
  Jun 24 12:31:16.524: INFO: Created: latency-svc-ccnsh
  Jun 24 12:31:16.560: INFO: Got endpoints: latency-svc-chtnw [748.488275ms]
  Jun 24 12:31:16.572: INFO: Created: latency-svc-n7rg8
  Jun 24 12:31:16.610: INFO: Got endpoints: latency-svc-xpkct [749.091122ms]
  Jun 24 12:31:16.620: INFO: Created: latency-svc-j8gsf
  Jun 24 12:31:16.665: INFO: Got endpoints: latency-svc-gdrmr [752.034282ms]
  Jun 24 12:31:16.678: INFO: Created: latency-svc-nchrr
  Jun 24 12:31:16.715: INFO: Got endpoints: latency-svc-k24jj [751.799389ms]
  Jun 24 12:31:16.730: INFO: Created: latency-svc-dx9gf
  Jun 24 12:31:16.762: INFO: Got endpoints: latency-svc-56f77 [749.838099ms]
  Jun 24 12:31:16.774: INFO: Created: latency-svc-cf7mq
  Jun 24 12:31:16.813: INFO: Got endpoints: latency-svc-b6npx [748.979539ms]
  Jun 24 12:31:16.825: INFO: Created: latency-svc-bbwjg
  Jun 24 12:31:16.861: INFO: Got endpoints: latency-svc-57z4t [747.635626ms]
  Jun 24 12:31:16.873: INFO: Created: latency-svc-59n6w
  Jun 24 12:31:16.913: INFO: Got endpoints: latency-svc-gwgvw [751.206663ms]
  Jun 24 12:31:16.925: INFO: Created: latency-svc-qqbwz
  Jun 24 12:31:16.962: INFO: Got endpoints: latency-svc-rb2gd [746.13566ms]
  Jun 24 12:31:16.976: INFO: Created: latency-svc-bsqct
  Jun 24 12:31:17.012: INFO: Got endpoints: latency-svc-x6lwx [749.580076ms]
  Jun 24 12:31:17.026: INFO: Created: latency-svc-2njf7
  Jun 24 12:31:17.061: INFO: Got endpoints: latency-svc-9h7wc [749.115721ms]
  Jun 24 12:31:17.076: INFO: Created: latency-svc-p9gm5
  Jun 24 12:31:17.114: INFO: Got endpoints: latency-svc-kmldn [749.04195ms]
  Jun 24 12:31:17.126: INFO: Created: latency-svc-fvswz
  Jun 24 12:31:17.163: INFO: Got endpoints: latency-svc-vlb5m [751.681208ms]
  Jun 24 12:31:17.178: INFO: Created: latency-svc-l2dk4
  Jun 24 12:31:17.211: INFO: Got endpoints: latency-svc-7ljdw [749.927509ms]
  Jun 24 12:31:17.227: INFO: Created: latency-svc-qfv9n
  Jun 24 12:31:17.262: INFO: Got endpoints: latency-svc-ccnsh [752.047161ms]
  Jun 24 12:31:17.275: INFO: Created: latency-svc-vxcm5
  Jun 24 12:31:17.313: INFO: Got endpoints: latency-svc-n7rg8 [752.997782ms]
  Jun 24 12:31:17.327: INFO: Created: latency-svc-zghnz
  Jun 24 12:31:17.362: INFO: Got endpoints: latency-svc-j8gsf [751.966691ms]
  Jun 24 12:31:17.378: INFO: Created: latency-svc-5vvg4
  Jun 24 12:31:17.412: INFO: Got endpoints: latency-svc-nchrr [747.482104ms]
  Jun 24 12:31:17.426: INFO: Created: latency-svc-qt67n
  Jun 24 12:31:17.465: INFO: Got endpoints: latency-svc-dx9gf [749.802018ms]
  Jun 24 12:31:17.477: INFO: Created: latency-svc-sr4sz
  Jun 24 12:31:17.513: INFO: Got endpoints: latency-svc-cf7mq [751.018082ms]
  Jun 24 12:31:17.527: INFO: Created: latency-svc-j49gn
  Jun 24 12:31:17.565: INFO: Got endpoints: latency-svc-bbwjg [751.973471ms]
  Jun 24 12:31:17.579: INFO: Created: latency-svc-rvjpp
  Jun 24 12:31:17.613: INFO: Got endpoints: latency-svc-59n6w [751.331435ms]
  Jun 24 12:31:17.628: INFO: Created: latency-svc-jc9cq
  Jun 24 12:31:17.660: INFO: Got endpoints: latency-svc-qqbwz [747.627356ms]
  Jun 24 12:31:17.673: INFO: Created: latency-svc-c5x9v
  Jun 24 12:31:17.714: INFO: Got endpoints: latency-svc-bsqct [751.415216ms]
  Jun 24 12:31:17.727: INFO: Created: latency-svc-dtgkh
  Jun 24 12:31:17.763: INFO: Got endpoints: latency-svc-2njf7 [751.227414ms]
  Jun 24 12:31:17.780: INFO: Created: latency-svc-hvk6d
  Jun 24 12:31:17.811: INFO: Got endpoints: latency-svc-p9gm5 [749.292073ms]
  Jun 24 12:31:17.824: INFO: Created: latency-svc-87c6z
  Jun 24 12:31:17.862: INFO: Got endpoints: latency-svc-fvswz [747.937459ms]
  Jun 24 12:31:17.875: INFO: Created: latency-svc-h5ddb
  Jun 24 12:31:17.912: INFO: Got endpoints: latency-svc-l2dk4 [748.338233ms]
  Jun 24 12:31:17.925: INFO: Created: latency-svc-t6tzx
  Jun 24 12:31:17.962: INFO: Got endpoints: latency-svc-qfv9n [750.760249ms]
  Jun 24 12:31:17.975: INFO: Created: latency-svc-zwjwj
  Jun 24 12:31:18.014: INFO: Got endpoints: latency-svc-vxcm5 [751.214224ms]
  Jun 24 12:31:18.027: INFO: Created: latency-svc-j8fml
  Jun 24 12:31:18.060: INFO: Got endpoints: latency-svc-zghnz [746.733947ms]
  Jun 24 12:31:18.074: INFO: Created: latency-svc-g2xgq
  Jun 24 12:31:18.117: INFO: Got endpoints: latency-svc-5vvg4 [754.387526ms]
  Jun 24 12:31:18.128: INFO: Created: latency-svc-dwdgk
  Jun 24 12:31:18.163: INFO: Got endpoints: latency-svc-qt67n [750.298124ms]
  Jun 24 12:31:18.181: INFO: Created: latency-svc-tntt6
  Jun 24 12:31:18.212: INFO: Got endpoints: latency-svc-sr4sz [747.632556ms]
  Jun 24 12:31:18.233: INFO: Created: latency-svc-wwnfv
  Jun 24 12:31:18.268: INFO: Got endpoints: latency-svc-j49gn [754.972243ms]
  Jun 24 12:31:18.283: INFO: Created: latency-svc-clcq9
  Jun 24 12:31:18.311: INFO: Got endpoints: latency-svc-rvjpp [745.407943ms]
  Jun 24 12:31:18.339: INFO: Created: latency-svc-64jxh
  Jun 24 12:31:18.362: INFO: Got endpoints: latency-svc-jc9cq [749.049141ms]
  Jun 24 12:31:18.375: INFO: Created: latency-svc-sxlp6
  Jun 24 12:31:18.412: INFO: Got endpoints: latency-svc-c5x9v [751.555127ms]
  Jun 24 12:31:18.425: INFO: Created: latency-svc-dvmtf
  Jun 24 12:31:18.461: INFO: Got endpoints: latency-svc-dtgkh [746.790627ms]
  Jun 24 12:31:18.473: INFO: Created: latency-svc-hnn7k
  Jun 24 12:31:18.513: INFO: Got endpoints: latency-svc-hvk6d [748.972161ms]
  Jun 24 12:31:18.527: INFO: Created: latency-svc-9wjjv
  Jun 24 12:31:18.561: INFO: Got endpoints: latency-svc-87c6z [750.356844ms]
  Jun 24 12:31:18.575: INFO: Created: latency-svc-wxc22
  Jun 24 12:31:18.611: INFO: Got endpoints: latency-svc-h5ddb [748.270892ms]
  Jun 24 12:31:18.624: INFO: Created: latency-svc-6t2fk
  Jun 24 12:31:18.664: INFO: Got endpoints: latency-svc-t6tzx [752.501467ms]
  Jun 24 12:31:18.678: INFO: Created: latency-svc-dc9q4
  Jun 24 12:31:18.716: INFO: Got endpoints: latency-svc-zwjwj [753.140443ms]
  Jun 24 12:31:18.730: INFO: Created: latency-svc-ftqwh
  Jun 24 12:31:18.764: INFO: Got endpoints: latency-svc-j8fml [750.384585ms]
  Jun 24 12:31:18.779: INFO: Created: latency-svc-6kvbp
  Jun 24 12:31:18.813: INFO: Got endpoints: latency-svc-g2xgq [752.518557ms]
  Jun 24 12:31:18.834: INFO: Created: latency-svc-5fgdm
  Jun 24 12:31:18.862: INFO: Got endpoints: latency-svc-dwdgk [745.786287ms]
  Jun 24 12:31:18.876: INFO: Created: latency-svc-cd9vq
  Jun 24 12:31:18.911: INFO: Got endpoints: latency-svc-tntt6 [747.909809ms]
  Jun 24 12:31:18.924: INFO: Created: latency-svc-xpk92
  Jun 24 12:31:18.962: INFO: Got endpoints: latency-svc-wwnfv [749.004341ms]
  Jun 24 12:31:18.977: INFO: Created: latency-svc-skhhd
  Jun 24 12:31:19.011: INFO: Got endpoints: latency-svc-clcq9 [743.341401ms]
  Jun 24 12:31:19.025: INFO: Created: latency-svc-g7c6j
  Jun 24 12:31:19.062: INFO: Got endpoints: latency-svc-64jxh [751.032762ms]
  Jun 24 12:31:19.076: INFO: Created: latency-svc-l8hbs
  Jun 24 12:31:19.113: INFO: Got endpoints: latency-svc-sxlp6 [750.148822ms]
  Jun 24 12:31:19.128: INFO: Created: latency-svc-hzfsp
  Jun 24 12:31:19.162: INFO: Got endpoints: latency-svc-dvmtf [750.114752ms]
  Jun 24 12:31:19.177: INFO: Created: latency-svc-llfk9
  Jun 24 12:31:19.213: INFO: Got endpoints: latency-svc-hnn7k [752.411096ms]
  Jun 24 12:31:19.228: INFO: Created: latency-svc-cp4b8
  Jun 24 12:31:19.264: INFO: Got endpoints: latency-svc-9wjjv [751.052012ms]
  Jun 24 12:31:19.278: INFO: Created: latency-svc-h8ql7
  Jun 24 12:31:19.313: INFO: Got endpoints: latency-svc-wxc22 [751.577857ms]
  Jun 24 12:31:19.324: INFO: Created: latency-svc-s72z4
  Jun 24 12:31:19.363: INFO: Got endpoints: latency-svc-6t2fk [751.482037ms]
  Jun 24 12:31:19.377: INFO: Created: latency-svc-kqtfz
  Jun 24 12:31:19.415: INFO: Got endpoints: latency-svc-dc9q4 [749.601241ms]
  Jun 24 12:31:19.429: INFO: Created: latency-svc-4kb9q
  Jun 24 12:31:19.462: INFO: Got endpoints: latency-svc-ftqwh [746.690845ms]
  Jun 24 12:31:19.477: INFO: Created: latency-svc-rkzzl
  Jun 24 12:31:19.511: INFO: Got endpoints: latency-svc-6kvbp [746.190254ms]
  Jun 24 12:31:19.523: INFO: Created: latency-svc-lnc4f
  Jun 24 12:31:19.562: INFO: Got endpoints: latency-svc-5fgdm [749.142918ms]
  Jun 24 12:31:19.579: INFO: Created: latency-svc-mqnjs
  Jun 24 12:31:19.612: INFO: Got endpoints: latency-svc-cd9vq [749.322063ms]
  Jun 24 12:31:19.624: INFO: Created: latency-svc-d9sk7
  Jun 24 12:31:19.664: INFO: Got endpoints: latency-svc-xpk92 [752.467667ms]
  Jun 24 12:31:19.676: INFO: Created: latency-svc-b6pkq
  Jun 24 12:31:19.714: INFO: Got endpoints: latency-svc-skhhd [751.812753ms]
  Jun 24 12:31:19.729: INFO: Created: latency-svc-8rnjs
  Jun 24 12:31:19.761: INFO: Got endpoints: latency-svc-g7c6j [749.378093ms]
  Jun 24 12:31:19.776: INFO: Created: latency-svc-hhdz2
  Jun 24 12:31:19.811: INFO: Got endpoints: latency-svc-l8hbs [749.017291ms]
  Jun 24 12:31:19.824: INFO: Created: latency-svc-tnt6r
  Jun 24 12:31:19.867: INFO: Got endpoints: latency-svc-hzfsp [754.204496ms]
  Jun 24 12:31:19.879: INFO: Created: latency-svc-94b2b
  Jun 24 12:31:19.914: INFO: Got endpoints: latency-svc-llfk9 [750.982648ms]
  Jun 24 12:31:19.929: INFO: Created: latency-svc-nnrrf
  Jun 24 12:31:19.961: INFO: Got endpoints: latency-svc-cp4b8 [746.870399ms]
  Jun 24 12:31:19.973: INFO: Created: latency-svc-5l2rs
  Jun 24 12:31:20.012: INFO: Got endpoints: latency-svc-h8ql7 [748.129465ms]
  Jun 24 12:31:20.025: INFO: Created: latency-svc-lhhkc
  Jun 24 12:31:20.066: INFO: Got endpoints: latency-svc-s72z4 [753.097117ms]
  Jun 24 12:31:20.078: INFO: Created: latency-svc-xlbnr
  Jun 24 12:31:20.114: INFO: Got endpoints: latency-svc-kqtfz [751.308282ms]
  Jun 24 12:31:20.125: INFO: Created: latency-svc-px2sr
  Jun 24 12:31:20.164: INFO: Got endpoints: latency-svc-4kb9q [749.30021ms]
  Jun 24 12:31:20.178: INFO: Created: latency-svc-zlms8
  Jun 24 12:31:20.211: INFO: Got endpoints: latency-svc-rkzzl [748.681034ms]
  Jun 24 12:31:20.226: INFO: Created: latency-svc-llmcx
  Jun 24 12:31:20.261: INFO: Got endpoints: latency-svc-lnc4f [749.891766ms]
  Jun 24 12:31:20.274: INFO: Created: latency-svc-xffkf
  Jun 24 12:31:20.312: INFO: Got endpoints: latency-svc-mqnjs [749.941146ms]
  Jun 24 12:31:20.327: INFO: Created: latency-svc-nndjl
  Jun 24 12:31:20.363: INFO: Got endpoints: latency-svc-d9sk7 [750.996437ms]
  Jun 24 12:31:20.376: INFO: Created: latency-svc-n5zgq
  Jun 24 12:31:20.413: INFO: Got endpoints: latency-svc-b6pkq [749.386231ms]
  Jun 24 12:31:20.432: INFO: Created: latency-svc-lt4xv
  Jun 24 12:31:20.461: INFO: Got endpoints: latency-svc-8rnjs [746.648133ms]
  Jun 24 12:31:20.476: INFO: Created: latency-svc-88g4d
  Jun 24 12:31:20.515: INFO: Got endpoints: latency-svc-hhdz2 [753.591332ms]
  Jun 24 12:31:20.527: INFO: Created: latency-svc-rxr5m
  Jun 24 12:31:20.561: INFO: Got endpoints: latency-svc-tnt6r [750.028897ms]
  Jun 24 12:31:20.573: INFO: Created: latency-svc-4xvfm
  Jun 24 12:31:20.614: INFO: Got endpoints: latency-svc-94b2b [746.784875ms]
  Jun 24 12:31:20.628: INFO: Created: latency-svc-t48wg
  Jun 24 12:31:20.663: INFO: Got endpoints: latency-svc-nnrrf [748.906176ms]
  Jun 24 12:31:20.677: INFO: Created: latency-svc-dk2sq
  Jun 24 12:31:20.711: INFO: Got endpoints: latency-svc-5l2rs [750.184049ms]
  Jun 24 12:31:20.726: INFO: Created: latency-svc-nmm7b
  Jun 24 12:31:20.764: INFO: Got endpoints: latency-svc-lhhkc [751.733054ms]
  Jun 24 12:31:20.777: INFO: Created: latency-svc-hss6m
  Jun 24 12:31:20.812: INFO: Got endpoints: latency-svc-xlbnr [745.435332ms]
  Jun 24 12:31:20.825: INFO: Created: latency-svc-slqxt
  Jun 24 12:31:20.862: INFO: Got endpoints: latency-svc-px2sr [747.390621ms]
  Jun 24 12:31:20.875: INFO: Created: latency-svc-kslk9
  Jun 24 12:31:20.913: INFO: Got endpoints: latency-svc-zlms8 [748.911287ms]
  Jun 24 12:31:20.926: INFO: Created: latency-svc-822mx
  Jun 24 12:31:20.960: INFO: Got endpoints: latency-svc-llmcx [749.023497ms]
  Jun 24 12:31:20.973: INFO: Created: latency-svc-scdt7
  Jun 24 12:31:21.011: INFO: Got endpoints: latency-svc-xffkf [750.625354ms]
  Jun 24 12:31:21.024: INFO: Created: latency-svc-zlrvv
  Jun 24 12:31:21.061: INFO: Got endpoints: latency-svc-nndjl [748.560803ms]
  Jun 24 12:31:21.075: INFO: Created: latency-svc-cnzzz
  Jun 24 12:31:21.115: INFO: Got endpoints: latency-svc-n5zgq [751.200179ms]
  Jun 24 12:31:21.129: INFO: Created: latency-svc-w4c7w
  Jun 24 12:31:21.164: INFO: Got endpoints: latency-svc-lt4xv [750.621353ms]
  Jun 24 12:31:21.175: INFO: Created: latency-svc-8jt94
  Jun 24 12:31:21.214: INFO: Got endpoints: latency-svc-88g4d [752.430291ms]
  Jun 24 12:31:21.230: INFO: Created: latency-svc-mdgrp
  Jun 24 12:31:21.264: INFO: Got endpoints: latency-svc-rxr5m [749.29818ms]
  Jun 24 12:31:21.281: INFO: Created: latency-svc-b2tnq
  Jun 24 12:31:21.312: INFO: Got endpoints: latency-svc-4xvfm [750.981897ms]
  Jun 24 12:31:21.325: INFO: Created: latency-svc-vn6qf
  Jun 24 12:31:21.363: INFO: Got endpoints: latency-svc-t48wg [748.960057ms]
  Jun 24 12:31:21.375: INFO: Created: latency-svc-qpl8b
  Jun 24 12:31:21.411: INFO: Got endpoints: latency-svc-dk2sq [747.541142ms]
  Jun 24 12:31:21.425: INFO: Created: latency-svc-59j9z
  Jun 24 12:31:21.462: INFO: Got endpoints: latency-svc-nmm7b [750.680724ms]
  Jun 24 12:31:21.477: INFO: Created: latency-svc-ljgxd
  Jun 24 12:31:21.512: INFO: Got endpoints: latency-svc-hss6m [748.105798ms]
  Jun 24 12:31:21.527: INFO: Created: latency-svc-dgvkf
  Jun 24 12:31:21.566: INFO: Got endpoints: latency-svc-slqxt [753.861946ms]
  Jun 24 12:31:21.578: INFO: Created: latency-svc-s4s4c
  Jun 24 12:31:21.610: INFO: Got endpoints: latency-svc-kslk9 [747.721705ms]
  Jun 24 12:31:21.622: INFO: Created: latency-svc-9ntd8
  Jun 24 12:31:21.662: INFO: Got endpoints: latency-svc-822mx [749.27054ms]
  Jun 24 12:31:21.676: INFO: Created: latency-svc-5sblv
  Jun 24 12:31:21.711: INFO: Got endpoints: latency-svc-scdt7 [750.758845ms]
  Jun 24 12:31:21.727: INFO: Created: latency-svc-hn4rv
  Jun 24 12:31:21.760: INFO: Got endpoints: latency-svc-zlrvv [748.876105ms]
  Jun 24 12:31:21.772: INFO: Created: latency-svc-qn225
  Jun 24 12:31:21.814: INFO: Got endpoints: latency-svc-cnzzz [752.234779ms]
  Jun 24 12:31:21.825: INFO: Created: latency-svc-br2nl
  Jun 24 12:31:21.861: INFO: Got endpoints: latency-svc-w4c7w [746.490452ms]
  Jun 24 12:31:21.875: INFO: Created: latency-svc-spgfk
  Jun 24 12:31:21.911: INFO: Got endpoints: latency-svc-8jt94 [746.3042ms]
  Jun 24 12:31:21.926: INFO: Created: latency-svc-489s7
  Jun 24 12:31:21.964: INFO: Got endpoints: latency-svc-mdgrp [749.584203ms]
  Jun 24 12:31:21.976: INFO: Created: latency-svc-26lng
  Jun 24 12:31:22.011: INFO: Got endpoints: latency-svc-b2tnq [746.739275ms]
  Jun 24 12:31:22.026: INFO: Created: latency-svc-mjtms
  Jun 24 12:31:22.061: INFO: Got endpoints: latency-svc-vn6qf [749.25967ms]
  Jun 24 12:31:22.112: INFO: Got endpoints: latency-svc-qpl8b [749.032097ms]
  Jun 24 12:31:22.164: INFO: Got endpoints: latency-svc-59j9z [753.158728ms]
  Jun 24 12:31:22.213: INFO: Got endpoints: latency-svc-ljgxd [750.386901ms]
  Jun 24 12:31:22.270: INFO: Got endpoints: latency-svc-dgvkf [756.996407ms]
  Jun 24 12:31:22.312: INFO: Got endpoints: latency-svc-s4s4c [745.945077ms]
  Jun 24 12:31:22.361: INFO: Got endpoints: latency-svc-9ntd8 [750.731654ms]
  Jun 24 12:31:22.413: INFO: Got endpoints: latency-svc-5sblv [749.894206ms]
  Jun 24 12:31:22.461: INFO: Got endpoints: latency-svc-hn4rv [749.636273ms]
  Jun 24 12:31:22.512: INFO: Got endpoints: latency-svc-qn225 [751.510792ms]
  Jun 24 12:31:22.563: INFO: Got endpoints: latency-svc-br2nl [749.706244ms]
  Jun 24 12:31:22.612: INFO: Got endpoints: latency-svc-spgfk [750.905296ms]
  Jun 24 12:31:22.661: INFO: Got endpoints: latency-svc-489s7 [750.109278ms]
  Jun 24 12:31:22.715: INFO: Got endpoints: latency-svc-26lng [750.548043ms]
  Jun 24 12:31:22.764: INFO: Got endpoints: latency-svc-mjtms [753.248649ms]
  Jun 24 12:31:22.765: INFO: Latencies: [39.569994ms 63.076689ms 63.505823ms 83.28637ms 89.320244ms 102.070537ms 103.667084ms 104.586633ms 116.329056ms 116.571639ms 124.117858ms 128.324062ms 135.102632ms 144.42812ms 146.802405ms 147.820985ms 152.714237ms 153.719557ms 161.035754ms 164.993315ms 167.506471ms 171.075379ms 175.907489ms 177.134292ms 182.589739ms 184.502739ms 185.440618ms 186.638031ms 219.881769ms 220.366834ms 222.672738ms 229.011634ms 235.418371ms 240.482324ms 244.014301ms 281.035318ms 286.447785ms 295.789472ms 297.420519ms 304.737606ms 309.89874ms 313.141364ms 330.408954ms 334.997042ms 345.154238ms 378.037473ms 380.287805ms 428.49031ms 523.653514ms 544.428072ms 619.82585ms 634.797687ms 671.487851ms 737.45882ms 742.031628ms 743.341401ms 745.10785ms 745.173891ms 745.407943ms 745.435332ms 745.786287ms 745.945077ms 746.13566ms 746.158331ms 746.190091ms 746.190254ms 746.3042ms 746.490452ms 746.648133ms 746.690845ms 746.733947ms 746.739275ms 746.784875ms 746.790627ms 746.870399ms 747.390621ms 747.482104ms 747.541142ms 747.584096ms 747.627356ms 747.632556ms 747.635626ms 747.721705ms 747.909809ms 747.937459ms 748.105798ms 748.129465ms 748.270892ms 748.338233ms 748.341583ms 748.488275ms 748.560803ms 748.681034ms 748.731567ms 748.876105ms 748.906176ms 748.911287ms 748.960057ms 748.972161ms 748.97598ms 748.979539ms 749.004341ms 749.017291ms 749.023497ms 749.032097ms 749.04195ms 749.049141ms 749.091122ms 749.115721ms 749.142918ms 749.149972ms 749.175943ms 749.25967ms 749.27054ms 749.292073ms 749.29818ms 749.30021ms 749.322063ms 749.378093ms 749.386231ms 749.580076ms 749.581647ms 749.584203ms 749.601241ms 749.636273ms 749.706244ms 749.802018ms 749.838099ms 749.891766ms 749.894206ms 749.927509ms 749.941146ms 750.028897ms 750.109278ms 750.114752ms 750.148822ms 750.184049ms 750.203663ms 750.282774ms 750.298124ms 750.356844ms 750.384585ms 750.386901ms 750.548043ms 750.621353ms 750.625354ms 750.680724ms 750.731654ms 750.758845ms 750.760249ms 750.82193ms 750.87592ms 750.905296ms 750.981897ms 750.982648ms 750.996437ms 751.018082ms 751.032762ms 751.052012ms 751.200179ms 751.206663ms 751.214224ms 751.227414ms 751.308282ms 751.331435ms 751.377286ms 751.415216ms 751.482037ms 751.510792ms 751.542507ms 751.555127ms 751.577857ms 751.580488ms 751.607427ms 751.681208ms 751.733054ms 751.799389ms 751.812753ms 751.966691ms 751.973471ms 752.034282ms 752.047161ms 752.234779ms 752.411096ms 752.430291ms 752.467667ms 752.501467ms 752.518557ms 752.997782ms 753.097117ms 753.140443ms 753.158728ms 753.248649ms 753.591332ms 753.861946ms 754.204496ms 754.387526ms 754.972243ms 756.996407ms 794.667288ms]
  Jun 24 12:31:22.765: INFO: 50 %ile: 748.979539ms
  Jun 24 12:31:22.766: INFO: 90 %ile: 752.034282ms
  Jun 24 12:31:22.766: INFO: 99 %ile: 756.996407ms
  Jun 24 12:31:22.766: INFO: Total sample count: 200
  Jun 24 12:31:22.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7475" for this suite. @ 06/24/23 12:31:22.773
• [9.794 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 06/24/23 12:31:22.784
  Jun 24 12:31:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sysctl @ 06/24/23 12:31:22.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:22.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:22.81
  STEP: Creating a pod with one valid and two invalid sysctls @ 06/24/23 12:31:22.82
  Jun 24 12:31:22.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2995" for this suite. @ 06/24/23 12:31:22.831
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 06/24/23 12:31:22.841
  Jun 24 12:31:22.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:31:22.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:22.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:22.865
  STEP: Creating configMap with name projected-configmap-test-volume-map-d253d1f7-0fa2-41a8-a485-071b9e39f82d @ 06/24/23 12:31:22.87
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:31:22.876
  STEP: Saw pod success @ 06/24/23 12:31:26.903
  Jun 24 12:31:26.908: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-05fb2c97-b539-4d81-b910-86b79654bbd5 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 12:31:26.93
  Jun 24 12:31:26.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1687" for this suite. @ 06/24/23 12:31:26.953
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 06/24/23 12:31:26.964
  Jun 24 12:31:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:31:26.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:26.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:26.993
  STEP: Setting up server cert @ 06/24/23 12:31:27.023
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:31:27.586
  STEP: Deploying the webhook pod @ 06/24/23 12:31:27.594
  STEP: Wait for the deployment to be ready @ 06/24/23 12:31:27.608
  Jun 24 12:31:27.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/24/23 12:31:29.638
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:31:29.651
  Jun 24 12:31:30.651: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 24 12:31:31.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 06/24/23 12:31:31.656
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 06/24/23 12:31:31.685
  STEP: Creating a configMap that should not be mutated @ 06/24/23 12:31:31.697
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 06/24/23 12:31:31.713
  STEP: Creating a configMap that should be mutated @ 06/24/23 12:31:31.724
  Jun 24 12:31:31.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6717" for this suite. @ 06/24/23 12:31:31.842
  STEP: Destroying namespace "webhook-markers-7646" for this suite. @ 06/24/23 12:31:31.854
• [4.902 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 06/24/23 12:31:31.867
  Jun 24 12:31:31.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:31:31.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:31.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:31.896
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/24/23 12:31:31.901
  STEP: Saw pod success @ 06/24/23 12:31:35.932
  Jun 24 12:31:35.936: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-61bc4543-6897-4792-b150-c57827ce9569 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:31:35.944
  Jun 24 12:31:35.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6467" for this suite. @ 06/24/23 12:31:35.969
• [4.111 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 06/24/23 12:31:35.978
  Jun 24 12:31:35.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 12:31:35.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:35.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:36.001
  STEP: create the deployment @ 06/24/23 12:31:36.007
  W0624 12:31:36.015706      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/24/23 12:31:36.015
  STEP: delete the deployment @ 06/24/23 12:31:36.532
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 06/24/23 12:31:36.54
  STEP: Gathering metrics @ 06/24/23 12:31:37.078
  W0624 12:31:37.084657      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 12:31:37.084: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 12:31:37.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3282" for this suite. @ 06/24/23 12:31:37.091
• [1.125 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 06/24/23 12:31:37.104
  Jun 24 12:31:37.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 12:31:37.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:37.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:37.133
  STEP: Create a Replicaset @ 06/24/23 12:31:37.147
  STEP: Verify that the required pods have come up. @ 06/24/23 12:31:37.156
  Jun 24 12:31:37.160: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun 24 12:31:42.166: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 12:31:42.166
  STEP: Getting /status @ 06/24/23 12:31:42.166
  Jun 24 12:31:42.171: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 06/24/23 12:31:42.171
  Jun 24 12:31:42.183: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 06/24/23 12:31:42.183
  Jun 24 12:31:42.186: INFO: Observed &ReplicaSet event: ADDED
  Jun 24 12:31:42.186: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.186: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.186: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.186: INFO: Found replicaset test-rs in namespace replicaset-193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 24 12:31:42.186: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 06/24/23 12:31:42.186
  Jun 24 12:31:42.186: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 24 12:31:42.194: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 06/24/23 12:31:42.194
  Jun 24 12:31:42.197: INFO: Observed &ReplicaSet event: ADDED
  Jun 24 12:31:42.197: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.198: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.199: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.199: INFO: Observed replicaset test-rs in namespace replicaset-193 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 12:31:42.200: INFO: Observed &ReplicaSet event: MODIFIED
  Jun 24 12:31:42.200: INFO: Found replicaset test-rs in namespace replicaset-193 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jun 24 12:31:42.200: INFO: Replicaset test-rs has a patched status
  Jun 24 12:31:42.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-193" for this suite. @ 06/24/23 12:31:42.207
• [5.111 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 06/24/23 12:31:42.216
  Jun 24 12:31:42.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:31:42.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:31:42.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:31:42.244
  STEP: Creating pod liveness-18f92c4a-e6e5-4893-9fb7-84478556eb6a in namespace container-probe-1207 @ 06/24/23 12:31:42.251
  Jun 24 12:31:44.281: INFO: Started pod liveness-18f92c4a-e6e5-4893-9fb7-84478556eb6a in namespace container-probe-1207
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 12:31:44.282
  Jun 24 12:31:44.286: INFO: Initial restart count of pod liveness-18f92c4a-e6e5-4893-9fb7-84478556eb6a is 0
  Jun 24 12:32:04.343: INFO: Restart count of pod container-probe-1207/liveness-18f92c4a-e6e5-4893-9fb7-84478556eb6a is now 1 (20.056950103s elapsed)
  Jun 24 12:32:04.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:32:04.348
  STEP: Destroying namespace "container-probe-1207" for this suite. @ 06/24/23 12:32:04.365
• [22.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 06/24/23 12:32:04.38
  Jun 24 12:32:04.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 12:32:04.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:04.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:32:04.404
  STEP: Creating a job @ 06/24/23 12:32:04.408
  STEP: Ensuring job reaches completions @ 06/24/23 12:32:04.416
  Jun 24 12:32:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4676" for this suite. @ 06/24/23 12:32:14.425
• [10.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 06/24/23 12:32:14.437
  Jun 24 12:32:14.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:32:14.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:14.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:32:14.464
  STEP: Starting the proxy @ 06/24/23 12:32:14.469
  Jun 24 12:32:14.469: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-528 proxy --unix-socket=/tmp/kubectl-proxy-unix1044151054/test'
  STEP: retrieving proxy /api/ output @ 06/24/23 12:32:14.529
  Jun 24 12:32:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-528" for this suite. @ 06/24/23 12:32:14.535
• [0.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 06/24/23 12:32:14.546
  Jun 24 12:32:14.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:32:14.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:14.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:32:14.57
  STEP: Creating projection with secret that has name projected-secret-test-map-d3e93fb2-fbd1-46df-b61e-38b59ea75f8b @ 06/24/23 12:32:14.576
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:32:14.583
  STEP: Saw pod success @ 06/24/23 12:32:18.609
  Jun 24 12:32:18.613: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-ba321382-f4ed-405e-b694-996d2a90484d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:32:18.621
  Jun 24 12:32:18.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5587" for this suite. @ 06/24/23 12:32:18.645
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 06/24/23 12:32:18.657
  Jun 24 12:32:18.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:32:18.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:18.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:32:18.681
  STEP: Creating a test namespace @ 06/24/23 12:32:18.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:18.704
  STEP: Creating a pod in the namespace @ 06/24/23 12:32:18.71
  STEP: Waiting for the pod to have running status @ 06/24/23 12:32:18.722
  STEP: Deleting the namespace @ 06/24/23 12:32:20.732
  STEP: Waiting for the namespace to be removed. @ 06/24/23 12:32:20.741
  STEP: Recreating the namespace @ 06/24/23 12:32:31.746
  STEP: Verifying there are no pods in the namespace @ 06/24/23 12:32:31.765
  Jun 24 12:32:31.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4212" for this suite. @ 06/24/23 12:32:31.779
  STEP: Destroying namespace "nsdeletetest-3558" for this suite. @ 06/24/23 12:32:31.785
  Jun 24 12:32:31.790: INFO: Namespace nsdeletetest-3558 was already deleted
  STEP: Destroying namespace "nsdeletetest-378" for this suite. @ 06/24/23 12:32:31.791
• [13.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 06/24/23 12:32:31.801
  Jun 24 12:32:31.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 12:32:31.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:32:31.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:32:31.827
  STEP: create the rc @ 06/24/23 12:32:31.836
  W0624 12:32:31.843106      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/24/23 12:32:37.853
  STEP: wait for the rc to be deleted @ 06/24/23 12:32:37.861
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 06/24/23 12:32:42.871
  STEP: Gathering metrics @ 06/24/23 12:33:12.883
  W0624 12:33:12.889155      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 12:33:12.889: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 12:33:12.889: INFO: Deleting pod "simpletest.rc-2bwd5" in namespace "gc-2801"
  Jun 24 12:33:12.902: INFO: Deleting pod "simpletest.rc-2d9cs" in namespace "gc-2801"
  Jun 24 12:33:12.919: INFO: Deleting pod "simpletest.rc-2zrsp" in namespace "gc-2801"
  Jun 24 12:33:12.938: INFO: Deleting pod "simpletest.rc-4bz4t" in namespace "gc-2801"
  Jun 24 12:33:12.954: INFO: Deleting pod "simpletest.rc-4cw8k" in namespace "gc-2801"
  Jun 24 12:33:12.971: INFO: Deleting pod "simpletest.rc-4p64b" in namespace "gc-2801"
  Jun 24 12:33:12.989: INFO: Deleting pod "simpletest.rc-4t7qh" in namespace "gc-2801"
  Jun 24 12:33:13.009: INFO: Deleting pod "simpletest.rc-5cr7n" in namespace "gc-2801"
  Jun 24 12:33:13.027: INFO: Deleting pod "simpletest.rc-5l5qp" in namespace "gc-2801"
  Jun 24 12:33:13.046: INFO: Deleting pod "simpletest.rc-5q5kk" in namespace "gc-2801"
  Jun 24 12:33:13.063: INFO: Deleting pod "simpletest.rc-5qp5v" in namespace "gc-2801"
  Jun 24 12:33:13.079: INFO: Deleting pod "simpletest.rc-6cb5c" in namespace "gc-2801"
  Jun 24 12:33:13.098: INFO: Deleting pod "simpletest.rc-6ksr8" in namespace "gc-2801"
  Jun 24 12:33:13.116: INFO: Deleting pod "simpletest.rc-75jp4" in namespace "gc-2801"
  Jun 24 12:33:13.135: INFO: Deleting pod "simpletest.rc-7hk86" in namespace "gc-2801"
  Jun 24 12:33:13.153: INFO: Deleting pod "simpletest.rc-7jh7f" in namespace "gc-2801"
  Jun 24 12:33:13.175: INFO: Deleting pod "simpletest.rc-8cmc8" in namespace "gc-2801"
  Jun 24 12:33:13.192: INFO: Deleting pod "simpletest.rc-8hrrq" in namespace "gc-2801"
  Jun 24 12:33:13.210: INFO: Deleting pod "simpletest.rc-8n97d" in namespace "gc-2801"
  Jun 24 12:33:13.229: INFO: Deleting pod "simpletest.rc-8smrv" in namespace "gc-2801"
  Jun 24 12:33:13.245: INFO: Deleting pod "simpletest.rc-8td7p" in namespace "gc-2801"
  Jun 24 12:33:13.260: INFO: Deleting pod "simpletest.rc-9bx74" in namespace "gc-2801"
  Jun 24 12:33:13.280: INFO: Deleting pod "simpletest.rc-9jvmh" in namespace "gc-2801"
  Jun 24 12:33:13.295: INFO: Deleting pod "simpletest.rc-bccjn" in namespace "gc-2801"
  Jun 24 12:33:13.313: INFO: Deleting pod "simpletest.rc-bjpqq" in namespace "gc-2801"
  Jun 24 12:33:13.329: INFO: Deleting pod "simpletest.rc-cf86m" in namespace "gc-2801"
  Jun 24 12:33:13.347: INFO: Deleting pod "simpletest.rc-cmr2r" in namespace "gc-2801"
  Jun 24 12:33:13.365: INFO: Deleting pod "simpletest.rc-csn6n" in namespace "gc-2801"
  Jun 24 12:33:13.376: INFO: Deleting pod "simpletest.rc-dcs8z" in namespace "gc-2801"
  Jun 24 12:33:13.393: INFO: Deleting pod "simpletest.rc-djqg5" in namespace "gc-2801"
  Jun 24 12:33:13.413: INFO: Deleting pod "simpletest.rc-dpbhk" in namespace "gc-2801"
  Jun 24 12:33:13.427: INFO: Deleting pod "simpletest.rc-dqdfj" in namespace "gc-2801"
  Jun 24 12:33:13.441: INFO: Deleting pod "simpletest.rc-dzqzw" in namespace "gc-2801"
  Jun 24 12:33:13.462: INFO: Deleting pod "simpletest.rc-f7p79" in namespace "gc-2801"
  Jun 24 12:33:13.479: INFO: Deleting pod "simpletest.rc-g7g99" in namespace "gc-2801"
  Jun 24 12:33:13.495: INFO: Deleting pod "simpletest.rc-gbvtt" in namespace "gc-2801"
  Jun 24 12:33:13.512: INFO: Deleting pod "simpletest.rc-ggs6h" in namespace "gc-2801"
  Jun 24 12:33:13.527: INFO: Deleting pod "simpletest.rc-gvbp9" in namespace "gc-2801"
  Jun 24 12:33:13.540: INFO: Deleting pod "simpletest.rc-gzkb2" in namespace "gc-2801"
  Jun 24 12:33:13.561: INFO: Deleting pod "simpletest.rc-hbtfj" in namespace "gc-2801"
  Jun 24 12:33:13.580: INFO: Deleting pod "simpletest.rc-hpc7p" in namespace "gc-2801"
  Jun 24 12:33:13.596: INFO: Deleting pod "simpletest.rc-ht4lr" in namespace "gc-2801"
  Jun 24 12:33:13.609: INFO: Deleting pod "simpletest.rc-hzdg6" in namespace "gc-2801"
  Jun 24 12:33:13.624: INFO: Deleting pod "simpletest.rc-j2ng8" in namespace "gc-2801"
  Jun 24 12:33:13.639: INFO: Deleting pod "simpletest.rc-jn48k" in namespace "gc-2801"
  Jun 24 12:33:13.654: INFO: Deleting pod "simpletest.rc-knf95" in namespace "gc-2801"
  Jun 24 12:33:13.668: INFO: Deleting pod "simpletest.rc-kqvmt" in namespace "gc-2801"
  Jun 24 12:33:13.683: INFO: Deleting pod "simpletest.rc-krhbg" in namespace "gc-2801"
  Jun 24 12:33:13.699: INFO: Deleting pod "simpletest.rc-l727m" in namespace "gc-2801"
  Jun 24 12:33:13.714: INFO: Deleting pod "simpletest.rc-l8d8m" in namespace "gc-2801"
  Jun 24 12:33:13.731: INFO: Deleting pod "simpletest.rc-ljjdw" in namespace "gc-2801"
  Jun 24 12:33:13.745: INFO: Deleting pod "simpletest.rc-mchtl" in namespace "gc-2801"
  Jun 24 12:33:13.756: INFO: Deleting pod "simpletest.rc-mfgth" in namespace "gc-2801"
  Jun 24 12:33:13.774: INFO: Deleting pod "simpletest.rc-mt2tv" in namespace "gc-2801"
  Jun 24 12:33:13.790: INFO: Deleting pod "simpletest.rc-ngn82" in namespace "gc-2801"
  Jun 24 12:33:13.805: INFO: Deleting pod "simpletest.rc-nt2n2" in namespace "gc-2801"
  Jun 24 12:33:13.818: INFO: Deleting pod "simpletest.rc-p5kdd" in namespace "gc-2801"
  Jun 24 12:33:13.834: INFO: Deleting pod "simpletest.rc-pb52n" in namespace "gc-2801"
  Jun 24 12:33:13.853: INFO: Deleting pod "simpletest.rc-pf69x" in namespace "gc-2801"
  Jun 24 12:33:13.868: INFO: Deleting pod "simpletest.rc-pp8ww" in namespace "gc-2801"
  Jun 24 12:33:13.884: INFO: Deleting pod "simpletest.rc-ppbj5" in namespace "gc-2801"
  Jun 24 12:33:13.900: INFO: Deleting pod "simpletest.rc-ptq9v" in namespace "gc-2801"
  Jun 24 12:33:13.915: INFO: Deleting pod "simpletest.rc-q4pvd" in namespace "gc-2801"
  Jun 24 12:33:13.931: INFO: Deleting pod "simpletest.rc-q977p" in namespace "gc-2801"
  Jun 24 12:33:13.947: INFO: Deleting pod "simpletest.rc-qfmxz" in namespace "gc-2801"
  Jun 24 12:33:13.967: INFO: Deleting pod "simpletest.rc-qg4cq" in namespace "gc-2801"
  Jun 24 12:33:13.989: INFO: Deleting pod "simpletest.rc-qjjww" in namespace "gc-2801"
  Jun 24 12:33:14.003: INFO: Deleting pod "simpletest.rc-qt9wb" in namespace "gc-2801"
  Jun 24 12:33:14.023: INFO: Deleting pod "simpletest.rc-r2fpc" in namespace "gc-2801"
  Jun 24 12:33:14.038: INFO: Deleting pod "simpletest.rc-rgp8c" in namespace "gc-2801"
  Jun 24 12:33:14.054: INFO: Deleting pod "simpletest.rc-rl7v4" in namespace "gc-2801"
  Jun 24 12:33:14.072: INFO: Deleting pod "simpletest.rc-rrcbb" in namespace "gc-2801"
  Jun 24 12:33:14.090: INFO: Deleting pod "simpletest.rc-rsz6s" in namespace "gc-2801"
  Jun 24 12:33:14.149: INFO: Deleting pod "simpletest.rc-rwkpb" in namespace "gc-2801"
  Jun 24 12:33:14.189: INFO: Deleting pod "simpletest.rc-rx28p" in namespace "gc-2801"
  Jun 24 12:33:14.237: INFO: Deleting pod "simpletest.rc-s9gq9" in namespace "gc-2801"
  Jun 24 12:33:14.292: INFO: Deleting pod "simpletest.rc-s9r5g" in namespace "gc-2801"
  Jun 24 12:33:14.337: INFO: Deleting pod "simpletest.rc-scfzf" in namespace "gc-2801"
  Jun 24 12:33:14.398: INFO: Deleting pod "simpletest.rc-sr72d" in namespace "gc-2801"
  Jun 24 12:33:14.441: INFO: Deleting pod "simpletest.rc-svddc" in namespace "gc-2801"
  Jun 24 12:33:14.488: INFO: Deleting pod "simpletest.rc-sz8rp" in namespace "gc-2801"
  Jun 24 12:33:14.543: INFO: Deleting pod "simpletest.rc-t59k5" in namespace "gc-2801"
  Jun 24 12:33:14.584: INFO: Deleting pod "simpletest.rc-td9x9" in namespace "gc-2801"
  Jun 24 12:33:14.634: INFO: Deleting pod "simpletest.rc-tgcdd" in namespace "gc-2801"
  Jun 24 12:33:14.687: INFO: Deleting pod "simpletest.rc-twflc" in namespace "gc-2801"
  Jun 24 12:33:14.741: INFO: Deleting pod "simpletest.rc-v4xkf" in namespace "gc-2801"
  Jun 24 12:33:14.789: INFO: Deleting pod "simpletest.rc-v6jnt" in namespace "gc-2801"
  Jun 24 12:33:14.839: INFO: Deleting pod "simpletest.rc-v8ftp" in namespace "gc-2801"
  Jun 24 12:33:14.891: INFO: Deleting pod "simpletest.rc-vf6gf" in namespace "gc-2801"
  Jun 24 12:33:14.942: INFO: Deleting pod "simpletest.rc-vl4wj" in namespace "gc-2801"
  Jun 24 12:33:14.990: INFO: Deleting pod "simpletest.rc-vv6lj" in namespace "gc-2801"
  Jun 24 12:33:15.037: INFO: Deleting pod "simpletest.rc-w5nw9" in namespace "gc-2801"
  Jun 24 12:33:15.086: INFO: Deleting pod "simpletest.rc-w92cr" in namespace "gc-2801"
  Jun 24 12:33:15.135: INFO: Deleting pod "simpletest.rc-wkbpm" in namespace "gc-2801"
  Jun 24 12:33:15.191: INFO: Deleting pod "simpletest.rc-wx5jm" in namespace "gc-2801"
  Jun 24 12:33:15.237: INFO: Deleting pod "simpletest.rc-xrgj5" in namespace "gc-2801"
  Jun 24 12:33:15.290: INFO: Deleting pod "simpletest.rc-xrzbc" in namespace "gc-2801"
  Jun 24 12:33:15.346: INFO: Deleting pod "simpletest.rc-zpdd7" in namespace "gc-2801"
  Jun 24 12:33:15.390: INFO: Deleting pod "simpletest.rc-ztf4v" in namespace "gc-2801"
  Jun 24 12:33:15.444: INFO: Deleting pod "simpletest.rc-zvljc" in namespace "gc-2801"
  Jun 24 12:33:15.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2801" for this suite. @ 06/24/23 12:33:15.527
• [43.779 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 06/24/23 12:33:15.581
  Jun 24 12:33:15.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:33:15.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:15.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:15.607
  STEP: creating pod @ 06/24/23 12:33:15.613
  Jun 24 12:33:25.673: INFO: Pod pod-hostip-4e7ba0a9-1a7e-4dd9-87ec-3c6368fdae6a has hostIP: 172.31.19.205
  Jun 24 12:33:25.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3919" for this suite. @ 06/24/23 12:33:25.678
• [10.105 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 06/24/23 12:33:25.687
  Jun 24 12:33:25.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:33:25.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:25.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:25.714
  STEP: Creating a pod to test downward api env vars @ 06/24/23 12:33:25.718
  STEP: Saw pod success @ 06/24/23 12:33:29.769
  Jun 24 12:33:29.773: INFO: Trying to get logs from node ip-172-31-19-205 pod downward-api-14f35799-3ae2-43ff-b47c-e6c472ccca18 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:33:29.784
  Jun 24 12:33:29.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1346" for this suite. @ 06/24/23 12:33:29.809
• [4.134 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 06/24/23 12:33:29.821
  Jun 24 12:33:29.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 12:33:29.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:29.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:29.897
  STEP: create the rc @ 06/24/23 12:33:29.91
  W0624 12:33:29.917794      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/24/23 12:33:35.931
  STEP: wait for the rc to be deleted @ 06/24/23 12:33:35.941
  Jun 24 12:33:37.097: INFO: 80 pods remaining
  Jun 24 12:33:37.108: INFO: 80 pods has nil DeletionTimestamp
  Jun 24 12:33:37.108: INFO: 
  Jun 24 12:33:37.977: INFO: 72 pods remaining
  Jun 24 12:33:37.980: INFO: 71 pods has nil DeletionTimestamp
  Jun 24 12:33:37.980: INFO: 
  Jun 24 12:33:38.960: INFO: 60 pods remaining
  Jun 24 12:33:38.960: INFO: 60 pods has nil DeletionTimestamp
  Jun 24 12:33:38.960: INFO: 
  Jun 24 12:33:39.960: INFO: 40 pods remaining
  Jun 24 12:33:39.960: INFO: 40 pods has nil DeletionTimestamp
  Jun 24 12:33:39.960: INFO: 
  Jun 24 12:33:40.964: INFO: 32 pods remaining
  Jun 24 12:33:40.964: INFO: 32 pods has nil DeletionTimestamp
  Jun 24 12:33:40.965: INFO: 
  Jun 24 12:33:41.963: INFO: 20 pods remaining
  Jun 24 12:33:41.964: INFO: 20 pods has nil DeletionTimestamp
  Jun 24 12:33:41.964: INFO: 
  STEP: Gathering metrics @ 06/24/23 12:33:42.963
  W0624 12:33:42.969547      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 12:33:42.969: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 12:33:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8081" for this suite. @ 06/24/23 12:33:42.98
• [13.169 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 06/24/23 12:33:42.991
  Jun 24 12:33:42.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename runtimeclass @ 06/24/23 12:33:42.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:43.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:43.034
  STEP: getting /apis @ 06/24/23 12:33:43.041
  STEP: getting /apis/node.k8s.io @ 06/24/23 12:33:43.05
  STEP: getting /apis/node.k8s.io/v1 @ 06/24/23 12:33:43.052
  STEP: creating @ 06/24/23 12:33:43.055
  STEP: watching @ 06/24/23 12:33:43.095
  Jun 24 12:33:43.095: INFO: starting watch
  STEP: getting @ 06/24/23 12:33:43.107
  STEP: listing @ 06/24/23 12:33:43.121
  STEP: patching @ 06/24/23 12:33:43.127
  STEP: updating @ 06/24/23 12:33:43.138
  Jun 24 12:33:43.151: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 06/24/23 12:33:43.152
  STEP: deleting a collection @ 06/24/23 12:33:43.189
  Jun 24 12:33:43.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6295" for this suite. @ 06/24/23 12:33:43.24
• [0.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 06/24/23 12:33:43.261
  Jun 24 12:33:43.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:33:43.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:43.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:43.338
  STEP: creating the pod @ 06/24/23 12:33:43.345
  STEP: submitting the pod to kubernetes @ 06/24/23 12:33:43.345
  STEP: verifying the pod is in kubernetes @ 06/24/23 12:33:53.405
  STEP: updating the pod @ 06/24/23 12:33:53.414
  Jun 24 12:33:53.932: INFO: Successfully updated pod "pod-update-533679f4-deb0-418d-9cb7-8644bc295849"
  STEP: verifying the updated pod is in kubernetes @ 06/24/23 12:33:53.937
  Jun 24 12:33:53.942: INFO: Pod update OK
  Jun 24 12:33:53.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3477" for this suite. @ 06/24/23 12:33:53.949
• [10.696 seconds]
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 06/24/23 12:33:53.957
  Jun 24 12:33:53.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl-logs @ 06/24/23 12:33:53.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:33:53.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:33:53.989
  STEP: creating an pod @ 06/24/23 12:33:53.993
  Jun 24 12:33:53.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jun 24 12:33:54.234: INFO: stderr: ""
  Jun 24 12:33:54.234: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 06/24/23 12:33:54.234
  Jun 24 12:33:54.234: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jun 24 12:33:58.248: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 06/24/23 12:33:58.248
  Jun 24 12:33:58.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator'
  Jun 24 12:33:58.349: INFO: stderr: ""
  Jun 24 12:33:58.349: INFO: stdout: "I0624 12:33:56.408204       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/n58 293\nI0624 12:33:56.608324       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/b56 494\nI0624 12:33:56.808922       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/sglx 258\nI0624 12:33:57.009285       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/rcw 371\nI0624 12:33:57.208642       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/5nc 477\nI0624 12:33:57.409002       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/vr5 451\nI0624 12:33:57.608287       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/8f9q 501\nI0624 12:33:57.809004       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/tgk9 215\nI0624 12:33:58.008284       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/gc8 397\nI0624 12:33:58.208657       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/6shx 478\n"
  STEP: limiting log lines @ 06/24/23 12:33:58.349
  Jun 24 12:33:58.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator --tail=1'
  Jun 24 12:33:58.435: INFO: stderr: ""
  Jun 24 12:33:58.435: INFO: stdout: "I0624 12:33:58.408988       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/mlh6 262\n"
  Jun 24 12:33:58.435: INFO: got output "I0624 12:33:58.408988       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/mlh6 262\n"
  STEP: limiting log bytes @ 06/24/23 12:33:58.435
  Jun 24 12:33:58.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator --limit-bytes=1'
  Jun 24 12:33:58.528: INFO: stderr: ""
  Jun 24 12:33:58.528: INFO: stdout: "I"
  Jun 24 12:33:58.528: INFO: got output "I"
  STEP: exposing timestamps @ 06/24/23 12:33:58.528
  Jun 24 12:33:58.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator --tail=1 --timestamps'
  Jun 24 12:33:58.617: INFO: stderr: ""
  Jun 24 12:33:58.617: INFO: stdout: "2023-06-24T12:33:58.608394062Z I0624 12:33:58.608266       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/g4wd 470\n"
  Jun 24 12:33:58.617: INFO: got output "2023-06-24T12:33:58.608394062Z I0624 12:33:58.608266       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/g4wd 470\n"
  STEP: restricting to a time range @ 06/24/23 12:33:58.618
  Jun 24 12:34:01.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator --since=1s'
  Jun 24 12:34:01.212: INFO: stderr: ""
  Jun 24 12:34:01.212: INFO: stdout: "I0624 12:34:00.208978       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/fdjk 520\nI0624 12:34:00.408263       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/t5l 564\nI0624 12:34:00.608673       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/flmz 530\nI0624 12:34:00.808991       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/9vmz 575\nI0624 12:34:01.008256       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/sm6b 320\nI0624 12:34:01.208615       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/d84j 421\n"
  Jun 24 12:34:01.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 logs logs-generator logs-generator --since=24h'
  Jun 24 12:34:01.308: INFO: stderr: ""
  Jun 24 12:34:01.308: INFO: stdout: "I0624 12:33:56.408204       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/n58 293\nI0624 12:33:56.608324       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/b56 494\nI0624 12:33:56.808922       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/sglx 258\nI0624 12:33:57.009285       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/rcw 371\nI0624 12:33:57.208642       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/5nc 477\nI0624 12:33:57.409002       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/vr5 451\nI0624 12:33:57.608287       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/8f9q 501\nI0624 12:33:57.809004       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/tgk9 215\nI0624 12:33:58.008284       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/gc8 397\nI0624 12:33:58.208657       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/6shx 478\nI0624 12:33:58.408988       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/mlh6 262\nI0624 12:33:58.608266       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/g4wd 470\nI0624 12:33:58.808583       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/8lt 538\nI0624 12:33:59.008922       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/knn7 570\nI0624 12:33:59.209189       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/kqvb 206\nI0624 12:33:59.408548       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/l7m 264\nI0624 12:33:59.608904       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/zhmt 321\nI0624 12:33:59.809270       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/lsj 536\nI0624 12:34:00.008621       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/7vn 413\nI0624 12:34:00.208978       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/fdjk 520\nI0624 12:34:00.408263       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/t5l 564\nI0624 12:34:00.608673       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/flmz 530\nI0624 12:34:00.808991       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/9vmz 575\nI0624 12:34:01.008256       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/sm6b 320\nI0624 12:34:01.208615       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/d84j 421\n"
  Jun 24 12:34:01.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-logs-1001 delete pod logs-generator'
  Jun 24 12:34:02.288: INFO: stderr: ""
  Jun 24 12:34:02.288: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jun 24 12:34:02.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-1001" for this suite. @ 06/24/23 12:34:02.294
• [8.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 06/24/23 12:34:02.303
  Jun 24 12:34:02.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:34:02.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:02.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:02.33
  Jun 24 12:34:04.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:34:04.360: INFO: Deleting pod "var-expansion-de189403-f09f-4f7a-b979-1f4193614c49" in namespace "var-expansion-5420"
  Jun 24 12:34:04.372: INFO: Wait up to 5m0s for pod "var-expansion-de189403-f09f-4f7a-b979-1f4193614c49" to be fully deleted
  STEP: Destroying namespace "var-expansion-5420" for this suite. @ 06/24/23 12:34:06.383
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 06/24/23 12:34:06.394
  Jun 24 12:34:06.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:34:06.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:06.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:06.423
  STEP: creating the pod @ 06/24/23 12:34:06.428
  STEP: submitting the pod to kubernetes @ 06/24/23 12:34:06.428
  W0624 12:34:06.439370      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 06/24/23 12:34:08.455
  STEP: updating the pod @ 06/24/23 12:34:08.46
  Jun 24 12:34:08.974: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2168a1ad-0d2b-4201-b23b-be6e5a3c975e"
  Jun 24 12:34:12.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1433" for this suite. @ 06/24/23 12:34:12.994
• [6.608 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 06/24/23 12:34:13.004
  Jun 24 12:34:13.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 12:34:13.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:13.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:13.031
  STEP: creating a ReplicationController @ 06/24/23 12:34:13.043
  STEP: waiting for RC to be added @ 06/24/23 12:34:13.051
  STEP: waiting for available Replicas @ 06/24/23 12:34:13.051
  STEP: patching ReplicationController @ 06/24/23 12:34:14.099
  STEP: waiting for RC to be modified @ 06/24/23 12:34:14.11
  STEP: patching ReplicationController status @ 06/24/23 12:34:14.11
  STEP: waiting for RC to be modified @ 06/24/23 12:34:14.116
  STEP: waiting for available Replicas @ 06/24/23 12:34:14.117
  STEP: fetching ReplicationController status @ 06/24/23 12:34:14.125
  STEP: patching ReplicationController scale @ 06/24/23 12:34:14.13
  STEP: waiting for RC to be modified @ 06/24/23 12:34:14.137
  STEP: waiting for ReplicationController's scale to be the max amount @ 06/24/23 12:34:14.138
  STEP: fetching ReplicationController; ensuring that it's patched @ 06/24/23 12:34:15.732
  STEP: updating ReplicationController status @ 06/24/23 12:34:15.736
  STEP: waiting for RC to be modified @ 06/24/23 12:34:15.747
  STEP: listing all ReplicationControllers @ 06/24/23 12:34:15.748
  STEP: checking that ReplicationController has expected values @ 06/24/23 12:34:15.752
  STEP: deleting ReplicationControllers by collection @ 06/24/23 12:34:15.752
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 06/24/23 12:34:15.766
  Jun 24 12:34:15.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 12:34:15.853671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-1981" for this suite. @ 06/24/23 12:34:15.86
• [2.864 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 06/24/23 12:34:15.87
  Jun 24 12:34:15.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:34:15.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:15.898
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:15.905
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:34:15.91
  E0624 12:34:16.854305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:17.854689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:18.855146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:19.855261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:34:19.944
  Jun 24 12:34:19.948: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-82205161-3f26-4a77-a3f2-68a44ce93da6 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:34:19.955
  Jun 24 12:34:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9576" for this suite. @ 06/24/23 12:34:19.978
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 06/24/23 12:34:19.987
  Jun 24 12:34:19.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:34:19.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:20.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:20.012
  Jun 24 12:34:20.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:34:20.855354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/24/23 12:34:21.599
  Jun 24 12:34:21.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-2784 --namespace=crd-publish-openapi-2784 create -f -'
  E0624 12:34:21.856081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:34:22.603: INFO: stderr: ""
  Jun 24 12:34:22.603: INFO: stdout: "e2e-test-crd-publish-openapi-4206-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 24 12:34:22.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-2784 --namespace=crd-publish-openapi-2784 delete e2e-test-crd-publish-openapi-4206-crds test-cr'
  Jun 24 12:34:22.742: INFO: stderr: ""
  Jun 24 12:34:22.742: INFO: stdout: "e2e-test-crd-publish-openapi-4206-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jun 24 12:34:22.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-2784 --namespace=crd-publish-openapi-2784 apply -f -'
  E0624 12:34:22.857744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:34:23.589: INFO: stderr: ""
  Jun 24 12:34:23.589: INFO: stdout: "e2e-test-crd-publish-openapi-4206-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun 24 12:34:23.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-2784 --namespace=crd-publish-openapi-2784 delete e2e-test-crd-publish-openapi-4206-crds test-cr'
  Jun 24 12:34:23.714: INFO: stderr: ""
  Jun 24 12:34:23.714: INFO: stdout: "e2e-test-crd-publish-openapi-4206-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 06/24/23 12:34:23.714
  Jun 24 12:34:23.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-2784 explain e2e-test-crd-publish-openapi-4206-crds'
  E0624 12:34:23.857778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:34:24.250: INFO: stderr: ""
  Jun 24 12:34:24.250: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-4206-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0624 12:34:24.858240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:34:25.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2784" for this suite. @ 06/24/23 12:34:25.839
• [5.869 seconds]
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 06/24/23 12:34:25.857
  Jun 24 12:34:25.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:34:25.858993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:34:25.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:25.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:25.883
  STEP: creating secret secrets-5917/secret-test-1cff5b34-99f0-4689-b380-aac2fa99bd3e @ 06/24/23 12:34:25.887
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:34:25.893
  E0624 12:34:26.860007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:27.860844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:28.860950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:29.861147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:34:29.922
  Jun 24 12:34:29.928: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-547bce55-9662-4804-8575-c9f1d8aa554d container env-test: <nil>
  STEP: delete the pod @ 06/24/23 12:34:29.937
  Jun 24 12:34:29.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5917" for this suite. @ 06/24/23 12:34:29.967
• [4.121 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 06/24/23 12:34:29.98
  Jun 24 12:34:29.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 12:34:29.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:30.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:30.018
  STEP: Creating replication controller my-hostname-basic-6dfb2a5b-b555-4225-92cd-76c96e8eca4c @ 06/24/23 12:34:30.024
  Jun 24 12:34:30.039: INFO: Pod name my-hostname-basic-6dfb2a5b-b555-4225-92cd-76c96e8eca4c: Found 0 pods out of 1
  E0624 12:34:30.861106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:31.861354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:32.862196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:33.862342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:34.862518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:34:35.046: INFO: Pod name my-hostname-basic-6dfb2a5b-b555-4225-92cd-76c96e8eca4c: Found 1 pods out of 1
  Jun 24 12:34:35.046: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6dfb2a5b-b555-4225-92cd-76c96e8eca4c" are running
  Jun 24 12:34:35.051: INFO: Pod "my-hostname-basic-6dfb2a5b-b555-4225-92cd-76c96e8eca4c-5gvxl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 12:34:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 12:34:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 12:34:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 12:34:30 +0000 UTC Reason: Message:}])
  Jun 24 12:34:35.051: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/24/23 12:34:35.052
  Jun 24 12:34:35.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7222" for this suite. @ 06/24/23 12:34:35.071
• [5.099 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 06/24/23 12:34:35.08
  Jun 24 12:34:35.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 12:34:35.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:34:35.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:34:35.112
  Jun 24 12:34:35.139: INFO: created pod
  E0624 12:34:35.862637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:36.862671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:37.863137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:38.863523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:34:39.155
  E0624 12:34:39.863991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:40.864051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:41.864168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:42.864271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:43.864826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:44.864956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:45.865074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:46.865909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:47.866024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:48.866143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:49.866261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:50.866379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:51.866505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:52.866907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:53.867096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:54.867200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:55.867311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:56.867929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:57.868035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:58.868152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:34:59.868268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:00.868380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:01.868474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:02.868585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:03.868693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:04.869310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:05.869643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:06.870492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:07.870546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:08.870784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:35:09.155: INFO: polling logs
  Jun 24 12:35:09.165: INFO: Pod logs: 
  I0624 12:34:36.073671       1 log.go:198] OK: Got token
  I0624 12:34:36.073727       1 log.go:198] validating with in-cluster discovery
  I0624 12:34:36.074083       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0624 12:34:36.074124       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1897:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687610675, NotBefore:1687610075, IssuedAt:1687610075, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1897", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cd4ccf5-886d-4fe4-bf9f-2deef34f8619"}}}
  I0624 12:34:36.089372       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0624 12:34:36.098687       1 log.go:198] OK: Validated signature on JWT
  I0624 12:34:36.098878       1 log.go:198] OK: Got valid claims from token!
  I0624 12:34:36.098918       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1897:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687610675, NotBefore:1687610075, IssuedAt:1687610075, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1897", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"2cd4ccf5-886d-4fe4-bf9f-2deef34f8619"}}}

  Jun 24 12:35:09.165: INFO: completed pod
  Jun 24 12:35:09.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1897" for this suite. @ 06/24/23 12:35:09.177
• [34.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 06/24/23 12:35:09.188
  Jun 24 12:35:09.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/24/23 12:35:09.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:09.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:09.216
  STEP: create the container to handle the HTTPGet hook request. @ 06/24/23 12:35:09.225
  E0624 12:35:09.871053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:10.871082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/24/23 12:35:11.25
  E0624 12:35:11.872225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:12.872376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 06/24/23 12:35:13.274
  E0624 12:35:13.873302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:14.873911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 06/24/23 12:35:15.294
  Jun 24 12:35:15.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6818" for this suite. @ 06/24/23 12:35:15.323
• [6.143 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 06/24/23 12:35:15.331
  Jun 24 12:35:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:35:15.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:15.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:15.356
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:35:15.361
  E0624 12:35:15.874804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:16.875038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:17.875198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:18.875445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:35:19.388
  Jun 24 12:35:19.391: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-c193c98e-e190-4cd2-9ed9-eb880cc73dd1 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:35:19.398
  Jun 24 12:35:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2371" for this suite. @ 06/24/23 12:35:19.424
• [4.100 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 06/24/23 12:35:19.431
  Jun 24 12:35:19.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubelet-test @ 06/24/23 12:35:19.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:19.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:19.458
  E0624 12:35:19.876188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:20.876643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:35:21.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3958" for this suite. @ 06/24/23 12:35:21.496
• [2.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 06/24/23 12:35:21.505
  Jun 24 12:35:21.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context @ 06/24/23 12:35:21.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:21.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:21.531
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/24/23 12:35:21.534
  E0624 12:35:21.877023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:22.877302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:23.878255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:24.878480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:35:25.556
  Jun 24 12:35:25.559: INFO: Trying to get logs from node ip-172-31-19-205 pod security-context-f86db380-61d9-4a2e-b7bf-f2ab27cec110 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:35:25.568
  Jun 24 12:35:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3231" for this suite. @ 06/24/23 12:35:25.596
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 06/24/23 12:35:25.606
  Jun 24 12:35:25.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subpath @ 06/24/23 12:35:25.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:25.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:25.632
  STEP: Setting up data @ 06/24/23 12:35:25.636
  STEP: Creating pod pod-subpath-test-downwardapi-x57n @ 06/24/23 12:35:25.648
  STEP: Creating a pod to test atomic-volume-subpath @ 06/24/23 12:35:25.648
  E0624 12:35:25.879128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:26.879690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:27.880076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:28.880181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:29.880826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:30.881401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:31.881610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:32.881765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:33.882145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:34.882821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:35.883175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:36.883353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:37.883476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:38.883743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:39.884453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:40.885135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:41.885531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:42.885652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:43.886311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:44.886601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:45.886971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:46.887924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:47.888730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:48.888922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:35:49.724
  Jun 24 12:35:49.728: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-subpath-test-downwardapi-x57n container test-container-subpath-downwardapi-x57n: <nil>
  STEP: delete the pod @ 06/24/23 12:35:49.737
  STEP: Deleting pod pod-subpath-test-downwardapi-x57n @ 06/24/23 12:35:49.755
  Jun 24 12:35:49.755: INFO: Deleting pod "pod-subpath-test-downwardapi-x57n" in namespace "subpath-9753"
  Jun 24 12:35:49.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9753" for this suite. @ 06/24/23 12:35:49.765
• [24.166 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 06/24/23 12:35:49.774
  Jun 24 12:35:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:35:49.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:35:49.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:35:49.806
  E0624 12:35:49.889452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:50.890370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:51.891081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:52.891999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:53.892553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:54.893262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:55.893537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:56.894165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:57.895237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:58.896181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:35:59.896776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:00.897746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:01.898570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:02.899374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:03.899737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:04.899865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:05.900026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:06.900367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:07.900738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:08.900873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:09.900986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:10.901797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:11.901881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:12.901975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:13.902097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:14.902206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:15.902335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:16.902752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:17.902847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:18.902939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:19.903824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:20.904041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:21.905067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:22.905513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:23.905620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:24.906521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:25.906597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:26.906996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:27.907083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:28.907183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:29.907850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:30.908635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:31.909379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:32.910227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:33.910881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:34.910985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:35.911087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:36.912009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:37.912779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:38.913066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:39.913417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:40.914125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:41.914160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:42.915035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:43.915198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:44.915323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:45.915699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:46.915995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:47.916806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:48.916908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:36:49.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-8273" for this suite. @ 06/24/23 12:36:49.829
• [60.062 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 06/24/23 12:36:49.836
  Jun 24 12:36:49.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:36:49.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:36:49.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:36:49.868
  STEP: Counting existing ResourceQuota @ 06/24/23 12:36:49.872
  E0624 12:36:49.917152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:50.917999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:51.918271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:52.918587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:53.918952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 12:36:54.876
  STEP: Ensuring resource quota status is calculated @ 06/24/23 12:36:54.887
  E0624 12:36:54.919388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:55.919621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 06/24/23 12:36:56.892
  STEP: Ensuring resource quota status captures replication controller creation @ 06/24/23 12:36:56.905
  E0624 12:36:56.920024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:57.920161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 06/24/23 12:36:58.91
  STEP: Ensuring resource quota status released usage @ 06/24/23 12:36:58.918
  E0624 12:36:58.920483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:36:59.920584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:00.921129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:00.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5274" for this suite. @ 06/24/23 12:37:00.928
• [11.100 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 06/24/23 12:37:00.938
  Jun 24 12:37:00.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:37:00.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:00.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:00.963
  STEP: creating service in namespace services-5959 @ 06/24/23 12:37:00.968
  STEP: creating service affinity-clusterip-transition in namespace services-5959 @ 06/24/23 12:37:00.968
  STEP: creating replication controller affinity-clusterip-transition in namespace services-5959 @ 06/24/23 12:37:00.983
  I0624 12:37:00.993360      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-5959, replica count: 3
  E0624 12:37:01.922178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:02.923120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:03.923395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 12:37:04.045954      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:37:04.057: INFO: Creating new exec pod
  E0624 12:37:04.923591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:05.923749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:06.924410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:07.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-5959 exec execpod-affinityk8fmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jun 24 12:37:07.249: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jun 24 12:37:07.249: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:37:07.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-5959 exec execpod-affinityk8fmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.50 80'
  Jun 24 12:37:07.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.50 80\nConnection to 10.152.183.50 80 port [tcp/http] succeeded!\n"
  Jun 24 12:37:07.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:37:07.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-5959 exec execpod-affinityk8fmf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.50:80/ ; done'
  Jun 24 12:37:07.654: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n"
  Jun 24 12:37:07.654: INFO: stdout: "\naffinity-clusterip-transition-45kbp\naffinity-clusterip-transition-45kbp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-45kbp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-45kbp\naffinity-clusterip-transition-s2pxj\naffinity-clusterip-transition-45kbp"
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-45kbp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-45kbp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-45kbp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.654: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.655: INFO: Received response from host: affinity-clusterip-transition-45kbp
  Jun 24 12:37:07.655: INFO: Received response from host: affinity-clusterip-transition-s2pxj
  Jun 24 12:37:07.655: INFO: Received response from host: affinity-clusterip-transition-45kbp
  Jun 24 12:37:07.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-5959 exec execpod-affinityk8fmf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.50:80/ ; done'
  Jun 24 12:37:07.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.50:80/\n"
  Jun 24 12:37:07.919: INFO: stdout: "\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp\naffinity-clusterip-transition-fbsrp"
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.919: INFO: Received response from host: affinity-clusterip-transition-fbsrp
  Jun 24 12:37:07.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 12:37:07.924687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:07.925: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5959, will wait for the garbage collector to delete the pods @ 06/24/23 12:37:07.94
  Jun 24 12:37:08.002: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.309285ms
  Jun 24 12:37:08.103: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.403228ms
  E0624 12:37:08.925461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:09.925688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5959" for this suite. @ 06/24/23 12:37:10.225
• [9.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 06/24/23 12:37:10.238
  Jun 24 12:37:10.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:37:10.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:10.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:10.262
  STEP: Counting existing ResourceQuota @ 06/24/23 12:37:10.267
  E0624 12:37:10.926290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:11.926926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:12.927808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:13.928055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:14.928652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 12:37:15.271
  STEP: Ensuring resource quota status is calculated @ 06/24/23 12:37:15.279
  E0624 12:37:15.928786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:16.928936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 06/24/23 12:37:17.284
  STEP: Ensuring resource quota status captures replicaset creation @ 06/24/23 12:37:17.3
  E0624 12:37:17.929041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:18.930007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 06/24/23 12:37:19.304
  STEP: Ensuring resource quota status released usage @ 06/24/23 12:37:19.312
  E0624 12:37:19.930058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:20.930163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:21.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7133" for this suite. @ 06/24/23 12:37:21.322
• [11.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 06/24/23 12:37:21.335
  Jun 24 12:37:21.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:37:21.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:21.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:21.36
  Jun 24 12:37:21.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: creating the pod @ 06/24/23 12:37:21.365
  STEP: submitting the pod to kubernetes @ 06/24/23 12:37:21.365
  E0624 12:37:21.931219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:22.931363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5951" for this suite. @ 06/24/23 12:37:23.425
• [2.098 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 06/24/23 12:37:23.433
  Jun 24 12:37:23.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:37:23.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:23.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:23.458
  Jun 24 12:37:23.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 create -f -'
  Jun 24 12:37:23.813: INFO: stderr: ""
  Jun 24 12:37:23.813: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jun 24 12:37:23.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 create -f -'
  E0624 12:37:23.931646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:24.136: INFO: stderr: ""
  Jun 24 12:37:24.136: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/24/23 12:37:24.136
  E0624 12:37:24.932467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:25.141: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:37:25.141: INFO: Found 1 / 1
  Jun 24 12:37:25.141: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 24 12:37:25.145: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:37:25.145: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 24 12:37:25.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 describe pod agnhost-primary-cd2qg'
  Jun 24 12:37:25.246: INFO: stderr: ""
  Jun 24 12:37:25.246: INFO: stdout: "Name:             agnhost-primary-cd2qg\nNamespace:        kubectl-6168\nPriority:         0\nService Account:  default\nNode:             ip-172-31-19-205/172.31.19.205\nStart Time:       Sat, 24 Jun 2023 12:37:23 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.150.202\nIPs:\n  IP:           192.168.150.202\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://000e934bc54c1e48fc8c2fd779dafdbeda007503f2ff4bd0bdb03ff8737ca9d3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 24 Jun 2023 12:37:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2gxfh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2gxfh:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6168/agnhost-primary-cd2qg to ip-172-31-19-205\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jun 24 12:37:25.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 describe rc agnhost-primary'
  Jun 24 12:37:25.346: INFO: stderr: ""
  Jun 24 12:37:25.346: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6168\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-cd2qg\n"
  Jun 24 12:37:25.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 describe service agnhost-primary'
  Jun 24 12:37:25.444: INFO: stderr: ""
  Jun 24 12:37:25.444: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6168\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.62\nIPs:               10.152.183.62\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.150.202:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jun 24 12:37:25.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 describe node ip-172-31-15-136'
  Jun 24 12:37:25.582: INFO: stderr: ""
  Jun 24 12:37:25.582: INFO: stdout: "Name:               ip-172-31-15-136\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-15-136\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 24 Jun 2023 11:53:31 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-15-136\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 24 Jun 2023 12:37:24 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 24 Jun 2023 12:35:18 +0000   Sat, 24 Jun 2023 11:54:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 24 Jun 2023 12:35:18 +0000   Sat, 24 Jun 2023 11:54:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 24 Jun 2023 12:35:18 +0000   Sat, 24 Jun 2023 11:54:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 24 Jun 2023 12:35:18 +0000   Sat, 24 Jun 2023 11:55:40 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.15.136\n  Hostname:    ip-172-31-15-136\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      16069568Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16115248Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    2\n  ephemeral-storage:      14809713845\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16012848Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                      ec2d0e5608683f2d4263b4c1d48b38df\n  System UUID:                     ec2d0e56-0868-3f2d-4263-b4c1d48b38df\n  Boot ID:                         6d1ca507-c8a7-4076-a1d6-ebb171a3ae01\n  Kernel Version:                  5.19.0-1027-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.3\n  Kube-Proxy Version:              v1.27.3\nNon-terminated Pods:               (4 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-9295m           0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                      calico-kube-controllers-69b5565d84-5wz9f                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         35m\n  sonobuoy                         sonobuoy-e2e-job-a6e2d2ee4f8b45eb                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests  Limits\n  --------               --------  ------\n  cpu                    0 (0%)    0 (0%)\n  memory                 0 (0%)    0 (0%)\n  ephemeral-storage      0 (0%)    0 (0%)\n  hugepages-1Gi          0 (0%)    0 (0%)\n  hugepages-2Mi          0 (0%)    0 (0%)\n  scheduling.k8s.io/foo  0         0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 43m                kube-proxy       \n  Normal   Starting                 41m                kube-proxy       \n  Normal   Starting                 43m                kube-proxy       \n  Normal   Starting                 43m                kube-proxy       \n  Warning  InvalidDiskCapacity      43m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  43m                kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    43m (x2 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     43m (x2 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           43m                node-controller  Node ip-172-31-15-136 event: Registered Node ip-172-31-15-136 in Controller\n  Normal   Starting                 43m                kubelet          Starting kubelet.\n  Normal   NodeHasSufficientPID     43m (x3 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  43m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasNoDiskPressure    43m (x4 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 43m                kubelet          Starting kubelet.\n  Normal   NodeAllocatableEnforced  43m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  43m (x3 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     43m (x5 over 43m)  kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientPID\n  Normal   NodeHasSufficientMemory  42m                kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      42m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  42m                kubelet          Updated Node Allocatable limit across pods\n  Normal   Starting                 42m                kubelet          Starting kubelet.\n  Normal   NodeHasNoDiskPressure    42m                kubelet          Node ip-172-31-15-136 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     42m                kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientPID\n  Normal   NodeReady                42m                kubelet          Node ip-172-31-15-136 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  41m                kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientMemory\n  Normal   Starting                 41m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      41m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasNoDiskPressure    41m                kubelet          Node ip-172-31-15-136 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     41m                kubelet          Node ip-172-31-15-136 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             41m                kubelet          Node ip-172-31-15-136 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  41m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                41m                kubelet          Node ip-172-31-15-136 status is now: NodeReady\n  Normal   RegisteredNode           40m                node-controller  Node ip-172-31-15-136 event: Registered Node ip-172-31-15-136 in Controller\n"
  Jun 24 12:37:25.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-6168 describe namespace kubectl-6168'
  Jun 24 12:37:25.678: INFO: stderr: ""
  Jun 24 12:37:25.678: INFO: stdout: "Name:         kubectl-6168\nLabels:       e2e-framework=kubectl\n              e2e-run=f5149a8b-d2eb-4506-8cd6-d2dd259ec86e\n              kubernetes.io/metadata.name=kubectl-6168\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jun 24 12:37:25.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6168" for this suite. @ 06/24/23 12:37:25.682
• [2.257 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 06/24/23 12:37:25.69
  Jun 24 12:37:25.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:37:25.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:25.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:25.716
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 06/24/23 12:37:25.722
  Jun 24 12:37:25.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:37:25.933119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:26.933950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:27.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:37:27.934537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:28.935064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:29.935792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:30.936708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:31.937550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:32.937857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:33.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1823" for this suite. @ 06/24/23 12:37:33.545
• [7.865 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 06/24/23 12:37:33.559
  Jun 24 12:37:33.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:37:33.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:33.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:33.586
  STEP: Setting up server cert @ 06/24/23 12:37:33.621
  E0624 12:37:33.938703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:37:34.207
  STEP: Deploying the webhook pod @ 06/24/23 12:37:34.216
  STEP: Wait for the deployment to be ready @ 06/24/23 12:37:34.23
  Jun 24 12:37:34.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 12:37:34.939339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:35.939413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 12:37:36.253
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:37:36.264
  E0624 12:37:36.940058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:37.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 06/24/23 12:37:37.27
  STEP: create a configmap that should be updated by the webhook @ 06/24/23 12:37:37.291
  Jun 24 12:37:37.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1116" for this suite. @ 06/24/23 12:37:37.372
  STEP: Destroying namespace "webhook-markers-4479" for this suite. @ 06/24/23 12:37:37.38
• [3.830 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 06/24/23 12:37:37.389
  Jun 24 12:37:37.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 12:37:37.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:37.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:37.414
  E0624 12:37:37.940143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:38.940256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:39.940793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:40.941014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:41.941893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:42.942012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:43.942726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:44.942841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:45.942947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:46.943008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:47.943868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:48.944103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:49.944795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:50.944914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:51.945812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:52.946103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:53.946997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:54.947043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:55.948095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:56.949094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:57.949989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:37:58.950932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:37:59.490: INFO: Container started at 2023-06-24 12:37:38 +0000 UTC, pod became ready at 2023-06-24 12:37:57 +0000 UTC
  Jun 24 12:37:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5231" for this suite. @ 06/24/23 12:37:59.496
• [22.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 06/24/23 12:37:59.504
  Jun 24 12:37:59.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:37:59.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:37:59.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:37:59.536
  STEP: Creating secret with name secret-test-f5ba6a31-6758-4c76-ab91-4737698eada5 @ 06/24/23 12:37:59.541
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:37:59.548
  E0624 12:37:59.951838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:00.952077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:01.952337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:02.952507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:38:03.576
  Jun 24 12:38:03.580: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-d51296dd-3911-4a0a-87f6-16183a9f1655 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:38:03.589
  Jun 24 12:38:03.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7158" for this suite. @ 06/24/23 12:38:03.609
• [4.112 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 06/24/23 12:38:03.619
  Jun 24 12:38:03.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 12:38:03.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:38:03.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:38:03.646
  STEP: Creating a job @ 06/24/23 12:38:03.651
  STEP: Ensuring active pods == parallelism @ 06/24/23 12:38:03.66
  E0624 12:38:03.953118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:04.953176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:05.953889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:06.954017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 06/24/23 12:38:07.665
  E0624 12:38:07.954051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:38:08.183: INFO: Successfully updated pod "adopt-release-g7jx5"
  STEP: Checking that the Job readopts the Pod @ 06/24/23 12:38:08.183
  E0624 12:38:08.954164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:09.954271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 06/24/23 12:38:10.192
  Jun 24 12:38:10.706: INFO: Successfully updated pod "adopt-release-g7jx5"
  STEP: Checking that the Job releases the Pod @ 06/24/23 12:38:10.706
  E0624 12:38:10.955326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:11.955924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:38:12.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6610" for this suite. @ 06/24/23 12:38:12.721
• [9.113 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 06/24/23 12:38:12.732
  Jun 24 12:38:12.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:38:12.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:38:12.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:38:12.76
  STEP: Creating secret with name s-test-opt-del-9ac06d83-4e15-491a-b8e8-846c7f8e4096 @ 06/24/23 12:38:12.769
  STEP: Creating secret with name s-test-opt-upd-881c0c58-05c7-405e-ab15-14caae74bfee @ 06/24/23 12:38:12.774
  STEP: Creating the pod @ 06/24/23 12:38:12.78
  E0624 12:38:12.956711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:13.956957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-9ac06d83-4e15-491a-b8e8-846c7f8e4096 @ 06/24/23 12:38:14.845
  STEP: Updating secret s-test-opt-upd-881c0c58-05c7-405e-ab15-14caae74bfee @ 06/24/23 12:38:14.853
  STEP: Creating secret with name s-test-opt-create-0e31970b-09fd-411c-9d8c-77cf49717464 @ 06/24/23 12:38:14.86
  STEP: waiting to observe update in volume @ 06/24/23 12:38:14.866
  E0624 12:38:14.957799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:15.957803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:16.958011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:17.958239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:18.958428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:19.958532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:20.959417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:21.960262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:22.961201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:23.961423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:24.961668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:25.961785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:26.962348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:27.962436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:28.963523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:29.963682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:30.964417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:31.965264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:32.965866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:33.965972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:34.966183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:35.966297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:36.967202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:37.967322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:38.967385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:39.967537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:40.967699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:41.968600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:42.968729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:43.968813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:44.969188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:45.969274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:46.969591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:47.969660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:48.969772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:49.970136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:50.970252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:51.971245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:52.971363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:53.971947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:54.972776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:55.972890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:56.973021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:57.973332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:58.973708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:38:59.973804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:00.973931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:01.974379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:02.974616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:03.974723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:04.974829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:05.974918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:06.975643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:07.975732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:08.976047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:09.976161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:10.976252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:11.977235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:12.977569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:13.977660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:14.977764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:15.977848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:16.978448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:17.978534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:18.978647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:19.978868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:20.978989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:21.979474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:22.979721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:23.979970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:24.980138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:39:25.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8679" for this suite. @ 06/24/23 12:39:25.252
• [72.530 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 06/24/23 12:39:25.263
  Jun 24 12:39:25.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename watch @ 06/24/23 12:39:25.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:39:25.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:39:25.29
  STEP: creating a new configmap @ 06/24/23 12:39:25.293
  STEP: modifying the configmap once @ 06/24/23 12:39:25.299
  STEP: modifying the configmap a second time @ 06/24/23 12:39:25.318
  STEP: deleting the configmap @ 06/24/23 12:39:25.327
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 06/24/23 12:39:25.334
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 06/24/23 12:39:25.336
  Jun 24 12:39:25.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1654  49512a02-91a2-435c-9575-9be15bd55284 22352 0 2023-06-24 12:39:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-24 12:39:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:39:25.337: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1654  49512a02-91a2-435c-9575-9be15bd55284 22353 0 2023-06-24 12:39:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-24 12:39:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 12:39:25.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1654" for this suite. @ 06/24/23 12:39:25.342
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 06/24/23 12:39:25.353
  Jun 24 12:39:25.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 12:39:25.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:39:25.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:39:25.382
  STEP: Creating a test headless service @ 06/24/23 12:39:25.386
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_tcp@PTR;sleep 1; done
   @ 06/24/23 12:39:25.407
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8980.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8980.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8980.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8980.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_udp@PTR;check="$$(dig +tcp +noall +answer +search 21.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.21_tcp@PTR;sleep 1; done
   @ 06/24/23 12:39:25.407
  STEP: creating a pod to probe DNS @ 06/24/23 12:39:25.408
  STEP: submitting the pod to kubernetes @ 06/24/23 12:39:25.408
  E0624 12:39:25.980615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:26.981208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 12:39:27.432
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:39:27.436
  Jun 24 12:39:27.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.447: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.451: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.456: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.480: INFO: Unable to read jessie_udp@dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.485: INFO: Unable to read jessie_tcp@dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.489: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.494: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local from pod dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3: the server could not find the requested resource (get pods dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3)
  Jun 24 12:39:27.513: INFO: Lookups using dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3 failed for: [wheezy_udp@dns-test-service.dns-8980.svc.cluster.local wheezy_tcp@dns-test-service.dns-8980.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local jessie_udp@dns-test-service.dns-8980.svc.cluster.local jessie_tcp@dns-test-service.dns-8980.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8980.svc.cluster.local]

  E0624 12:39:27.981898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:28.981996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:29.982097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:30.982221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:31.982230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:39:32.581: INFO: DNS probes using dns-8980/dns-test-c1d594d0-403f-4c5e-972d-d18034c937c3 succeeded

  Jun 24 12:39:32.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:39:32.586
  STEP: deleting the test service @ 06/24/23 12:39:32.6
  STEP: deleting the test headless service @ 06/24/23 12:39:32.621
  STEP: Destroying namespace "dns-8980" for this suite. @ 06/24/23 12:39:32.635
• [7.290 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 06/24/23 12:39:32.651
  Jun 24 12:39:32.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 12:39:32.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:39:32.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:39:32.697
  Jun 24 12:39:32.724: INFO: created pod pod-service-account-defaultsa
  Jun 24 12:39:32.724: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jun 24 12:39:32.732: INFO: created pod pod-service-account-mountsa
  Jun 24 12:39:32.732: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jun 24 12:39:32.737: INFO: created pod pod-service-account-nomountsa
  Jun 24 12:39:32.737: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jun 24 12:39:32.746: INFO: created pod pod-service-account-defaultsa-mountspec
  Jun 24 12:39:32.746: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jun 24 12:39:32.754: INFO: created pod pod-service-account-mountsa-mountspec
  Jun 24 12:39:32.754: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jun 24 12:39:32.762: INFO: created pod pod-service-account-nomountsa-mountspec
  Jun 24 12:39:32.762: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jun 24 12:39:32.768: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jun 24 12:39:32.768: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jun 24 12:39:32.777: INFO: created pod pod-service-account-mountsa-nomountspec
  Jun 24 12:39:32.777: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jun 24 12:39:32.783: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jun 24 12:39:32.783: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jun 24 12:39:32.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2550" for this suite. @ 06/24/23 12:39:32.797
• [0.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 06/24/23 12:39:32.807
  Jun 24 12:39:32.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 12:39:32.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:39:32.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:39:32.849
  STEP: Creating service test in namespace statefulset-7011 @ 06/24/23 12:39:32.854
  STEP: Creating a new StatefulSet @ 06/24/23 12:39:32.863
  Jun 24 12:39:32.877: INFO: Found 0 stateful pods, waiting for 3
  E0624 12:39:32.982728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:33.983309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:34.983052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:35.983665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:36.984141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:37.984222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:38.984325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:39.984449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:40.984786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:41.985224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:39:42.884: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:39:42.884: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:39:42.884: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:39:42.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-7011 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0624 12:39:42.985875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:39:43.088: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:39:43.088: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:39:43.088: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0624 12:39:43.985899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:44.986089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:45.986113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:46.987084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:47.987327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:48.987417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:49.987532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:50.987703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:51.988617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:52.988725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/24/23 12:39:53.107
  Jun 24 12:39:53.128: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/24/23 12:39:53.128
  E0624 12:39:53.988856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:54.988978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:55.989082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:56.989540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:57.990542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:58.990865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:39:59.990958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:00.991125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:01.991969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:02.992764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 06/24/23 12:40:03.144
  Jun 24 12:40:03.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-7011 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 12:40:03.318: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 12:40:03.318: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:40:03.318: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0624 12:40:03.992921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:04.993057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:05.993142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:06.994140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:07.994262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:08.994314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:09.994435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:10.994630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:11.995074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:12.995174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:13.995761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:14.995862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:15.996106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:16.996180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:17.996364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:18.996697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:19.997271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:20.997457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:21.998488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:22.999166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 06/24/23 12:40:23.34
  Jun 24 12:40:23.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-7011 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:40:23.494: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:40:23.494: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:40:23.494: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0624 12:40:23.999778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:24.999986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:26.000084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:27.000540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:28.001605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:29.001682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:30.001780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:31.001879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:32.002318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:33.002487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:40:33.531: INFO: Updating stateful set ss2
  E0624 12:40:34.002624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:35.002722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:36.002838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:37.003105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:38.003315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:39.003393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:40.003496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:41.004223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:42.005043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:43.005128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 06/24/23 12:40:43.548
  Jun 24 12:40:43.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-7011 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 12:40:43.716: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 12:40:43.716: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:40:43.716: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0624 12:40:44.005160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:45.005301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:46.005404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:47.005912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:48.005990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:49.006154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:50.006266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:51.006433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:52.007224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:53.007349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:40:53.740: INFO: Deleting all statefulset in ns statefulset-7011
  Jun 24 12:40:53.743: INFO: Scaling statefulset ss2 to 0
  E0624 12:40:54.007772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:55.008789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:56.008885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:57.008994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:58.009110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:40:59.009212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:00.009356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:01.009456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:02.010345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:03.010436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:03.764: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:41:03.767: INFO: Deleting statefulset ss2
  Jun 24 12:41:03.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7011" for this suite. @ 06/24/23 12:41:03.786
• [90.989 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 06/24/23 12:41:03.798
  Jun 24 12:41:03.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:41:03.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:03.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:03.823
  E0624 12:41:04.010622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:05.011176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:05.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:41:05.858: INFO: Deleting pod "var-expansion-b39d27f4-4286-4173-a0b7-284359cf9e79" in namespace "var-expansion-1722"
  Jun 24 12:41:05.867: INFO: Wait up to 5m0s for pod "var-expansion-b39d27f4-4286-4173-a0b7-284359cf9e79" to be fully deleted
  E0624 12:41:06.012217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:07.013146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-1722" for this suite. @ 06/24/23 12:41:07.876
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 06/24/23 12:41:07.888
  Jun 24 12:41:07.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:41:07.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:07.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:07.91
  STEP: validating api versions @ 06/24/23 12:41:07.916
  Jun 24 12:41:07.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-3624 api-versions'
  Jun 24 12:41:07.994: INFO: stderr: ""
  Jun 24 12:41:07.994: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jun 24 12:41:07.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3624" for this suite. @ 06/24/23 12:41:07.999
• [0.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 06/24/23 12:41:08.007
  Jun 24 12:41:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:41:08.008
  E0624 12:41:08.013619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:08.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:08.029
  STEP: Creating the pod @ 06/24/23 12:41:08.034
  E0624 12:41:09.014464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:10.015024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:10.591: INFO: Successfully updated pod "labelsupdate3fb2365a-701f-4c5d-8488-a82976707faa"
  E0624 12:41:11.015134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:12.015229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:12.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-402" for this suite. @ 06/24/23 12:41:12.63
• [4.631 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 06/24/23 12:41:12.638
  Jun 24 12:41:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:41:12.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:12.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:12.662
  STEP: Creating secret with name secret-test-29512b00-e907-4dfd-a0f4-befbc10ece38 @ 06/24/23 12:41:12.67
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:41:12.676
  E0624 12:41:13.015416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:14.015491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:15.015586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:16.015875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:41:16.702
  Jun 24 12:41:16.706: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-81a265fb-7ee0-46db-8940-29c677b1bd6f container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:41:16.715
  Jun 24 12:41:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6855" for this suite. @ 06/24/23 12:41:16.733
• [4.103 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 06/24/23 12:41:16.741
  Jun 24 12:41:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:41:16.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:16.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:16.767
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/24/23 12:41:16.77
  E0624 12:41:17.016738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:18.016913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:19.017919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:20.018037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:41:20.792
  Jun 24 12:41:20.796: INFO: Trying to get logs from node ip-172-31-15-136 pod pod-7a608ef9-20be-4ba4-b3fc-78361845f34e container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:41:20.819
  Jun 24 12:41:20.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5428" for this suite. @ 06/24/23 12:41:20.837
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 06/24/23 12:41:20.849
  Jun 24 12:41:20.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 12:41:20.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:20.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:20.872
  Jun 24 12:41:20.880: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0624 12:41:21.018652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 06/24/23 12:41:21.893
  STEP: Checking rc "condition-test" has the desired failure condition set @ 06/24/23 12:41:21.899
  E0624 12:41:22.019426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 06/24/23 12:41:22.908
  Jun 24 12:41:22.919: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 06/24/23 12:41:22.92
  E0624 12:41:23.019426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:23.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4154" for this suite. @ 06/24/23 12:41:23.931
• [3.090 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 06/24/23 12:41:23.94
  Jun 24 12:41:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename proxy @ 06/24/23 12:41:23.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:23.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:23.966
  STEP: starting an echo server on multiple ports @ 06/24/23 12:41:23.979
  STEP: creating replication controller proxy-service-9gllh in namespace proxy-4931 @ 06/24/23 12:41:23.979
  I0624 12:41:23.990756      19 runners.go:194] Created replication controller with name: proxy-service-9gllh, namespace: proxy-4931, replica count: 1
  E0624 12:41:24.019657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:25.019756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 12:41:25.042046      19 runners.go:194] proxy-service-9gllh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0624 12:41:26.019812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 12:41:26.043042      19 runners.go:194] proxy-service-9gllh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:41:26.048: INFO: setup took 2.078050306s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 06/24/23 12:41:26.048
  Jun 24 12:41:26.058: INFO: (0) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.193626ms)
  Jun 24 12:41:26.059: INFO: (0) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 10.383079ms)
  Jun 24 12:41:26.059: INFO: (0) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.49788ms)
  Jun 24 12:41:26.061: INFO: (0) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 12.90992ms)
  Jun 24 12:41:26.061: INFO: (0) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.282011ms)
  Jun 24 12:41:26.061: INFO: (0) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.941689ms)
  Jun 24 12:41:26.068: INFO: (0) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 19.678833ms)
  Jun 24 12:41:26.068: INFO: (0) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 19.766274ms)
  Jun 24 12:41:26.068: INFO: (0) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 20.025477ms)
  Jun 24 12:41:26.070: INFO: (0) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 21.0502ms)
  Jun 24 12:41:26.070: INFO: (0) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 21.454665ms)
  Jun 24 12:41:26.070: INFO: (0) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 21.998692ms)
  Jun 24 12:41:26.070: INFO: (0) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 21.613816ms)
  Jun 24 12:41:26.070: INFO: (0) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 22.553658ms)
  Jun 24 12:41:26.073: INFO: (0) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 25.169291ms)
  Jun 24 12:41:26.074: INFO: (0) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 25.852279ms)
  Jun 24 12:41:26.085: INFO: (1) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 10.802654ms)
  Jun 24 12:41:26.086: INFO: (1) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 11.597064ms)
  Jun 24 12:41:26.086: INFO: (1) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 11.606174ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 12.348693ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.653567ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 12.620656ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 12.601896ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.712887ms)
  Jun 24 12:41:26.087: INFO: (1) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.036051ms)
  Jun 24 12:41:26.088: INFO: (1) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 13.658649ms)
  Jun 24 12:41:26.088: INFO: (1) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 13.630118ms)
  Jun 24 12:41:26.088: INFO: (1) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 13.79314ms)
  Jun 24 12:41:26.088: INFO: (1) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.183155ms)
  Jun 24 12:41:26.088: INFO: (1) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 13.785641ms)
  Jun 24 12:41:26.089: INFO: (1) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.246936ms)
  Jun 24 12:41:26.089: INFO: (1) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 14.458989ms)
  Jun 24 12:41:26.095: INFO: (2) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 6.045985ms)
  Jun 24 12:41:26.099: INFO: (2) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 8.962771ms)
  Jun 24 12:41:26.101: INFO: (2) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.727895ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 12.515575ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 12.448093ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 12.360832ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.703256ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 13.070361ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 12.833008ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 12.912439ms)
  Jun 24 12:41:26.102: INFO: (2) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.563355ms)
  Jun 24 12:41:26.103: INFO: (2) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.80279ms)
  Jun 24 12:41:26.104: INFO: (2) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.400758ms)
  Jun 24 12:41:26.104: INFO: (2) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 14.57752ms)
  Jun 24 12:41:26.104: INFO: (2) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.487689ms)
  Jun 24 12:41:26.104: INFO: (2) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 14.5662ms)
  Jun 24 12:41:26.112: INFO: (3) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 7.163368ms)
  Jun 24 12:41:26.113: INFO: (3) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 7.839667ms)
  Jun 24 12:41:26.113: INFO: (3) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 8.212812ms)
  Jun 24 12:41:26.114: INFO: (3) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 9.244434ms)
  Jun 24 12:41:26.114: INFO: (3) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 9.090742ms)
  Jun 24 12:41:26.115: INFO: (3) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 10.51076ms)
  Jun 24 12:41:26.115: INFO: (3) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.016953ms)
  Jun 24 12:41:26.116: INFO: (3) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 10.698262ms)
  Jun 24 12:41:26.116: INFO: (3) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 11.435491ms)
  Jun 24 12:41:26.117: INFO: (3) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 11.517152ms)
  Jun 24 12:41:26.117: INFO: (3) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 11.898246ms)
  Jun 24 12:41:26.118: INFO: (3) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.695757ms)
  Jun 24 12:41:26.118: INFO: (3) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 13.637968ms)
  Jun 24 12:41:26.119: INFO: (3) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.836941ms)
  Jun 24 12:41:26.119: INFO: (3) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 13.886662ms)
  Jun 24 12:41:26.120: INFO: (3) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.453878ms)
  Jun 24 12:41:26.126: INFO: (4) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 6.129216ms)
  Jun 24 12:41:26.129: INFO: (4) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 8.677097ms)
  Jun 24 12:41:26.130: INFO: (4) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 9.626089ms)
  Jun 24 12:41:26.130: INFO: (4) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 9.184043ms)
  Jun 24 12:41:26.131: INFO: (4) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 10.630321ms)
  Jun 24 12:41:26.131: INFO: (4) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 11.250179ms)
  Jun 24 12:41:26.132: INFO: (4) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 11.32745ms)
  Jun 24 12:41:26.132: INFO: (4) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.858886ms)
  Jun 24 12:41:26.133: INFO: (4) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 12.534845ms)
  Jun 24 12:41:26.133: INFO: (4) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.400153ms)
  Jun 24 12:41:26.133: INFO: (4) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 12.425843ms)
  Jun 24 12:41:26.135: INFO: (4) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 13.889652ms)
  Jun 24 12:41:26.136: INFO: (4) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 16.038518ms)
  Jun 24 12:41:26.137: INFO: (4) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 16.004828ms)
  Jun 24 12:41:26.137: INFO: (4) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 16.692346ms)
  Jun 24 12:41:26.137: INFO: (4) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 17.257592ms)
  Jun 24 12:41:26.147: INFO: (5) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 8.883089ms)
  Jun 24 12:41:26.148: INFO: (5) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 10.400048ms)
  Jun 24 12:41:26.148: INFO: (5) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.286747ms)
  Jun 24 12:41:26.149: INFO: (5) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 10.786433ms)
  Jun 24 12:41:26.149: INFO: (5) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 10.704162ms)
  Jun 24 12:41:26.151: INFO: (5) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 12.329272ms)
  Jun 24 12:41:26.151: INFO: (5) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 12.210771ms)
  Jun 24 12:41:26.151: INFO: (5) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 12.050439ms)
  Jun 24 12:41:26.152: INFO: (5) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 13.488236ms)
  Jun 24 12:41:26.152: INFO: (5) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 13.574388ms)
  Jun 24 12:41:26.153: INFO: (5) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 13.880791ms)
  Jun 24 12:41:26.153: INFO: (5) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.067053ms)
  Jun 24 12:41:26.153: INFO: (5) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.538299ms)
  Jun 24 12:41:26.155: INFO: (5) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 16.539424ms)
  Jun 24 12:41:26.160: INFO: (5) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 22.926283ms)
  Jun 24 12:41:26.161: INFO: (5) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 22.203794ms)
  Jun 24 12:41:26.168: INFO: (6) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 6.51225ms)
  Jun 24 12:41:26.169: INFO: (6) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 7.755506ms)
  Jun 24 12:41:26.169: INFO: (6) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 7.370921ms)
  Jun 24 12:41:26.171: INFO: (6) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 9.487267ms)
  Jun 24 12:41:26.171: INFO: (6) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 9.369566ms)
  Jun 24 12:41:26.171: INFO: (6) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 10.094405ms)
  Jun 24 12:41:26.172: INFO: (6) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 10.071124ms)
  Jun 24 12:41:26.172: INFO: (6) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.232367ms)
  Jun 24 12:41:26.173: INFO: (6) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 10.850284ms)
  Jun 24 12:41:26.172: INFO: (6) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.859714ms)
  Jun 24 12:41:26.173: INFO: (6) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 11.258069ms)
  Jun 24 12:41:26.173: INFO: (6) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.457961ms)
  Jun 24 12:41:26.174: INFO: (6) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 12.032488ms)
  Jun 24 12:41:26.174: INFO: (6) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 12.009828ms)
  Jun 24 12:41:26.176: INFO: (6) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 14.678941ms)
  Jun 24 12:41:26.176: INFO: (6) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.491158ms)
  Jun 24 12:41:26.184: INFO: (7) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 7.164079ms)
  Jun 24 12:41:26.185: INFO: (7) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 8.11411ms)
  Jun 24 12:41:26.185: INFO: (7) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 8.13326ms)
  Jun 24 12:41:26.186: INFO: (7) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 9.438196ms)
  Jun 24 12:41:26.188: INFO: (7) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 10.973106ms)
  Jun 24 12:41:26.188: INFO: (7) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 11.33127ms)
  Jun 24 12:41:26.189: INFO: (7) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.283552ms)
  Jun 24 12:41:26.190: INFO: (7) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 13.436606ms)
  Jun 24 12:41:26.190: INFO: (7) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 13.538587ms)
  Jun 24 12:41:26.190: INFO: (7) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 13.288124ms)
  Jun 24 12:41:26.191: INFO: (7) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 13.971233ms)
  Jun 24 12:41:26.191: INFO: (7) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 13.874931ms)
  Jun 24 12:41:26.191: INFO: (7) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 14.100374ms)
  Jun 24 12:41:26.191: INFO: (7) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.380328ms)
  Jun 24 12:41:26.191: INFO: (7) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.285896ms)
  Jun 24 12:41:26.194: INFO: (7) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 17.040631ms)
  Jun 24 12:41:26.207: INFO: (8) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.575535ms)
  Jun 24 12:41:26.207: INFO: (8) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 12.701977ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 13.098882ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 12.898419ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 12.881039ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 13.543637ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 13.366395ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 13.550987ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 13.696379ms)
  Jun 24 12:41:26.208: INFO: (8) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 13.77278ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 13.990032ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 13.993383ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 14.61495ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 14.367488ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.629641ms)
  Jun 24 12:41:26.209: INFO: (8) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 14.447479ms)
  Jun 24 12:41:26.218: INFO: (9) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 8.1365ms)
  Jun 24 12:41:26.219: INFO: (9) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 9.100762ms)
  Jun 24 12:41:26.220: INFO: (9) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.481719ms)
  Jun 24 12:41:26.220: INFO: (9) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.678091ms)
  Jun 24 12:41:26.221: INFO: (9) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.738312ms)
  Jun 24 12:41:26.221: INFO: (9) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 11.096067ms)
  Jun 24 12:41:26.222: INFO: (9) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 12.689206ms)
  Jun 24 12:41:26.224: INFO: (9) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 14.303036ms)
  Jun 24 12:41:26.224: INFO: (9) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 13.719599ms)
  Jun 24 12:41:26.224: INFO: (9) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.180905ms)
  Jun 24 12:41:26.224: INFO: (9) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.217056ms)
  Jun 24 12:41:26.225: INFO: (9) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 15.042725ms)
  Jun 24 12:41:26.225: INFO: (9) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 14.786243ms)
  Jun 24 12:41:26.225: INFO: (9) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 15.46135ms)
  Jun 24 12:41:26.225: INFO: (9) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 15.055556ms)
  Jun 24 12:41:26.225: INFO: (9) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 15.3997ms)
  Jun 24 12:41:26.233: INFO: (10) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 7.529893ms)
  Jun 24 12:41:26.233: INFO: (10) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 7.614004ms)
  Jun 24 12:41:26.239: INFO: (10) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 13.890111ms)
  Jun 24 12:41:26.239: INFO: (10) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 13.715979ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 13.981303ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 13.950402ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.245506ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.071854ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.293876ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 14.124744ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 14.381217ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 14.59617ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 14.437539ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.802703ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 14.274186ms)
  Jun 24 12:41:26.240: INFO: (10) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 14.162274ms)
  Jun 24 12:41:26.251: INFO: (11) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 10.061785ms)
  Jun 24 12:41:26.253: INFO: (11) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 12.041788ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 13.302584ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.352205ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 13.451956ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 13.877851ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.035153ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.219725ms)
  Jun 24 12:41:26.255: INFO: (11) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.333237ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 14.768332ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.679852ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.761422ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 14.879324ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 15.064106ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 15.232758ms)
  Jun 24 12:41:26.256: INFO: (11) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 15.36772ms)
  Jun 24 12:41:26.265: INFO: (12) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 7.29301ms)
  Jun 24 12:41:26.266: INFO: (12) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 8.12228ms)
  Jun 24 12:41:26.267: INFO: (12) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 9.592308ms)
  Jun 24 12:41:26.269: INFO: (12) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 11.780676ms)
  Jun 24 12:41:26.271: INFO: (12) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 13.419666ms)
  Jun 24 12:41:26.271: INFO: (12) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 13.225524ms)
  Jun 24 12:41:26.272: INFO: (12) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.245946ms)
  Jun 24 12:41:26.272: INFO: (12) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 14.62937ms)
  Jun 24 12:41:26.272: INFO: (12) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.366857ms)
  Jun 24 12:41:26.272: INFO: (12) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 14.626471ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 14.725072ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 15.448911ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 15.265918ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 15.34973ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 15.617933ms)
  Jun 24 12:41:26.273: INFO: (12) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 15.882536ms)
  Jun 24 12:41:26.281: INFO: (13) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 7.151898ms)
  Jun 24 12:41:26.285: INFO: (13) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.935375ms)
  Jun 24 12:41:26.285: INFO: (13) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.381251ms)
  Jun 24 12:41:26.287: INFO: (13) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 13.81329ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 13.689419ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 14.329627ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 14.674741ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 14.521569ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 14.485188ms)
  Jun 24 12:41:26.288: INFO: (13) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 14.269996ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 14.423838ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 14.413068ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 14.722112ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 15.234167ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 14.753742ms)
  Jun 24 12:41:26.289: INFO: (13) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 16.055478ms)
  Jun 24 12:41:26.298: INFO: (14) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 7.644255ms)
  Jun 24 12:41:26.298: INFO: (14) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 7.669194ms)
  Jun 24 12:41:26.299: INFO: (14) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 9.019581ms)
  Jun 24 12:41:26.300: INFO: (14) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 9.462807ms)
  Jun 24 12:41:26.300: INFO: (14) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 10.184196ms)
  Jun 24 12:41:26.300: INFO: (14) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 10.411019ms)
  Jun 24 12:41:26.301: INFO: (14) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 10.257877ms)
  Jun 24 12:41:26.301: INFO: (14) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 11.30575ms)
  Jun 24 12:41:26.301: INFO: (14) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 11.32471ms)
  Jun 24 12:41:26.302: INFO: (14) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 11.776106ms)
  Jun 24 12:41:26.302: INFO: (14) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 11.531242ms)
  Jun 24 12:41:26.302: INFO: (14) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 12.11967ms)
  Jun 24 12:41:26.302: INFO: (14) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 12.263692ms)
  Jun 24 12:41:26.303: INFO: (14) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 12.528235ms)
  Jun 24 12:41:26.303: INFO: (14) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 12.91423ms)
  Jun 24 12:41:26.305: INFO: (14) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 14.816893ms)
  Jun 24 12:41:26.312: INFO: (15) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 6.550731ms)
  Jun 24 12:41:26.312: INFO: (15) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 6.50961ms)
  Jun 24 12:41:26.313: INFO: (15) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 8.040839ms)
  Jun 24 12:41:26.315: INFO: (15) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 9.319145ms)
  Jun 24 12:41:26.316: INFO: (15) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.53575ms)
  Jun 24 12:41:26.317: INFO: (15) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 11.155978ms)
  Jun 24 12:41:26.317: INFO: (15) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 11.601313ms)
  Jun 24 12:41:26.317: INFO: (15) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 11.599223ms)
  Jun 24 12:41:26.318: INFO: (15) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.722865ms)
  Jun 24 12:41:26.318: INFO: (15) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 11.916427ms)
  Jun 24 12:41:26.318: INFO: (15) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 12.845889ms)
  Jun 24 12:41:26.319: INFO: (15) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 13.531627ms)
  Jun 24 12:41:26.320: INFO: (15) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.108614ms)
  Jun 24 12:41:26.320: INFO: (15) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 13.664288ms)
  Jun 24 12:41:26.320: INFO: (15) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 13.964432ms)
  Jun 24 12:41:26.320: INFO: (15) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 13.805101ms)
  Jun 24 12:41:26.327: INFO: (16) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 6.661333ms)
  Jun 24 12:41:26.328: INFO: (16) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 7.347041ms)
  Jun 24 12:41:26.328: INFO: (16) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 8.09742ms)
  Jun 24 12:41:26.330: INFO: (16) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 9.949333ms)
  Jun 24 12:41:26.330: INFO: (16) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 9.533118ms)
  Jun 24 12:41:26.331: INFO: (16) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 11.165507ms)
  Jun 24 12:41:26.331: INFO: (16) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 10.741862ms)
  Jun 24 12:41:26.332: INFO: (16) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 11.660684ms)
  Jun 24 12:41:26.332: INFO: (16) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 12.036618ms)
  Jun 24 12:41:26.332: INFO: (16) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 11.668524ms)
  Jun 24 12:41:26.333: INFO: (16) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 12.572235ms)
  Jun 24 12:41:26.333: INFO: (16) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 12.589885ms)
  Jun 24 12:41:26.334: INFO: (16) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 13.080971ms)
  Jun 24 12:41:26.334: INFO: (16) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 13.488096ms)
  Jun 24 12:41:26.334: INFO: (16) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.134762ms)
  Jun 24 12:41:26.334: INFO: (16) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 13.376665ms)
  Jun 24 12:41:26.342: INFO: (17) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 8.07519ms)
  Jun 24 12:41:26.342: INFO: (17) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 7.983889ms)
  Jun 24 12:41:26.343: INFO: (17) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 8.392924ms)
  Jun 24 12:41:26.343: INFO: (17) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 8.655297ms)
  Jun 24 12:41:26.343: INFO: (17) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 9.016102ms)
  Jun 24 12:41:26.344: INFO: (17) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.098685ms)
  Jun 24 12:41:26.344: INFO: (17) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 9.70141ms)
  Jun 24 12:41:26.345: INFO: (17) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 10.380008ms)
  Jun 24 12:41:26.346: INFO: (17) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 11.443271ms)
  Jun 24 12:41:26.347: INFO: (17) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 12.549125ms)
  Jun 24 12:41:26.347: INFO: (17) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 12.438503ms)
  Jun 24 12:41:26.348: INFO: (17) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.770848ms)
  Jun 24 12:41:26.348: INFO: (17) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.179162ms)
  Jun 24 12:41:26.348: INFO: (17) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 12.97925ms)
  Jun 24 12:41:26.348: INFO: (17) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 13.445936ms)
  Jun 24 12:41:26.349: INFO: (17) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 14.022903ms)
  Jun 24 12:41:26.356: INFO: (18) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 6.909026ms)
  Jun 24 12:41:26.357: INFO: (18) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 7.705526ms)
  Jun 24 12:41:26.358: INFO: (18) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 8.894139ms)
  Jun 24 12:41:26.358: INFO: (18) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 8.730348ms)
  Jun 24 12:41:26.360: INFO: (18) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 10.614551ms)
  Jun 24 12:41:26.360: INFO: (18) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.50599ms)
  Jun 24 12:41:26.360: INFO: (18) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 10.706993ms)
  Jun 24 12:41:26.360: INFO: (18) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 11.428421ms)
  Jun 24 12:41:26.360: INFO: (18) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 11.258889ms)
  Jun 24 12:41:26.361: INFO: (18) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 12.17818ms)
  Jun 24 12:41:26.361: INFO: (18) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 11.808826ms)
  Jun 24 12:41:26.361: INFO: (18) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 12.038758ms)
  Jun 24 12:41:26.362: INFO: (18) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 12.537645ms)
  Jun 24 12:41:26.362: INFO: (18) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 13.357814ms)
  Jun 24 12:41:26.363: INFO: (18) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.934132ms)
  Jun 24 12:41:26.364: INFO: (18) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 14.218076ms)
  Jun 24 12:41:26.370: INFO: (19) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:443/proxy/tlsrewritem... (200; 6.039615ms)
  Jun 24 12:41:26.371: INFO: (19) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:462/proxy/: tls qux (200; 6.880315ms)
  Jun 24 12:41:26.373: INFO: (19) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8/proxy/rewriteme">test</a> (200; 8.774999ms)
  Jun 24 12:41:26.374: INFO: (19) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:1080/proxy/rewriteme">test<... (200; 10.117475ms)
  Jun 24 12:41:26.374: INFO: (19) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.091975ms)
  Jun 24 12:41:26.375: INFO: (19) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:160/proxy/: foo (200; 10.273577ms)
  Jun 24 12:41:26.375: INFO: (19) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/: <a href="/api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:1080/proxy/rewriteme">... (200; 10.399729ms)
  Jun 24 12:41:26.375: INFO: (19) /api/v1/namespaces/proxy-4931/pods/http:proxy-service-9gllh-55kf8:162/proxy/: bar (200; 10.750083ms)
  Jun 24 12:41:26.376: INFO: (19) /api/v1/namespaces/proxy-4931/pods/https:proxy-service-9gllh-55kf8:460/proxy/: tls baz (200; 11.702725ms)
  Jun 24 12:41:26.376: INFO: (19) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname2/proxy/: bar (200; 12.010159ms)
  Jun 24 12:41:26.376: INFO: (19) /api/v1/namespaces/proxy-4931/services/proxy-service-9gllh:portname1/proxy/: foo (200; 12.286072ms)
  Jun 24 12:41:26.377: INFO: (19) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname2/proxy/: tls qux (200; 12.681537ms)
  Jun 24 12:41:26.377: INFO: (19) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname2/proxy/: bar (200; 13.148782ms)
  Jun 24 12:41:26.377: INFO: (19) /api/v1/namespaces/proxy-4931/services/https:proxy-service-9gllh:tlsportname1/proxy/: tls baz (200; 13.134923ms)
  Jun 24 12:41:26.377: INFO: (19) /api/v1/namespaces/proxy-4931/services/http:proxy-service-9gllh:portname1/proxy/: foo (200; 13.308124ms)
  Jun 24 12:41:26.377: INFO: (19) /api/v1/namespaces/proxy-4931/pods/proxy-service-9gllh-55kf8:162/proxy/: bar (200; 13.150502ms)
  Jun 24 12:41:26.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-9gllh in namespace proxy-4931, will wait for the garbage collector to delete the pods @ 06/24/23 12:41:26.383
  Jun 24 12:41:26.445: INFO: Deleting ReplicationController proxy-service-9gllh took: 7.768356ms
  Jun 24 12:41:26.545: INFO: Terminating ReplicationController proxy-service-9gllh pods took: 100.51021ms
  E0624 12:41:27.020717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:28.021492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:29.022100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-4931" for this suite. @ 06/24/23 12:41:29.446
• [5.513 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 06/24/23 12:41:29.457
  Jun 24 12:41:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:41:29.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:29.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:29.478
  STEP: creating the pod @ 06/24/23 12:41:29.482
  STEP: setting up watch @ 06/24/23 12:41:29.482
  STEP: submitting the pod to kubernetes @ 06/24/23 12:41:29.586
  STEP: verifying the pod is in kubernetes @ 06/24/23 12:41:29.598
  STEP: verifying pod creation was observed @ 06/24/23 12:41:29.602
  E0624 12:41:30.022183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:31.022331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 06/24/23 12:41:31.628
  STEP: verifying pod deletion was observed @ 06/24/23 12:41:31.638
  E0624 12:41:32.023305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:33.023690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:34.023793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:34.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5976" for this suite. @ 06/24/23 12:41:34.437
• [4.987 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 06/24/23 12:41:34.449
  Jun 24 12:41:34.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:41:34.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:34.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:34.476
  Jun 24 12:41:34.481: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jun 24 12:41:34.492: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0624 12:41:35.024862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:36.024950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:37.025074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:38.025315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:39.025426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:39.498: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 12:41:39.498
  Jun 24 12:41:39.498: INFO: Creating deployment "test-rolling-update-deployment"
  Jun 24 12:41:39.505: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jun 24 12:41:39.514: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0624 12:41:40.025617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:41.025661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:41.524: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jun 24 12:41:41.527: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jun 24 12:41:41.538: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5361  50cff0f1-db42-4e64-80c8-168f11852c0f 23708 1 2023-06-24 12:41:39 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-24 12:41:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002834168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-24 12:41:39 +0000 UTC,LastTransitionTime:2023-06-24 12:41:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-06-24 12:41:40 +0000 UTC,LastTransitionTime:2023-06-24 12:41:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 24 12:41:41.542: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-5361  9808a4bb-a237-4c5e-8b2c-23c9f2d4d933 23698 1 2023-06-24 12:41:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 50cff0f1-db42-4e64-80c8-168f11852c0f 0xc002834647 0xc002834648}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:41:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50cff0f1-db42-4e64-80c8-168f11852c0f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:41:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002834718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:41:41.542: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jun 24 12:41:41.542: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5361  c142f121-4ce9-42da-a620-6dab69f63b38 23707 2 2023-06-24 12:41:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 50cff0f1-db42-4e64-80c8-168f11852c0f 0xc002834517 0xc002834518}] [] [{e2e.test Update apps/v1 2023-06-24 12:41:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:41:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50cff0f1-db42-4e64-80c8-168f11852c0f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:41:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028345d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:41:41.546: INFO: Pod "test-rolling-update-deployment-656d657cd8-b27zk" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-b27zk test-rolling-update-deployment-656d657cd8- deployment-5361  b95e43d6-fdb9-471a-8776-0e8ceb03791e 23697 0 2023-06-24 12:41:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 9808a4bb-a237-4c5e-8b2c-23c9f2d4d933 0xc002834b67 0xc002834b68}] [] [{kube-controller-manager Update v1 2023-06-24 12:41:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9808a4bb-a237-4c5e-8b2c-23c9f2d4d933\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:41:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk7gk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk7gk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:41:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:41:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:41:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.247,StartTime:2023-06-24 12:41:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:41:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://2ff171ddd3c626763c7eb9f662a1f524bf82b89ed515f6be41fb1db7f55ece9c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.247,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:41:41.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5361" for this suite. @ 06/24/23 12:41:41.551
• [7.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 06/24/23 12:41:41.562
  Jun 24 12:41:41.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename runtimeclass @ 06/24/23 12:41:41.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:41.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:41.591
  STEP: Deleting RuntimeClass runtimeclass-1786-delete-me @ 06/24/23 12:41:41.601
  STEP: Waiting for the RuntimeClass to disappear @ 06/24/23 12:41:41.607
  Jun 24 12:41:41.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1786" for this suite. @ 06/24/23 12:41:41.623
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 06/24/23 12:41:41.631
  Jun 24 12:41:41.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename containers @ 06/24/23 12:41:41.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:41.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:41.654
  E0624 12:41:42.026279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:43.026491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:43.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4212" for this suite. @ 06/24/23 12:41:43.69
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 06/24/23 12:41:43.7
  Jun 24 12:41:43.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:41:43.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:43.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:43.723
  STEP: creating service endpoint-test2 in namespace services-1839 @ 06/24/23 12:41:43.726
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1839 to expose endpoints map[] @ 06/24/23 12:41:43.738
  Jun 24 12:41:43.743: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0624 12:41:44.027293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:44.752: INFO: successfully validated that service endpoint-test2 in namespace services-1839 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1839 @ 06/24/23 12:41:44.752
  E0624 12:41:45.028134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:46.028221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1839 to expose endpoints map[pod1:[80]] @ 06/24/23 12:41:46.778
  Jun 24 12:41:46.789: INFO: successfully validated that service endpoint-test2 in namespace services-1839 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 06/24/23 12:41:46.789
  Jun 24 12:41:46.789: INFO: Creating new exec pod
  E0624 12:41:47.028291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:48.028797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:49.029618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:49.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 24 12:41:49.963: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:49.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:41:49.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.51 80'
  E0624 12:41:50.029737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:50.124: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.51 80\nConnection to 10.152.183.51 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:50.124: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-1839 @ 06/24/23 12:41:50.124
  E0624 12:41:51.030159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:52.030266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1839 to expose endpoints map[pod1:[80] pod2:[80]] @ 06/24/23 12:41:52.142
  Jun 24 12:41:52.157: INFO: successfully validated that service endpoint-test2 in namespace services-1839 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 06/24/23 12:41:52.157
  E0624 12:41:53.030927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:53.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 24 12:41:53.322: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:53.322: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:41:53.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.51 80'
  Jun 24 12:41:53.492: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.51 80\nConnection to 10.152.183.51 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:53.492: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1839 @ 06/24/23 12:41:53.492
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1839 to expose endpoints map[pod2:[80]] @ 06/24/23 12:41:53.505
  E0624 12:41:54.031066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:54.533: INFO: successfully validated that service endpoint-test2 in namespace services-1839 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 06/24/23 12:41:54.533
  E0624 12:41:55.031952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:55.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun 24 12:41:55.693: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:55.693: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:41:55.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1839 exec execpodsfqr8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.51 80'
  Jun 24 12:41:55.852: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.51 80\nConnection to 10.152.183.51 80 port [tcp/http] succeeded!\n"
  Jun 24 12:41:55.852: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-1839 @ 06/24/23 12:41:55.852
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1839 to expose endpoints map[] @ 06/24/23 12:41:55.868
  E0624 12:41:56.032096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:41:56.890: INFO: successfully validated that service endpoint-test2 in namespace services-1839 exposes endpoints map[]
  Jun 24 12:41:56.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1839" for this suite. @ 06/24/23 12:41:56.911
• [13.220 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 06/24/23 12:41:56.922
  Jun 24 12:41:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/24/23 12:41:56.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:41:56.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:41:56.952
  Jun 24 12:41:56.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:41:57.032075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:58.032757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:41:59.032800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:00.033664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:00.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5862" for this suite. @ 06/24/23 12:42:00.213
• [3.299 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 06/24/23 12:42:00.223
  Jun 24 12:42:00.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:42:00.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:00.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:00.247
  STEP: Setting up server cert @ 06/24/23 12:42:00.285
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:42:00.913
  STEP: Deploying the webhook pod @ 06/24/23 12:42:00.923
  STEP: Wait for the deployment to be ready @ 06/24/23 12:42:00.937
  Jun 24 12:42:00.946: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 12:42:01.034106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:02.034226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 12:42:02.963
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:42:02.973
  E0624 12:42:03.034987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:03.973: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/24/23 12:42:03.978
  Jun 24 12:42:03.998: INFO: Waiting for webhook configuration to be ready...
  E0624 12:42:04.035409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a pod @ 06/24/23 12:42:04.11
  E0624 12:42:05.036225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:06.036804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 06/24/23 12:42:06.129
  Jun 24 12:42:06.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=webhook-1780 attach --namespace=webhook-1780 to-be-attached-pod -i -c=container1'
  Jun 24 12:42:06.228: INFO: rc: 1
  Jun 24 12:42:06.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1780" for this suite. @ 06/24/23 12:42:06.287
  STEP: Destroying namespace "webhook-markers-1730" for this suite. @ 06/24/23 12:42:06.297
• [6.083 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 06/24/23 12:42:06.306
  Jun 24 12:42:06.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 12:42:06.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:06.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:06.329
  STEP: Create a ReplicaSet @ 06/24/23 12:42:06.334
  STEP: Verify that the required pods have come up @ 06/24/23 12:42:06.341
  Jun 24 12:42:06.345: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0624 12:42:07.037124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:08.037249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:09.037705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:10.038095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:11.038297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:11.351: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 06/24/23 12:42:11.351
  Jun 24 12:42:11.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 06/24/23 12:42:11.355
  STEP: DeleteCollection of the ReplicaSets @ 06/24/23 12:42:11.36
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 06/24/23 12:42:11.372
  Jun 24 12:42:11.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3852" for this suite. @ 06/24/23 12:42:11.381
• [5.088 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 06/24/23 12:42:11.396
  Jun 24 12:42:11.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:42:11.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:11.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:11.437
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/24/23 12:42:11.444
  E0624 12:42:12.038334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:13.039280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:14.040293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:15.040314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:42:15.469
  Jun 24 12:42:15.473: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-6a60146e-2025-49e6-8564-7f5f9a35f116 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:42:15.48
  Jun 24 12:42:15.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1440" for this suite. @ 06/24/23 12:42:15.498
• [4.111 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 06/24/23 12:42:15.508
  Jun 24 12:42:15.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename certificates @ 06/24/23 12:42:15.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:15.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:15.536
  E0624 12:42:16.041081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 06/24/23 12:42:16.64
  STEP: getting /apis/certificates.k8s.io @ 06/24/23 12:42:16.645
  STEP: getting /apis/certificates.k8s.io/v1 @ 06/24/23 12:42:16.646
  STEP: creating @ 06/24/23 12:42:16.648
  STEP: getting @ 06/24/23 12:42:16.671
  STEP: listing @ 06/24/23 12:42:16.674
  STEP: watching @ 06/24/23 12:42:16.679
  Jun 24 12:42:16.679: INFO: starting watch
  STEP: patching @ 06/24/23 12:42:16.681
  STEP: updating @ 06/24/23 12:42:16.69
  Jun 24 12:42:16.697: INFO: waiting for watch events with expected annotations
  Jun 24 12:42:16.698: INFO: saw patched and updated annotations
  STEP: getting /approval @ 06/24/23 12:42:16.698
  STEP: patching /approval @ 06/24/23 12:42:16.702
  STEP: updating /approval @ 06/24/23 12:42:16.71
  STEP: getting /status @ 06/24/23 12:42:16.717
  STEP: patching /status @ 06/24/23 12:42:16.721
  STEP: updating /status @ 06/24/23 12:42:16.73
  STEP: deleting @ 06/24/23 12:42:16.738
  STEP: deleting a collection @ 06/24/23 12:42:16.752
  Jun 24 12:42:16.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6834" for this suite. @ 06/24/23 12:42:16.776
• [1.276 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 06/24/23 12:42:16.786
  Jun 24 12:42:16.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:42:16.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:16.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:16.81
  STEP: Creating a pod to test downward api env vars @ 06/24/23 12:42:16.853
  E0624 12:42:17.041963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:18.042696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:19.043146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:20.043277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:42:20.877
  Jun 24 12:42:20.881: INFO: Trying to get logs from node ip-172-31-19-205 pod downward-api-f9335bb5-c0c4-43a3-ac99-fac64fc98bc9 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:42:20.889
  Jun 24 12:42:20.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7260" for this suite. @ 06/24/23 12:42:20.909
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 06/24/23 12:42:20.92
  Jun 24 12:42:20.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:42:20.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:20.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:20.944
  STEP: creating Agnhost RC @ 06/24/23 12:42:20.949
  Jun 24 12:42:20.949: INFO: namespace kubectl-2640
  Jun 24 12:42:20.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2640 create -f -'
  E0624 12:42:21.044282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:21.520: INFO: stderr: ""
  Jun 24 12:42:21.520: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/24/23 12:42:21.52
  E0624 12:42:22.045333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:22.524: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:42:22.524: INFO: Found 0 / 1
  E0624 12:42:23.045562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:23.524: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:42:23.524: INFO: Found 1 / 1
  Jun 24 12:42:23.524: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun 24 12:42:23.528: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun 24 12:42:23.528: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun 24 12:42:23.528: INFO: wait on agnhost-primary startup in kubectl-2640 
  Jun 24 12:42:23.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2640 logs agnhost-primary-stpwt agnhost-primary'
  Jun 24 12:42:23.636: INFO: stderr: ""
  Jun 24 12:42:23.636: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 06/24/23 12:42:23.636
  Jun 24 12:42:23.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2640 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jun 24 12:42:23.739: INFO: stderr: ""
  Jun 24 12:42:23.739: INFO: stdout: "service/rm2 exposed\n"
  Jun 24 12:42:23.745: INFO: Service rm2 in namespace kubectl-2640 found.
  E0624 12:42:24.046009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:25.046225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 06/24/23 12:42:25.753
  Jun 24 12:42:25.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2640 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jun 24 12:42:25.855: INFO: stderr: ""
  Jun 24 12:42:25.855: INFO: stdout: "service/rm3 exposed\n"
  Jun 24 12:42:25.859: INFO: Service rm3 in namespace kubectl-2640 found.
  E0624 12:42:26.047040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:27.047058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:27.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2640" for this suite. @ 06/24/23 12:42:27.872
• [6.960 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 06/24/23 12:42:27.885
  Jun 24 12:42:27.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 12:42:27.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:27.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:27.91
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 06/24/23 12:42:27.914
  E0624 12:42:28.048078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:29.048338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 06/24/23 12:42:29.937
  STEP: Then the orphan pod is adopted @ 06/24/23 12:42:29.943
  E0624 12:42:30.048566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 06/24/23 12:42:30.952
  Jun 24 12:42:30.956: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/24/23 12:42:30.968
  E0624 12:42:31.049353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:31.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8374" for this suite. @ 06/24/23 12:42:31.983
• [4.106 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 06/24/23 12:42:31.992
  Jun 24 12:42:31.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:42:31.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:32.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:32.017
  STEP: Creating a pod to test emptydir volume type on node default medium @ 06/24/23 12:42:32.022
  E0624 12:42:32.050410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:33.050789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:34.051207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:35.051530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:42:36.046
  E0624 12:42:36.052293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:42:36.054: INFO: Trying to get logs from node ip-172-31-15-136 pod pod-cbdeb3e2-569a-4831-8020-2f5ff82f6839 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 12:42:36.065
  Jun 24 12:42:36.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8385" for this suite. @ 06/24/23 12:42:36.084
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 06/24/23 12:42:36.093
  Jun 24 12:42:36.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-runtime @ 06/24/23 12:42:36.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:36.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:36.121
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 06/24/23 12:42:36.138
  E0624 12:42:37.053270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:38.053366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:39.053472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:40.053621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:41.053758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:42.054327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:43.054718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:44.054821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:45.054845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:46.054972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:47.055707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:48.056770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:49.056885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:50.057039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:51.057145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:52.057368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 06/24/23 12:42:52.222
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 06/24/23 12:42:52.226
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 06/24/23 12:42:52.234
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 06/24/23 12:42:52.235
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 06/24/23 12:42:52.261
  E0624 12:42:53.058437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:54.058611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:55.058692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 06/24/23 12:42:55.279
  E0624 12:42:56.058852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 06/24/23 12:42:56.288
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 06/24/23 12:42:56.296
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 06/24/23 12:42:56.296
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 06/24/23 12:42:56.323
  E0624 12:42:57.059295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 06/24/23 12:42:57.332
  E0624 12:42:58.059469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:42:59.059632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 06/24/23 12:42:59.344
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 06/24/23 12:42:59.351
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 06/24/23 12:42:59.351
  Jun 24 12:42:59.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9628" for this suite. @ 06/24/23 12:42:59.379
• [23.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 06/24/23 12:42:59.394
  Jun 24 12:42:59.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubelet-test @ 06/24/23 12:42:59.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:42:59.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:42:59.42
  STEP: Waiting for pod completion @ 06/24/23 12:42:59.435
  E0624 12:43:00.060552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:01.059961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:02.060515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:03.061127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2528" for this suite. @ 06/24/23 12:43:03.465
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 06/24/23 12:43:03.475
  Jun 24 12:43:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:43:03.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:03.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:03.501
  STEP: creating a secret @ 06/24/23 12:43:03.506
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 06/24/23 12:43:03.511
  STEP: patching the secret @ 06/24/23 12:43:03.516
  STEP: deleting the secret using a LabelSelector @ 06/24/23 12:43:03.526
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 06/24/23 12:43:03.536
  Jun 24 12:43:03.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4086" for this suite. @ 06/24/23 12:43:03.545
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 06/24/23 12:43:03.555
  Jun 24 12:43:03.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:43:03.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:03.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:03.581
  STEP: Create a pod @ 06/24/23 12:43:03.586
  E0624 12:43:04.061256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:05.061361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/24/23 12:43:05.604
  Jun 24 12:43:05.615: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jun 24 12:43:05.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6523" for this suite. @ 06/24/23 12:43:05.619
• [2.073 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 06/24/23 12:43:05.628
  Jun 24 12:43:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:43:05.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:05.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:05.653
  STEP: Creating secret with name secret-test-57176c42-d60a-4508-9ea5-4e7068d45c35 @ 06/24/23 12:43:05.657
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:43:05.663
  E0624 12:43:06.062142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:07.063116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:08.063231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:09.063690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:43:09.688
  Jun 24 12:43:09.692: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-8fc5b506-6e8c-423c-9dd3-290e73e5298f container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:43:09.701
  Jun 24 12:43:09.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9839" for this suite. @ 06/24/23 12:43:09.723
• [4.105 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 06/24/23 12:43:09.734
  Jun 24 12:43:09.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 12:43:09.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:09.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:09.764
  STEP: Creating a pod to test substitution in container's args @ 06/24/23 12:43:09.77
  E0624 12:43:10.064152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:11.064786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:12.065391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:13.066469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:43:13.801
  Jun 24 12:43:13.806: INFO: Trying to get logs from node ip-172-31-19-205 pod var-expansion-0fccd1ff-c19f-458e-b44d-cad0401036c7 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:43:13.815
  Jun 24 12:43:13.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8863" for this suite. @ 06/24/23 12:43:13.842
• [4.122 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 06/24/23 12:43:13.856
  Jun 24 12:43:13.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context-test @ 06/24/23 12:43:13.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:13.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:13.888
  E0624 12:43:14.066577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:15.067172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:16.067522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:17.068174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:17.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-30" for this suite. @ 06/24/23 12:43:17.935
• [4.087 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 06/24/23 12:43:17.944
  Jun 24 12:43:17.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pod-network-test @ 06/24/23 12:43:17.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:17.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:17.978
  STEP: Performing setup for networking test in namespace pod-network-test-4848 @ 06/24/23 12:43:17.981
  STEP: creating a selector @ 06/24/23 12:43:17.981
  STEP: Creating the service pods in kubernetes @ 06/24/23 12:43:17.981
  Jun 24 12:43:17.982: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0624 12:43:18.069124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:19.069373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:20.070269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:21.070386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:22.071136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:23.071583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:24.072246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:25.072365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:26.072977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:27.073100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:28.074098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:29.074202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:30.075399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/24/23 12:43:30.075
  E0624 12:43:31.075526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:32.075931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:32.094: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 24 12:43:32.094: INFO: Breadth first check of 192.168.116.202 on host 172.31.15.136...
  Jun 24 12:43:32.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.194:9080/dial?request=hostname&protocol=udp&host=192.168.116.202&port=8081&tries=1'] Namespace:pod-network-test-4848 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:43:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:43:32.099: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:43:32.099: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4848/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.194%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.116.202%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:43:32.181: INFO: Waiting for responses: map[]
  Jun 24 12:43:32.181: INFO: reached 192.168.116.202 after 0/1 tries
  Jun 24 12:43:32.181: INFO: Breadth first check of 192.168.150.254 on host 172.31.19.205...
  Jun 24 12:43:32.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.194:9080/dial?request=hostname&protocol=udp&host=192.168.150.254&port=8081&tries=1'] Namespace:pod-network-test-4848 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:43:32.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:43:32.186: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:43:32.186: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4848/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.194%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.150.254%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:43:32.272: INFO: Waiting for responses: map[]
  Jun 24 12:43:32.272: INFO: reached 192.168.150.254 after 0/1 tries
  Jun 24 12:43:32.272: INFO: Breadth first check of 192.168.144.128 on host 172.31.89.202...
  Jun 24 12:43:32.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.194:9080/dial?request=hostname&protocol=udp&host=192.168.144.128&port=8081&tries=1'] Namespace:pod-network-test-4848 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:43:32.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:43:32.277: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:43:32.277: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4848/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.194%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.144.128%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun 24 12:43:32.353: INFO: Waiting for responses: map[]
  Jun 24 12:43:32.353: INFO: reached 192.168.144.128 after 0/1 tries
  Jun 24 12:43:32.353: INFO: Going to retry 0 out of 3 pods....
  Jun 24 12:43:32.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4848" for this suite. @ 06/24/23 12:43:32.358
• [14.422 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 06/24/23 12:43:32.367
  Jun 24 12:43:32.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:43:32.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:32.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:32.393
  Jun 24 12:43:32.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:43:33.076644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/24/23 12:43:33.847
  Jun 24 12:43:33.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-5430 --namespace=crd-publish-openapi-5430 create -f -'
  E0624 12:43:34.077092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:34.643: INFO: stderr: ""
  Jun 24 12:43:34.643: INFO: stdout: "e2e-test-crd-publish-openapi-1831-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 24 12:43:34.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-5430 --namespace=crd-publish-openapi-5430 delete e2e-test-crd-publish-openapi-1831-crds test-cr'
  Jun 24 12:43:34.733: INFO: stderr: ""
  Jun 24 12:43:34.733: INFO: stdout: "e2e-test-crd-publish-openapi-1831-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jun 24 12:43:34.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-5430 --namespace=crd-publish-openapi-5430 apply -f -'
  E0624 12:43:35.077180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:35.090: INFO: stderr: ""
  Jun 24 12:43:35.091: INFO: stdout: "e2e-test-crd-publish-openapi-1831-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun 24 12:43:35.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-5430 --namespace=crd-publish-openapi-5430 delete e2e-test-crd-publish-openapi-1831-crds test-cr'
  Jun 24 12:43:35.183: INFO: stderr: ""
  Jun 24 12:43:35.183: INFO: stdout: "e2e-test-crd-publish-openapi-1831-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/24/23 12:43:35.183
  Jun 24 12:43:35.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-5430 explain e2e-test-crd-publish-openapi-1831-crds'
  Jun 24 12:43:35.551: INFO: stderr: ""
  Jun 24 12:43:35.551: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-1831-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0624 12:43:36.078056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:37.078277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:43:37.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5430" for this suite. @ 06/24/23 12:43:37.512
• [5.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 06/24/23 12:43:37.521
  Jun 24 12:43:37.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:43:37.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:37.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:37.546
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:43:37.55
  E0624 12:43:38.079382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:39.079692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:40.080537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:41.080699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:43:41.575
  Jun 24 12:43:41.580: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-32f91fdb-2e0f-4dba-bc35-9c564207c374 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:43:41.589
  Jun 24 12:43:41.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6388" for this suite. @ 06/24/23 12:43:41.612
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 06/24/23 12:43:41.622
  Jun 24 12:43:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:43:41.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:43:41.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:43:41.649
  STEP: Creating configMap with name configmap-test-upd-70a1d044-8922-49f2-afc1-27af1ec90098 @ 06/24/23 12:43:41.658
  STEP: Creating the pod @ 06/24/23 12:43:41.664
  E0624 12:43:42.081339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:43.082043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-70a1d044-8922-49f2-afc1-27af1ec90098 @ 06/24/23 12:43:43.691
  STEP: waiting to observe update in volume @ 06/24/23 12:43:43.698
  E0624 12:43:44.082861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:45.082979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:46.083267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:47.083822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:48.084601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:49.084714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:50.084793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:51.084891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:52.085038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:53.085283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:54.086017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:55.086167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:56.086628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:57.087233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:58.088343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:43:59.088775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:00.089624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:01.090270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:02.090304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:03.090420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:04.091423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:05.091693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:06.092134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:07.092816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:08.093627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:09.093713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:10.093792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:11.093864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:12.094813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:13.094900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:14.094939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:15.095026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:16.095931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:17.096511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:18.097084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:19.097811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:20.098374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:21.099251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:22.099360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:23.099696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:24.100783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:25.100956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:26.101408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:27.101500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:28.102010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:29.102208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:30.103169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:31.103379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:32.104370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:33.104451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:34.104987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:35.105137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:36.105247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:37.106138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:38.106205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:39.106547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:40.106597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:41.106722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:42.107956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:43.108043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:44.108424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:45.108488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:46.108942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:47.109458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:48.109747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:49.109825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:50.111765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:51.111828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:52.111931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:53.112066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:54.112504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:55.113025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:56.113904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:57.114612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:58.115438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:44:59.115538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:00.116231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:01.116376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:02.117437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:03.117551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:04.118094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:05.118460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:06.118810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:07.119186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:08.120369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:09.121194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:10.121801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:11.121900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:12.122238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:13.122331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:14.122522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:45:14.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7515" for this suite. @ 06/24/23 12:45:14.161
• [92.547 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 06/24/23 12:45:14.172
  Jun 24 12:45:14.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename endpointslice @ 06/24/23 12:45:14.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:45:14.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:45:14.197
  E0624 12:45:15.122747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:16.123010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:17.123030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:18.123523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:19.123700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 06/24/23 12:45:19.278
  E0624 12:45:20.124702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:21.124801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:22.125220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:23.125317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:24.125428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 06/24/23 12:45:24.287
  E0624 12:45:25.125564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:26.125645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:27.126583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:28.126703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:29.126779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 06/24/23 12:45:29.297
  E0624 12:45:30.127864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:31.127732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:32.128161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:33.128435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:34.128783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 06/24/23 12:45:34.308
  Jun 24 12:45:34.333: INFO: EndpointSlice for Service endpointslice-8128/example-named-port not found
  E0624 12:45:35.129831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:36.130162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:37.130525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:38.131066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:39.131721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:40.131832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:41.132780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:42.132887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:43.132996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:44.133100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:45:44.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8128" for this suite. @ 06/24/23 12:45:44.348
• [30.184 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 06/24/23 12:45:44.356
  Jun 24 12:45:44.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:45:44.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:45:44.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:45:44.383
  STEP: Creating resourceQuota "e2e-rq-status-f48pr" @ 06/24/23 12:45:44.391
  Jun 24 12:45:44.400: INFO: Resource quota "e2e-rq-status-f48pr" reports spec: hard cpu limit of 500m
  Jun 24 12:45:44.400: INFO: Resource quota "e2e-rq-status-f48pr" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-f48pr" /status @ 06/24/23 12:45:44.4
  STEP: Confirm /status for "e2e-rq-status-f48pr" resourceQuota via watch @ 06/24/23 12:45:44.411
  Jun 24 12:45:44.413: INFO: observed resourceQuota "e2e-rq-status-f48pr" in namespace "resourcequota-2792" with hard status: v1.ResourceList(nil)
  Jun 24 12:45:44.413: INFO: Found resourceQuota "e2e-rq-status-f48pr" in namespace "resourcequota-2792" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 24 12:45:44.414: INFO: ResourceQuota "e2e-rq-status-f48pr" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 06/24/23 12:45:44.418
  Jun 24 12:45:44.426: INFO: Resource quota "e2e-rq-status-f48pr" reports spec: hard cpu limit of 1
  Jun 24 12:45:44.426: INFO: Resource quota "e2e-rq-status-f48pr" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-f48pr" /status @ 06/24/23 12:45:44.426
  STEP: Confirm /status for "e2e-rq-status-f48pr" resourceQuota via watch @ 06/24/23 12:45:44.436
  Jun 24 12:45:44.438: INFO: observed resourceQuota "e2e-rq-status-f48pr" in namespace "resourcequota-2792" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun 24 12:45:44.438: INFO: Found resourceQuota "e2e-rq-status-f48pr" in namespace "resourcequota-2792" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jun 24 12:45:44.438: INFO: ResourceQuota "e2e-rq-status-f48pr" /status was patched
  STEP: Get "e2e-rq-status-f48pr" /status @ 06/24/23 12:45:44.438
  Jun 24 12:45:44.443: INFO: Resourcequota "e2e-rq-status-f48pr" reports status: hard cpu of 1
  Jun 24 12:45:44.443: INFO: Resourcequota "e2e-rq-status-f48pr" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-f48pr" /status before checking Spec is unchanged @ 06/24/23 12:45:44.448
  Jun 24 12:45:44.453: INFO: Resourcequota "e2e-rq-status-f48pr" reports status: hard cpu of 2
  Jun 24 12:45:44.453: INFO: Resourcequota "e2e-rq-status-f48pr" reports status: hard memory of 2Gi
  Jun 24 12:45:44.455: INFO: Found resourceQuota "e2e-rq-status-f48pr" in namespace "resourcequota-2792" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0624 12:45:45.134041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:46.134134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:47.135159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:48.135386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:49.135589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:50.135797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:51.136794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:52.137492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:53.137605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:54.137747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:55.138707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:56.138889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:57.138931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:58.139112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:45:59.139175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:00.140047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:01.140315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:02.140972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:03.141083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:04.141141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:05.142180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:06.143145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:07.143218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:08.143699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:09.143808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:10.143957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:11.144221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:12.144761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:13.144979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:14.145197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:15.145391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:16.145488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:17.146099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:18.146345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:19.146590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:20.146699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:21.146813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:22.147344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:23.147735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:24.148797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:25.148876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:26.149134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:27.150015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:28.150120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:29.150246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:30.150529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:31.150789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:32.151352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:33.151701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:34.152801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:35.152913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:36.153274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:37.154332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:38.154473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:39.154590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:40.155687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:41.155692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:42.156473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:43.156568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:44.156637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:45.156716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:46.156838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:47.157625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:48.157679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:49.157803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:50.158268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:51.158518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:52.159477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:53.159709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:54.159792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:55.159973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:56.160775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:57.160863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:58.161019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:46:59.161129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:00.161783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:01.161862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:02.161949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:03.162337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:04.163396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:04.464: INFO: ResourceQuota "e2e-rq-status-f48pr" Spec was unchanged and /status reset
  Jun 24 12:47:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2792" for this suite. @ 06/24/23 12:47:04.469
• [80.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 06/24/23 12:47:04.479
  Jun 24 12:47:04.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:47:04.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:04.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:04.507
  STEP: Creating a pod to test downward api env vars @ 06/24/23 12:47:04.511
  E0624 12:47:05.164614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:06.165060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:07.165133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:08.165251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:47:08.546
  Jun 24 12:47:08.550: INFO: Trying to get logs from node ip-172-31-19-205 pod downward-api-f78b700b-b91d-43c7-b2fc-fa2ba5af5c36 container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 12:47:08.571
  Jun 24 12:47:08.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1298" for this suite. @ 06/24/23 12:47:08.593
• [4.121 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 06/24/23 12:47:08.602
  Jun 24 12:47:08.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:47:08.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:08.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:08.628
  STEP: validating cluster-info @ 06/24/23 12:47:08.633
  Jun 24 12:47:08.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-3137 cluster-info'
  Jun 24 12:47:08.720: INFO: stderr: ""
  Jun 24 12:47:08.720: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jun 24 12:47:08.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3137" for this suite. @ 06/24/23 12:47:08.725
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 06/24/23 12:47:08.733
  Jun 24 12:47:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename endpointslicemirroring @ 06/24/23 12:47:08.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:08.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:08.759
  STEP: mirroring a new custom Endpoint @ 06/24/23 12:47:08.78
  Jun 24 12:47:08.793: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0624 12:47:09.165933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:10.166055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 06/24/23 12:47:10.798
  Jun 24 12:47:10.808: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0624 12:47:11.166646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:12.167106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 06/24/23 12:47:12.812
  Jun 24 12:47:12.823: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0624 12:47:13.167461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:14.167774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:14.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-4105" for this suite. @ 06/24/23 12:47:14.831
• [6.106 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 06/24/23 12:47:14.839
  Jun 24 12:47:14.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:47:14.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:14.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:14.867
  STEP: Creating configMap with name projected-configmap-test-volume-map-7543c65a-d21e-460e-b8eb-1cac0391bc85 @ 06/24/23 12:47:14.872
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:47:14.877
  E0624 12:47:15.168385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:16.168611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:17.169048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:18.169572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:47:18.902
  Jun 24 12:47:18.907: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-c84a8304-4306-470c-a90e-19e747349f37 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 12:47:18.915
  Jun 24 12:47:18.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6543" for this suite. @ 06/24/23 12:47:18.936
• [4.106 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 06/24/23 12:47:18.946
  Jun 24 12:47:18.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:47:18.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:18.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:18.983
  STEP: Creating namespace "e2e-ns-89h4s" @ 06/24/23 12:47:18.989
  Jun 24 12:47:19.013: INFO: Namespace "e2e-ns-89h4s-79" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-89h4s-79" @ 06/24/23 12:47:19.013
  Jun 24 12:47:19.027: INFO: Namespace "e2e-ns-89h4s-79" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-89h4s-79" @ 06/24/23 12:47:19.027
  Jun 24 12:47:19.039: INFO: Namespace "e2e-ns-89h4s-79" has []v1.FinalizerName{"kubernetes"}
  Jun 24 12:47:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4131" for this suite. @ 06/24/23 12:47:19.045
  STEP: Destroying namespace "e2e-ns-89h4s-79" for this suite. @ 06/24/23 12:47:19.054
• [0.122 seconds]
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 06/24/23 12:47:19.069
  Jun 24 12:47:19.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename endpointslice @ 06/24/23 12:47:19.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:19.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:19.103
  STEP: getting /apis @ 06/24/23 12:47:19.109
  STEP: getting /apis/discovery.k8s.io @ 06/24/23 12:47:19.117
  STEP: getting /apis/discovery.k8s.iov1 @ 06/24/23 12:47:19.12
  STEP: creating @ 06/24/23 12:47:19.123
  STEP: getting @ 06/24/23 12:47:19.147
  STEP: listing @ 06/24/23 12:47:19.151
  STEP: watching @ 06/24/23 12:47:19.156
  Jun 24 12:47:19.156: INFO: starting watch
  STEP: cluster-wide listing @ 06/24/23 12:47:19.158
  STEP: cluster-wide watching @ 06/24/23 12:47:19.163
  Jun 24 12:47:19.163: INFO: starting watch
  STEP: patching @ 06/24/23 12:47:19.166
  E0624 12:47:19.170256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating @ 06/24/23 12:47:19.174
  Jun 24 12:47:19.189: INFO: waiting for watch events with expected annotations
  Jun 24 12:47:19.189: INFO: saw patched and updated annotations
  STEP: deleting @ 06/24/23 12:47:19.189
  STEP: deleting a collection @ 06/24/23 12:47:19.209
  Jun 24 12:47:19.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5802" for this suite. @ 06/24/23 12:47:19.237
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 06/24/23 12:47:19.25
  Jun 24 12:47:19.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename podtemplate @ 06/24/23 12:47:19.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:19.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:19.286
  STEP: Create set of pod templates @ 06/24/23 12:47:19.292
  Jun 24 12:47:19.300: INFO: created test-podtemplate-1
  Jun 24 12:47:19.310: INFO: created test-podtemplate-2
  Jun 24 12:47:19.318: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 06/24/23 12:47:19.319
  STEP: delete collection of pod templates @ 06/24/23 12:47:19.324
  Jun 24 12:47:19.324: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 06/24/23 12:47:19.348
  Jun 24 12:47:19.348: INFO: requesting list of pod templates to confirm quantity
  Jun 24 12:47:19.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4439" for this suite. @ 06/24/23 12:47:19.358
• [0.119 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 06/24/23 12:47:19.369
  Jun 24 12:47:19.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:47:19.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:19.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:19.402
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 06/24/23 12:47:19.408
  Jun 24 12:47:19.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:47:20.170390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:47:21.171076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:22.172046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:23.172129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:24.172807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:25.173458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:26.174019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:27.174755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:27.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4373" for this suite. @ 06/24/23 12:47:27.774
• [8.414 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 06/24/23 12:47:27.786
  Jun 24 12:47:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/24/23 12:47:27.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:27.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:27.817
  E0624 12:47:28.175701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:29.175829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:29.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 06/24/23 12:47:29.878
  STEP: Cleaning up the configmap @ 06/24/23 12:47:29.887
  STEP: Cleaning up the pod @ 06/24/23 12:47:29.895
  STEP: Destroying namespace "emptydir-wrapper-7494" for this suite. @ 06/24/23 12:47:29.911
• [2.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 06/24/23 12:47:29.923
  Jun 24 12:47:29.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:47:29.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:29.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:29.947
  STEP: creating a Namespace @ 06/24/23 12:47:29.951
  STEP: patching the Namespace @ 06/24/23 12:47:29.974
  STEP: get the Namespace and ensuring it has the label @ 06/24/23 12:47:29.983
  Jun 24 12:47:29.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3606" for this suite. @ 06/24/23 12:47:29.993
  STEP: Destroying namespace "nspatchtest-ac6feb30-9eb0-429c-a051-5a6c95eb2052-9817" for this suite. @ 06/24/23 12:47:30.002
• [0.091 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 06/24/23 12:47:30.015
  Jun 24 12:47:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 12:47:30.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:30.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:30.045
  STEP: Creating Pod @ 06/24/23 12:47:30.051
  E0624 12:47:30.176445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:31.176741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 06/24/23 12:47:32.079
  Jun 24 12:47:32.080: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9411 PodName:pod-sharedvolume-3699bac1-6cad-405b-b254-a0723925ac8d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:47:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:47:32.081: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:47:32.081: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-9411/pods/pod-sharedvolume-3699bac1-6cad-405b-b254-a0723925ac8d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jun 24 12:47:32.147: INFO: Exec stderr: ""
  Jun 24 12:47:32.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9411" for this suite. @ 06/24/23 12:47:32.152
• [2.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 06/24/23 12:47:32.165
  Jun 24 12:47:32.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 12:47:32.166
  E0624 12:47:32.176827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:32.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:32.191
  STEP: Creating service test in namespace statefulset-4400 @ 06/24/23 12:47:32.197
  STEP: Creating statefulset ss in namespace statefulset-4400 @ 06/24/23 12:47:32.205
  Jun 24 12:47:32.217: INFO: Found 0 stateful pods, waiting for 1
  E0624 12:47:33.177342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:34.177575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:35.177728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:36.177894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:37.178318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:38.178472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:39.178577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:40.178990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:41.179644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:42.180245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:42.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 06/24/23 12:47:42.233
  STEP: updating a scale subresource @ 06/24/23 12:47:42.236
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/24/23 12:47:42.244
  STEP: Patch a scale subresource @ 06/24/23 12:47:42.25
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/24/23 12:47:42.261
  Jun 24 12:47:42.265: INFO: Deleting all statefulset in ns statefulset-4400
  Jun 24 12:47:42.272: INFO: Scaling statefulset ss to 0
  E0624 12:47:43.180915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:44.181662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:45.181967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:46.181998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:47.182093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:48.182190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:49.182300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:50.182436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:51.182523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:52.183293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:47:52.315: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:47:52.319: INFO: Deleting statefulset ss
  Jun 24 12:47:52.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4400" for this suite. @ 06/24/23 12:47:52.337
• [20.181 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 06/24/23 12:47:52.347
  Jun 24 12:47:52.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:47:52.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:52.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:52.374
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:47:52.379
  E0624 12:47:53.183705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:54.184794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:55.185719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:56.185788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:47:56.405
  Jun 24 12:47:56.408: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-4c00b1ab-6907-4c69-87c4-ca1fb12b4e75 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:47:56.42
  Jun 24 12:47:56.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7232" for this suite. @ 06/24/23 12:47:56.44
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 06/24/23 12:47:56.448
  Jun 24 12:47:56.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:47:56.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:47:56.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:47:56.472
  STEP: creating service in namespace services-4421 @ 06/24/23 12:47:56.476
  STEP: creating service affinity-nodeport-transition in namespace services-4421 @ 06/24/23 12:47:56.477
  STEP: creating replication controller affinity-nodeport-transition in namespace services-4421 @ 06/24/23 12:47:56.494
  I0624 12:47:56.506475      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-4421, replica count: 3
  E0624 12:47:57.186371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:58.187030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:47:59.187297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 12:47:59.557086      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 12:47:59.568: INFO: Creating new exec pod
  E0624 12:48:00.188350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:01.188635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:02.188874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:02.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jun 24 12:48:02.761: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jun 24 12:48:02.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:48:02.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.118 80'
  Jun 24 12:48:02.923: INFO: stderr: "+ nc -v -t -w 2 10.152.183.118 80\nConnection to 10.152.183.118 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 24 12:48:02.923: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:48:02.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.136 31617'
  Jun 24 12:48:03.076: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.136 31617\nConnection to 172.31.15.136 31617 port [tcp/*] succeeded!\n"
  Jun 24 12:48:03.076: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:48:03.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.89.202 31617'
  E0624 12:48:03.189645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:03.232: INFO: stderr: "+ nc -v -t -w 2 172.31.89.202 31617\n+ echo hostName\nConnection to 172.31.89.202 31617 port [tcp/*] succeeded!\n"
  Jun 24 12:48:03.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 12:48:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.136:31617/ ; done'
  Jun 24 12:48:03.516: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n"
  Jun 24 12:48:03.517: INFO: stdout: "\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-cxzsc\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-cxzsc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-cxzsc\naffinity-nodeport-transition-cdc7v\naffinity-nodeport-transition-cxzsc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-cxzsc\naffinity-nodeport-transition-cdc7v"
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cxzsc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cxzsc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cxzsc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cxzsc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cxzsc
  Jun 24 12:48:03.517: INFO: Received response from host: affinity-nodeport-transition-cdc7v
  Jun 24 12:48:03.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4421 exec execpod-affinitygjkm9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.136:31617/ ; done'
  Jun 24 12:48:03.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:31617/\n"
  Jun 24 12:48:03.802: INFO: stdout: "\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc\naffinity-nodeport-transition-jt6xc"
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Received response from host: affinity-nodeport-transition-jt6xc
  Jun 24 12:48:03.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 12:48:03.807: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4421, will wait for the garbage collector to delete the pods @ 06/24/23 12:48:03.821
  Jun 24 12:48:03.884: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.315105ms
  Jun 24 12:48:03.985: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.970945ms
  E0624 12:48:04.190424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:05.190939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4421" for this suite. @ 06/24/23 12:48:05.911
• [9.475 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 06/24/23 12:48:05.923
  Jun 24 12:48:05.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-runtime @ 06/24/23 12:48:05.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:05.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:05.954
  STEP: create the container @ 06/24/23 12:48:05.958
  W0624 12:48:05.970145      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 06/24/23 12:48:05.97
  E0624 12:48:06.191160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:07.191541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:08.192548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/24/23 12:48:08.987
  STEP: the container should be terminated @ 06/24/23 12:48:08.991
  STEP: the termination message should be set @ 06/24/23 12:48:08.991
  Jun 24 12:48:08.991: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/24/23 12:48:08.992
  Jun 24 12:48:09.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8991" for this suite. @ 06/24/23 12:48:09.012
• [3.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 06/24/23 12:48:09.038
  Jun 24 12:48:09.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:48:09.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:09.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:09.113
  STEP: Creating configMap with name configmap-test-upd-0b56cebd-4f34-4688-8c82-01b847a4b646 @ 06/24/23 12:48:09.123
  STEP: Creating the pod @ 06/24/23 12:48:09.13
  E0624 12:48:09.193100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:10.193206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 06/24/23 12:48:11.148
  STEP: Waiting for pod with binary data @ 06/24/23 12:48:11.156
  Jun 24 12:48:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6700" for this suite. @ 06/24/23 12:48:11.17
• [2.141 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 06/24/23 12:48:11.179
  Jun 24 12:48:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-pred @ 06/24/23 12:48:11.18
  E0624 12:48:11.193900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:11.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:11.207
  Jun 24 12:48:11.211: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 24 12:48:11.221: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 12:48:11.225: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-136 before test
  Jun 24 12:48:11.230: INFO: nginx-ingress-controller-kubernetes-worker-9295m from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:39 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.230: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: calico-kube-controllers-69b5565d84-5wz9f from kube-system started at 2023-06-24 12:02:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.230: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: sonobuoy-e2e-job-a6e2d2ee4f8b45eb from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.230: INFO: 	Container e2e ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57 from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:48:11.230: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-205 before test
  Jun 24 12:48:11.237: INFO: pod-configmaps-9afeb56b-d856-4a02-b0df-d1e1d4d6c78e from configmap-6700 started at 2023-06-24 12:48:09 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.237: INFO: 	Container agnhost-container ready: true, restart count 0
  Jun 24 12:48:11.237: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
  Jun 24 12:48:11.237: INFO: nginx-ingress-controller-kubernetes-worker-6vrl7 from ingress-nginx-kubernetes-worker started at 2023-06-24 12:03:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.237: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:48:11.237: INFO: sonobuoy from sonobuoy started at 2023-06-24 12:08:47 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.237: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 24 12:48:11.237: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-2vcbs from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.237: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:48:11.237: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:48:11.237: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-202 before test
  Jun 24 12:48:11.244: INFO: default-http-backend-kubernetes-worker-65fc475d49-cs8f9 from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: nginx-ingress-controller-kubernetes-worker-bvt8s from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: coredns-5c7f76ccb8-d25lj from kube-system started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container coredns ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: kube-state-metrics-5b95b4459c-pcn82 from kube-system started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: metrics-server-v0.5.2-6cf8c8b69c-m28bd from kube-system started at 2023-06-24 11:54:03 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: dashboard-metrics-scraper-6b8586b5c9-c7tbk from kubernetes-dashboard started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: kubernetes-dashboard-6869f4cd5f-6rqxs from kubernetes-dashboard started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 24 12:48:11.244: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-ldg4s from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:48:11.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:48:11.245: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/24/23 12:48:11.245
  E0624 12:48:12.194534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:13.194680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:14.195413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:15.195740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/24/23 12:48:15.271
  STEP: Trying to apply a random label on the found node. @ 06/24/23 12:48:15.282
  STEP: verifying the node has the label kubernetes.io/e2e-78fd7311-1011-4983-9e73-954780fe15c4 42 @ 06/24/23 12:48:15.292
  STEP: Trying to relaunch the pod, now with labels. @ 06/24/23 12:48:15.298
  E0624 12:48:16.196807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:17.197268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-78fd7311-1011-4983-9e73-954780fe15c4 off the node ip-172-31-19-205 @ 06/24/23 12:48:17.319
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-78fd7311-1011-4983-9e73-954780fe15c4 @ 06/24/23 12:48:17.335
  Jun 24 12:48:17.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9154" for this suite. @ 06/24/23 12:48:17.344
• [6.173 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 06/24/23 12:48:17.352
  Jun 24 12:48:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:48:17.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:17.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:17.383
  STEP: Updating Namespace "namespaces-1395" @ 06/24/23 12:48:17.387
  Jun 24 12:48:17.397: INFO: Namespace "namespaces-1395" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f5149a8b-d2eb-4506-8cd6-d2dd259ec86e", "kubernetes.io/metadata.name":"namespaces-1395", "namespaces-1395":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jun 24 12:48:17.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1395" for this suite. @ 06/24/23 12:48:17.402
• [0.058 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 06/24/23 12:48:17.412
  Jun 24 12:48:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:48:17.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:17.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:17.439
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:48:17.444
  E0624 12:48:18.198176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:19.198318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:20.198426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:21.199181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:48:21.469
  Jun 24 12:48:21.473: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-e85262c2-7a63-4db4-b5ae-85e9cfbc2e98 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:48:21.48
  Jun 24 12:48:21.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9909" for this suite. @ 06/24/23 12:48:21.504
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 06/24/23 12:48:21.516
  Jun 24 12:48:21.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 12:48:21.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:21.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:21.544
  STEP: Creating simple DaemonSet "daemon-set" @ 06/24/23 12:48:21.575
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 12:48:21.583
  Jun 24 12:48:21.588: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:21.588: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:21.592: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:48:21.592: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 12:48:22.199901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:22.597: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:22.597: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:22.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 12:48:22.602: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 12:48:23.202378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:23.599: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:23.599: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 12:48:23.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 12:48:23.603: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 06/24/23 12:48:23.607
  Jun 24 12:48:23.611: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 06/24/23 12:48:23.611
  Jun 24 12:48:23.621: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 06/24/23 12:48:23.622
  Jun 24 12:48:23.624: INFO: Observed &DaemonSet event: ADDED
  Jun 24 12:48:23.624: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.625: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.625: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.625: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.625: INFO: Found daemon set daemon-set in namespace daemonsets-2056 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 24 12:48:23.625: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 06/24/23 12:48:23.625
  STEP: watching for the daemon set status to be patched @ 06/24/23 12:48:23.634
  Jun 24 12:48:23.636: INFO: Observed &DaemonSet event: ADDED
  Jun 24 12:48:23.637: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.637: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.638: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.638: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.638: INFO: Observed daemon set daemon-set in namespace daemonsets-2056 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 24 12:48:23.638: INFO: Observed &DaemonSet event: MODIFIED
  Jun 24 12:48:23.638: INFO: Found daemon set daemon-set in namespace daemonsets-2056 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jun 24 12:48:23.638: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 12:48:23.643
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2056, will wait for the garbage collector to delete the pods @ 06/24/23 12:48:23.643
  Jun 24 12:48:23.708: INFO: Deleting DaemonSet.extensions daemon-set took: 10.319951ms
  Jun 24 12:48:23.809: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.964483ms
  E0624 12:48:24.203188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:25.203696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:25.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 12:48:25.615: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 12:48:25.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26567"},"items":null}

  Jun 24 12:48:25.623: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26567"},"items":null}

  Jun 24 12:48:25.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2056" for this suite. @ 06/24/23 12:48:25.647
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 06/24/23 12:48:25.658
  Jun 24 12:48:25.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:48:25.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:25.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:25.685
  Jun 24 12:48:25.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:48:26.203941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/24/23 12:48:27.163
  Jun 24 12:48:27.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-3513 --namespace=crd-publish-openapi-3513 create -f -'
  E0624 12:48:27.204196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:27.896: INFO: stderr: ""
  Jun 24 12:48:27.896: INFO: stdout: "e2e-test-crd-publish-openapi-5216-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 24 12:48:27.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-3513 --namespace=crd-publish-openapi-3513 delete e2e-test-crd-publish-openapi-5216-crds test-cr'
  Jun 24 12:48:27.992: INFO: stderr: ""
  Jun 24 12:48:27.992: INFO: stdout: "e2e-test-crd-publish-openapi-5216-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jun 24 12:48:27.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-3513 --namespace=crd-publish-openapi-3513 apply -f -'
  E0624 12:48:28.204309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:28.823: INFO: stderr: ""
  Jun 24 12:48:28.824: INFO: stdout: "e2e-test-crd-publish-openapi-5216-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun 24 12:48:28.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-3513 --namespace=crd-publish-openapi-3513 delete e2e-test-crd-publish-openapi-5216-crds test-cr'
  Jun 24 12:48:28.911: INFO: stderr: ""
  Jun 24 12:48:28.911: INFO: stdout: "e2e-test-crd-publish-openapi-5216-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/24/23 12:48:28.911
  Jun 24 12:48:28.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-3513 explain e2e-test-crd-publish-openapi-5216-crds'
  Jun 24 12:48:29.150: INFO: stderr: ""
  Jun 24 12:48:29.150: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-5216-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0624 12:48:29.204360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:30.205397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:31.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3513" for this suite. @ 06/24/23 12:48:31.188
• [5.538 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 06/24/23 12:48:31.199
  Jun 24 12:48:31.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 12:48:31.2
  E0624 12:48:31.206164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:48:31.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:48:31.235
  STEP: Creating service test in namespace statefulset-2922 @ 06/24/23 12:48:31.241
  STEP: Creating stateful set ss in namespace statefulset-2922 @ 06/24/23 12:48:31.25
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2922 @ 06/24/23 12:48:31.26
  Jun 24 12:48:31.265: INFO: Found 0 stateful pods, waiting for 1
  E0624 12:48:32.207047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:33.207183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:34.207266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:35.207368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:36.207733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:37.208476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:38.208582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:39.208708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:40.208817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:41.208927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:41.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 06/24/23 12:48:41.272
  Jun 24 12:48:41.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:48:41.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:48:41.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:48:41.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:48:41.447: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0624 12:48:42.209849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:43.209894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:44.210074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:45.210295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:46.211255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:47.211672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:48.212756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:49.213666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:50.213836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:48:51.213984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:51.453: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:48:51.453: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:48:51.473: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun 24 12:48:51.473: INFO: ss-0  ip-172-31-19-205  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC  }]
  Jun 24 12:48:51.473: INFO: 
  Jun 24 12:48:51.473: INFO: StatefulSet ss has not reached scale 3, at 1
  E0624 12:48:52.214301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:52.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995967108s
  E0624 12:48:53.215158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:53.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991130186s
  E0624 12:48:54.216212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:54.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98499543s
  E0624 12:48:55.216792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:55.494: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97943476s
  E0624 12:48:56.216866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:56.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974384826s
  E0624 12:48:57.216964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:57.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.968392571s
  E0624 12:48:58.217120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:58.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962001652s
  E0624 12:48:59.217705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:48:59.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951506288s
  E0624 12:49:00.217784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:00.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.408278ms
  E0624 12:49:01.217898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2922 @ 06/24/23 12:49:01.529
  Jun 24 12:49:01.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 12:49:01.695: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 12:49:01.695: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:49:01.695: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 12:49:01.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 12:49:01.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 24 12:49:01.857: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:49:01.857: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 12:49:01.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 12:49:02.033: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun 24 12:49:02.033: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:49:02.033: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 12:49:02.038: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0624 12:49:02.218929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:03.219152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:04.219253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:05.219391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:06.219487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:07.220287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:08.220382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:09.220525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:10.220665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:11.220844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:12.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:49:12.044: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:49:12.044: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 06/24/23 12:49:12.044
  Jun 24 12:49:12.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:49:12.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:49:12.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:49:12.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:49:12.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0624 12:49:12.221258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:12.366: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:49:12.366: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:49:12.366: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:49:12.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-2922 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:49:12.526: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:49:12.526: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:49:12.526: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:49:12.526: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:49:12.531: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0624 12:49:13.221347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:14.221431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:15.221854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:16.221949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:17.222059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:18.222162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:19.222273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:20.222365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:21.222475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:22.222519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:22.541: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:49:22.541: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:49:22.541: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:49:22.556: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun 24 12:49:22.556: INFO: ss-0  ip-172-31-19-205  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC  }]
  Jun 24 12:49:22.556: INFO: ss-1  ip-172-31-15-136  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  }]
  Jun 24 12:49:22.556: INFO: ss-2  ip-172-31-89-202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  }]
  Jun 24 12:49:22.556: INFO: 
  Jun 24 12:49:22.556: INFO: StatefulSet ss has not reached scale 0, at 3
  E0624 12:49:23.222614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:23.561: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jun 24 12:49:23.561: INFO: ss-0  ip-172-31-19-205  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:31 +0000 UTC  }]
  Jun 24 12:49:23.561: INFO: ss-1  ip-172-31-15-136  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  }]
  Jun 24 12:49:23.561: INFO: ss-2  ip-172-31-89-202  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:49:12 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-24 12:48:51 +0000 UTC  }]
  Jun 24 12:49:23.561: INFO: 
  Jun 24 12:49:23.561: INFO: StatefulSet ss has not reached scale 0, at 3
  E0624 12:49:24.222715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:24.566: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990562817s
  E0624 12:49:25.222828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:25.571: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.985210171s
  E0624 12:49:26.222953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:26.575: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98115049s
  E0624 12:49:27.224050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:27.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976005716s
  E0624 12:49:28.224788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:28.585: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971108396s
  E0624 12:49:29.225743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:29.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966555849s
  E0624 12:49:30.225854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:30.596: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961173662s
  E0624 12:49:31.225961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:31.600: INFO: Verifying statefulset ss doesn't scale past 0 for another 956.030749ms
  E0624 12:49:32.226535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2922 @ 06/24/23 12:49:32.6
  Jun 24 12:49:32.605: INFO: Scaling statefulset ss to 0
  Jun 24 12:49:32.619: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:49:32.622: INFO: Deleting all statefulset in ns statefulset-2922
  Jun 24 12:49:32.627: INFO: Scaling statefulset ss to 0
  Jun 24 12:49:32.640: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:49:32.644: INFO: Deleting statefulset ss
  Jun 24 12:49:32.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2922" for this suite. @ 06/24/23 12:49:32.664
• [61.474 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 06/24/23 12:49:32.673
  Jun 24 12:49:32.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:49:32.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:49:32.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:49:32.699
  STEP: Creating projection with secret that has name projected-secret-test-map-f48b028a-c457-4021-a45b-a70e210fddeb @ 06/24/23 12:49:32.705
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:49:32.712
  E0624 12:49:33.226660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:34.226769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:35.227635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:36.227705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:49:36.738
  Jun 24 12:49:36.742: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-65806489-aafc-461c-a1a6-1cd6c3dc6d49 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:49:36.75
  Jun 24 12:49:36.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8794" for this suite. @ 06/24/23 12:49:36.771
• [4.106 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 06/24/23 12:49:36.781
  Jun 24 12:49:36.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:49:36.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:49:36.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:49:36.806
  STEP: Setting up server cert @ 06/24/23 12:49:36.838
  E0624 12:49:37.228289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:49:37.487
  STEP: Deploying the webhook pod @ 06/24/23 12:49:37.497
  STEP: Wait for the deployment to be ready @ 06/24/23 12:49:37.512
  Jun 24 12:49:37.519: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0624 12:49:38.228409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:39.228550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 12:49:39.533
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:49:39.547
  E0624 12:49:40.228699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:40.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 06/24/23 12:49:40.552
  STEP: create a pod that should be updated by the webhook @ 06/24/23 12:49:40.572
  Jun 24 12:49:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4111" for this suite. @ 06/24/23 12:49:40.659
  STEP: Destroying namespace "webhook-markers-7540" for this suite. @ 06/24/23 12:49:40.667
• [3.894 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 06/24/23 12:49:40.678
  Jun 24 12:49:40.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename prestop @ 06/24/23 12:49:40.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:49:40.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:49:40.705
  STEP: Creating server pod server in namespace prestop-5880 @ 06/24/23 12:49:40.71
  STEP: Waiting for pods to come up. @ 06/24/23 12:49:40.72
  E0624 12:49:41.229514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:42.229868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-5880 @ 06/24/23 12:49:42.734
  E0624 12:49:43.230083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:44.230071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 06/24/23 12:49:44.75
  E0624 12:49:45.230227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:46.230248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:47.230396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:48.230987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:49.231154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:49:49.768: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jun 24 12:49:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 06/24/23 12:49:49.773
  STEP: Destroying namespace "prestop-5880" for this suite. @ 06/24/23 12:49:49.79
• [9.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 06/24/23 12:49:49.801
  Jun 24 12:49:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:49:49.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:49:49.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:49:49.829
  STEP: Discovering how many secrets are in namespace by default @ 06/24/23 12:49:49.834
  E0624 12:49:50.231742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:51.232782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:52.233370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:53.233462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:54.233553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/24/23 12:49:54.839
  E0624 12:49:55.234525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:56.234659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:57.234711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:58.235163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:49:59.236038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 12:49:59.844
  STEP: Ensuring resource quota status is calculated @ 06/24/23 12:49:59.854
  E0624 12:50:00.236797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:01.237000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 06/24/23 12:50:01.858
  STEP: Ensuring resource quota status captures secret creation @ 06/24/23 12:50:01.876
  E0624 12:50:02.237386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:03.237491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 06/24/23 12:50:03.883
  STEP: Ensuring resource quota status released usage @ 06/24/23 12:50:03.89
  E0624 12:50:04.238428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:05.238474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:50:05.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6852" for this suite. @ 06/24/23 12:50:05.9
• [16.106 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 06/24/23 12:50:05.908
  Jun 24 12:50:05.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:50:05.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:05.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:05.937
  STEP: Creating a ResourceQuota @ 06/24/23 12:50:05.942
  STEP: Getting a ResourceQuota @ 06/24/23 12:50:05.948
  STEP: Updating a ResourceQuota @ 06/24/23 12:50:05.951
  STEP: Verifying a ResourceQuota was modified @ 06/24/23 12:50:05.958
  STEP: Deleting a ResourceQuota @ 06/24/23 12:50:05.964
  STEP: Verifying the deleted ResourceQuota @ 06/24/23 12:50:05.97
  Jun 24 12:50:05.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-434" for this suite. @ 06/24/23 12:50:05.979
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 06/24/23 12:50:05.987
  Jun 24 12:50:05.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subpath @ 06/24/23 12:50:05.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:06.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:06.013
  STEP: Setting up data @ 06/24/23 12:50:06.018
  STEP: Creating pod pod-subpath-test-projected-tg9d @ 06/24/23 12:50:06.029
  STEP: Creating a pod to test atomic-volume-subpath @ 06/24/23 12:50:06.029
  E0624 12:50:06.239071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:07.239148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:08.240177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:09.240719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:10.240812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:11.241179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:12.241249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:13.241408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:14.242280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:15.243040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:16.243061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:17.243152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:18.243636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:19.244107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:20.245108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:21.245228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:22.245323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:23.245435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:24.245876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:25.246036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:26.246887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:27.247045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:28.247437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:29.247733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:50:30.105
  Jun 24 12:50:30.110: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-subpath-test-projected-tg9d container test-container-subpath-projected-tg9d: <nil>
  STEP: delete the pod @ 06/24/23 12:50:30.122
  STEP: Deleting pod pod-subpath-test-projected-tg9d @ 06/24/23 12:50:30.138
  Jun 24 12:50:30.138: INFO: Deleting pod "pod-subpath-test-projected-tg9d" in namespace "subpath-1626"
  Jun 24 12:50:30.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1626" for this suite. @ 06/24/23 12:50:30.147
• [24.167 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 06/24/23 12:50:30.155
  Jun 24 12:50:30.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename events @ 06/24/23 12:50:30.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:30.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:30.18
  STEP: creating a test event @ 06/24/23 12:50:30.186
  STEP: listing all events in all namespaces @ 06/24/23 12:50:30.195
  STEP: patching the test event @ 06/24/23 12:50:30.207
  STEP: fetching the test event @ 06/24/23 12:50:30.216
  STEP: updating the test event @ 06/24/23 12:50:30.221
  STEP: getting the test event @ 06/24/23 12:50:30.233
  STEP: deleting the test event @ 06/24/23 12:50:30.237
  STEP: listing all events in all namespaces @ 06/24/23 12:50:30.245
  E0624 12:50:30.248731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:50:30.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-908" for this suite. @ 06/24/23 12:50:30.261
• [0.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 06/24/23 12:50:30.268
  Jun 24 12:50:30.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 12:50:30.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:30.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:30.295
  STEP: Creating a test headless service @ 06/24/23 12:50:30.3
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6453.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6453.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 06/24/23 12:50:30.304
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6453.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6453.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 06/24/23 12:50:30.304
  STEP: creating a pod to probe DNS @ 06/24/23 12:50:30.304
  STEP: submitting the pod to kubernetes @ 06/24/23 12:50:30.305
  E0624 12:50:31.248871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:32.249005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 12:50:32.329
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:50:32.334
  Jun 24 12:50:32.352: INFO: DNS probes using dns-6453/dns-test-a3fb300c-6577-4b90-b752-645bf860b8cc succeeded

  Jun 24 12:50:32.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:50:32.358
  STEP: deleting the test headless service @ 06/24/23 12:50:32.37
  STEP: Destroying namespace "dns-6453" for this suite. @ 06/24/23 12:50:32.386
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 06/24/23 12:50:32.399
  Jun 24 12:50:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 12:50:32.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:32.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:32.425
  STEP: Creating Indexed job @ 06/24/23 12:50:32.43
  STEP: Ensuring job reaches completions @ 06/24/23 12:50:32.438
  E0624 12:50:33.249128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:34.249595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:35.249732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:36.250042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:37.250969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:38.251874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:39.251999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:40.252112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 06/24/23 12:50:40.444
  Jun 24 12:50:40.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3296" for this suite. @ 06/24/23 12:50:40.453
• [8.061 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 06/24/23 12:50:40.461
  Jun 24 12:50:40.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename endpointslice @ 06/24/23 12:50:40.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:40.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:40.49
  Jun 24 12:50:40.506: INFO: Endpoints addresses: [172.31.15.72 172.31.26.147] , ports: [6443]
  Jun 24 12:50:40.506: INFO: EndpointSlices addresses: [172.31.15.72 172.31.26.147] , ports: [6443]
  Jun 24 12:50:40.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8593" for this suite. @ 06/24/23 12:50:40.511
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 06/24/23 12:50:40.521
  Jun 24 12:50:40.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 12:50:40.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:40.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:40.545
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 06/24/23 12:50:40.549
  Jun 24 12:50:40.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:50:41.252161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:42.253100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:43.253245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:44.254018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:45.254762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:46.255321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 06/24/23 12:50:46.808
  Jun 24 12:50:46.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:50:47.256128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:48.256700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:50:48.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 12:50:49.256742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:50.256946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:51.270297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:52.270425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:53.271315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:54.271343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:50:54.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3006" for this suite. @ 06/24/23 12:50:54.333
• [13.820 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 06/24/23 12:50:54.344
  Jun 24 12:50:54.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename proxy @ 06/24/23 12:50:54.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:54.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:54.372
  Jun 24 12:50:54.377: INFO: Creating pod...
  E0624 12:50:55.271726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:56.272815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:50:56.398: INFO: Creating service...
  Jun 24 12:50:56.421: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/DELETE
  Jun 24 12:50:56.429: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 24 12:50:56.429: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/GET
  Jun 24 12:50:56.434: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 24 12:50:56.434: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/HEAD
  Jun 24 12:50:56.438: INFO: http.Client request:HEAD | StatusCode:200
  Jun 24 12:50:56.438: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/OPTIONS
  Jun 24 12:50:56.443: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 24 12:50:56.443: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/PATCH
  Jun 24 12:50:56.448: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 24 12:50:56.448: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/POST
  Jun 24 12:50:56.454: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 24 12:50:56.455: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/pods/agnhost/proxy/some/path/with/PUT
  Jun 24 12:50:56.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 24 12:50:56.459: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/DELETE
  Jun 24 12:50:56.465: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun 24 12:50:56.465: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/GET
  Jun 24 12:50:56.472: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun 24 12:50:56.472: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/HEAD
  Jun 24 12:50:56.479: INFO: http.Client request:HEAD | StatusCode:200
  Jun 24 12:50:56.479: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/OPTIONS
  Jun 24 12:50:56.485: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun 24 12:50:56.485: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/PATCH
  Jun 24 12:50:56.491: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun 24 12:50:56.491: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/POST
  Jun 24 12:50:56.498: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun 24 12:50:56.498: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-751/services/test-service/proxy/some/path/with/PUT
  Jun 24 12:50:56.504: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun 24 12:50:56.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-751" for this suite. @ 06/24/23 12:50:56.511
• [2.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 06/24/23 12:50:56.526
  Jun 24 12:50:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:50:56.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:50:56.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:50:56.551
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 12:50:56.555
  E0624 12:50:57.273413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:58.273467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:50:59.273592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:00.273758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:51:00.585
  Jun 24 12:51:00.589: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-2681bbf7-d8ca-49f6-9dfa-42f7ec8d2ece container client-container: <nil>
  STEP: delete the pod @ 06/24/23 12:51:00.598
  Jun 24 12:51:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2530" for this suite. @ 06/24/23 12:51:00.617
• [4.098 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 06/24/23 12:51:00.625
  Jun 24 12:51:00.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 12:51:00.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:51:00.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:51:00.664
  STEP: Creating a suspended job @ 06/24/23 12:51:00.671
  STEP: Patching the Job @ 06/24/23 12:51:00.677
  STEP: Watching for Job to be patched @ 06/24/23 12:51:00.695
  Jun 24 12:51:00.698: INFO: Event ADDED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun 24 12:51:00.698: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun 24 12:51:00.698: INFO: Event MODIFIED found for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 06/24/23 12:51:00.698
  STEP: Watching for Job to be updated @ 06/24/23 12:51:00.709
  Jun 24 12:51:00.711: INFO: Event MODIFIED found for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:00.711: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 06/24/23 12:51:00.711
  Jun 24 12:51:00.716: INFO: Job: e2e-7mjvd as labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd]
  STEP: Waiting for job to complete @ 06/24/23 12:51:00.716
  E0624 12:51:01.273831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:02.274429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:03.274573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:04.274683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:05.274957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:06.275091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:07.275212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:08.275326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 06/24/23 12:51:08.72
  STEP: Watching for Job to be deleted @ 06/24/23 12:51:08.729
  Jun 24 12:51:08.731: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:08.731: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:08.731: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:08.732: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:08.732: INFO: Event MODIFIED observed for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun 24 12:51:08.732: INFO: Event DELETED found for Job e2e-7mjvd in namespace job-5904 with labels: map[e2e-7mjvd:patched e2e-job-label:e2e-7mjvd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 06/24/23 12:51:08.732
  Jun 24 12:51:08.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5904" for this suite. @ 06/24/23 12:51:08.739
• [8.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 06/24/23 12:51:08.765
  Jun 24 12:51:08.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-pred @ 06/24/23 12:51:08.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:51:08.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:51:08.792
  Jun 24 12:51:08.796: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 24 12:51:08.807: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 12:51:08.811: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-136 before test
  Jun 24 12:51:08.817: INFO: nginx-ingress-controller-kubernetes-worker-9295m from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:39 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.817: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:51:08.817: INFO: calico-kube-controllers-69b5565d84-5wz9f from kube-system started at 2023-06-24 12:02:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.817: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 24 12:51:08.818: INFO: sonobuoy-e2e-job-a6e2d2ee4f8b45eb from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:51:08.818: INFO: 	Container e2e ready: true, restart count 0
  Jun 24 12:51:08.818: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:51:08.818: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57 from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:51:08.818: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:51:08.818: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:51:08.818: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-205 before test
  Jun 24 12:51:08.824: INFO: nginx-ingress-controller-kubernetes-worker-6vrl7 from ingress-nginx-kubernetes-worker started at 2023-06-24 12:03:18 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.825: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:51:08.825: INFO: sonobuoy from sonobuoy started at 2023-06-24 12:08:47 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.825: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 24 12:51:08.825: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-2vcbs from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:51:08.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:51:08.825: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 12:51:08.825: INFO: webhook-to-be-mutated from webhook-4111 started at 2023-06-24 12:49:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.825: INFO: 	Container example ready: false, restart count 0
  Jun 24 12:51:08.825: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-202 before test
  Jun 24 12:51:08.832: INFO: default-http-backend-kubernetes-worker-65fc475d49-cs8f9 from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.832: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 24 12:51:08.832: INFO: nginx-ingress-controller-kubernetes-worker-bvt8s from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.832: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 12:51:08.832: INFO: coredns-5c7f76ccb8-d25lj from kube-system started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.832: INFO: 	Container coredns ready: true, restart count 0
  Jun 24 12:51:08.832: INFO: kube-state-metrics-5b95b4459c-pcn82 from kube-system started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.833: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: metrics-server-v0.5.2-6cf8c8b69c-m28bd from kube-system started at 2023-06-24 11:54:03 +0000 UTC (2 container statuses recorded)
  Jun 24 12:51:08.833: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: dashboard-metrics-scraper-6b8586b5c9-c7tbk from kubernetes-dashboard started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.833: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: kubernetes-dashboard-6869f4cd5f-6rqxs from kubernetes-dashboard started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 12:51:08.833: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-ldg4s from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 12:51:08.833: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 12:51:08.833: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/24/23 12:51:08.833
  E0624 12:51:09.275903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:10.276772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/24/23 12:51:10.856
  STEP: Trying to apply a random label on the found node. @ 06/24/23 12:51:10.872
  STEP: verifying the node has the label kubernetes.io/e2e-1eaabc36-49ba-4778-a57c-9582eed21c62 95 @ 06/24/23 12:51:10.884
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 06/24/23 12:51:10.887
  E0624 12:51:11.277415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:12.277584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:13.278224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:14.278361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.19.205 on the node which pod4 resides and expect not scheduled @ 06/24/23 12:51:14.914
  E0624 12:51:15.279044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:16.279167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:17.279272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:18.279499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:19.280229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:20.280800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:21.281195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:22.281746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:23.281837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:24.281935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:25.283060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:26.283159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:27.283251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:28.283360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:29.284127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:30.284230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:31.284648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:32.285369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:33.285508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:34.285604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:35.286388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:36.286506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:37.286948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:38.287084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:39.287220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:40.287652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:41.288429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:42.288748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:43.289575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:44.289662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:45.289856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:46.290418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:47.290852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:48.291102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:49.291205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:50.291327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:51.291943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:52.292770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:53.293138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:54.293240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:55.293810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:56.293890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:57.294552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:58.294612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:51:59.294800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:00.294922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:01.295028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:02.295656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:03.296583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:04.297045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:05.297136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:06.297409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:07.297513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:08.297642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:09.298101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:10.298221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:11.299006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:12.299638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:13.300767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:14.300846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:15.301634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:16.301720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:17.302165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:18.302261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:19.302338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:20.302437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:21.302810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:22.302927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:23.303430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:24.303528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:25.304197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:26.304338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:27.304770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:28.304859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:29.305847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:30.305956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:31.306831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:32.306910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:33.307659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:34.307764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:35.308582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:36.309417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:37.309771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:38.309874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:39.310935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:40.311128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:41.311419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:42.311449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:43.311798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:44.311961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:45.312385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:46.312649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:47.313500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:48.313770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:49.314386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:50.314750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:51.315326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:52.315421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:53.316393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:54.317302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:55.317859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:56.317964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:57.318995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:58.319130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:52:59.319805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:00.319908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:01.320211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:02.320310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:03.321277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:04.321405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:05.322142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:06.322444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:07.322717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:08.323103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:09.324158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:10.324256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:11.325328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:12.325429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:13.325861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:14.325989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:15.326923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:16.327028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:17.327923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:18.328041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:19.328746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:20.328851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:21.329887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:22.330538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:23.331246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:24.331400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:25.331915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:26.332021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:27.332156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:28.332790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:29.333539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:30.334367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:31.335334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:32.335445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:33.336150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:34.336364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:35.336471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:36.336554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:37.336934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:38.337048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:39.337280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:40.337615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:41.338614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:42.339679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:43.339966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:44.340077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:45.340660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:46.340823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:47.341888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:48.342058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:49.342146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:50.342511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:51.343503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:52.343638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:53.344040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:54.344149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:55.344672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:56.344818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:57.345848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:58.345932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:53:59.346758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:00.346889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:01.347644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:02.348807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:03.349351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:04.349472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:05.350342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:06.350475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:07.351273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:08.351997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:09.352210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:10.352915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:11.353120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:12.353253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:13.353348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:14.353458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:15.353571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:16.353709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:17.354417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:18.355042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:19.355147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:20.355252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:21.355370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:22.356229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:23.356876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:24.357753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:25.357883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:26.358875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:27.359525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:28.359731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:29.360870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:30.361290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:31.361585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:32.361657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:33.361743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:34.361846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:35.362217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:36.362347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:37.362462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:38.362625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:39.362676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:40.362782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:41.362902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:42.363673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:43.363717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:44.363845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:45.364866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:46.364990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:47.365122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:48.365234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:49.365586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:50.365920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:51.366025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:52.366796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:53.366909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:54.367573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:55.367206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:56.367692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:57.368736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:58.368830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:54:59.368936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:00.369186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:01.369302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:02.369392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:03.369496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:04.369593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:05.369693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:06.370220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:07.370530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:08.370891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:09.371104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:10.371206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:11.371528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:12.371991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:13.373018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:14.373131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:15.373252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:16.373351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:17.373567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:18.374624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:19.374942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:20.375057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:21.375178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:22.376333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:23.376754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:24.377361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:25.377624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:26.378300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:27.378963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:28.379027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:29.379118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:30.379233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:31.379337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:32.379438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:33.379740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:34.379840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:35.379946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:36.380052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:37.380101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:38.380454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:39.380687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:40.380890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:41.380988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:42.381993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:43.382289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:44.382421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:45.382719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:46.382866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:47.383315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:48.383677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:49.384901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:50.384969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:51.385239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:52.386012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:53.386151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:54.387090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:55.387192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:56.387682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:57.388536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:58.388791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:55:59.389051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:00.389132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:01.389466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:02.389654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:03.389746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:04.389801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:05.389890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:06.389975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:07.390624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:08.391272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:09.391375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:10.391703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:11.392881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:12.393118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:13.393445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:14.394447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-1eaabc36-49ba-4778-a57c-9582eed21c62 off the node ip-172-31-19-205 @ 06/24/23 12:56:14.923
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-1eaabc36-49ba-4778-a57c-9582eed21c62 @ 06/24/23 12:56:14.937
  Jun 24 12:56:14.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6519" for this suite. @ 06/24/23 12:56:14.946
• [306.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 06/24/23 12:56:14.955
  Jun 24 12:56:14.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 12:56:14.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:56:14.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:56:14.981
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-0d33b985-b716-4a13-a8fa-6d3cc9b47cc1 @ 06/24/23 12:56:14.99
  STEP: Creating the pod @ 06/24/23 12:56:14.995
  E0624 12:56:15.395434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:16.395692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-0d33b985-b716-4a13-a8fa-6d3cc9b47cc1 @ 06/24/23 12:56:17.043
  STEP: waiting to observe update in volume @ 06/24/23 12:56:17.049
  E0624 12:56:17.395844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:18.395960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:19.396890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:20.397016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:21.397116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:22.397231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:23.398124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:24.398260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:25.398358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:26.398507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:27.398562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:28.398735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:29.399594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:30.399791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:31.400228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:32.400357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:33.400552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:34.401108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:35.402083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:36.402219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:37.402579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:38.402795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:39.402938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:40.403664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:41.404328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:42.404467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:43.405284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:44.405398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:45.406025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:46.406122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:47.406981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:48.407170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:49.407410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:50.407649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:51.408197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:52.408299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:53.408402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:54.411279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:55.411763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:56.412908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:57.413297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:58.413525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:56:59.414070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:00.414209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:01.414311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:02.414394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:03.414803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:04.414962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:05.415055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:06.415205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:07.415827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:08.415978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:09.416675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:10.416933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:11.417302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:12.417438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:13.418264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:14.418373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:15.419247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:16.419420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:17.420101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:18.420292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:19.421001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:20.421143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2944" for this suite. @ 06/24/23 12:57:21.368
• [66.421 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 06/24/23 12:57:21.377
  Jun 24 12:57:21.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename podtemplate @ 06/24/23 12:57:21.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:21.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:21.418
  E0624 12:57:21.421643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create a pod template @ 06/24/23 12:57:21.422
  STEP: Replace a pod template @ 06/24/23 12:57:21.429
  Jun 24 12:57:21.437: INFO: Found updated podtemplate annotation: "true"

  Jun 24 12:57:21.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3022" for this suite. @ 06/24/23 12:57:21.442
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 06/24/23 12:57:21.454
  Jun 24 12:57:21.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:57:21.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:21.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:21.48
  STEP: Creating configMap with name configmap-test-volume-ae03f799-4e40-4938-b154-8a087cfa0ef5 @ 06/24/23 12:57:21.484
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:57:21.488
  E0624 12:57:22.422416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:23.422743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:24.423363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:25.423743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:57:25.51
  Jun 24 12:57:25.514: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-2ace847c-b3ff-4115-8064-292a5727f577 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 12:57:25.523
  Jun 24 12:57:25.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9283" for this suite. @ 06/24/23 12:57:25.542
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 06/24/23 12:57:25.555
  Jun 24 12:57:25.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:57:25.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:25.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:25.583
  Jun 24 12:57:25.598: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0624 12:57:26.423826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:27.424664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:28.424986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:29.425269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:30.425648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:30.603: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 12:57:30.603
  Jun 24 12:57:30.603: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 06/24/23 12:57:30.614
  Jun 24 12:57:30.629: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2889  09f9b482-cbb1-4d1d-a0e0-ecf16ace80b6 28765 1 2023-06-24 12:57:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-24 12:57:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021d1d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 24 12:57:30.633: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jun 24 12:57:30.633: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jun 24 12:57:30.633: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2889  abe0dce0-2a07-426c-b90b-4ab847546cdf 28768 1 2023-06-24 12:57:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 09f9b482-cbb1-4d1d-a0e0-ecf16ace80b6 0xc005d82c97 0xc005d82c98}] [] [{e2e.test Update apps/v1 2023-06-24 12:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:57:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-24 12:57:30 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"09f9b482-cbb1-4d1d-a0e0-ecf16ace80b6\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d82d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:57:30.638: INFO: Pod "test-cleanup-controller-kv79q" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-kv79q test-cleanup-controller- deployment-2889  8a57562f-c4eb-43a1-9868-6f621e36c609 28743 0 2023-06-24 12:57:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller abe0dce0-2a07-426c-b90b-4ab847546cdf 0xc005d83057 0xc005d83058}] [] [{kube-controller-manager Update v1 2023-06-24 12:57:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abe0dce0-2a07-426c-b90b-4ab847546cdf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:57:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjk9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjk9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.240,StartTime:2023-06-24 12:57:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 12:57:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a6ce0e6c2ded27e371c0dbc0bed1e05da0024357816e7a6c063853a89ecb5cd8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.240,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:57:30.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2889" for this suite. @ 06/24/23 12:57:30.642
• [5.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 06/24/23 12:57:30.656
  Jun 24 12:57:30.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:57:30.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:30.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:30.69
  STEP: Create set of pods @ 06/24/23 12:57:30.693
  Jun 24 12:57:30.704: INFO: created test-pod-1
  Jun 24 12:57:30.711: INFO: created test-pod-2
  Jun 24 12:57:30.717: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 06/24/23 12:57:30.717
  E0624 12:57:31.425827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:32.425924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 06/24/23 12:57:32.765
  Jun 24 12:57:32.769: INFO: Pod quantity 3 is different from expected quantity 0
  E0624 12:57:33.425962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:33.774: INFO: Pod quantity 2 is different from expected quantity 0
  E0624 12:57:34.426061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:34.774: INFO: Pod quantity 2 is different from expected quantity 0
  E0624 12:57:35.426162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:35.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3896" for this suite. @ 06/24/23 12:57:35.782
• [5.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 06/24/23 12:57:35.793
  Jun 24 12:57:35.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:57:35.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:35.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:35.824
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/24/23 12:57:35.829
  Jun 24 12:57:35.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-3558 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun 24 12:57:35.913: INFO: stderr: ""
  Jun 24 12:57:35.913: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 06/24/23 12:57:35.913
  Jun 24 12:57:35.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-3558 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jun 24 12:57:36.010: INFO: stderr: ""
  Jun 24 12:57:36.010: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/24/23 12:57:36.01
  Jun 24 12:57:36.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-3558 delete pods e2e-test-httpd-pod'
  E0624 12:57:36.426781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:37.427615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:38.427722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:38.729: INFO: stderr: ""
  Jun 24 12:57:38.729: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 24 12:57:38.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3558" for this suite. @ 06/24/23 12:57:38.734
• [2.947 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 06/24/23 12:57:38.742
  Jun 24 12:57:38.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/24/23 12:57:38.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:38.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:38.768
  STEP: create the container to handle the HTTPGet hook request. @ 06/24/23 12:57:38.777
  E0624 12:57:39.427834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:40.427945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/24/23 12:57:40.801
  E0624 12:57:41.428855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:42.428973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 06/24/23 12:57:42.82
  STEP: delete the pod with lifecycle hook @ 06/24/23 12:57:42.844
  E0624 12:57:43.429095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:44.429208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:44.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3389" for this suite. @ 06/24/23 12:57:44.869
• [6.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 06/24/23 12:57:44.877
  Jun 24 12:57:44.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 12:57:44.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:44.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:44.903
  Jun 24 12:57:44.907: INFO: Creating deployment "test-recreate-deployment"
  Jun 24 12:57:44.920: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jun 24 12:57:44.928: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0624 12:57:45.429667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:46.429868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:57:46.937: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jun 24 12:57:46.940: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jun 24 12:57:46.951: INFO: Updating deployment test-recreate-deployment
  Jun 24 12:57:46.952: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jun 24 12:57:47.051: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-962  229df66a-91b1-45f2-a9f2-7cf72c6f45bd 29055 2 2023-06-24 12:57:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-24 12:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:57:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ab698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-24 12:57:47 +0000 UTC,LastTransitionTime:2023-06-24 12:57:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-06-24 12:57:47 +0000 UTC,LastTransitionTime:2023-06-24 12:57:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jun 24 12:57:47.056: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-962  8df5e3e1-78ff-4447-a5e9-ef18276bcf1f 29052 1 2023-06-24 12:57:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 229df66a-91b1-45f2-a9f2-7cf72c6f45bd 0xc0040bfa47 0xc0040bfa48}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"229df66a-91b1-45f2-a9f2-7cf72c6f45bd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:57:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040bfc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:57:47.056: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jun 24 12:57:47.056: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-962  f46306a1-0c52-4e67-8725-139178035442 29043 2 2023-06-24 12:57:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 229df66a-91b1-45f2-a9f2-7cf72c6f45bd 0xc0040bfd47 0xc0040bfd48}] [] [{kube-controller-manager Update apps/v1 2023-06-24 12:57:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"229df66a-91b1-45f2-a9f2-7cf72c6f45bd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 12:57:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040bff08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 12:57:47.060: INFO: Pod "test-recreate-deployment-54757ffd6c-zcsj2" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-zcsj2 test-recreate-deployment-54757ffd6c- deployment-962  f2e5bfba-532c-412a-99b6-529f30919ccd 29053 0 2023-06-24 12:57:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 8df5e3e1-78ff-4447-a5e9-ef18276bcf1f 0xc0040acf57 0xc0040acf58}] [] [{kube-controller-manager Update v1 2023-06-24 12:57:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8df5e3e1-78ff-4447-a5e9-ef18276bcf1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 12:57:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrd86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrd86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 12:57:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:,StartTime:2023-06-24 12:57:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 12:57:47.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-962" for this suite. @ 06/24/23 12:57:47.066
• [2.195 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 06/24/23 12:57:47.076
  Jun 24 12:57:47.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 12:57:47.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:47.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:47.102
  STEP: Creating secret with name secret-test-d680fb82-3258-4bd9-99a7-43ab74c37579 @ 06/24/23 12:57:47.11
  STEP: Creating a pod to test consume secrets @ 06/24/23 12:57:47.117
  E0624 12:57:47.430553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:48.430732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:49.430849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:50.431860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:57:51.149
  Jun 24 12:57:51.153: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-1a0d08dd-6df1-42ad-95d3-166efa2f7c18 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 12:57:51.161
  Jun 24 12:57:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9318" for this suite. @ 06/24/23 12:57:51.186
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 06/24/23 12:57:51.198
  Jun 24 12:57:51.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/24/23 12:57:51.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:51.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:51.224
  STEP: create the container to handle the HTTPGet hook request. @ 06/24/23 12:57:51.236
  E0624 12:57:51.432219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:52.432326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/24/23 12:57:53.264
  E0624 12:57:53.432882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:54.433012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 06/24/23 12:57:55.286
  E0624 12:57:55.433468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:56.433629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 06/24/23 12:57:57.303
  Jun 24 12:57:57.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8712" for this suite. @ 06/24/23 12:57:57.317
• [6.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 06/24/23 12:57:57.325
  Jun 24 12:57:57.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 12:57:57.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:57.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:57.354
  Jun 24 12:57:57.361: INFO: Got root ca configmap in namespace "svcaccounts-5423"
  Jun 24 12:57:57.367: INFO: Deleted root ca configmap in namespace "svcaccounts-5423"
  E0624 12:57:57.434180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 06/24/23 12:57:57.869
  Jun 24 12:57:57.872: INFO: Recreated root ca configmap in namespace "svcaccounts-5423"
  Jun 24 12:57:57.878: INFO: Updated root ca configmap in namespace "svcaccounts-5423"
  STEP: waiting for the root ca configmap reconciled @ 06/24/23 12:57:58.379
  Jun 24 12:57:58.383: INFO: Reconciled root ca configmap in namespace "svcaccounts-5423"
  Jun 24 12:57:58.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5423" for this suite. @ 06/24/23 12:57:58.388
• [1.071 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 06/24/23 12:57:58.397
  Jun 24 12:57:58.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 12:57:58.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:57:58.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:57:58.422
  STEP: Creating configMap with name configmap-test-volume-map-a063a01d-ae80-4f66-a4b6-95bcd206709a @ 06/24/23 12:57:58.426
  STEP: Creating a pod to test consume configMaps @ 06/24/23 12:57:58.432
  E0624 12:57:58.435053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:57:59.435183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:00.435589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:01.435872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:02.436628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 12:58:02.459
  Jun 24 12:58:02.463: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-91b151a1-4928-42b1-9b22-465d04298f35 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 12:58:02.472
  Jun 24 12:58:02.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-614" for this suite. @ 06/24/23 12:58:02.497
• [4.109 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 06/24/23 12:58:02.506
  Jun 24 12:58:02.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename namespaces @ 06/24/23 12:58:02.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:02.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:02.536
  STEP: Read namespace status @ 06/24/23 12:58:02.544
  Jun 24 12:58:02.551: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 06/24/23 12:58:02.551
  Jun 24 12:58:02.560: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 06/24/23 12:58:02.56
  Jun 24 12:58:02.573: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jun 24 12:58:02.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-41" for this suite. @ 06/24/23 12:58:02.578
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 06/24/23 12:58:02.589
  Jun 24 12:58:02.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 12:58:02.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:02.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:02.619
  STEP: creating a Service @ 06/24/23 12:58:02.629
  STEP: watching for the Service to be added @ 06/24/23 12:58:02.642
  Jun 24 12:58:02.646: INFO: Found Service test-service-npgg5 in namespace services-245 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jun 24 12:58:02.646: INFO: Service test-service-npgg5 created
  STEP: Getting /status @ 06/24/23 12:58:02.646
  Jun 24 12:58:02.653: INFO: Service test-service-npgg5 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 06/24/23 12:58:02.653
  STEP: watching for the Service to be patched @ 06/24/23 12:58:02.663
  Jun 24 12:58:02.665: INFO: observed Service test-service-npgg5 in namespace services-245 with annotations: map[] & LoadBalancer: {[]}
  Jun 24 12:58:02.665: INFO: Found Service test-service-npgg5 in namespace services-245 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jun 24 12:58:02.665: INFO: Service test-service-npgg5 has service status patched
  STEP: updating the ServiceStatus @ 06/24/23 12:58:02.665
  Jun 24 12:58:02.680: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 06/24/23 12:58:02.68
  Jun 24 12:58:02.684: INFO: Observed Service test-service-npgg5 in namespace services-245 with annotations: map[] & Conditions: {[]}
  Jun 24 12:58:02.684: INFO: Observed event: &Service{ObjectMeta:{test-service-npgg5  services-245  a8eddcd1-ba8c-4a22-b96f-c66e37d89ce4 29276 0 2023-06-24 12:58:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-24 12:58:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-24 12:58:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.29,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.29],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jun 24 12:58:02.685: INFO: Found Service test-service-npgg5 in namespace services-245 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun 24 12:58:02.685: INFO: Service test-service-npgg5 has service status updated
  STEP: patching the service @ 06/24/23 12:58:02.685
  STEP: watching for the Service to be patched @ 06/24/23 12:58:02.699
  Jun 24 12:58:02.701: INFO: observed Service test-service-npgg5 in namespace services-245 with labels: map[test-service-static:true]
  Jun 24 12:58:02.701: INFO: observed Service test-service-npgg5 in namespace services-245 with labels: map[test-service-static:true]
  Jun 24 12:58:02.701: INFO: observed Service test-service-npgg5 in namespace services-245 with labels: map[test-service-static:true]
  Jun 24 12:58:02.701: INFO: Found Service test-service-npgg5 in namespace services-245 with labels: map[test-service:patched test-service-static:true]
  Jun 24 12:58:02.701: INFO: Service test-service-npgg5 patched
  STEP: deleting the service @ 06/24/23 12:58:02.702
  STEP: watching for the Service to be deleted @ 06/24/23 12:58:02.717
  Jun 24 12:58:02.719: INFO: Observed event: ADDED
  Jun 24 12:58:02.720: INFO: Observed event: MODIFIED
  Jun 24 12:58:02.720: INFO: Observed event: MODIFIED
  Jun 24 12:58:02.720: INFO: Observed event: MODIFIED
  Jun 24 12:58:02.720: INFO: Found Service test-service-npgg5 in namespace services-245 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jun 24 12:58:02.720: INFO: Service test-service-npgg5 deleted
  Jun 24 12:58:02.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-245" for this suite. @ 06/24/23 12:58:02.727
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 06/24/23 12:58:02.735
  Jun 24 12:58:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 12:58:02.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:02.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:02.762
  STEP: Creating ReplicationController "e2e-rc-w2c6b" @ 06/24/23 12:58:02.766
  Jun 24 12:58:02.773: INFO: Get Replication Controller "e2e-rc-w2c6b" to confirm replicas
  E0624 12:58:03.436842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:03.777: INFO: Get Replication Controller "e2e-rc-w2c6b" to confirm replicas
  Jun 24 12:58:03.781: INFO: Found 1 replicas for "e2e-rc-w2c6b" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-w2c6b" @ 06/24/23 12:58:03.781
  STEP: Updating a scale subresource @ 06/24/23 12:58:03.785
  STEP: Verifying replicas where modified for replication controller "e2e-rc-w2c6b" @ 06/24/23 12:58:03.792
  Jun 24 12:58:03.792: INFO: Get Replication Controller "e2e-rc-w2c6b" to confirm replicas
  E0624 12:58:04.437012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:04.796: INFO: Get Replication Controller "e2e-rc-w2c6b" to confirm replicas
  Jun 24 12:58:04.800: INFO: Found 2 replicas for "e2e-rc-w2c6b" replication controller
  Jun 24 12:58:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-53" for this suite. @ 06/24/23 12:58:04.805
• [2.078 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 06/24/23 12:58:04.814
  Jun 24 12:58:04.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:58:04.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:04.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:04.842
  STEP: Creating a ResourceQuota @ 06/24/23 12:58:04.845
  STEP: Getting a ResourceQuota @ 06/24/23 12:58:04.851
  STEP: Listing all ResourceQuotas with LabelSelector @ 06/24/23 12:58:04.854
  STEP: Patching the ResourceQuota @ 06/24/23 12:58:04.858
  STEP: Deleting a Collection of ResourceQuotas @ 06/24/23 12:58:04.864
  STEP: Verifying the deleted ResourceQuota @ 06/24/23 12:58:04.874
  Jun 24 12:58:04.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-552" for this suite. @ 06/24/23 12:58:04.881
• [0.076 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 06/24/23 12:58:04.891
  Jun 24 12:58:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 06/24/23 12:58:04.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:04.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:04.921
  STEP: Setting up the test @ 06/24/23 12:58:04.925
  STEP: Creating hostNetwork=false pod @ 06/24/23 12:58:04.926
  E0624 12:58:05.437945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:06.438059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 06/24/23 12:58:06.952
  E0624 12:58:07.438445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:08.438404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 06/24/23 12:58:08.97
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 06/24/23 12:58:08.97
  Jun 24 12:58:08.970: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:08.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:08.971: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:08.971: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 24 12:58:09.061: INFO: Exec stderr: ""
  Jun 24 12:58:09.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.062: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.062: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 24 12:58:09.132: INFO: Exec stderr: ""
  Jun 24 12:58:09.133: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.134: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.134: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 24 12:58:09.209: INFO: Exec stderr: ""
  Jun 24 12:58:09.209: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.209: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.209: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 24 12:58:09.284: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 06/24/23 12:58:09.284
  Jun 24 12:58:09.284: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.285: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.286: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 24 12:58:09.356: INFO: Exec stderr: ""
  Jun 24 12:58:09.356: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.357: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.358: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun 24 12:58:09.428: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 06/24/23 12:58:09.428
  Jun 24 12:58:09.428: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.429: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.429: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0624 12:58:09.439319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:09.511: INFO: Exec stderr: ""
  Jun 24 12:58:09.512: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.513: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.513: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun 24 12:58:09.586: INFO: Exec stderr: ""
  Jun 24 12:58:09.586: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.587: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.587: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 24 12:58:09.662: INFO: Exec stderr: ""
  Jun 24 12:58:09.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 12:58:09.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 12:58:09.663: INFO: ExecWithOptions: Clientset creation
  Jun 24 12:58:09.663: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun 24 12:58:09.729: INFO: Exec stderr: ""
  Jun 24 12:58:09.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-8123" for this suite. @ 06/24/23 12:58:09.735
• [4.851 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 06/24/23 12:58:09.744
  Jun 24 12:58:09.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 12:58:09.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:09.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:09.769
  STEP: Creating a test headless service @ 06/24/23 12:58:09.774
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7247 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7247;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7247 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7247;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7247.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7247.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7247.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7247.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7247.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7247.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7247.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7247.svc;check="$$(dig +notcp +noall +answer +search 118.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.118_tcp@PTR;sleep 1; done
   @ 06/24/23 12:58:09.792
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7247 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7247;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7247 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7247;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7247.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7247.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7247.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7247.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7247.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7247.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7247.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7247.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7247.svc;check="$$(dig +notcp +noall +answer +search 118.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.118_tcp@PTR;sleep 1; done
   @ 06/24/23 12:58:09.792
  STEP: creating a pod to probe DNS @ 06/24/23 12:58:09.792
  STEP: submitting the pod to kubernetes @ 06/24/23 12:58:09.792
  E0624 12:58:10.440064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:11.440161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:12.440813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:13.440959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 12:58:13.824
  STEP: looking for the results for each expected name from probers @ 06/24/23 12:58:13.828
  Jun 24 12:58:13.834: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.839: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-7247 from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7247 from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.858: INFO: Unable to read wheezy_udp@dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.862: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.867: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.872: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.893: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.898: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.902: INFO: Unable to read jessie_udp@dns-test-service.dns-7247 from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.906: INFO: Unable to read jessie_tcp@dns-test-service.dns-7247 from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.912: INFO: Unable to read jessie_udp@dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.916: INFO: Unable to read jessie_tcp@dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.921: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.926: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7247.svc from pod dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2: the server could not find the requested resource (get pods dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2)
  Jun 24 12:58:13.945: INFO: Lookups using dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7247 wheezy_tcp@dns-test-service.dns-7247 wheezy_udp@dns-test-service.dns-7247.svc wheezy_tcp@dns-test-service.dns-7247.svc wheezy_udp@_http._tcp.dns-test-service.dns-7247.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7247.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7247 jessie_tcp@dns-test-service.dns-7247 jessie_udp@dns-test-service.dns-7247.svc jessie_tcp@dns-test-service.dns-7247.svc jessie_udp@_http._tcp.dns-test-service.dns-7247.svc jessie_tcp@_http._tcp.dns-test-service.dns-7247.svc]

  E0624 12:58:14.441280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:15.442337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:16.442544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:17.442648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:18.442771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:19.057: INFO: DNS probes using dns-7247/dns-test-f3b42b67-976c-41de-8f6d-cdbc14aaa4e2 succeeded

  Jun 24 12:58:19.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 12:58:19.063
  STEP: deleting the test service @ 06/24/23 12:58:19.077
  STEP: deleting the test headless service @ 06/24/23 12:58:19.103
  STEP: Destroying namespace "dns-7247" for this suite. @ 06/24/23 12:58:19.116
• [9.383 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 06/24/23 12:58:19.127
  Jun 24 12:58:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 12:58:19.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:19.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:19.15
  Jun 24 12:58:19.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: creating the pod @ 06/24/23 12:58:19.155
  STEP: submitting the pod to kubernetes @ 06/24/23 12:58:19.156
  E0624 12:58:19.443034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:20.443698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6700" for this suite. @ 06/24/23 12:58:21.27
• [2.151 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 06/24/23 12:58:21.28
  Jun 24 12:58:21.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 12:58:21.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:21.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:21.306
  STEP: Creating a ResourceQuota with terminating scope @ 06/24/23 12:58:21.31
  STEP: Ensuring ResourceQuota status is calculated @ 06/24/23 12:58:21.317
  E0624 12:58:21.444446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:22.444875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 06/24/23 12:58:23.321
  STEP: Ensuring ResourceQuota status is calculated @ 06/24/23 12:58:23.328
  E0624 12:58:23.445582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:24.445770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 06/24/23 12:58:25.333
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 06/24/23 12:58:25.351
  E0624 12:58:25.446256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:26.446362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 06/24/23 12:58:27.356
  E0624 12:58:27.447177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:28.447263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/24/23 12:58:29.361
  STEP: Ensuring resource quota status released the pod usage @ 06/24/23 12:58:29.372
  E0624 12:58:29.448246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:30.448342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 06/24/23 12:58:31.378
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 06/24/23 12:58:31.39
  E0624 12:58:31.448939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:32.449464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 06/24/23 12:58:33.396
  E0624 12:58:33.449532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:34.449672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/24/23 12:58:35.4
  STEP: Ensuring resource quota status released the pod usage @ 06/24/23 12:58:35.413
  E0624 12:58:35.449976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:36.451039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:37.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-330" for this suite. @ 06/24/23 12:58:37.423
• [16.152 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 06/24/23 12:58:37.433
  Jun 24 12:58:37.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 12:58:37.434
  E0624 12:58:37.451304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:37.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:37.462
  STEP: creating a replication controller @ 06/24/23 12:58:37.466
  Jun 24 12:58:37.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 create -f -'
  E0624 12:58:38.451789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:38.749: INFO: stderr: ""
  Jun 24 12:58:38.749: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/24/23 12:58:38.749
  Jun 24 12:58:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 12:58:38.848: INFO: stderr: ""
  Jun 24 12:58:38.848: INFO: stdout: "update-demo-nautilus-8khrr update-demo-nautilus-p8btp "
  Jun 24 12:58:38.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-8khrr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:38.941: INFO: stderr: ""
  Jun 24 12:58:38.942: INFO: stdout: ""
  Jun 24 12:58:38.942: INFO: update-demo-nautilus-8khrr is created but not running
  E0624 12:58:39.452721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:40.452828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:41.452921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:42.453513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:43.453553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:43.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 12:58:44.029: INFO: stderr: ""
  Jun 24 12:58:44.029: INFO: stdout: "update-demo-nautilus-8khrr update-demo-nautilus-p8btp "
  Jun 24 12:58:44.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-8khrr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:44.112: INFO: stderr: ""
  Jun 24 12:58:44.112: INFO: stdout: "true"
  Jun 24 12:58:44.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-8khrr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 12:58:44.195: INFO: stderr: ""
  Jun 24 12:58:44.195: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 12:58:44.195: INFO: validating pod update-demo-nautilus-8khrr
  Jun 24 12:58:44.201: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 12:58:44.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 12:58:44.201: INFO: update-demo-nautilus-8khrr is verified up and running
  Jun 24 12:58:44.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:44.287: INFO: stderr: ""
  Jun 24 12:58:44.287: INFO: stdout: "true"
  Jun 24 12:58:44.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 12:58:44.369: INFO: stderr: ""
  Jun 24 12:58:44.369: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 12:58:44.369: INFO: validating pod update-demo-nautilus-p8btp
  Jun 24 12:58:44.376: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 12:58:44.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 12:58:44.376: INFO: update-demo-nautilus-p8btp is verified up and running
  STEP: scaling down the replication controller @ 06/24/23 12:58:44.376
  Jun 24 12:58:44.378: INFO: scanned /root for discovery docs: <nil>
  Jun 24 12:58:44.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0624 12:58:44.454344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:45.454473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:45.490: INFO: stderr: ""
  Jun 24 12:58:45.490: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/24/23 12:58:45.49
  Jun 24 12:58:45.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 12:58:45.579: INFO: stderr: ""
  Jun 24 12:58:45.579: INFO: stdout: "update-demo-nautilus-8khrr update-demo-nautilus-p8btp "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 06/24/23 12:58:45.579
  E0624 12:58:46.455423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:47.455752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:48.456785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:49.456883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:50.456991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:50.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 12:58:50.668: INFO: stderr: ""
  Jun 24 12:58:50.668: INFO: stdout: "update-demo-nautilus-p8btp "
  Jun 24 12:58:50.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:50.751: INFO: stderr: ""
  Jun 24 12:58:50.751: INFO: stdout: "true"
  Jun 24 12:58:50.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 12:58:50.836: INFO: stderr: ""
  Jun 24 12:58:50.836: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 12:58:50.836: INFO: validating pod update-demo-nautilus-p8btp
  Jun 24 12:58:50.841: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 12:58:50.841: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 12:58:50.841: INFO: update-demo-nautilus-p8btp is verified up and running
  STEP: scaling up the replication controller @ 06/24/23 12:58:50.841
  Jun 24 12:58:50.842: INFO: scanned /root for discovery docs: <nil>
  Jun 24 12:58:50.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0624 12:58:51.457976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:51.949: INFO: stderr: ""
  Jun 24 12:58:51.949: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/24/23 12:58:51.949
  Jun 24 12:58:51.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 12:58:52.041: INFO: stderr: ""
  Jun 24 12:58:52.041: INFO: stdout: "update-demo-nautilus-kcmv7 update-demo-nautilus-p8btp "
  Jun 24 12:58:52.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-kcmv7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:52.122: INFO: stderr: ""
  Jun 24 12:58:52.122: INFO: stdout: "true"
  Jun 24 12:58:52.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-kcmv7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 12:58:52.205: INFO: stderr: ""
  Jun 24 12:58:52.205: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 12:58:52.205: INFO: validating pod update-demo-nautilus-kcmv7
  Jun 24 12:58:52.212: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 12:58:52.212: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 12:58:52.212: INFO: update-demo-nautilus-kcmv7 is verified up and running
  Jun 24 12:58:52.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 12:58:52.299: INFO: stderr: ""
  Jun 24 12:58:52.299: INFO: stdout: "true"
  Jun 24 12:58:52.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods update-demo-nautilus-p8btp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 12:58:52.383: INFO: stderr: ""
  Jun 24 12:58:52.383: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 12:58:52.383: INFO: validating pod update-demo-nautilus-p8btp
  Jun 24 12:58:52.388: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 12:58:52.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 12:58:52.388: INFO: update-demo-nautilus-p8btp is verified up and running
  STEP: using delete to clean up resources @ 06/24/23 12:58:52.388
  Jun 24 12:58:52.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 delete --grace-period=0 --force -f -'
  E0624 12:58:52.458134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:52.492: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 12:58:52.492: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 24 12:58:52.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get rc,svc -l name=update-demo --no-headers'
  Jun 24 12:58:52.608: INFO: stderr: "No resources found in kubectl-8341 namespace.\n"
  Jun 24 12:58:52.608: INFO: stdout: ""
  Jun 24 12:58:52.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8341 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 24 12:58:52.701: INFO: stderr: ""
  Jun 24 12:58:52.701: INFO: stdout: ""
  Jun 24 12:58:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8341" for this suite. @ 06/24/23 12:58:52.707
• [15.282 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 06/24/23 12:58:52.721
  Jun 24 12:58:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 12:58:52.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:52.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:52.746
  STEP: Creating the pod @ 06/24/23 12:58:52.75
  E0624 12:58:53.458280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:54.458928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:55.299: INFO: Successfully updated pod "annotationupdate3bd9fbd8-9f1c-4777-b5da-057d554eaeec"
  E0624 12:58:55.459481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:56.459771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:58:57.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3776" for this suite. @ 06/24/23 12:58:57.323
• [4.611 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 06/24/23 12:58:57.335
  Jun 24 12:58:57.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:58:57.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:58:57.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:58:57.361
  STEP: Setting up server cert @ 06/24/23 12:58:57.398
  E0624 12:58:57.460105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:58:57.993
  STEP: Deploying the webhook pod @ 06/24/23 12:58:58.008
  STEP: Wait for the deployment to be ready @ 06/24/23 12:58:58.023
  Jun 24 12:58:58.033: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 12:58:58.460681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:58:59.460906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 12:59:00.047
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:59:00.058
  E0624 12:59:00.461816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:01.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 06/24/23 12:59:01.063
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 06/24/23 12:59:01.065
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 06/24/23 12:59:01.065
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 06/24/23 12:59:01.065
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 06/24/23 12:59:01.066
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/24/23 12:59:01.066
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/24/23 12:59:01.068
  Jun 24 12:59:01.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3315" for this suite. @ 06/24/23 12:59:01.115
  STEP: Destroying namespace "webhook-markers-619" for this suite. @ 06/24/23 12:59:01.123
• [3.797 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 06/24/23 12:59:01.134
  Jun 24 12:59:01.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 12:59:01.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:59:01.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:59:01.165
  STEP: Setting up server cert @ 06/24/23 12:59:01.198
  E0624 12:59:01.461986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 12:59:01.7
  STEP: Deploying the webhook pod @ 06/24/23 12:59:01.712
  STEP: Wait for the deployment to be ready @ 06/24/23 12:59:01.833
  Jun 24 12:59:01.849: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 12:59:02.462580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:03.462658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 12:59:03.862
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 12:59:03.874
  E0624 12:59:04.462748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:04.875: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/24/23 12:59:04.88
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/24/23 12:59:04.901
  STEP: Creating a dummy validating-webhook-configuration object @ 06/24/23 12:59:04.922
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 06/24/23 12:59:04.934
  STEP: Creating a dummy mutating-webhook-configuration object @ 06/24/23 12:59:04.942
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 06/24/23 12:59:04.954
  Jun 24 12:59:04.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9509" for this suite. @ 06/24/23 12:59:05.027
  STEP: Destroying namespace "webhook-markers-4983" for this suite. @ 06/24/23 12:59:05.034
• [3.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 06/24/23 12:59:05.045
  Jun 24 12:59:05.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 12:59:05.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 12:59:05.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 12:59:05.072
  STEP: Creating service test in namespace statefulset-5303 @ 06/24/23 12:59:05.077
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 06/24/23 12:59:05.085
  STEP: Creating stateful set ss in namespace statefulset-5303 @ 06/24/23 12:59:05.091
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5303 @ 06/24/23 12:59:05.104
  Jun 24 12:59:05.109: INFO: Found 0 stateful pods, waiting for 1
  E0624 12:59:05.463673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:06.463916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:07.464794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:08.465075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:09.465250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:10.465370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:11.465747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:12.466236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:13.466343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:14.466762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:15.114: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 06/24/23 12:59:15.114
  Jun 24 12:59:15.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:59:15.281: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:59:15.281: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:59:15.281: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:59:15.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0624 12:59:15.467631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:16.467746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:17.468454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:18.468664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:19.468786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:20.468902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:21.469108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:22.469236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:23.469491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:24.469612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:25.291: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:59:25.291: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:59:25.310: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999977s
  E0624 12:59:25.470416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:26.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994966231s
  E0624 12:59:26.470999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:27.320: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989510626s
  E0624 12:59:27.471929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:28.325: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985307385s
  E0624 12:59:28.472761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:29.329: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980467888s
  E0624 12:59:29.473848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:30.333: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976464879s
  E0624 12:59:30.474211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:31.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971906725s
  E0624 12:59:31.475234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:32.343: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966954126s
  E0624 12:59:32.475718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:33.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962433442s
  E0624 12:59:33.476485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:34.352: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.262081ms
  E0624 12:59:34.477361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5303 @ 06/24/23 12:59:35.353
  Jun 24 12:59:35.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0624 12:59:35.478189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:35.518: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 12:59:35.518: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 12:59:35.518: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 12:59:35.522: INFO: Found 1 stateful pods, waiting for 3
  E0624 12:59:36.478691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:37.479621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:38.480594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:39.480823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:40.481850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:41.481972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:42.483057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:43.483407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:44.483515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:45.483727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:45.527: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:59:45.527: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 12:59:45.527: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 06/24/23 12:59:45.528
  STEP: Scale down will halt with unhealthy stateful pod @ 06/24/23 12:59:45.528
  Jun 24 12:59:45.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:59:45.686: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:59:45.686: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:59:45.686: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:59:45.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:59:45.855: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:59:45.855: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:59:45.855: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:59:45.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun 24 12:59:46.021: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun 24 12:59:46.021: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun 24 12:59:46.021: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun 24 12:59:46.021: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 12:59:46.026: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0624 12:59:46.484033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:47.484149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:48.484307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:49.484483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:50.484768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:51.484783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:52.484847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:53.485060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:54.485552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 12:59:55.485781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:56.034: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:59:56.034: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:59:56.034: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun 24 12:59:56.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999979s
  E0624 12:59:56.485899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:57.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992451222s
  E0624 12:59:57.486860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:58.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988032305s
  E0624 12:59:58.486977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 12:59:59.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983582017s
  E0624 12:59:59.487977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:00.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977642145s
  E0624 13:00:00.488768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:01.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972931115s
  E0624 13:00:01.488884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:02.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967534958s
  E0624 13:00:02.488993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:03.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961525505s
  E0624 13:00:03.489791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:04.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955539172s
  E0624 13:00:04.489857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:05.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.696501ms
  E0624 13:00:05.490658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5303 @ 06/24/23 13:00:06.102
  Jun 24 13:00:06.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 13:00:06.276: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 13:00:06.276: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 13:00:06.276: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 13:00:06.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun 24 13:00:06.442: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 13:00:06.442: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 13:00:06.442: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 13:00:06.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=statefulset-5303 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0624 13:00:06.491032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:06.625: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun 24 13:00:06.625: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun 24 13:00:06.625: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun 24 13:00:06.625: INFO: Scaling statefulset ss to 0
  E0624 13:00:07.491243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:08.492131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:09.492112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:10.492847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:11.492959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:12.493070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:13.493181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:14.493312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:15.493591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:16.493703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 06/24/23 13:00:16.646
  Jun 24 13:00:16.646: INFO: Deleting all statefulset in ns statefulset-5303
  Jun 24 13:00:16.649: INFO: Scaling statefulset ss to 0
  Jun 24 13:00:16.662: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 13:00:16.665: INFO: Deleting statefulset ss
  Jun 24 13:00:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5303" for this suite. @ 06/24/23 13:00:16.684
• [71.647 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 06/24/23 13:00:16.694
  Jun 24 13:00:16.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/24/23 13:00:16.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:16.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:16.722
  Jun 24 13:00:16.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:00:17.494325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:18.494405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:19.494479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:20.495374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:21.495806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:22.496439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:23.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2327" for this suite. @ 06/24/23 13:00:23.049
• [6.363 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 06/24/23 13:00:23.058
  Jun 24 13:00:23.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 13:00:23.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:23.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:23.091
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 06/24/23 13:00:23.095
  Jun 24 13:00:23.106: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-749  e1932dbd-bf8a-4618-b29f-88c1764cdb92 30498 0 2023-06-24 13:00:23 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-24 13:00:23 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxssw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxssw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0624 13:00:23.496654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:24.496776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 06/24/23 13:00:25.115
  Jun 24 13:00:25.115: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-749 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:00:25.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:00:25.116: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:00:25.116: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-749/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 06/24/23 13:00:25.201
  Jun 24 13:00:25.202: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-749 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:00:25.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:00:25.202: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:00:25.203: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-749/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun 24 13:00:25.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 13:00:25.295: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-749" for this suite. @ 06/24/23 13:00:25.311
• [2.260 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 06/24/23 13:00:25.319
  Jun 24 13:00:25.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename init-container @ 06/24/23 13:00:25.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:25.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:25.342
  STEP: creating the pod @ 06/24/23 13:00:25.346
  Jun 24 13:00:25.346: INFO: PodSpec: initContainers in spec.initContainers
  E0624 13:00:25.497264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:26.498074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:27.498708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:28.499376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:29.499684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:29.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8956" for this suite. @ 06/24/23 13:00:29.584
• [4.275 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 06/24/23 13:00:29.595
  Jun 24 13:00:29.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 13:00:29.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:29.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:29.626
  STEP: Setting up server cert @ 06/24/23 13:00:29.655
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 13:00:30.397
  STEP: Deploying the webhook pod @ 06/24/23 13:00:30.406
  STEP: Wait for the deployment to be ready @ 06/24/23 13:00:30.425
  Jun 24 13:00:30.440: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 13:00:30.499819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:31.500222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:00:32.452
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:00:32.465
  E0624 13:00:32.500662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:33.465: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 06/24/23 13:00:33.47
  STEP: Creating a custom resource definition that should be denied by the webhook @ 06/24/23 13:00:33.489
  Jun 24 13:00:33.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:00:33.500940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2390" for this suite. @ 06/24/23 13:00:33.56
  STEP: Destroying namespace "webhook-markers-73" for this suite. @ 06/24/23 13:00:33.57
• [3.985 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 06/24/23 13:00:33.581
  Jun 24 13:00:33.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 13:00:33.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:33.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:33.61
  STEP: Creating a pod to test service account token:  @ 06/24/23 13:00:33.615
  E0624 13:00:34.501977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:35.502147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:36.502216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:37.502325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:00:37.642
  Jun 24 13:00:37.646: INFO: Trying to get logs from node ip-172-31-19-205 pod test-pod-93734a77-8261-4358-98a9-bbacd38d9f1d container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:00:37.666
  Jun 24 13:00:37.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2499" for this suite. @ 06/24/23 13:00:37.686
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 06/24/23 13:00:37.696
  Jun 24 13:00:37.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-pred @ 06/24/23 13:00:37.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:37.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:37.719
  Jun 24 13:00:37.724: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun 24 13:00:37.734: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 13:00:37.738: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-136 before test
  Jun 24 13:00:37.744: INFO: nginx-ingress-controller-kubernetes-worker-9295m from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:39 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.745: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: calico-kube-controllers-69b5565d84-5wz9f from kube-system started at 2023-06-24 12:02:18 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.745: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: sonobuoy-e2e-job-a6e2d2ee4f8b45eb from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 13:00:37.745: INFO: 	Container e2e ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-xmz57 from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 13:00:37.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 13:00:37.745: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-205 before test
  Jun 24 13:00:37.752: INFO: nginx-ingress-controller-kubernetes-worker-6vrl7 from ingress-nginx-kubernetes-worker started at 2023-06-24 12:03:18 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.752: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 13:00:37.752: INFO: sonobuoy from sonobuoy started at 2023-06-24 12:08:47 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.752: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun 24 13:00:37.752: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-2vcbs from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 13:00:37.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 13:00:37.752: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun 24 13:00:37.752: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-89-202 before test
  Jun 24 13:00:37.758: INFO: default-http-backend-kubernetes-worker-65fc475d49-cs8f9 from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: nginx-ingress-controller-kubernetes-worker-bvt8s from ingress-nginx-kubernetes-worker started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: coredns-5c7f76ccb8-d25lj from kube-system started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container coredns ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: kube-state-metrics-5b95b4459c-pcn82 from kube-system started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: metrics-server-v0.5.2-6cf8c8b69c-m28bd from kube-system started at 2023-06-24 11:54:03 +0000 UTC (2 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container metrics-server ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: dashboard-metrics-scraper-6b8586b5c9-c7tbk from kubernetes-dashboard started at 2023-06-24 11:54:03 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: kubernetes-dashboard-6869f4cd5f-6rqxs from kubernetes-dashboard started at 2023-06-24 11:53:40 +0000 UTC (1 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: sonobuoy-systemd-logs-daemon-set-792ce0975a3d424f-ldg4s from sonobuoy started at 2023-06-24 12:08:50 +0000 UTC (2 container statuses recorded)
  Jun 24 13:00:37.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun 24 13:00:37.759: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 06/24/23 13:00:37.759
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.176b99d7dcfa14f8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 06/24/23 13:00:37.793
  E0624 13:00:38.502407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:38.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2108" for this suite. @ 06/24/23 13:00:38.794
• [1.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 06/24/23 13:00:38.811
  Jun 24 13:00:38.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename containers @ 06/24/23 13:00:38.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:38.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:38.837
  STEP: Creating a pod to test override command @ 06/24/23 13:00:38.84
  E0624 13:00:39.503234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:40.503476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:41.503685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:42.503717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:00:42.864
  Jun 24 13:00:42.868: INFO: Trying to get logs from node ip-172-31-19-205 pod client-containers-de484bd6-e92b-4923-a25b-57856443aee6 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:00:42.877
  Jun 24 13:00:42.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-158" for this suite. @ 06/24/23 13:00:42.899
• [4.096 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 06/24/23 13:00:42.907
  Jun 24 13:00:42.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 13:00:42.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:42.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:42.938
  STEP: Setting up server cert @ 06/24/23 13:00:42.966
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 13:00:43.336
  STEP: Deploying the webhook pod @ 06/24/23 13:00:43.342
  STEP: Wait for the deployment to be ready @ 06/24/23 13:00:43.355
  Jun 24 13:00:43.364: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0624 13:00:43.504700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:44.504866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:00:45.376
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:00:45.386
  E0624 13:00:45.505717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:46.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/24/23 13:00:46.466
  E0624 13:00:46.506248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/24/23 13:00:46.516
  STEP: Deleting the collection of validation webhooks @ 06/24/23 13:00:46.562
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/24/23 13:00:46.621
  Jun 24 13:00:46.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4992" for this suite. @ 06/24/23 13:00:46.685
  STEP: Destroying namespace "webhook-markers-5124" for this suite. @ 06/24/23 13:00:46.693
• [3.793 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 06/24/23 13:00:46.7
  Jun 24 13:00:46.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename endpointslice @ 06/24/23 13:00:46.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:46.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:46.73
  E0624 13:00:47.506439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:48.506559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:00:48.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4838" for this suite. @ 06/24/23 13:00:48.79
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 06/24/23 13:00:48.802
  Jun 24 13:00:48.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:00:48.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:48.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:48.825
  STEP: Creating configMap with name projected-configmap-test-volume-da6c1d74-37f4-42a0-ac33-392a35fa0d30 @ 06/24/23 13:00:48.829
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:00:48.834
  E0624 13:00:49.506741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:50.507249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:51.507364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:52.507733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:00:52.859
  Jun 24 13:00:52.863: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-3f481eea-6de4-479b-b7eb-f6e65c8b3fe6 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 13:00:52.871
  Jun 24 13:00:52.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5405" for this suite. @ 06/24/23 13:00:52.894
• [4.099 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 06/24/23 13:00:52.901
  Jun 24 13:00:52.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:00:52.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:52.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:52.93
  STEP: Creating configMap with name projected-configmap-test-volume-0890cc03-771a-4f62-9fbf-f5575e664393 @ 06/24/23 13:00:52.934
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:00:52.942
  E0624 13:00:53.508640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:54.509071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:55.509676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:56.510276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:00:56.968
  Jun 24 13:00:56.972: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-702a0133-51c9-4c7a-a111-a4a9bd1e5741 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:00:56.98
  Jun 24 13:00:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5743" for this suite. @ 06/24/23 13:00:57.003
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 06/24/23 13:00:57.015
  Jun 24 13:00:57.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename init-container @ 06/24/23 13:00:57.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:00:57.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:00:57.042
  STEP: creating the pod @ 06/24/23 13:00:57.047
  Jun 24 13:00:57.047: INFO: PodSpec: initContainers in spec.initContainers
  E0624 13:00:57.510471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:58.511162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:00:59.511527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:00.512086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:01.512208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:01.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1735" for this suite. @ 06/24/23 13:01:01.731
• [4.724 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 06/24/23 13:01:01.741
  Jun 24 13:01:01.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 13:01:01.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:01.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:01.777
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2974.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2974.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 06/24/23 13:01:01.781
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2974.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2974.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 06/24/23 13:01:01.781
  STEP: creating a pod to probe /etc/hosts @ 06/24/23 13:01:01.781
  STEP: submitting the pod to kubernetes @ 06/24/23 13:01:01.781
  E0624 13:01:02.513261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:03.513938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 13:01:03.805
  STEP: looking for the results for each expected name from probers @ 06/24/23 13:01:03.809
  Jun 24 13:01:03.827: INFO: DNS probes using dns-2974/dns-test-4ee0a95b-33d9-46eb-a072-e1a34b72c438 succeeded

  Jun 24 13:01:03.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:01:03.833
  STEP: Destroying namespace "dns-2974" for this suite. @ 06/24/23 13:01:03.846
• [2.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 06/24/23 13:01:03.855
  Jun 24 13:01:03.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 13:01:03.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:03.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:03.885
  Jun 24 13:01:03.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  W0624 13:01:03.891222      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005de3530 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0624 13:01:04.514600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:05.515028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0624 13:01:06.456074      19 warnings.go:70] unknown field "alpha"
  W0624 13:01:06.456110      19 warnings.go:70] unknown field "beta"
  W0624 13:01:06.456118      19 warnings.go:70] unknown field "delta"
  W0624 13:01:06.456124      19 warnings.go:70] unknown field "epsilon"
  W0624 13:01:06.456132      19 warnings.go:70] unknown field "gamma"
  Jun 24 13:01:06.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4223" for this suite. @ 06/24/23 13:01:06.5
• [2.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 06/24/23 13:01:06.514
  Jun 24 13:01:06.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename csistoragecapacity @ 06/24/23 13:01:06.515
  E0624 13:01:06.515455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:06.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:06.542
  STEP: getting /apis @ 06/24/23 13:01:06.547
  STEP: getting /apis/storage.k8s.io @ 06/24/23 13:01:06.552
  STEP: getting /apis/storage.k8s.io/v1 @ 06/24/23 13:01:06.554
  STEP: creating @ 06/24/23 13:01:06.555
  STEP: watching @ 06/24/23 13:01:06.573
  Jun 24 13:01:06.573: INFO: starting watch
  STEP: getting @ 06/24/23 13:01:06.583
  STEP: listing in namespace @ 06/24/23 13:01:06.59
  STEP: listing across namespaces @ 06/24/23 13:01:06.595
  STEP: patching @ 06/24/23 13:01:06.599
  STEP: updating @ 06/24/23 13:01:06.604
  Jun 24 13:01:06.610: INFO: waiting for watch events with expected annotations in namespace
  Jun 24 13:01:06.610: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 06/24/23 13:01:06.611
  STEP: deleting a collection @ 06/24/23 13:01:06.626
  Jun 24 13:01:06.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1066" for this suite. @ 06/24/23 13:01:06.649
• [0.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 06/24/23 13:01:06.658
  Jun 24 13:01:06.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 06/24/23 13:01:06.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:06.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:06.685
  STEP: creating a target pod @ 06/24/23 13:01:06.689
  E0624 13:01:07.516048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:08.516151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 06/24/23 13:01:08.714
  E0624 13:01:09.516807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:10.517320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 06/24/23 13:01:10.741
  Jun 24 13:01:10.741: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1072 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:01:10.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:01:10.742: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:01:10.742: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-1072/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jun 24 13:01:10.820: INFO: Exec stderr: ""
  Jun 24 13:01:10.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-1072" for this suite. @ 06/24/23 13:01:10.833
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 06/24/23 13:01:10.844
  Jun 24 13:01:10.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename aggregator @ 06/24/23 13:01:10.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:10.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:10.872
  Jun 24 13:01:10.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Registering the sample API server. @ 06/24/23 13:01:10.878
  Jun 24 13:01:11.300: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jun 24 13:01:11.333: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0624 13:01:11.517901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:12.518043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:13.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:13.518427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:14.518529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:15.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:15.519226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:16.519393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:17.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:17.520013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:18.521039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:19.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:19.521735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:20.521924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:21.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:21.522470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:22.522822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:23.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:23.523225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:24.523298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:25.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:25.523642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:26.523717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:27.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:27.524554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:28.524677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:29.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:29.525218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:30.525824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:31.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:31.526826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:32.526934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:33.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:01:33.527648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:34.527749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:35.512: INFO: Waited 121.244672ms for the sample-apiserver to be ready to handle requests.
  E0624 13:01:35.527727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Read Status for v1alpha1.wardle.example.com @ 06/24/23 13:01:35.569
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 06/24/23 13:01:35.574
  STEP: List APIServices @ 06/24/23 13:01:35.585
  Jun 24 13:01:35.594: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 06/24/23 13:01:35.594
  Jun 24 13:01:35.616: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 06/24/23 13:01:35.616
  Jun 24 13:01:35.629: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.June, 24, 13, 1, 35, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 06/24/23 13:01:35.629
  Jun 24 13:01:35.636: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-06-24 13:01:35 +0000 UTC Passed all checks passed}
  Jun 24 13:01:35.636: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 13:01:35.636: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 06/24/23 13:01:35.636
  Jun 24 13:01:35.652: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-314679988" @ 06/24/23 13:01:35.652
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 06/24/23 13:01:35.672
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 06/24/23 13:01:35.682
  STEP: Patch APIService Status @ 06/24/23 13:01:35.689
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 06/24/23 13:01:35.707
  Jun 24 13:01:35.720: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-06-24 13:01:35 +0000 UTC Passed all checks passed}
  Jun 24 13:01:35.720: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 13:01:35.720: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jun 24 13:01:35.720: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 06/24/23 13:01:35.721
  STEP: Confirm that the generated APIService has been deleted @ 06/24/23 13:01:35.728
  Jun 24 13:01:35.728: INFO: Requesting list of APIServices to confirm quantity
  Jun 24 13:01:35.734: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jun 24 13:01:35.734: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jun 24 13:01:35.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-9470" for this suite. @ 06/24/23 13:01:35.904
• [25.070 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 06/24/23 13:01:35.915
  Jun 24 13:01:35.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replication-controller @ 06/24/23 13:01:35.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:35.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:35.947
  STEP: Given a ReplicationController is created @ 06/24/23 13:01:35.953
  STEP: When the matched label of one of its pods change @ 06/24/23 13:01:35.961
  Jun 24 13:01:35.965: INFO: Pod name pod-release: Found 0 pods out of 1
  E0624 13:01:36.528609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:37.529659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:38.529730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:39.529978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:40.531064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:40.971: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/24/23 13:01:40.986
  E0624 13:01:41.532121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:41.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8823" for this suite. @ 06/24/23 13:01:42.006
• [6.100 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 06/24/23 13:01:42.016
  Jun 24 13:01:42.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-webhook @ 06/24/23 13:01:42.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:42.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:42.05
  STEP: Setting up server cert @ 06/24/23 13:01:42.055
  E0624 13:01:42.532718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/24/23 13:01:42.694
  STEP: Deploying the custom resource conversion webhook pod @ 06/24/23 13:01:42.7
  STEP: Wait for the deployment to be ready @ 06/24/23 13:01:42.713
  Jun 24 13:01:42.722: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0624 13:01:43.533191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:44.533304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:01:44.734
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:01:44.744
  E0624 13:01:45.533439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:45.745: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 24 13:01:45.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:01:46.533606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:47.533712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 06/24/23 13:01:48.339
  STEP: v2 custom resource should be converted @ 06/24/23 13:01:48.346
  Jun 24 13:01:48.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 13:01:48.534844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-1769" for this suite. @ 06/24/23 13:01:48.926
• [6.919 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 06/24/23 13:01:48.936
  Jun 24 13:01:48.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption @ 06/24/23 13:01:48.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:48.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:48.97
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:01:48.982
  E0624 13:01:49.535671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:50.535697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 06/24/23 13:01:51.017
  Jun 24 13:01:51.022: INFO: running pods: 0 < 3
  E0624 13:01:51.535888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:52.535999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:01:53.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2052" for this suite. @ 06/24/23 13:01:53.036
• [4.108 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 06/24/23 13:01:53.045
  Jun 24 13:01:53.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:01:53.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:53.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:53.072
  STEP: Creating configMap with name configmap-test-volume-b74d2e04-c261-44e9-977d-0e7769406d83 @ 06/24/23 13:01:53.076
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:01:53.081
  E0624 13:01:53.536267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:54.536372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:55.536559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:56.536636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:01:57.107
  Jun 24 13:01:57.112: INFO: Trying to get logs from node ip-172-31-15-136 pod pod-configmaps-8394c116-c422-4a48-b4f9-66b5c83a8838 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:01:57.132
  Jun 24 13:01:57.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3164" for this suite. @ 06/24/23 13:01:57.158
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 06/24/23 13:01:57.179
  Jun 24 13:01:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pods @ 06/24/23 13:01:57.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:01:57.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:01:57.209
  E0624 13:01:57.537365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:58.537427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:01:59.537539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:00.537757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:01.538598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:02.538691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:02:03.272
  Jun 24 13:02:03.276: INFO: Trying to get logs from node ip-172-31-19-205 pod client-envvars-ec137b89-8b54-432a-ba20-bfb5fc2538fb container env3cont: <nil>
  STEP: delete the pod @ 06/24/23 13:02:03.289
  Jun 24 13:02:03.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3086" for this suite. @ 06/24/23 13:02:03.309
• [6.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 06/24/23 13:02:03.32
  Jun 24 13:02:03.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 13:02:03.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:03.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:03.347
  Jun 24 13:02:03.384: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"657df1e6-20d7-4096-8ad8-12357deaa0d3", Controller:(*bool)(0xc0043b1336), BlockOwnerDeletion:(*bool)(0xc0043b1337)}}
  Jun 24 13:02:03.393: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aa0cef3c-77ca-47ba-97f9-f714124c1cef", Controller:(*bool)(0xc0043b18f6), BlockOwnerDeletion:(*bool)(0xc0043b18f7)}}
  Jun 24 13:02:03.401: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c6ed1d58-5489-498d-ab81-557b04ff0b14", Controller:(*bool)(0xc001e39c96), BlockOwnerDeletion:(*bool)(0xc001e39c97)}}
  E0624 13:02:03.538783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:04.538888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:05.539578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:06.539794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:07.539975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:02:08.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2856" for this suite. @ 06/24/23 13:02:08.418
• [5.106 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 06/24/23 13:02:08.426
  Jun 24 13:02:08.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename watch @ 06/24/23 13:02:08.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:08.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:08.454
  STEP: creating a watch on configmaps @ 06/24/23 13:02:08.459
  STEP: creating a new configmap @ 06/24/23 13:02:08.461
  STEP: modifying the configmap once @ 06/24/23 13:02:08.467
  STEP: closing the watch once it receives two notifications @ 06/24/23 13:02:08.475
  Jun 24 13:02:08.475: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9096  942c9f8a-8b23-43bd-bfba-4df2a3ae4715 31757 0 2023-06-24 13:02:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-24 13:02:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:02:08.476: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9096  942c9f8a-8b23-43bd-bfba-4df2a3ae4715 31759 0 2023-06-24 13:02:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-24 13:02:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 06/24/23 13:02:08.476
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 06/24/23 13:02:08.487
  STEP: deleting the configmap @ 06/24/23 13:02:08.489
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 06/24/23 13:02:08.496
  Jun 24 13:02:08.497: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9096  942c9f8a-8b23-43bd-bfba-4df2a3ae4715 31761 0 2023-06-24 13:02:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-24 13:02:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:02:08.497: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9096  942c9f8a-8b23-43bd-bfba-4df2a3ae4715 31764 0 2023-06-24 13:02:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-24 13:02:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:02:08.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9096" for this suite. @ 06/24/23 13:02:08.502
• [0.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 06/24/23 13:02:08.513
  Jun 24 13:02:08.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 13:02:08.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:08.536
  E0624 13:02:08.540003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:08.54
  STEP: Creating simple DaemonSet "daemon-set" @ 06/24/23 13:02:08.572
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 13:02:08.579
  Jun 24 13:02:08.585: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:08.585: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:08.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:02:08.589: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:02:09.540284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:02:09.599: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:09.599: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:09.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 13:02:09.604: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:02:10.540766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:02:10.596: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:10.596: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:02:10.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 13:02:10.600: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 06/24/23 13:02:10.604
  STEP: DeleteCollection of the DaemonSets @ 06/24/23 13:02:10.609
  STEP: Verify that ReplicaSets have been deleted @ 06/24/23 13:02:10.618
  Jun 24 13:02:10.631: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31833"},"items":null}

  Jun 24 13:02:10.638: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31837"},"items":[{"metadata":{"name":"daemon-set-95r64","generateName":"daemon-set-","namespace":"daemonsets-1603","uid":"10d410d7-d08c-40e0-acb9-4ef081abb211","resourceVersion":"31835","creationTimestamp":"2023-06-24T13:02:08Z","deletionTimestamp":"2023-06-24T13:02:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dbd720a6-74e4-4b16-878f-d871a4c62358","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbd720a6-74e4-4b16-878f-d871a4c62358\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.144.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-smmss","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-smmss","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-89-202","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-89-202"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"}],"hostIP":"172.31.89.202","podIP":"192.168.144.140","podIPs":[{"ip":"192.168.144.140"}],"startTime":"2023-06-24T13:02:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-24T13:02:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://8c0bfc2d86f13d05ede75319aff44c56e88ff661fc5fb46c9647a06264c16ae8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ps6rj","generateName":"daemon-set-","namespace":"daemonsets-1603","uid":"705271c5-7457-4f2f-b004-a77a3a36432d","resourceVersion":"31837","creationTimestamp":"2023-06-24T13:02:08Z","deletionTimestamp":"2023-06-24T13:02:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dbd720a6-74e4-4b16-878f-d871a4c62358","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbd720a6-74e4-4b16-878f-d871a4c62358\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.116.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8ph8s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8ph8s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-15-136","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-15-136"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"}],"hostIP":"172.31.15.136","podIP":"192.168.116.247","podIPs":[{"ip":"192.168.116.247"}],"startTime":"2023-06-24T13:02:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-24T13:02:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6ce72827d99099374b224bf516d7d7351f1f27a05c85c91a4aa415e7039517db","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-r2828","generateName":"daemon-set-","namespace":"daemonsets-1603","uid":"09e85eb5-0c15-499c-8c16-a22f30eac14e","resourceVersion":"31836","creationTimestamp":"2023-06-24T13:02:08Z","deletionTimestamp":"2023-06-24T13:02:40Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"dbd720a6-74e4-4b16-878f-d871a4c62358","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbd720a6-74e4-4b16-878f-d871a4c62358\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-24T13:02:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vbh8f","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vbh8f","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-19-205","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-19-205"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-24T13:02:08Z"}],"hostIP":"172.31.19.205","podIP":"192.168.150.209","podIPs":[{"ip":"192.168.150.209"}],"startTime":"2023-06-24T13:02:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-24T13:02:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://54b46339d7df2d022eadd0514efe278adf74b27a8fd67a862e0aec679674f3d8","started":true}],"qosClass":"BestEffort"}}]}

  Jun 24 13:02:10.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1603" for this suite. @ 06/24/23 13:02:10.661
• [2.156 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 06/24/23 13:02:10.671
  Jun 24 13:02:10.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:02:10.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:10.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:10.697
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/24/23 13:02:10.705
  E0624 13:02:11.541284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:12.541399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:13.541526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:14.541622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:02:14.728
  Jun 24 13:02:14.732: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-dbf742be-6bd6-4399-bc29-0823b6f14af5 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:02:14.74
  Jun 24 13:02:14.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3796" for this suite. @ 06/24/23 13:02:14.761
• [4.098 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 06/24/23 13:02:14.77
  Jun 24 13:02:14.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/24/23 13:02:14.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:14.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:14.793
  Jun 24 13:02:14.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:02:15.542467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:02:15.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7909" for this suite. @ 06/24/23 13:02:15.83
• [1.069 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 06/24/23 13:02:15.844
  Jun 24 13:02:15.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 13:02:15.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:02:15.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:02:15.874
  STEP: Creating pod test-grpc-d551af9c-c560-4cef-802d-cd6a4c22b73e in namespace container-probe-9477 @ 06/24/23 13:02:15.878
  E0624 13:02:16.542591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:17.543227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:02:17.897: INFO: Started pod test-grpc-d551af9c-c560-4cef-802d-cd6a4c22b73e in namespace container-probe-9477
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 13:02:17.897
  Jun 24 13:02:17.901: INFO: Initial restart count of pod test-grpc-d551af9c-c560-4cef-802d-cd6a4c22b73e is 0
  E0624 13:02:18.543729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:19.543778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:20.543927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:21.544461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:22.544552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:23.544670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:24.544902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:25.544967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:26.545079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:27.545601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:28.545748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:29.545835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:30.545963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:31.546070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:32.546400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:33.546906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:34.547034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:35.547410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:36.547585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:37.547694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:38.547704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:39.547869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:40.547996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:41.548146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:42.548812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:43.548913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:44.549026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:45.549345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:46.549365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:47.550178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:48.550304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:49.550384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:50.551275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:51.551399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:52.551732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:53.551826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:54.551949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:55.552786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:56.553038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:57.553663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:58.554257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:02:59.554382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:00.554492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:01.554581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:02.554683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:03.554807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:04.555638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:05.555996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:06.556213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:07.556343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:08.557417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:09.557539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:10.558266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:11.558398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:12.559200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:13.559268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:14.559725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:15.560764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:16.561014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:17.561184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:18.561961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:19.562139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:20.562329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:21.562691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:03:22.061: INFO: Restart count of pod container-probe-9477/test-grpc-d551af9c-c560-4cef-802d-cd6a4c22b73e is now 1 (1m4.160448357s elapsed)
  Jun 24 13:03:22.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:03:22.066
  STEP: Destroying namespace "container-probe-9477" for this suite. @ 06/24/23 13:03:22.078
• [66.243 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 06/24/23 13:03:22.087
  Jun 24 13:03:22.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 13:03:22.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:03:22.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:03:22.114
  STEP: Creating service test in namespace statefulset-531 @ 06/24/23 13:03:22.118
  Jun 24 13:03:22.137: INFO: Found 0 stateful pods, waiting for 1
  E0624 13:03:22.563059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:23.563147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:24.563274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:25.563357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:26.563659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:27.563725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:28.563848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:29.563926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:30.564030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:31.564172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:03:32.144: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 06/24/23 13:03:32.152
  W0624 13:03:32.165335      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 24 13:03:32.173: INFO: Found 1 stateful pods, waiting for 2
  E0624 13:03:32.564315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:33.564497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:34.564597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:35.564716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:36.564799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:37.564881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:38.565328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:39.565343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:40.566408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:41.566495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:03:42.179: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 13:03:42.179: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 06/24/23 13:03:42.188
  STEP: Delete all of the StatefulSets @ 06/24/23 13:03:42.192
  STEP: Verify that StatefulSets have been deleted @ 06/24/23 13:03:42.202
  Jun 24 13:03:42.205: INFO: Deleting all statefulset in ns statefulset-531
  Jun 24 13:03:42.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-531" for this suite. @ 06/24/23 13:03:42.22
• [20.146 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 06/24/23 13:03:42.237
  Jun 24 13:03:42.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-webhook @ 06/24/23 13:03:42.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:03:42.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:03:42.266
  STEP: Setting up server cert @ 06/24/23 13:03:42.27
  E0624 13:03:42.569197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/24/23 13:03:42.914
  STEP: Deploying the custom resource conversion webhook pod @ 06/24/23 13:03:42.925
  STEP: Wait for the deployment to be ready @ 06/24/23 13:03:42.94
  Jun 24 13:03:42.949: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0624 13:03:43.569853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:44.570265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:03:44.961
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:03:44.975
  E0624 13:03:45.570360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:03:45.975: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun 24 13:03:45.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:03:46.571265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:47.571696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:48.572034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 06/24/23 13:03:48.575
  STEP: Create a v2 custom resource @ 06/24/23 13:03:48.595
  STEP: List CRs in v1 @ 06/24/23 13:03:48.605
  STEP: List CRs in v2 @ 06/24/23 13:03:48.656
  Jun 24 13:03:48.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1575" for this suite. @ 06/24/23 13:03:49.235
• [7.007 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 06/24/23 13:03:49.245
  Jun 24 13:03:49.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:03:49.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:03:49.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:03:49.279
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 13:03:49.285
  E0624 13:03:49.572816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:50.572996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:51.573083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:52.573156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:03:53.313
  Jun 24 13:03:53.317: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-ae74af65-2c26-4dc0-9a64-e5e1eaf30728 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 13:03:53.338
  Jun 24 13:03:53.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3316" for this suite. @ 06/24/23 13:03:53.362
• [4.125 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 06/24/23 13:03:53.371
  Jun 24 13:03:53.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:03:53.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:03:53.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:03:53.408
  STEP: Creating configMap with name configmap-test-volume-map-d6113630-ada0-4cb8-b5f0-5c0610f15a3b @ 06/24/23 13:03:53.412
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:03:53.418
  E0624 13:03:53.574177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:54.574282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:55.574972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:56.575080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:03:57.445
  Jun 24 13:03:57.452: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-40b9b94d-8f35-477b-8d86-a20ade62a7d5 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:03:57.463
  Jun 24 13:03:57.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3549" for this suite. @ 06/24/23 13:03:57.487
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 06/24/23 13:03:57.496
  Jun 24 13:03:57.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-runtime @ 06/24/23 13:03:57.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:03:57.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:03:57.525
  STEP: create the container @ 06/24/23 13:03:57.528
  W0624 13:03:57.539281      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/24/23 13:03:57.539
  E0624 13:03:57.575867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:58.576134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:03:59.577097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/24/23 13:04:00.562
  STEP: the container should be terminated @ 06/24/23 13:04:00.567
  STEP: the termination message should be set @ 06/24/23 13:04:00.567
  Jun 24 13:04:00.567: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 06/24/23 13:04:00.567
  E0624 13:04:00.577603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:00.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3095" for this suite. @ 06/24/23 13:04:00.592
• [3.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 06/24/23 13:04:00.605
  Jun 24 13:04:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 13:04:00.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:00.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:00.634
  STEP: Creating secret with name secret-test-map-85e77ab6-07c5-4ae0-8aa6-a3182efb9bdf @ 06/24/23 13:04:00.638
  STEP: Creating a pod to test consume secrets @ 06/24/23 13:04:00.644
  E0624 13:04:01.577966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:02.578072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:03.578137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:04.578486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:04:04.671
  Jun 24 13:04:04.675: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-32a64d12-b017-4c9a-94e7-b4b3a87d5701 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 13:04:04.683
  Jun 24 13:04:04.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5605" for this suite. @ 06/24/23 13:04:04.705
• [4.107 seconds]
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 06/24/23 13:04:04.713
  Jun 24 13:04:04.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:04:04.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:04.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:04.741
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-1422 @ 06/24/23 13:04:04.744
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/24/23 13:04:04.771
  STEP: creating service externalsvc in namespace services-1422 @ 06/24/23 13:04:04.771
  STEP: creating replication controller externalsvc in namespace services-1422 @ 06/24/23 13:04:04.785
  I0624 13:04:04.795864      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-1422, replica count: 2
  E0624 13:04:05.578841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:06.578902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:07.579095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 13:04:07.846565      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 06/24/23 13:04:07.851
  Jun 24 13:04:07.873: INFO: Creating new exec pod
  E0624 13:04:08.579175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:09.579823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:09.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-1422 exec execpod87r6j -- /bin/sh -x -c nslookup nodeport-service.services-1422.svc.cluster.local'
  Jun 24 13:04:10.095: INFO: stderr: "+ nslookup nodeport-service.services-1422.svc.cluster.local\n"
  Jun 24 13:04:10.095: INFO: stdout: "Server:\t\t10.152.183.180\nAddress:\t10.152.183.180#53\n\nnodeport-service.services-1422.svc.cluster.local\tcanonical name = externalsvc.services-1422.svc.cluster.local.\nName:\texternalsvc.services-1422.svc.cluster.local\nAddress: 10.152.183.108\n\n"
  Jun 24 13:04:10.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-1422, will wait for the garbage collector to delete the pods @ 06/24/23 13:04:10.101
  Jun 24 13:04:10.163: INFO: Deleting ReplicationController externalsvc took: 7.385565ms
  Jun 24 13:04:10.263: INFO: Terminating ReplicationController externalsvc pods took: 100.18903ms
  E0624 13:04:10.580611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:11.581259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:12.582135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:12.780: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-1422" for this suite. @ 06/24/23 13:04:12.793
• [8.089 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 06/24/23 13:04:12.802
  Jun 24 13:04:12.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename containers @ 06/24/23 13:04:12.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:12.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:12.828
  STEP: Creating a pod to test override arguments @ 06/24/23 13:04:12.832
  E0624 13:04:13.583163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:14.583263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:15.583741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:16.584821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:04:16.86
  Jun 24 13:04:16.863: INFO: Trying to get logs from node ip-172-31-19-205 pod client-containers-915bb69a-4aed-4b9b-b831-b422d6b7c6c2 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:04:16.87
  Jun 24 13:04:16.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3315" for this suite. @ 06/24/23 13:04:16.89
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 06/24/23 13:04:16.902
  Jun 24 13:04:16.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:04:16.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:16.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:16.933
  STEP: create deployment with httpd image @ 06/24/23 13:04:16.938
  Jun 24 13:04:16.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1293 create -f -'
  E0624 13:04:17.585633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:18.089: INFO: stderr: ""
  Jun 24 13:04:18.089: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 06/24/23 13:04:18.089
  Jun 24 13:04:18.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1293 diff -f -'
  E0624 13:04:18.586350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:18.616: INFO: rc: 1
  Jun 24 13:04:18.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1293 delete -f -'
  Jun 24 13:04:18.701: INFO: stderr: ""
  Jun 24 13:04:18.701: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jun 24 13:04:18.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1293" for this suite. @ 06/24/23 13:04:18.706
• [1.812 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 06/24/23 13:04:18.714
  Jun 24 13:04:18.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:04:18.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:18.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:18.74
  STEP: creating a replication controller @ 06/24/23 13:04:18.744
  Jun 24 13:04:18.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 create -f -'
  Jun 24 13:04:19.148: INFO: stderr: ""
  Jun 24 13:04:19.148: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/24/23 13:04:19.148
  Jun 24 13:04:19.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 13:04:19.244: INFO: stderr: ""
  Jun 24 13:04:19.244: INFO: stdout: "update-demo-nautilus-86kl4 update-demo-nautilus-w9l74 "
  Jun 24 13:04:19.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods update-demo-nautilus-86kl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 13:04:19.330: INFO: stderr: ""
  Jun 24 13:04:19.330: INFO: stdout: ""
  Jun 24 13:04:19.330: INFO: update-demo-nautilus-86kl4 is created but not running
  E0624 13:04:19.587134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:20.587308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:21.588247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:22.588447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:23.588792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:24.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun 24 13:04:24.418: INFO: stderr: ""
  Jun 24 13:04:24.418: INFO: stdout: "update-demo-nautilus-86kl4 update-demo-nautilus-w9l74 "
  Jun 24 13:04:24.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods update-demo-nautilus-86kl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 13:04:24.499: INFO: stderr: ""
  Jun 24 13:04:24.499: INFO: stdout: "true"
  Jun 24 13:04:24.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods update-demo-nautilus-86kl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0624 13:04:24.589832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:04:24.592: INFO: stderr: ""
  Jun 24 13:04:24.592: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 13:04:24.592: INFO: validating pod update-demo-nautilus-86kl4
  Jun 24 13:04:24.600: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 13:04:24.600: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 13:04:24.600: INFO: update-demo-nautilus-86kl4 is verified up and running
  Jun 24 13:04:24.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods update-demo-nautilus-w9l74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun 24 13:04:24.684: INFO: stderr: ""
  Jun 24 13:04:24.684: INFO: stdout: "true"
  Jun 24 13:04:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods update-demo-nautilus-w9l74 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun 24 13:04:24.770: INFO: stderr: ""
  Jun 24 13:04:24.770: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun 24 13:04:24.770: INFO: validating pod update-demo-nautilus-w9l74
  Jun 24 13:04:24.775: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun 24 13:04:24.775: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun 24 13:04:24.775: INFO: update-demo-nautilus-w9l74 is verified up and running
  STEP: using delete to clean up resources @ 06/24/23 13:04:24.775
  Jun 24 13:04:24.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 delete --grace-period=0 --force -f -'
  Jun 24 13:04:24.860: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:04:24.860: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun 24 13:04:24.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get rc,svc -l name=update-demo --no-headers'
  Jun 24 13:04:24.994: INFO: stderr: "No resources found in kubectl-1909 namespace.\n"
  Jun 24 13:04:24.994: INFO: stdout: ""
  Jun 24 13:04:24.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-1909 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 24 13:04:25.113: INFO: stderr: ""
  Jun 24 13:04:25.114: INFO: stdout: ""
  Jun 24 13:04:25.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1909" for this suite. @ 06/24/23 13:04:25.119
• [6.415 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 06/24/23 13:04:25.13
  Jun 24 13:04:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:04:25.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:25.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:25.159
  Jun 24 13:04:25.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-9327 version'
  Jun 24 13:04:25.330: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jun 24 13:04:25.330: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T02:06:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jun 24 13:04:25.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9327" for this suite. @ 06/24/23 13:04:25.335
• [0.212 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 06/24/23 13:04:25.343
  Jun 24 13:04:25.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename ingress @ 06/24/23 13:04:25.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:25.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:25.366
  STEP: getting /apis @ 06/24/23 13:04:25.37
  STEP: getting /apis/networking.k8s.io @ 06/24/23 13:04:25.375
  STEP: getting /apis/networking.k8s.iov1 @ 06/24/23 13:04:25.377
  STEP: creating @ 06/24/23 13:04:25.378
  STEP: getting @ 06/24/23 13:04:25.401
  STEP: listing @ 06/24/23 13:04:25.414
  STEP: watching @ 06/24/23 13:04:25.418
  Jun 24 13:04:25.419: INFO: starting watch
  STEP: cluster-wide listing @ 06/24/23 13:04:25.421
  STEP: cluster-wide watching @ 06/24/23 13:04:25.425
  Jun 24 13:04:25.425: INFO: starting watch
  STEP: patching @ 06/24/23 13:04:25.426
  STEP: updating @ 06/24/23 13:04:25.434
  Jun 24 13:04:25.449: INFO: waiting for watch events with expected annotations
  Jun 24 13:04:25.450: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/24/23 13:04:25.451
  STEP: updating /status @ 06/24/23 13:04:25.464
  STEP: get /status @ 06/24/23 13:04:25.477
  STEP: deleting @ 06/24/23 13:04:25.482
  STEP: deleting a collection @ 06/24/23 13:04:25.504
  Jun 24 13:04:25.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7836" for this suite. @ 06/24/23 13:04:25.536
• [0.203 seconds]
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 06/24/23 13:04:25.548
  Jun 24 13:04:25.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sysctl @ 06/24/23 13:04:25.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:25.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:25.58
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 06/24/23 13:04:25.585
  E0624 13:04:25.591731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Watching for error events or started pod @ 06/24/23 13:04:25.608
  E0624 13:04:26.592785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:27.593018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 06/24/23 13:04:27.613
  E0624 13:04:28.593109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:29.593523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 06/24/23 13:04:29.626
  STEP: Getting logs from the pod @ 06/24/23 13:04:29.626
  STEP: Checking that the sysctl is actually updated @ 06/24/23 13:04:29.634
  Jun 24 13:04:29.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-4392" for this suite. @ 06/24/23 13:04:29.639
• [4.100 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 06/24/23 13:04:29.649
  Jun 24 13:04:29.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context @ 06/24/23 13:04:29.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:29.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:29.679
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/24/23 13:04:29.683
  E0624 13:04:30.593596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:31.593690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:32.594408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:33.594539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:04:33.711
  Jun 24 13:04:33.715: INFO: Trying to get logs from node ip-172-31-19-205 pod security-context-e490f233-2796-4750-8a6a-0636073a1d18 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:04:33.723
  Jun 24 13:04:33.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6341" for this suite. @ 06/24/23 13:04:33.745
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 06/24/23 13:04:33.757
  Jun 24 13:04:33.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:04:33.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:33.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:33.782
  STEP: Creating configMap with name configmap-test-volume-df9b45dd-2f95-4455-a75c-bd382e07d712 @ 06/24/23 13:04:33.786
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:04:33.794
  E0624 13:04:34.594653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:35.594741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:36.595349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:37.595689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:04:37.821
  Jun 24 13:04:37.825: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-b9b42603-ccbc-4c71-8011-cb312866b103 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:04:37.832
  Jun 24 13:04:37.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5355" for this suite. @ 06/24/23 13:04:37.859
• [4.109 seconds]
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 06/24/23 13:04:37.867
  Jun 24 13:04:37.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename taint-multiple-pods @ 06/24/23 13:04:37.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:04:37.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:04:37.89
  Jun 24 13:04:37.895: INFO: Waiting up to 1m0s for all nodes to be ready
  E0624 13:04:38.595846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:39.596810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:40.596988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:41.597299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:42.597460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:43.597604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:44.597716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:45.597811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:46.598476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:47.598627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:48.598694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:49.598785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:50.598956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:51.599178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:52.600364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:53.600342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:54.601109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:55.601534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:56.602497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:57.603248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:58.604062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:04:59.604779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:00.604886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:01.605006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:02.605874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:03.606183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:04.607127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:05.607348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:06.607489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:07.607717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:08.608683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:09.608792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:10.608913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:11.609223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:12.609376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:13.609985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:14.610177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:15.610285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:16.610828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:17.610899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:18.610996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:19.611196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:20.611713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:21.611816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:22.612784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:23.613000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:24.613115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:25.613797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:26.613924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:27.614279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:28.614356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:29.614647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:30.615250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:31.615357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:32.615811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:33.616821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:34.616933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:35.617362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:36.617495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:37.617663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:05:37.915: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 13:05:37.920: INFO: Starting informer...
  STEP: Starting pods... @ 06/24/23 13:05:37.92
  Jun 24 13:05:38.143: INFO: Pod1 is running on ip-172-31-19-205. Tainting Node
  E0624 13:05:38.619656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:39.620510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:05:40.366: INFO: Pod2 is running on ip-172-31-19-205. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/24/23 13:05:40.366
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/24/23 13:05:40.378
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 06/24/23 13:05:40.382
  E0624 13:05:40.621343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:41.621535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:42.621572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:43.621652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:44.621775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:45.622420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:05:46.167: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0624 13:05:46.622630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:47.622733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:48.622933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:49.623482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:50.624686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:51.625184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:52.625649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:53.625413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:54.625482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:55.625573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:56.625686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:57.625834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:58.626026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:05:59.626161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:00.626260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:01.626359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:02.626479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:03.626573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:04.626685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:05.627481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:06:06.205: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jun 24 13:06:06.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/24/23 13:06:06.225
  STEP: Destroying namespace "taint-multiple-pods-2881" for this suite. @ 06/24/23 13:06:06.229
• [88.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 06/24/23 13:06:06.237
  Jun 24 13:06:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:06:06.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:06:06.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:06:06.286
  STEP: Creating secret with name s-test-opt-del-36d76357-6370-4d13-9cfd-cc2393c6f026 @ 06/24/23 13:06:06.295
  STEP: Creating secret with name s-test-opt-upd-9e92894e-71d1-4d90-bb45-02537226a1e6 @ 06/24/23 13:06:06.3
  STEP: Creating the pod @ 06/24/23 13:06:06.307
  E0624 13:06:06.627711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:07.628708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-36d76357-6370-4d13-9cfd-cc2393c6f026 @ 06/24/23 13:06:08.376
  STEP: Updating secret s-test-opt-upd-9e92894e-71d1-4d90-bb45-02537226a1e6 @ 06/24/23 13:06:08.383
  STEP: Creating secret with name s-test-opt-create-254afb4b-1600-4b3f-b6a5-b70ed453b495 @ 06/24/23 13:06:08.389
  STEP: waiting to observe update in volume @ 06/24/23 13:06:08.395
  E0624 13:06:08.629686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:09.629822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:10.630804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:11.630879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:12.631407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:13.632291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:14.632406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:15.632809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:16.633143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:17.633265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:18.633853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:19.634173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:20.635007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:21.635342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:22.635698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:23.635842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:24.636171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:25.636289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:26.636688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:27.636794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:28.637074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:29.637312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:30.638068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:31.638164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:32.639052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:33.639207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:34.639947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:35.640066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:36.640437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:37.640733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:38.641158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:39.641647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:40.641723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:41.641890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:42.642866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:43.642972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:44.643481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:45.643707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:46.644693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:47.644823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:48.644964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:49.645496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:50.646385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:51.646746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:52.647622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:53.648105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:54.648727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:55.649117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:56.649832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:57.650478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:58.650823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:06:59.650965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:00.651422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:01.651795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:02.651918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:03.651997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:04.652085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:05.652206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:06.652319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:07.652435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:08.652647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:09.652760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:10.653014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:11.653147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:12.653261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:13.653292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:14.653403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:15.653521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:16.653613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:17.654485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:18.654585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:19.654697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:20.654788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:21.655152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:22.655263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:23.655939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:24.656066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:25.656714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:26.656826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:27.657001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:28.657143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:29.658120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:30.658383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:31.658818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:32.658920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:33.659792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:34.659901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:35.660096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:36.660193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:07:36.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1377" for this suite. @ 06/24/23 13:07:36.878
• [90.648 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 06/24/23 13:07:36.887
  Jun 24 13:07:36.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption @ 06/24/23 13:07:36.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:36.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:36.917
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:07:36.925
  E0624 13:07:37.660989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:38.661138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 06/24/23 13:07:38.933
  STEP: Waiting for all pods to be running @ 06/24/23 13:07:38.942
  Jun 24 13:07:38.947: INFO: running pods: 0 < 1
  E0624 13:07:39.661674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:40.661776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/24/23 13:07:40.953
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:07:40.966
  STEP: Patching PodDisruptionBudget status @ 06/24/23 13:07:40.974
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:07:40.986
  Jun 24 13:07:40.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8331" for this suite. @ 06/24/23 13:07:40.994
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 06/24/23 13:07:41.002
  Jun 24 13:07:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 13:07:41.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:41.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:41.025
  STEP: Counting existing ResourceQuota @ 06/24/23 13:07:41.029
  E0624 13:07:41.662196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:42.662410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:43.663267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:44.664139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:45.664645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 13:07:46.034
  STEP: Ensuring resource quota status is calculated @ 06/24/23 13:07:46.039
  E0624 13:07:46.665155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:47.665835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:07:48.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1174" for this suite. @ 06/24/23 13:07:48.05
• [7.057 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 06/24/23 13:07:48.061
  Jun 24 13:07:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:07:48.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:48.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:48.093
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/24/23 13:07:48.097
  E0624 13:07:48.665878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:49.665984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:50.667035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:51.667152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:07:52.121
  Jun 24 13:07:52.124: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-fbfb8cec-c453-417f-b78b-82853e4af028 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:07:52.132
  Jun 24 13:07:52.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6803" for this suite. @ 06/24/23 13:07:52.153
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 06/24/23 13:07:52.17
  Jun 24 13:07:52.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename ingressclass @ 06/24/23 13:07:52.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:52.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:52.203
  STEP: getting /apis @ 06/24/23 13:07:52.208
  STEP: getting /apis/networking.k8s.io @ 06/24/23 13:07:52.214
  STEP: getting /apis/networking.k8s.iov1 @ 06/24/23 13:07:52.216
  STEP: creating @ 06/24/23 13:07:52.217
  STEP: getting @ 06/24/23 13:07:52.234
  STEP: listing @ 06/24/23 13:07:52.238
  STEP: watching @ 06/24/23 13:07:52.242
  Jun 24 13:07:52.242: INFO: starting watch
  STEP: patching @ 06/24/23 13:07:52.243
  STEP: updating @ 06/24/23 13:07:52.25
  Jun 24 13:07:52.257: INFO: waiting for watch events with expected annotations
  Jun 24 13:07:52.257: INFO: saw patched and updated annotations
  STEP: deleting @ 06/24/23 13:07:52.257
  STEP: deleting a collection @ 06/24/23 13:07:52.271
  Jun 24 13:07:52.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-5858" for this suite. @ 06/24/23 13:07:52.295
• [0.132 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 06/24/23 13:07:52.303
  Jun 24 13:07:52.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 13:07:52.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:52.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:52.334
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 13:07:52.338
  E0624 13:07:52.667761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:53.667467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:54.668352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:55.668798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:07:56.365
  Jun 24 13:07:56.368: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-28aef819-ab8c-476c-8821-ce7aa38f6c7e container client-container: <nil>
  STEP: delete the pod @ 06/24/23 13:07:56.376
  Jun 24 13:07:56.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6834" for this suite. @ 06/24/23 13:07:56.4
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 06/24/23 13:07:56.412
  Jun 24 13:07:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 13:07:56.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:07:56.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:07:56.442
  STEP: Creating a test headless service @ 06/24/23 13:07:56.447
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7857.svc.cluster.local;sleep 1; done
   @ 06/24/23 13:07:56.454
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7857.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7857.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7857.svc.cluster.local;sleep 1; done
   @ 06/24/23 13:07:56.454
  STEP: creating a pod to probe DNS @ 06/24/23 13:07:56.454
  STEP: submitting the pod to kubernetes @ 06/24/23 13:07:56.455
  E0624 13:07:56.669438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:57.669567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 13:07:58.48
  STEP: looking for the results for each expected name from probers @ 06/24/23 13:07:58.484
  Jun 24 13:07:58.490: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.496: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.501: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.505: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.510: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.514: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.519: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.524: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7857.svc.cluster.local from pod dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe: the server could not find the requested resource (get pods dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe)
  Jun 24 13:07:58.524: INFO: Lookups using dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7857.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7857.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7857.svc.cluster.local jessie_udp@dns-test-service-2.dns-7857.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7857.svc.cluster.local]

  E0624 13:07:58.670628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:07:59.670765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:00.671049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:01.671163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:02.671265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:08:03.558: INFO: DNS probes using dns-7857/dns-test-15309cbb-d24e-4159-8c02-7c3e383b83fe succeeded

  Jun 24 13:08:03.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:08:03.563
  STEP: deleting the test headless service @ 06/24/23 13:08:03.576
  STEP: Destroying namespace "dns-7857" for this suite. @ 06/24/23 13:08:03.588
• [7.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 06/24/23 13:08:03.599
  Jun 24 13:08:03.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/24/23 13:08:03.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:08:03.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:08:03.651
  STEP: fetching the /apis discovery document @ 06/24/23 13:08:03.655
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 06/24/23 13:08:03.657
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 06/24/23 13:08:03.657
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 06/24/23 13:08:03.657
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 06/24/23 13:08:03.658
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 06/24/23 13:08:03.658
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 06/24/23 13:08:03.659
  Jun 24 13:08:03.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9806" for this suite. @ 06/24/23 13:08:03.664
  E0624 13:08:03.671928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.074 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 06/24/23 13:08:03.674
  Jun 24 13:08:03.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 13:08:03.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:08:03.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:08:03.703
  STEP: Creating service test in namespace statefulset-1658 @ 06/24/23 13:08:03.707
  STEP: Creating statefulset ss in namespace statefulset-1658 @ 06/24/23 13:08:03.718
  Jun 24 13:08:03.729: INFO: Found 0 stateful pods, waiting for 1
  E0624 13:08:04.672079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:05.672189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:06.672310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:07.672768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:08.672952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:09.673471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:10.673781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:11.673874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:12.674351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:13.675257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:08:13.733: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 06/24/23 13:08:13.74
  STEP: Getting /status @ 06/24/23 13:08:13.748
  Jun 24 13:08:13.753: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 06/24/23 13:08:13.753
  Jun 24 13:08:13.766: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 06/24/23 13:08:13.766
  Jun 24 13:08:13.768: INFO: Observed &StatefulSet event: ADDED
  Jun 24 13:08:13.768: INFO: Found Statefulset ss in namespace statefulset-1658 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun 24 13:08:13.768: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 06/24/23 13:08:13.768
  Jun 24 13:08:13.768: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun 24 13:08:13.779: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 06/24/23 13:08:13.779
  Jun 24 13:08:13.781: INFO: Observed &StatefulSet event: ADDED
  Jun 24 13:08:13.782: INFO: Deleting all statefulset in ns statefulset-1658
  Jun 24 13:08:13.786: INFO: Scaling statefulset ss to 0
  E0624 13:08:14.675417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:15.676075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:16.676210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:17.676861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:18.676986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:19.677092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:20.677233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:21.677279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:22.677560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:23.677676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:08:23.807: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 13:08:23.811: INFO: Deleting statefulset ss
  Jun 24 13:08:23.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1658" for this suite. @ 06/24/23 13:08:23.834
• [20.166 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 06/24/23 13:08:23.841
  Jun 24 13:08:23.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 13:08:23.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:08:23.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:08:23.87
  STEP: Counting existing ResourceQuota @ 06/24/23 13:08:23.875
  E0624 13:08:24.678406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:25.678542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:26.678659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:27.679627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:28.680756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 13:08:28.879
  STEP: Ensuring resource quota status is calculated @ 06/24/23 13:08:28.886
  E0624 13:08:29.680873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:30.680940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 06/24/23 13:08:30.891
  STEP: Creating a NodePort Service @ 06/24/23 13:08:30.91
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 06/24/23 13:08:30.937
  STEP: Ensuring resource quota status captures service creation @ 06/24/23 13:08:30.959
  E0624 13:08:31.681024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:32.681373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 06/24/23 13:08:32.965
  STEP: Ensuring resource quota status released usage @ 06/24/23 13:08:33.009
  E0624 13:08:33.682255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:34.682351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:08:35.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4472" for this suite. @ 06/24/23 13:08:35.019
• [11.188 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 06/24/23 13:08:35.03
  Jun 24 13:08:35.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 13:08:35.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:08:35.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:08:35.06
  Jun 24 13:08:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9108" for this suite. @ 06/24/23 13:08:35.115
• [0.095 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 06/24/23 13:08:35.126
  Jun 24 13:08:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename cronjob @ 06/24/23 13:08:35.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:08:35.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:08:35.151
  STEP: Creating a suspended cronjob @ 06/24/23 13:08:35.155
  STEP: Ensuring no jobs are scheduled @ 06/24/23 13:08:35.163
  E0624 13:08:35.682397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:36.682557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:37.682601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:38.682713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:39.683596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:40.683712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:41.684805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:42.684873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:43.684985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:44.685090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:45.685908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:46.685994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:47.686621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:48.686718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:49.687390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:50.687746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:51.687862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:52.687978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:53.688788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:54.688920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:55.689772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:56.689881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:57.689935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:58.690049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:08:59.690436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:00.690700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:01.691023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:02.691587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:03.691855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:04.691961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:05.692726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:06.693071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:07.693175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:08.693287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:09.693437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:10.693507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:11.694353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:12.695372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:13.696130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:14.696222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:15.696317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:16.696545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:17.697242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:18.697497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:19.698510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:20.698624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:21.699363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:22.699689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:23.699983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:24.700102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:25.700188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:26.701096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:27.701190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:28.701835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:29.702237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:30.702351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:31.702462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:32.702577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:33.702696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:34.702776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:35.702871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:36.703141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:37.703200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:38.703445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:39.704453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:40.704513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:41.704701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:42.704778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:43.704892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:44.705001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:45.705114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:46.705456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:47.706444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:48.706884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:49.707834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:50.707941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:51.708734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:52.708889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:53.709956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:54.710282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:55.710962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:56.711927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:57.712767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:58.713512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:09:59.714194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:00.714364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:01.715086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:02.716120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:03.716250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:04.716761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:05.717606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:06.717867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:07.718849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:08.719683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:09.720384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:10.721362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:11.722221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:12.722725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:13.723501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:14.723683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:15.724744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:16.724848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:17.724992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:18.726008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:19.726677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:20.726983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:21.727818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:22.728765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:23.729426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:24.729607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:25.730089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:26.730257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:27.731166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:28.731348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:29.731429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:30.731689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:31.731964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:32.732765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:33.733326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:34.733660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:35.734709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:36.735055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:37.735500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:38.735936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:39.736765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:40.737163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:41.737395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:42.737760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:43.737886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:44.738121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:45.738180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:46.738487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:47.738612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:48.738714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:49.738832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:50.739288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:51.739430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:52.739848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:53.740774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:54.741861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:55.742782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:56.742984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:57.743143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:58.743239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:10:59.743964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:00.744074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:01.744183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:02.744802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:03.744947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:04.745105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:05.745615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:06.745755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:07.745842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:08.745960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:09.746723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:10.746870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:11.746863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:12.747012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:13.747955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:14.748769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:15.749831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:16.749938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:17.750537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:18.750888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:19.752004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:20.752075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:21.752186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:22.752290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:23.752802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:24.752915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:25.753057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:26.753172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:27.754129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:28.754251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:29.754356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:30.754460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:31.754624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:32.754712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:33.754828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:34.755405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:35.755480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:36.755820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:37.756091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:38.756178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:39.756862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:40.757003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:41.757681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:42.757757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:43.757891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:44.758002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:45.758085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:46.758330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:47.758438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:48.758541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:49.758645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:50.758762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:51.759748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:52.759875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:53.760863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:54.760935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:55.761981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:56.762254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:57.762671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:58.762759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:11:59.763395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:00.763688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:01.764753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:02.765011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:03.765432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:04.765771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:05.766151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:06.766449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:07.766920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:08.767172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:09.767302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:10.767410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:11.767492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:12.767684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:13.768821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:14.768926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:15.769845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:16.769979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:17.770079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:18.770190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:19.770929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:20.771436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:21.772224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:22.772795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:23.772867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:24.773635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:25.774649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:26.774929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:27.775543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:28.775717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:29.776803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:30.776983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:31.777173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:32.777279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:33.777384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:34.777542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:35.778358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:36.779225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:37.780065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:38.780830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:39.781015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:40.781913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:41.782505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:42.782875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:43.783817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:44.784802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:45.785603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:46.786228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:47.787174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:48.787701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:49.787814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:50.788431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:51.788974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:52.789413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:53.790443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:54.790563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:55.790625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:56.790717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:57.791675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:58.791987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:12:59.792471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:00.792928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:01.793858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:02.794264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:03.795040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:04.795363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:05.795681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:06.795942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:07.796210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:08.796397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:09.796648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:10.797147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:11.798091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:12.798490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:13.799104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:14.799214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:15.800148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:16.800254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:17.800758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:18.800955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:19.801066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:20.801898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:21.802025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:22.802119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:23.802187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:24.802399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:25.802670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:26.802870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:27.803039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:28.803136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:29.803243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:30.803368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:31.803496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:32.803691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:33.804753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:34.805370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 06/24/23 13:13:35.171
  STEP: Removing cronjob @ 06/24/23 13:13:35.174
  Jun 24 13:13:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3995" for this suite. @ 06/24/23 13:13:35.194
• [300.084 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 06/24/23 13:13:35.211
  Jun 24 13:13:35.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 13:13:35.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:13:35.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:13:35.268
  STEP: Setting up server cert @ 06/24/23 13:13:35.306
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 13:13:35.718
  STEP: Deploying the webhook pod @ 06/24/23 13:13:35.727
  STEP: Wait for the deployment to be ready @ 06/24/23 13:13:35.745
  Jun 24 13:13:35.753: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0624 13:13:35.805774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:36.805990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:13:37.764
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:13:37.776
  E0624 13:13:37.806011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:13:38.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 24 13:13:38.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:13:38.806311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2184-crds.webhook.example.com via the AdmissionRegistration API @ 06/24/23 13:13:39.296
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/24/23 13:13:39.316
  E0624 13:13:39.806751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:40.806927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:13:41.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 13:13:41.807203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9413" for this suite. @ 06/24/23 13:13:41.957
  STEP: Destroying namespace "webhook-markers-1607" for this suite. @ 06/24/23 13:13:41.964
• [6.763 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 06/24/23 13:13:41.974
  Jun 24 13:13:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:13:41.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:13:41.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:13:42.001
  STEP: Creating configMap with name configmap-test-volume-6b0e9988-c031-4153-9e0c-5423da9a2257 @ 06/24/23 13:13:42.006
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:13:42.011
  E0624 13:13:42.807603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:43.807728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:44.807788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:45.808760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:13:46.034
  Jun 24 13:13:46.038: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-c181a530-a750-4d5d-b00d-69b18cc89f98 container configmap-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 13:13:46.062
  Jun 24 13:13:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2119" for this suite. @ 06/24/23 13:13:46.082
• [4.115 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 06/24/23 13:13:46.089
  Jun 24 13:13:46.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename cronjob @ 06/24/23 13:13:46.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:13:46.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:13:46.116
  STEP: Creating a ReplaceConcurrent cronjob @ 06/24/23 13:13:46.12
  STEP: Ensuring a job is scheduled @ 06/24/23 13:13:46.127
  E0624 13:13:46.808915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:47.809283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:48.809456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:49.809547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:50.810084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:51.810182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:52.810659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:53.810564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:54.810859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:55.811234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:56.811321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:57.811742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:58.811860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:13:59.811993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:00.812591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:01.812775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 06/24/23 13:14:02.131
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/24/23 13:14:02.135
  STEP: Ensuring the job is replaced with a new one @ 06/24/23 13:14:02.139
  E0624 13:14:02.813774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:03.813942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:04.814095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:05.814191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:06.814222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:07.814321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:08.814466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:09.814554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:10.815661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:11.815769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:12.816677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:13.816800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:14.816938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:15.817067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:16.817162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:17.817261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:18.817515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:19.817573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:20.818670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:21.818786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:22.819733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:23.819849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:24.820812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:25.820931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:26.821054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:27.821155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:28.821809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:29.821940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:30.822045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:31.822108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:32.822212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:33.822319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:34.822393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:35.822557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:36.823525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:37.823740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:38.824721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:39.825241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:40.825343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:41.825632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:42.825737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:43.825854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:44.825988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:45.826097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:46.826224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:47.826271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:48.826325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:49.826460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:50.826610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:51.826727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:52.827090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:53.827434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:54.827543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:55.827695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:56.828763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:57.828887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:58.828885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:14:59.828995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 06/24/23 13:15:00.147
  Jun 24 13:15:00.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8793" for this suite. @ 06/24/23 13:15:00.164
• [74.082 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 06/24/23 13:15:00.172
  Jun 24 13:15:00.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename watch @ 06/24/23 13:15:00.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:15:00.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:15:00.218
  STEP: creating a watch on configmaps with a certain label @ 06/24/23 13:15:00.222
  STEP: creating a new configmap @ 06/24/23 13:15:00.224
  STEP: modifying the configmap once @ 06/24/23 13:15:00.23
  STEP: changing the label value of the configmap @ 06/24/23 13:15:00.238
  STEP: Expecting to observe a delete notification for the watched object @ 06/24/23 13:15:00.248
  Jun 24 13:15:00.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34947 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:15:00.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34948 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:15:00.249: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34949 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 06/24/23 13:15:00.249
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 06/24/23 13:15:00.257
  E0624 13:15:00.829468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:01.829579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:02.829687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:03.830594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:04.831039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:05.831693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:06.831718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:07.832755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:08.833113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:09.833566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 06/24/23 13:15:10.258
  STEP: modifying the configmap a third time @ 06/24/23 13:15:10.269
  STEP: deleting the configmap @ 06/24/23 13:15:10.277
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 06/24/23 13:15:10.285
  Jun 24 13:15:10.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34985 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:15:10.286: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34986 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:15:10.286: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  797e121f-ca55-4440-92b4-dbd1a99d88da 34987 0 2023-06-24 13:15:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-24 13:15:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun 24 13:15:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3713" for this suite. @ 06/24/23 13:15:10.291
• [10.126 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 06/24/23 13:15:10.302
  Jun 24 13:15:10.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 13:15:10.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:15:10.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:15:10.328
  STEP: Creating pod busybox-8daf1306-0fab-40e4-abe1-a5f684802ffb in namespace container-probe-8153 @ 06/24/23 13:15:10.333
  E0624 13:15:10.834525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:11.834783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:15:12.357: INFO: Started pod busybox-8daf1306-0fab-40e4-abe1-a5f684802ffb in namespace container-probe-8153
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 13:15:12.357
  Jun 24 13:15:12.360: INFO: Initial restart count of pod busybox-8daf1306-0fab-40e4-abe1-a5f684802ffb is 0
  E0624 13:15:12.834954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:13.835047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:14.835832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:15.835939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:16.836051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:17.836132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:18.836804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:19.837300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:20.838258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:21.838375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:22.839375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:23.839713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:24.840319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:25.840553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:26.841617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:27.841740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:28.842559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:29.843056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:30.843822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:31.844792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:32.844878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:33.844983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:34.845122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:35.845203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:36.845313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:37.845447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:38.845558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:39.845670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:40.846490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:41.846991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:42.847653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:43.848029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:44.848906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:45.849292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:46.849708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:47.850072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:48.850154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:49.850890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:50.850989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:51.851297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:52.851618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:53.851723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:54.852600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:55.852970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:56.853721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:57.854134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:58.854185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:15:59.854402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:00.854704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:01.854983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:02.855230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:03.855387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:04.855911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:05.856767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:06.857678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:07.858044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:08.858832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:09.858998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:10.860031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:11.860139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:12.860333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:13.861207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:14.861968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:15.862143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:16.862533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:17.862985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:18.863539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:19.863682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:20.864402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:21.864746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:22.864851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:23.865160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:24.865376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:25.865674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:26.865850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:27.866314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:28.866906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:29.866992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:30.867766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:31.867896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:32.868570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:33.868680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:34.868792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:35.868907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:36.869810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:37.870067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:38.870168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:39.870589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:40.870644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:41.871054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:42.871899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:43.872758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:44.873363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:45.873527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:46.874327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:47.874435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:48.875010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:49.875142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:50.875692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:51.876074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:52.876561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:53.876860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:54.877725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:55.877842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:56.878767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:57.878893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:58.879455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:16:59.879721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:00.880454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:01.881257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:02.881742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:03.882380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:04.882471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:05.882790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:06.883054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:07.883460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:08.884469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:09.884891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:10.885596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:11.886069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:12.887070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:13.887171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:14.887376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:15.887683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:16.888518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:17.888629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:18.889144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:19.889338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:20.889814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:21.890667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:22.891388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:23.891504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:24.892499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:25.892750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:26.893519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:27.893878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:28.894356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:29.894700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:30.895332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:31.895737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:32.896069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:33.896768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:34.897264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:35.897445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:36.898463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:37.898747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:38.899619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:39.899822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:40.900352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:41.900947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:42.902015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:43.902525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:44.903610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:45.903702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:46.904085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:47.904771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:48.905649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:49.905743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:50.906507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:51.906679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:52.907029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:53.906888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:54.907681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:55.908765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:56.909252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:57.909383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:58.909950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:17:59.909965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:00.910632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:01.911140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:02.911850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:03.912770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:04.913745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:05.913986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:06.914016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:07.914233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:08.915052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:09.915236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:10.916450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:11.917005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:12.917135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:13.917242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:14.917990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:15.918091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:16.918933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:17.919051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:18.919475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:19.919684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:20.920429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:21.921401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:22.921768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:23.921967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:24.922698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:25.922877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:26.923223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:27.923852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:28.923946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:29.924753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:30.925099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:31.925380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:32.926391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:33.926682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:34.926803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:35.926897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:36.927723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:37.927901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:38.927962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:39.928485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:40.928563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:41.929464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:42.930401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:43.930925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:44.931646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:45.931707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:46.932314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:47.932425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:48.932683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:49.932807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:50.933095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:51.933535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:52.934601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:53.934741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:54.935098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:55.935631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:56.935825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:57.935962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:58.937011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:18:59.937140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:00.937953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:01.938278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:02.938285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:03.938558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:04.938822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:05.938973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:06.939754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:07.939878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:08.940233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:09.941216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:10.942005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:11.942243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:19:12.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 13:19:12.942485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod @ 06/24/23 13:19:12.943
  STEP: Destroying namespace "container-probe-8153" for this suite. @ 06/24/23 13:19:12.957
• [242.664 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 06/24/23 13:19:12.968
  Jun 24 13:19:12.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:19:12.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:19:12.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:19:12.991
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/24/23 13:19:12.996
  Jun 24 13:19:12.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2844 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jun 24 13:19:13.082: INFO: stderr: ""
  Jun 24 13:19:13.082: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/24/23 13:19:13.082
  Jun 24 13:19:13.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2844 delete pods e2e-test-httpd-pod'
  E0624 13:19:13.942596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:14.942710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:19:15.712: INFO: stderr: ""
  Jun 24 13:19:15.712: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun 24 13:19:15.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2844" for this suite. @ 06/24/23 13:19:15.717
• [2.756 seconds]
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 06/24/23 13:19:15.724
  Jun 24 13:19:15.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename taint-single-pod @ 06/24/23 13:19:15.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:19:15.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:19:15.753
  Jun 24 13:19:15.757: INFO: Waiting up to 1m0s for all nodes to be ready
  E0624 13:19:15.943428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:16.943932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:17.944815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:18.944937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:19.945968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:20.946094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:21.946513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:22.946959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:23.947889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:24.948117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:25.948474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:26.948718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:27.949117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:28.949211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:29.949470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:30.950064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:31.951035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:32.951863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:33.952347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:34.952769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:35.952885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:36.953689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:37.953960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:38.954078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:39.954165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:40.954257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:41.955141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:42.955664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:43.956011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:44.956239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:45.957373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:46.957931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:47.958658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:48.959179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:49.960034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:50.960484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:51.961057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:52.961572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:53.961741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:54.962000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:55.963060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:56.963634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:57.964606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:58.964714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:19:59.965328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:00.965440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:01.965973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:02.966081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:03.966292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:04.966689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:05.967689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:06.968446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:07.969236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:08.969492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:09.970367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:10.971381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:11.971675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:12.971713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:13.972536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:14.972640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:20:15.776: INFO: Waiting for terminating namespaces to be deleted...
  Jun 24 13:20:15.783: INFO: Starting informer...
  STEP: Starting pod... @ 06/24/23 13:20:15.783
  E0624 13:20:15.972836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:20:16.004: INFO: Pod is running on ip-172-31-19-205. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/24/23 13:20:16.004
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/24/23 13:20:16.019
  STEP: Waiting short time to make sure Pod is queued for deletion @ 06/24/23 13:20:16.027
  Jun 24 13:20:16.027: INFO: Pod wasn't evicted. Proceeding
  Jun 24 13:20:16.027: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/24/23 13:20:16.064
  STEP: Waiting some time to make sure that toleration time passed. @ 06/24/23 13:20:16.068
  E0624 13:20:16.973770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:17.973909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:18.974079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:19.974141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:20.974445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:21.975335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:22.975458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:23.975678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:24.976758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:25.977241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:26.978071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:27.978343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:28.978841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:29.979275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:30.979599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:31.980628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:32.980769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:33.981026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:34.981131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:35.981427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:36.982060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:37.982164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:38.982276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:39.982374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:40.982495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:41.983306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:42.983399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:43.983682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:44.984757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:45.985648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:46.986236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:47.986497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:48.986653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:49.986943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:50.987087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:51.988109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:52.988753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:53.989055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:54.989158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:55.989268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:56.989692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:57.989888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:58.989977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:20:59.990073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:00.990177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:01.990244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:02.990344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:03.990462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:04.990815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:05.990919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:06.990979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:07.991083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:08.991209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:09.991301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:10.991628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:11.992255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:12.992535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:13.992647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:14.992753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:15.992864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:16.993686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:17.993832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:18.993938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:19.994132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:20.994249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:21.994254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:22.994343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:23.994975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:24.995139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:25.995370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:26.996190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:27.996300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:28.996404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:29.996516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:30.996635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:31.069: INFO: Pod wasn't evicted. Test successful
  Jun 24 13:21:31.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-9248" for this suite. @ 06/24/23 13:21:31.076
• [135.359 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 06/24/23 13:21:31.086
  Jun 24 13:21:31.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:21:31.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:31.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:31.115
  STEP: Creating the pod @ 06/24/23 13:21:31.125
  E0624 13:21:31.997060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:32.997441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:33.685: INFO: Successfully updated pod "annotationupdateb9544f9c-7351-4e1c-8b94-5f4ded79db54"
  E0624 13:21:33.998227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:34.998346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:35.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2495" for this suite. @ 06/24/23 13:21:35.709
• [4.631 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 06/24/23 13:21:35.719
  Jun 24 13:21:35.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename pod-network-test @ 06/24/23 13:21:35.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:35.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:35.744
  STEP: Performing setup for networking test in namespace pod-network-test-4220 @ 06/24/23 13:21:35.749
  STEP: creating a selector @ 06/24/23 13:21:35.749
  STEP: Creating the service pods in kubernetes @ 06/24/23 13:21:35.749
  Jun 24 13:21:35.749: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0624 13:21:35.998465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:36.999245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:37.999959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:38.999990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:40.000855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:41.001037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:42.001859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:43.001969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:44.002422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:45.002556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:46.003407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:47.003988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/24/23 13:21:47.845
  E0624 13:21:48.004444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:49.004534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:49.881: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun 24 13:21:49.881: INFO: Going to poll 192.168.116.249 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 13:21:49.885: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.116.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:21:49.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:21:49.885: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:21:49.886: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4220/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.116.249+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0624 13:21:50.004999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:50.965: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun 24 13:21:50.965: INFO: Going to poll 192.168.150.239 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 13:21:50.969: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.150.239 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:21:50.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:21:50.970: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:21:50.970: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4220/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.150.239+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0624 13:21:51.005950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:52.006310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:52.039: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun 24 13:21:52.039: INFO: Going to poll 192.168.144.138 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun 24 13:21:52.044: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.144.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4220 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:21:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:21:52.045: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:21:52.045: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4220/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.144.138+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0624 13:21:53.006817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:53.125: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun 24 13:21:53.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4220" for this suite. @ 06/24/23 13:21:53.13
• [17.418 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 06/24/23 13:21:53.138
  Jun 24 13:21:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename discovery @ 06/24/23 13:21:53.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:53.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:53.164
  STEP: Setting up server cert @ 06/24/23 13:21:53.169
  Jun 24 13:21:53.766: INFO: Checking APIGroup: apiregistration.k8s.io
  Jun 24 13:21:53.767: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jun 24 13:21:53.767: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jun 24 13:21:53.767: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jun 24 13:21:53.767: INFO: Checking APIGroup: apps
  Jun 24 13:21:53.769: INFO: PreferredVersion.GroupVersion: apps/v1
  Jun 24 13:21:53.769: INFO: Versions found [{apps/v1 v1}]
  Jun 24 13:21:53.769: INFO: apps/v1 matches apps/v1
  Jun 24 13:21:53.769: INFO: Checking APIGroup: events.k8s.io
  Jun 24 13:21:53.770: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jun 24 13:21:53.770: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jun 24 13:21:53.770: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jun 24 13:21:53.770: INFO: Checking APIGroup: authentication.k8s.io
  Jun 24 13:21:53.771: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jun 24 13:21:53.771: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jun 24 13:21:53.771: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jun 24 13:21:53.771: INFO: Checking APIGroup: authorization.k8s.io
  Jun 24 13:21:53.772: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jun 24 13:21:53.772: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jun 24 13:21:53.772: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jun 24 13:21:53.772: INFO: Checking APIGroup: autoscaling
  Jun 24 13:21:53.773: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jun 24 13:21:53.773: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jun 24 13:21:53.773: INFO: autoscaling/v2 matches autoscaling/v2
  Jun 24 13:21:53.773: INFO: Checking APIGroup: batch
  Jun 24 13:21:53.774: INFO: PreferredVersion.GroupVersion: batch/v1
  Jun 24 13:21:53.774: INFO: Versions found [{batch/v1 v1}]
  Jun 24 13:21:53.774: INFO: batch/v1 matches batch/v1
  Jun 24 13:21:53.774: INFO: Checking APIGroup: certificates.k8s.io
  Jun 24 13:21:53.775: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jun 24 13:21:53.775: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jun 24 13:21:53.775: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jun 24 13:21:53.775: INFO: Checking APIGroup: networking.k8s.io
  Jun 24 13:21:53.776: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jun 24 13:21:53.776: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jun 24 13:21:53.776: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jun 24 13:21:53.776: INFO: Checking APIGroup: policy
  Jun 24 13:21:53.777: INFO: PreferredVersion.GroupVersion: policy/v1
  Jun 24 13:21:53.777: INFO: Versions found [{policy/v1 v1}]
  Jun 24 13:21:53.777: INFO: policy/v1 matches policy/v1
  Jun 24 13:21:53.777: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jun 24 13:21:53.778: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jun 24 13:21:53.778: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jun 24 13:21:53.778: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jun 24 13:21:53.778: INFO: Checking APIGroup: storage.k8s.io
  Jun 24 13:21:53.779: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jun 24 13:21:53.779: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jun 24 13:21:53.779: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jun 24 13:21:53.779: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jun 24 13:21:53.781: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jun 24 13:21:53.781: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jun 24 13:21:53.781: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jun 24 13:21:53.781: INFO: Checking APIGroup: apiextensions.k8s.io
  Jun 24 13:21:53.782: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jun 24 13:21:53.782: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jun 24 13:21:53.782: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jun 24 13:21:53.782: INFO: Checking APIGroup: scheduling.k8s.io
  Jun 24 13:21:53.783: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jun 24 13:21:53.783: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jun 24 13:21:53.783: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jun 24 13:21:53.783: INFO: Checking APIGroup: coordination.k8s.io
  Jun 24 13:21:53.784: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jun 24 13:21:53.784: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jun 24 13:21:53.784: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jun 24 13:21:53.784: INFO: Checking APIGroup: node.k8s.io
  Jun 24 13:21:53.785: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jun 24 13:21:53.785: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jun 24 13:21:53.785: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jun 24 13:21:53.785: INFO: Checking APIGroup: discovery.k8s.io
  Jun 24 13:21:53.786: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jun 24 13:21:53.786: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jun 24 13:21:53.786: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jun 24 13:21:53.786: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jun 24 13:21:53.787: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jun 24 13:21:53.787: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jun 24 13:21:53.787: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jun 24 13:21:53.787: INFO: Checking APIGroup: metrics.k8s.io
  Jun 24 13:21:53.789: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jun 24 13:21:53.789: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jun 24 13:21:53.789: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jun 24 13:21:53.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-779" for this suite. @ 06/24/23 13:21:53.795
• [0.663 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 06/24/23 13:21:53.803
  Jun 24 13:21:53.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 13:21:53.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:53.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:53.831
  Jun 24 13:21:53.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:21:54.007385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:55.007498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:56.007629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:21:56.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4490" for this suite. @ 06/24/23 13:21:56.482
• [2.688 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 06/24/23 13:21:56.492
  Jun 24 13:21:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 13:21:56.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:56.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:56.527
  STEP: creating a ServiceAccount @ 06/24/23 13:21:56.532
  STEP: watching for the ServiceAccount to be added @ 06/24/23 13:21:56.545
  STEP: patching the ServiceAccount @ 06/24/23 13:21:56.549
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 06/24/23 13:21:56.559
  STEP: deleting the ServiceAccount @ 06/24/23 13:21:56.564
  Jun 24 13:21:56.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9652" for this suite. @ 06/24/23 13:21:56.586
• [0.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 06/24/23 13:21:56.599
  Jun 24 13:21:56.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename job @ 06/24/23 13:21:56.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:56.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:56.626
  STEP: Creating a job @ 06/24/23 13:21:56.631
  STEP: Ensure pods equal to parallelism count is attached to the job @ 06/24/23 13:21:56.64
  E0624 13:21:57.010260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:21:58.010350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/24/23 13:21:58.645
  STEP: updating /status @ 06/24/23 13:21:58.654
  STEP: get /status @ 06/24/23 13:21:58.687
  Jun 24 13:21:58.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5958" for this suite. @ 06/24/23 13:21:58.698
• [2.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 06/24/23 13:21:58.709
  Jun 24 13:21:58.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename events @ 06/24/23 13:21:58.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:58.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:58.737
  STEP: Create set of events @ 06/24/23 13:21:58.741
  Jun 24 13:21:58.746: INFO: created test-event-1
  Jun 24 13:21:58.753: INFO: created test-event-2
  Jun 24 13:21:58.767: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 06/24/23 13:21:58.768
  STEP: delete collection of events @ 06/24/23 13:21:58.773
  Jun 24 13:21:58.773: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/24/23 13:21:58.801
  Jun 24 13:21:58.801: INFO: requesting list of events to confirm quantity
  Jun 24 13:21:58.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2092" for this suite. @ 06/24/23 13:21:58.814
• [0.113 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 06/24/23 13:21:58.823
  Jun 24 13:21:58.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 13:21:58.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:58.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:58.853
  STEP: apply creating a deployment @ 06/24/23 13:21:58.858
  Jun 24 13:21:58.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8393" for this suite. @ 06/24/23 13:21:58.882
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 06/24/23 13:21:58.906
  Jun 24 13:21:58.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 13:21:58.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:21:58.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:21:58.937
  STEP: Setting up server cert @ 06/24/23 13:21:58.986
  E0624 13:21:59.010557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 13:21:59.286
  STEP: Deploying the webhook pod @ 06/24/23 13:21:59.298
  STEP: Wait for the deployment to be ready @ 06/24/23 13:21:59.314
  Jun 24 13:21:59.324: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 13:22:00.010749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:01.010780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:22:01.336
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:22:01.348
  E0624 13:22:02.011286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:22:02.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 24 13:22:02.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9628-crds.webhook.example.com via the AdmissionRegistration API @ 06/24/23 13:22:02.864
  STEP: Creating a custom resource while v1 is storage version @ 06/24/23 13:22:02.886
  E0624 13:22:03.011953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:04.012062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 06/24/23 13:22:04.944
  STEP: Patching the custom resource while v2 is storage version @ 06/24/23 13:22:04.971
  E0624 13:22:05.012889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:22:05.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2618" for this suite. @ 06/24/23 13:22:05.621
  STEP: Destroying namespace "webhook-markers-4932" for this suite. @ 06/24/23 13:22:05.631
• [6.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 06/24/23 13:22:05.64
  Jun 24 13:22:05.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subpath @ 06/24/23 13:22:05.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:22:05.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:22:05.671
  STEP: Setting up data @ 06/24/23 13:22:05.675
  STEP: Creating pod pod-subpath-test-secret-xd9z @ 06/24/23 13:22:05.689
  STEP: Creating a pod to test atomic-volume-subpath @ 06/24/23 13:22:05.689
  E0624 13:22:06.013925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:07.014017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:08.014452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:09.014604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:10.014888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:11.015010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:12.015203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:13.015250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:14.015930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:15.016054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:16.016466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:17.017434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:18.018259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:19.018377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:20.018468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:21.019225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:22.020215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:23.021321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:24.021832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:25.021929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:26.022494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:27.023169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:28.023684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:29.024318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:22:29.767
  Jun 24 13:22:29.771: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-subpath-test-secret-xd9z container test-container-subpath-secret-xd9z: <nil>
  STEP: delete the pod @ 06/24/23 13:22:29.78
  STEP: Deleting pod pod-subpath-test-secret-xd9z @ 06/24/23 13:22:29.801
  Jun 24 13:22:29.802: INFO: Deleting pod "pod-subpath-test-secret-xd9z" in namespace "subpath-3923"
  Jun 24 13:22:29.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3923" for this suite. @ 06/24/23 13:22:29.811
• [24.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 06/24/23 13:22:29.824
  Jun 24 13:22:29.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename watch @ 06/24/23 13:22:29.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:22:29.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:22:29.865
  STEP: getting a starting resourceVersion @ 06/24/23 13:22:29.872
  STEP: starting a background goroutine to produce watch events @ 06/24/23 13:22:29.884
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 06/24/23 13:22:29.884
  E0624 13:22:30.025179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:31.025302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:32.025367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:22:32.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2569" for this suite. @ 06/24/23 13:22:32.681
• [2.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 06/24/23 13:22:32.738
  Jun 24 13:22:32.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 13:22:32.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:22:32.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:22:32.767
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 06/24/23 13:22:32.771
  Jun 24 13:22:32.782: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0624 13:22:33.025898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:34.026064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:35.026301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:36.026888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:37.026992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:22:37.788: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 13:22:37.788
  STEP: getting scale subresource @ 06/24/23 13:22:37.788
  STEP: updating a scale subresource @ 06/24/23 13:22:37.792
  STEP: verifying the replicaset Spec.Replicas was modified @ 06/24/23 13:22:37.799
  STEP: Patch a scale subresource @ 06/24/23 13:22:37.803
  Jun 24 13:22:37.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4522" for this suite. @ 06/24/23 13:22:37.82
• [5.090 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 06/24/23 13:22:37.83
  Jun 24 13:22:37.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename server-version @ 06/24/23 13:22:37.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:22:37.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:22:37.866
  STEP: Request ServerVersion @ 06/24/23 13:22:37.869
  STEP: Confirm major version @ 06/24/23 13:22:37.87
  Jun 24 13:22:37.871: INFO: Major version: 1
  STEP: Confirm minor version @ 06/24/23 13:22:37.871
  Jun 24 13:22:37.871: INFO: cleanMinorVersion: 27
  Jun 24 13:22:37.871: INFO: Minor version: 27
  Jun 24 13:22:37.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-89" for this suite. @ 06/24/23 13:22:37.877
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 06/24/23 13:22:37.888
  Jun 24 13:22:37.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename hostport @ 06/24/23 13:22:37.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:22:37.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:22:37.919
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 06/24/23 13:22:37.928
  E0624 13:22:38.027209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:39.027347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.19.205 on the node which pod1 resides and expect scheduled @ 06/24/23 13:22:39.949
  E0624 13:22:40.028243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:41.028341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:42.028829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:43.028955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:44.029649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:45.029942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:46.030390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:47.031164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:48.031448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:49.031811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:50.032248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:51.032383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.19.205 but use UDP protocol on the node which pod2 resides @ 06/24/23 13:22:51.985
  E0624 13:22:52.032629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:53.032741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:54.033197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:55.033370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 06/24/23 13:22:56.022
  Jun 24 13:22:56.022: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.19.205 http://127.0.0.1:54323/hostname] Namespace:hostport-7941 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:22:56.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:22:56.023: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:22:56.023: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7941/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.19.205+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0624 13:22:56.033915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.19.205, port: 54323 @ 06/24/23 13:22:56.1
  Jun 24 13:22:56.100: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.19.205:54323/hostname] Namespace:hostport-7941 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:22:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:22:56.101: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:22:56.102: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7941/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.19.205%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.19.205, port: 54323 UDP @ 06/24/23 13:22:56.182
  Jun 24 13:22:56.182: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.19.205 54323] Namespace:hostport-7941 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:22:56.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:22:56.183: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:22:56.183: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7941/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.19.205+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0624 13:22:57.034878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:58.035478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:22:59.035722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:00.035870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:01.035976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:23:01.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-7941" for this suite. @ 06/24/23 13:23:01.266
• [23.385 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 06/24/23 13:23:01.275
  Jun 24 13:23:01.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 13:23:01.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:01.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:01.299
  STEP: creating the pod @ 06/24/23 13:23:01.303
  STEP: waiting for pod running @ 06/24/23 13:23:01.316
  E0624 13:23:02.036490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:03.036615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 06/24/23 13:23:03.33
  Jun 24 13:23:03.333: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-397 PodName:var-expansion-84466953-e947-4535-acbe-6dfb28c4d701 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:23:03.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:23:03.334: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:23:03.334: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-397/pods/var-expansion-84466953-e947-4535-acbe-6dfb28c4d701/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 06/24/23 13:23:03.425
  Jun 24 13:23:03.430: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-397 PodName:var-expansion-84466953-e947-4535-acbe-6dfb28c4d701 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun 24 13:23:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  Jun 24 13:23:03.431: INFO: ExecWithOptions: Clientset creation
  Jun 24 13:23:03.431: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-397/pods/var-expansion-84466953-e947-4535-acbe-6dfb28c4d701/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 06/24/23 13:23:03.505
  Jun 24 13:23:04.021: INFO: Successfully updated pod "var-expansion-84466953-e947-4535-acbe-6dfb28c4d701"
  STEP: waiting for annotated pod running @ 06/24/23 13:23:04.021
  STEP: deleting the pod gracefully @ 06/24/23 13:23:04.025
  Jun 24 13:23:04.026: INFO: Deleting pod "var-expansion-84466953-e947-4535-acbe-6dfb28c4d701" in namespace "var-expansion-397"
  Jun 24 13:23:04.035: INFO: Wait up to 5m0s for pod "var-expansion-84466953-e947-4535-acbe-6dfb28c4d701" to be fully deleted
  E0624 13:23:04.037440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:05.037545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:06.037666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:07.038239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:08.038375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:09.038404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:10.038527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:11.038666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:12.039187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:13.039972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:14.040103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:15.040781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:16.040929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:17.041419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:18.041613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:19.041682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:20.041779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:21.041886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:22.042339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:23.042458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:24.042602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:25.042685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:26.042804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:27.043245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:28.043369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:29.043527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:30.043854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:31.044773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:32.045291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:33.046320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:34.046916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:35.046959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:36.047069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:23:36.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-397" for this suite. @ 06/24/23 13:23:36.131
• [34.864 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 06/24/23 13:23:36.141
  Jun 24 13:23:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/24/23 13:23:36.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:36.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:36.177
  STEP: create the container to handle the HTTPGet hook request. @ 06/24/23 13:23:36.186
  E0624 13:23:37.047078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:38.047483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/24/23 13:23:38.216
  E0624 13:23:39.048068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:40.048306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 06/24/23 13:23:40.234
  STEP: delete the pod with lifecycle hook @ 06/24/23 13:23:40.258
  E0624 13:23:41.048592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:42.049390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:23:42.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5292" for this suite. @ 06/24/23 13:23:42.282
• [6.150 seconds]
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 06/24/23 13:23:42.291
  Jun 24 13:23:42.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename conformance-tests @ 06/24/23 13:23:42.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:42.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:42.323
  STEP: Getting node addresses @ 06/24/23 13:23:42.331
  Jun 24 13:23:42.331: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jun 24 13:23:42.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-6805" for this suite. @ 06/24/23 13:23:42.352
• [0.068 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 06/24/23 13:23:42.361
  Jun 24 13:23:42.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubelet-test @ 06/24/23 13:23:42.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:42.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:42.384
  E0624 13:23:43.050052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:44.050349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:45.050954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:46.051051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:23:46.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2094" for this suite. @ 06/24/23 13:23:46.413
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 06/24/23 13:23:46.422
  Jun 24 13:23:46.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename runtimeclass @ 06/24/23 13:23:46.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:46.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:46.453
  Jun 24 13:23:46.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5941" for this suite. @ 06/24/23 13:23:46.472
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 06/24/23 13:23:46.482
  Jun 24 13:23:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:23:46.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:46.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:46.507
  STEP: Creating configMap that has name configmap-test-emptyKey-e8ab0a0b-a0ae-4d54-a068-34243586f0cd @ 06/24/23 13:23:46.511
  Jun 24 13:23:46.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5130" for this suite. @ 06/24/23 13:23:46.517
• [0.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 06/24/23 13:23:46.533
  Jun 24 13:23:46.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption @ 06/24/23 13:23:46.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:23:46.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:23:46.556
  Jun 24 13:23:46.577: INFO: Waiting up to 1m0s for all nodes to be ready
  E0624 13:23:47.051844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:48.051983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:49.052746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:50.052847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:51.053258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:52.054308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:53.055047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:54.055167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:55.056096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:56.056204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:57.057080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:58.057292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:23:59.057939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:00.058207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:01.059142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:02.059244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:03.060359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:04.060696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:05.061436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:06.061645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:07.061957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:08.062112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:09.062312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:10.062721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:11.063158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:12.063283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:13.064056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:14.064468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:15.065444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:16.065842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:17.066277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:18.067021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:19.068075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:20.068389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:21.068791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:22.069309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:23.069406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:24.069489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:25.069668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:26.069755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:27.070251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:28.070733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:29.070947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:30.071316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:31.071760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:32.072313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:33.072956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:34.073087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:35.073470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:36.073739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:37.074719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:38.074808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:39.074971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:40.075388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:41.076388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:42.076506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:43.077448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:44.077575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:45.077669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:46.077780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:24:46.601: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/24/23 13:24:46.606
  Jun 24 13:24:46.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/24/23 13:24:46.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:24:46.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:24:46.631
  Jun 24 13:24:46.658: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jun 24 13:24:46.663: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jun 24 13:24:46.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 13:24:46.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6675" for this suite. @ 06/24/23 13:24:46.772
  STEP: Destroying namespace "sched-preemption-480" for this suite. @ 06/24/23 13:24:46.781
• [60.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 06/24/23 13:24:46.795
  Jun 24 13:24:46.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename cronjob @ 06/24/23 13:24:46.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:24:46.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:24:46.824
  STEP: Creating a ForbidConcurrent cronjob @ 06/24/23 13:24:46.83
  STEP: Ensuring a job is scheduled @ 06/24/23 13:24:46.836
  E0624 13:24:47.077907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:48.078022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:49.078508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:50.078586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:51.079217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:52.079331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:53.079701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:54.079921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:55.080662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:56.080744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:57.081149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:58.081229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:24:59.081893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:00.081974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 06/24/23 13:25:00.84
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/24/23 13:25:00.845
  STEP: Ensuring no more jobs are scheduled @ 06/24/23 13:25:00.85
  E0624 13:25:01.082157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:02.082255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:03.083268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:04.084307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:05.085382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:06.085591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:07.086547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:08.086643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:09.087308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:10.087397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:11.087516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:12.087911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:13.088791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:14.088911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:15.089609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:16.089710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:17.090636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:18.090863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:19.091599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:20.091741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:21.092730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:22.093299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:23.093716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:24.093820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:25.094816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:26.094909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:27.096137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:28.096257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:29.096684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:30.096992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:31.097427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:32.098347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:33.098805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:34.098920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:35.099608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:36.099760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:37.100274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:38.100356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:39.100984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:40.101316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:41.101418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:42.102357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:43.102489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:44.102581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:45.103455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:46.103658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:47.104149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:48.104271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:49.105335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:50.105685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:51.106745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:52.107103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:53.108183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:54.108289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:55.109270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:56.109534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:57.110205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:58.110337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:25:59.111047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:00.111720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:01.112698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:02.113084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:03.113764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:04.113869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:05.114724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:06.114994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:07.115398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:08.115500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:09.115614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:10.115722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:11.116747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:12.117235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:13.117969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:14.118473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:15.118991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:16.119103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:17.120010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:18.121029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:19.121749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:20.122242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:21.122661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:22.123074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:23.124005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:24.124800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:25.124827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:26.125193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:27.125933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:28.126209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:29.126834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:30.126964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:31.127177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:32.127962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:33.128757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:34.129109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:35.129630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:36.129885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:37.130892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:38.131014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:39.131408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:40.131704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:41.132155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:42.132764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:43.133429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:44.133644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:45.134105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:46.134433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:47.135155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:48.135986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:49.136765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:50.137389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:51.138095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:52.138156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:53.138609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:54.138724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:55.138824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:56.138939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:57.139681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:58.139731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:26:59.140284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:00.140756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:01.141474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:02.141832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:03.142577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:04.142680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:05.142772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:06.142876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:07.144007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:08.144144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:09.144244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:10.144357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:11.145380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:12.145411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:13.146130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:14.146608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:15.146765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:16.146946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:17.147620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:18.147738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:19.147849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:20.147941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:21.148750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:22.149267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:23.150030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:24.150133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:25.151085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:26.151156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:27.151991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:28.152094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:29.152549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:30.152961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:31.153068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:32.153173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:33.153415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:34.153621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:35.154580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:36.154697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:37.155367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:38.156452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:39.157178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:40.157287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:41.157784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:42.158289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:43.160350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:44.160834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:45.161319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:46.161604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:47.162662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:48.162773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:49.163347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:50.163448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:51.163685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:52.164436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:53.164543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:54.165146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:55.165664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:56.165766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:57.166163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:58.166280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:27:59.166860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:00.166970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:01.167068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:02.167158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:03.167644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:04.168105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:05.168577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:06.168679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:07.169658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:08.169769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:09.170155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:10.170548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:11.170605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:12.170694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:13.171319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:14.171688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:15.172433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:16.172542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:17.172636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:18.172746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:19.172956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:20.173317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:21.173476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:22.174031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:23.174867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:24.175087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:25.176140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:26.176767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:27.176916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:28.177098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:29.178133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:30.178933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:31.179147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:32.179647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:33.179938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:34.180253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:35.180771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:36.181771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:37.182441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:38.182755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:39.183775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:40.184759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:41.184975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:42.185348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:43.185661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:44.185849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:45.186781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:46.186894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:47.187331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:48.187670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:49.187696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:50.187812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:51.188316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:52.188444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:53.188719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:54.188817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:55.189608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:56.189733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:57.190754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:58.191604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:28:59.192242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:00.192764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:01.193338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:02.194257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:03.194406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:04.194557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:05.195631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:06.195712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:07.196047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:08.196023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:09.196779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:10.197073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:11.197813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:12.198660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:13.199683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:14.199784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:15.200212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:16.200326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:17.200777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:18.200869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:19.201353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:20.202262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:21.202850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:22.203771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:23.203996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:24.204097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:25.204456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:26.204556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:27.204661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:28.204780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:29.205607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:30.205708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:31.206403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:32.206695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:33.207477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:34.208247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:35.208743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:36.209241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:37.210155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:38.210247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:39.210566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:40.210669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:41.211237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:42.211493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:43.212096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:44.212273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:45.212299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:46.212386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:47.212751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:48.213008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:49.213301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:50.213436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:51.214342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:52.214690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:53.214766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:54.214876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:55.215894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:56.216000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:57.216781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:58.217103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:29:59.217722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:00.217837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 06/24/23 13:30:00.86
  Jun 24 13:30:00.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4862" for this suite. @ 06/24/23 13:30:00.873
• [314.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 06/24/23 13:30:00.884
  Jun 24 13:30:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:30:00.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:00.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:00.913
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 06/24/23 13:30:00.922
  E0624 13:30:01.217988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:02.218404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:03.218914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:04.218957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:30:04.952
  Jun 24 13:30:04.956: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-6d260964-7ed0-4304-924f-9f4344a0066f container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:30:04.98
  Jun 24 13:30:04.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7627" for this suite. @ 06/24/23 13:30:05.001
• [4.125 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 06/24/23 13:30:05.009
  Jun 24 13:30:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:30:05.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:05.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:05.035
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 13:30:05.038
  E0624 13:30:05.218998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:06.219257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:07.219413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:08.219735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:30:09.062
  Jun 24 13:30:09.067: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-5605e2d2-d88f-4faa-baaa-bec951fbde00 container client-container: <nil>
  STEP: delete the pod @ 06/24/23 13:30:09.075
  Jun 24 13:30:09.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1315" for this suite. @ 06/24/23 13:30:09.096
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 06/24/23 13:30:09.108
  Jun 24 13:30:09.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/24/23 13:30:09.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:09.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:09.136
  Jun 24 13:30:09.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:30:09.220462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1217" for this suite. @ 06/24/23 13:30:09.711
• [0.611 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 06/24/23 13:30:09.719
  Jun 24 13:30:09.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/24/23 13:30:09.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:09.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:09.746
  STEP: creating @ 06/24/23 13:30:09.75
  STEP: getting @ 06/24/23 13:30:09.769
  STEP: listing @ 06/24/23 13:30:09.777
  STEP: deleting @ 06/24/23 13:30:09.781
  Jun 24 13:30:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-533" for this suite. @ 06/24/23 13:30:09.81
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 06/24/23 13:30:09.825
  Jun 24 13:30:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 13:30:09.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:09.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:09.854
  STEP: Counting existing ResourceQuota @ 06/24/23 13:30:09.864
  E0624 13:30:10.221390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:11.221676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:12.222473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:13.223380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:14.224314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 13:30:14.868
  STEP: Ensuring resource quota status is calculated @ 06/24/23 13:30:14.876
  E0624 13:30:15.224457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:16.224610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 06/24/23 13:30:16.881
  STEP: Ensuring ResourceQuota status captures the pod usage @ 06/24/23 13:30:16.898
  E0624 13:30:17.225403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:18.225573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 06/24/23 13:30:18.903
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 06/24/23 13:30:18.906
  STEP: Ensuring a pod cannot update its resource requirements @ 06/24/23 13:30:18.909
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 06/24/23 13:30:18.914
  E0624 13:30:19.225668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:20.225805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/24/23 13:30:20.92
  STEP: Ensuring resource quota status released the pod usage @ 06/24/23 13:30:20.931
  E0624 13:30:21.226314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:22.226468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:22.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2792" for this suite. @ 06/24/23 13:30:22.949
• [13.134 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 06/24/23 13:30:22.96
  Jun 24 13:30:22.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 13:30:22.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:22.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:22.986
  STEP: Creating a simple DaemonSet "daemon-set" @ 06/24/23 13:30:23.011
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 13:30:23.018
  Jun 24 13:30:23.022: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:23.022: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:23.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:30:23.026: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:30:23.226899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:24.031: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:24.031: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:24.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:30:24.036: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:30:24.227690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:25.031: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:25.031: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:25.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 13:30:25.035: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 06/24/23 13:30:25.039
  Jun 24 13:30:25.057: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:25.057: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:25.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:30:25.064: INFO: Node ip-172-31-19-205 is running 0 daemon pod, expected 1
  E0624 13:30:25.228683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:26.069: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:26.070: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:26.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:30:26.073: INFO: Node ip-172-31-19-205 is running 0 daemon pod, expected 1
  E0624 13:30:26.229379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:27.069: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:27.069: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:30:27.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 13:30:27.072: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 06/24/23 13:30:27.073
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 13:30:27.081
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6961, will wait for the garbage collector to delete the pods @ 06/24/23 13:30:27.081
  Jun 24 13:30:27.143: INFO: Deleting DaemonSet.extensions daemon-set took: 8.735151ms
  E0624 13:30:27.229915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:27.245: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.340187ms
  E0624 13:30:28.230082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:28.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:30:28.251: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 13:30:28.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38098"},"items":null}

  Jun 24 13:30:28.257: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38098"},"items":null}

  Jun 24 13:30:28.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6961" for this suite. @ 06/24/23 13:30:28.279
• [5.327 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 06/24/23 13:30:28.288
  Jun 24 13:30:28.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:30:28.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:28.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:28.313
  STEP: Creating configMap with name projected-configmap-test-volume-230ed174-fb38-4a8b-a857-9b07a53f0e25 @ 06/24/23 13:30:28.318
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:30:28.326
  E0624 13:30:29.230788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:30.231148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:31.231694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:32.232529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:30:32.354
  Jun 24 13:30:32.358: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-5557fb43-5b45-4748-9707-8fc480b06900 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:30:32.366
  Jun 24 13:30:32.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4234" for this suite. @ 06/24/23 13:30:32.389
• [4.109 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 06/24/23 13:30:32.399
  Jun 24 13:30:32.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:30:32.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:32.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:32.425
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/24/23 13:30:32.43
  E0624 13:30:33.233089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:34.233213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:35.233325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:36.233548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:30:36.455
  Jun 24 13:30:36.459: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-549be687-6639-46c2-b4f8-645be5e2ad75 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:30:36.467
  Jun 24 13:30:36.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4620" for this suite. @ 06/24/23 13:30:36.486
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 06/24/23 13:30:36.498
  Jun 24 13:30:36.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:30:36.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:36.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:36.524
  STEP: Creating configMap with name configmap-test-volume-map-5dda7f9b-b9c7-469f-88cd-cb5070ae42c7 @ 06/24/23 13:30:36.527
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:30:36.536
  E0624 13:30:37.234284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:38.235086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:39.235754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:40.235918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:30:40.56
  Jun 24 13:30:40.564: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-configmaps-e12a5e8f-c124-4a14-828b-f4347a057e9e container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:30:40.572
  Jun 24 13:30:40.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3056" for this suite. @ 06/24/23 13:30:40.594
• [4.106 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 06/24/23 13:30:40.605
  Jun 24 13:30:40.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 13:30:40.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:30:40.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:30:40.633
  STEP: Creating pod liveness-a424e462-a9fe-4534-9d67-656f428b65a2 in namespace container-probe-7253 @ 06/24/23 13:30:40.638
  E0624 13:30:41.236091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:42.236349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:30:42.679: INFO: Started pod liveness-a424e462-a9fe-4534-9d67-656f428b65a2 in namespace container-probe-7253
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 13:30:42.679
  Jun 24 13:30:42.684: INFO: Initial restart count of pod liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is 0
  E0624 13:30:43.236500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:44.236589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:45.237355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:46.237513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:47.238418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:48.238652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:49.239639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:50.239701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:51.240508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:52.240596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:53.241490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:54.242194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:55.242365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:56.242706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:57.242865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:58.242946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:30:59.243072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:00.243157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:01.243752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:02.244290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:31:02.737: INFO: Restart count of pod container-probe-7253/liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is now 1 (20.053077856s elapsed)
  E0624 13:31:03.244743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:04.244895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:05.245954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:06.246043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:07.246604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:08.246695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:09.246788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:10.246897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:11.247026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:12.247607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:13.247693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:14.247783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:15.247907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:16.248112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:17.248738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:18.249022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:19.249630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:20.249750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:21.250478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:22.250494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:31:22.786: INFO: Restart count of pod container-probe-7253/liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is now 2 (40.102061682s elapsed)
  E0624 13:31:23.251240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:24.251370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:25.252184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:26.252447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:27.252526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:28.252634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:29.253400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:30.253540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:31.254398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:32.254504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:33.255394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:34.255725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:35.256717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:36.256829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:37.257877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:38.257990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:39.258662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:40.258736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:41.258760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:42.259436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:31:42.834: INFO: Restart count of pod container-probe-7253/liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is now 3 (1m0.150483209s elapsed)
  E0624 13:31:43.259882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:44.259700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:45.259692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:46.259803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:47.260446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:48.260556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:49.261162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:50.261476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:51.261767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:52.262717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:53.262868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:54.262959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:55.263271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:56.263388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:57.264399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:58.264504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:31:59.264690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:00.265644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:01.265744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:02.266555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:32:02.883: INFO: Restart count of pod container-probe-7253/liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is now 4 (1m20.199313122s elapsed)
  E0624 13:32:03.267225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:04.267689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:05.268369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:06.268518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:07.268663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:08.268716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:09.269505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:10.269935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:11.270990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:12.271105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:13.271698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:14.271737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:15.272357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:16.272522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:17.273478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:18.273714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:19.274398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:20.274552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:21.275038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:22.275198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:23.275438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:24.275515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:25.276109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:26.276376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:27.277339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:28.277697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:29.278287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:30.278435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:31.279462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:32.279539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:33.279965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:34.280045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:35.280759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:36.280961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:37.281528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:38.281603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:39.282547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:40.282663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:41.283222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:42.283317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:43.284241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:44.284561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:45.285601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:46.285712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:47.285788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:48.285914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:49.286016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:50.286130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:51.286255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:52.286522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:53.287175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:54.287289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:55.287851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:56.287951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:57.288725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:58.288838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:32:59.289775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:00.289913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:01.290000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:02.290124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:03.290927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:04.291025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:05.291625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:06.291726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:07.292407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:08.292731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:09.293702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:10.293815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:11.294818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:12.295424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:13.296090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:14.296222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:33:15.058: INFO: Restart count of pod container-probe-7253/liveness-a424e462-a9fe-4534-9d67-656f428b65a2 is now 5 (2m32.374501779s elapsed)
  Jun 24 13:33:15.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:33:15.063
  STEP: Destroying namespace "container-probe-7253" for this suite. @ 06/24/23 13:33:15.076
• [154.478 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 06/24/23 13:33:15.09
  Jun 24 13:33:15.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subpath @ 06/24/23 13:33:15.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:33:15.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:33:15.117
  STEP: Setting up data @ 06/24/23 13:33:15.123
  STEP: Creating pod pod-subpath-test-configmap-qmz2 @ 06/24/23 13:33:15.136
  STEP: Creating a pod to test atomic-volume-subpath @ 06/24/23 13:33:15.137
  E0624 13:33:15.296910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:16.296971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:17.297551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:18.297664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:19.298719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:20.298908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:21.299761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:22.300537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:23.301601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:24.301759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:25.301939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:26.302828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:27.302914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:28.303005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:29.303804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:30.303909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:31.304840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:32.305523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:33.306614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:34.307308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:35.308301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:36.308502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:37.309173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:38.309275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:33:39.213
  Jun 24 13:33:39.216: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-subpath-test-configmap-qmz2 container test-container-subpath-configmap-qmz2: <nil>
  STEP: delete the pod @ 06/24/23 13:33:39.239
  STEP: Deleting pod pod-subpath-test-configmap-qmz2 @ 06/24/23 13:33:39.254
  Jun 24 13:33:39.254: INFO: Deleting pod "pod-subpath-test-configmap-qmz2" in namespace "subpath-8766"
  Jun 24 13:33:39.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8766" for this suite. @ 06/24/23 13:33:39.262
• [24.179 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 06/24/23 13:33:39.271
  Jun 24 13:33:39.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:33:39.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:33:39.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:33:39.298
  STEP: Creating projection with secret that has name projected-secret-test-587d221f-fd45-4424-8360-5eac26c9b0b9 @ 06/24/23 13:33:39.303
  STEP: Creating a pod to test consume secrets @ 06/24/23 13:33:39.309
  E0624 13:33:39.309619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:40.309533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:41.309858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:42.309998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:43.311169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:33:43.34
  Jun 24 13:33:43.345: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-02ea8c71-bc43-40e4-a554-d5f973127466 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 13:33:43.354
  Jun 24 13:33:43.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4176" for this suite. @ 06/24/23 13:33:43.378
• [4.115 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 06/24/23 13:33:43.387
  Jun 24 13:33:43.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:33:43.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:33:43.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:33:43.409
  STEP: Creating a pod to test downward API volume plugin @ 06/24/23 13:33:43.413
  E0624 13:33:44.311724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:45.311836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:46.312818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:47.313407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:33:47.436
  Jun 24 13:33:47.440: INFO: Trying to get logs from node ip-172-31-19-205 pod downwardapi-volume-8b552fd7-93b7-4eec-ad08-a1c8210430ef container client-container: <nil>
  STEP: delete the pod @ 06/24/23 13:33:47.449
  Jun 24 13:33:47.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7329" for this suite. @ 06/24/23 13:33:47.47
• [4.089 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 06/24/23 13:33:47.478
  Jun 24 13:33:47.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename cronjob @ 06/24/23 13:33:47.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:33:47.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:33:47.507
  STEP: Creating a cronjob @ 06/24/23 13:33:47.512
  STEP: Ensuring more than one job is running at a time @ 06/24/23 13:33:47.519
  E0624 13:33:48.314447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:49.314567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:50.314677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:51.314803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:52.314950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:53.315196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:54.315329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:55.315411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:56.315699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:57.315962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:58.316142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:33:59.316768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:00.317785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:01.317897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:02.318726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:03.319107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:04.319737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:05.319848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:06.320493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:07.320759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:08.321334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:09.321457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:10.321895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:11.322002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:12.322046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:13.322635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:14.322776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:15.322920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:16.322992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:17.323115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:18.323181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:19.323291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:20.323447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:21.323690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:22.324510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:23.324681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:24.325418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:25.325805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:26.326159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:27.326526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:28.326561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:29.326767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:30.326863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:31.326960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:32.327064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:33.327171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:34.327679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:35.327711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:36.327852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:37.327940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:38.328488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:39.328608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:40.328724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:41.328822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:42.329901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:43.330007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:44.330129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:45.330202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:46.330317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:47.330662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:48.331601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:49.331748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:50.331860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:51.331985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:52.332603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:53.333640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:54.334089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:55.334368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:56.334539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:57.334926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:58.335052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:34:59.335166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:00.335811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:01.336349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 06/24/23 13:35:01.524
  STEP: Removing cronjob @ 06/24/23 13:35:01.529
  Jun 24 13:35:01.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2443" for this suite. @ 06/24/23 13:35:01.541
• [74.071 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 06/24/23 13:35:01.549
  Jun 24 13:35:01.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename deployment @ 06/24/23 13:35:01.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:01.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:01.591
  Jun 24 13:35:01.594: INFO: Creating simple deployment test-new-deployment
  Jun 24 13:35:01.611: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0624 13:35:02.336751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:03.336899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 06/24/23 13:35:03.626
  STEP: updating a scale subresource @ 06/24/23 13:35:03.63
  STEP: verifying the deployment Spec.Replicas was modified @ 06/24/23 13:35:03.638
  STEP: Patch a scale subresource @ 06/24/23 13:35:03.642
  Jun 24 13:35:03.660: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-5753  38aed2f8-5cd4-4d65-b687-cb5a9ed3e6ce 39013 3 2023-06-24 13:35:01 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-24 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 13:35:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aebf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-24 13:35:02 +0000 UTC,LastTransitionTime:2023-06-24 13:35:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-06-24 13:35:02 +0000 UTC,LastTransitionTime:2023-06-24 13:35:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun 24 13:35:03.666: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-5753  c050ecc0-5012-4b43-8c56-f0bf6a58cc4e 39017 2 2023-06-24 13:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 38aed2f8-5cd4-4d65-b687-cb5a9ed3e6ce 0xc001e39b77 0xc001e39b78}] [] [{kube-controller-manager Update apps/v1 2023-06-24 13:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"38aed2f8-5cd4-4d65-b687-cb5a9ed3e6ce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-24 13:35:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e39c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun 24 13:35:03.671: INFO: Pod "test-new-deployment-67bd4bf6dc-9h74x" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-9h74x test-new-deployment-67bd4bf6dc- deployment-5753  f30301dc-0aa9-44a1-8586-35f96527b841 39016 0 2023-06-24 13:35:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c050ecc0-5012-4b43-8c56-f0bf6a58cc4e 0xc001e39fc7 0xc001e39fc8}] [] [{kube-controller-manager Update v1 2023-06-24 13:35:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c050ecc0-5012-4b43-8c56-f0bf6a58cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8m4z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8m4z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-136,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 13:35:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 13:35:03.672: INFO: Pod "test-new-deployment-67bd4bf6dc-x6p2n" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-x6p2n test-new-deployment-67bd4bf6dc- deployment-5753  0150b714-ceb1-4854-8b40-bfa6a55574c7 39006 0 2023-06-24 13:35:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c050ecc0-5012-4b43-8c56-f0bf6a58cc4e 0xc005d82130 0xc005d82131}] [] [{kube-controller-manager Update v1 2023-06-24 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c050ecc0-5012-4b43-8c56-f0bf6a58cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-24 13:35:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hzvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hzvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 13:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 13:35:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 13:35:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-24 13:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.205,PodIP:192.168.150.199,StartTime:2023-06-24 13:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-24 13:35:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ac94d28500c476d34f32440ed56ee0634c0cc59e28d33466275c69488814c8f2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.199,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun 24 13:35:03.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5753" for this suite. @ 06/24/23 13:35:03.677
• [2.139 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 06/24/23 13:35:03.69
  Jun 24 13:35:03.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:35:03.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:03.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:03.729
  STEP: Creating projection with secret that has name projected-secret-test-3606f34e-f987-4b6e-bfcb-35f97aa14a69 @ 06/24/23 13:35:03.732
  STEP: Creating a pod to test consume secrets @ 06/24/23 13:35:03.738
  E0624 13:35:04.337008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:05.337846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:06.338554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:07.339259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:35:07.766
  Jun 24 13:35:07.770: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-secrets-8fe7539c-8e96-4a54-9220-2f609290d18f container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/24/23 13:35:07.782
  Jun 24 13:35:07.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5460" for this suite. @ 06/24/23 13:35:07.806
• [4.126 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 06/24/23 13:35:07.817
  Jun 24 13:35:07.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:35:07.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:07.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:07.859
  Jun 24 13:35:07.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2660" for this suite. @ 06/24/23 13:35:07.933
• [0.126 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 06/24/23 13:35:07.945
  Jun 24 13:35:07.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename downward-api @ 06/24/23 13:35:07.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:07.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:07.974
  STEP: Creating a pod to test downward api env vars @ 06/24/23 13:35:07.979
  E0624 13:35:08.340261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:09.340519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:10.340614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:11.340726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:35:12.014
  Jun 24 13:35:12.018: INFO: Trying to get logs from node ip-172-31-15-136 pod downward-api-5a70ec2c-80bf-4b0c-accd-f4da78f0d59f container dapi-container: <nil>
  STEP: delete the pod @ 06/24/23 13:35:12.049
  Jun 24 13:35:12.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7461" for this suite. @ 06/24/23 13:35:12.077
• [4.140 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 06/24/23 13:35:12.085
  Jun 24 13:35:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:35:12.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:12.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:12.127
  STEP: creating service in namespace services-4002 @ 06/24/23 13:35:12.132
  STEP: creating service affinity-nodeport in namespace services-4002 @ 06/24/23 13:35:12.132
  STEP: creating replication controller affinity-nodeport in namespace services-4002 @ 06/24/23 13:35:12.155
  I0624 13:35:12.168326      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4002, replica count: 3
  E0624 13:35:12.341554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:13.342007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:14.342148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 13:35:15.219604      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 13:35:15.234: INFO: Creating new exec pod
  E0624 13:35:15.342720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:16.342989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:17.344014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:35:18.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4002 exec execpod-affinityvz4m5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0624 13:35:18.344801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:35:18.424: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jun 24 13:35:18.424: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:35:18.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4002 exec execpod-affinityvz4m5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jun 24 13:35:18.591: INFO: stderr: "+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 24 13:35:18.591: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:35:18.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4002 exec execpod-affinityvz4m5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.205 32719'
  Jun 24 13:35:18.755: INFO: stderr: "+ nc -v -t -w 2 172.31.19.205 32719\nConnection to 172.31.19.205 32719 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jun 24 13:35:18.755: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:35:18.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4002 exec execpod-affinityvz4m5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.136 32719'
  Jun 24 13:35:18.908: INFO: stderr: "+ nc -v -t -w 2 172.31.15.136 32719\n+ echo hostName\nConnection to 172.31.15.136 32719 port [tcp/*] succeeded!\n"
  Jun 24 13:35:18.908: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:35:18.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-4002 exec execpod-affinityvz4m5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.136:32719/ ; done'
  Jun 24 13:35:19.145: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.136:32719/\n"
  Jun 24 13:35:19.145: INFO: stdout: "\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67\naffinity-nodeport-rgw67"
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.145: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.146: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.146: INFO: Received response from host: affinity-nodeport-rgw67
  Jun 24 13:35:19.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 13:35:19.151: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4002, will wait for the garbage collector to delete the pods @ 06/24/23 13:35:19.167
  Jun 24 13:35:19.228: INFO: Deleting ReplicationController affinity-nodeport took: 7.207184ms
  Jun 24 13:35:19.329: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.616077ms
  E0624 13:35:19.344863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:20.345605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:21.345878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4002" for this suite. @ 06/24/23 13:35:21.859
• [9.780 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 06/24/23 13:35:21.866
  Jun 24 13:35:21.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:35:21.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:21.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:21.894
  STEP: fetching services @ 06/24/23 13:35:21.9
  Jun 24 13:35:21.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7360" for this suite. @ 06/24/23 13:35:21.912
• [0.056 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 06/24/23 13:35:21.922
  Jun 24 13:35:21.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename statefulset @ 06/24/23 13:35:21.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:35:21.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:35:21.971
  STEP: Creating service test in namespace statefulset-1060 @ 06/24/23 13:35:21.979
  STEP: Creating a new StatefulSet @ 06/24/23 13:35:21.99
  Jun 24 13:35:22.005: INFO: Found 0 stateful pods, waiting for 3
  E0624 13:35:22.346504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:23.346613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:24.347376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:25.347500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:26.347710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:27.348143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:28.348426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:29.348524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:30.348633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:31.348737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:35:32.012: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 13:35:32.012: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 13:35:32.012: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/24/23 13:35:32.024
  Jun 24 13:35:32.046: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/24/23 13:35:32.047
  E0624 13:35:32.349176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:33.349258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:34.349365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:35.349508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:36.349605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:37.350340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:38.350455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:39.350902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:40.351123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:41.351230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 06/24/23 13:35:42.071
  STEP: Performing a canary update @ 06/24/23 13:35:42.071
  Jun 24 13:35:42.096: INFO: Updating stateful set ss2
  Jun 24 13:35:42.107: INFO: Waiting for Pod statefulset-1060/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0624 13:35:42.352117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:43.352449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:44.352578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:45.353606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:46.354280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:47.354754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:48.354989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:49.355281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:50.356102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:51.356266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 06/24/23 13:35:52.116
  Jun 24 13:35:52.156: INFO: Found 1 stateful pods, waiting for 3
  E0624 13:35:52.356673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:53.357130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:54.357374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:55.357563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:56.357628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:57.357752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:58.357950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:35:59.358627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:00.359087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:01.359597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:02.161: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 13:36:02.161: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun 24 13:36:02.161: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 06/24/23 13:36:02.169
  Jun 24 13:36:02.193: INFO: Updating stateful set ss2
  Jun 24 13:36:02.203: INFO: Waiting for Pod statefulset-1060/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0624 13:36:02.359830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:03.360010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:04.360942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:05.360929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:06.361051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:07.361242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:08.361337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:09.361496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:10.361679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:11.361817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:12.235: INFO: Updating stateful set ss2
  Jun 24 13:36:12.245: INFO: Waiting for StatefulSet statefulset-1060/ss2 to complete update
  Jun 24 13:36:12.245: INFO: Waiting for Pod statefulset-1060/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0624 13:36:12.362603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:13.362685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:14.362851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:15.363297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:16.363443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:17.363486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:18.363697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:19.363896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:20.363986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:21.364142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:22.257: INFO: Deleting all statefulset in ns statefulset-1060
  Jun 24 13:36:22.260: INFO: Scaling statefulset ss2 to 0
  E0624 13:36:22.364578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:23.364715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:24.365098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:25.365372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:26.365486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:27.366172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:28.366568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:29.366712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:30.366817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:31.366963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:32.283: INFO: Waiting for statefulset status.replicas updated to 0
  Jun 24 13:36:32.287: INFO: Deleting statefulset ss2
  Jun 24 13:36:32.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1060" for this suite. @ 06/24/23 13:36:32.309
• [70.395 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 06/24/23 13:36:32.318
  Jun 24 13:36:32.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 13:36:32.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:36:32.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:36:32.344
  Jun 24 13:36:32.348: INFO: Creating ReplicaSet my-hostname-basic-3f8d848d-d326-4e47-aa7e-cdfd3166c70a
  Jun 24 13:36:32.361: INFO: Pod name my-hostname-basic-3f8d848d-d326-4e47-aa7e-cdfd3166c70a: Found 0 pods out of 1
  E0624 13:36:32.367479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:33.367833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:34.367894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:35.368015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:36.368137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:37.366: INFO: Pod name my-hostname-basic-3f8d848d-d326-4e47-aa7e-cdfd3166c70a: Found 1 pods out of 1
  Jun 24 13:36:37.366: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3f8d848d-d326-4e47-aa7e-cdfd3166c70a" is running
  E0624 13:36:37.368765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:36:37.370: INFO: Pod "my-hostname-basic-3f8d848d-d326-4e47-aa7e-cdfd3166c70a-r842g" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 13:36:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 13:36:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 13:36:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-24 13:36:32 +0000 UTC Reason: Message:}])
  Jun 24 13:36:37.370: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/24/23 13:36:37.37
  Jun 24 13:36:37.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9740" for this suite. @ 06/24/23 13:36:37.39
• [5.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 06/24/23 13:36:37.401
  Jun 24 13:36:37.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 13:36:37.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:36:37.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:36:37.429
  E0624 13:36:38.369124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:39.370079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:40.371157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:41.372227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:42.372459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:43.373032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:44.373184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:45.373231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:46.373353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:47.374148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:48.374259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:49.374375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:50.374481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:51.374607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:52.374697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:53.374810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:54.374878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/24/23 13:36:54.438
  E0624 13:36:55.375631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:56.375696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:57.375841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:58.375951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:36:59.376058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/24/23 13:36:59.444
  STEP: Ensuring resource quota status is calculated @ 06/24/23 13:36:59.449
  E0624 13:37:00.376784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:01.376904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 06/24/23 13:37:01.455
  STEP: Ensuring resource quota status captures configMap creation @ 06/24/23 13:37:01.471
  E0624 13:37:02.377021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:03.377131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 06/24/23 13:37:03.477
  STEP: Ensuring resource quota status released usage @ 06/24/23 13:37:03.484
  E0624 13:37:04.377217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:05.377321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:37:05.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9446" for this suite. @ 06/24/23 13:37:05.493
• [28.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 06/24/23 13:37:05.504
  Jun 24 13:37:05.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename var-expansion @ 06/24/23 13:37:05.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:37:05.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:37:05.533
  STEP: creating the pod with failed condition @ 06/24/23 13:37:05.537
  E0624 13:37:06.378052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:07.378860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:08.378867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:09.378968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:10.379088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:11.379407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:12.380340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:13.380585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:14.380737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:15.380901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:16.381709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:17.381862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:18.381974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:19.382129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:20.382267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:21.382365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:22.382923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:23.383161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:24.383273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:25.383378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:26.384645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:27.385273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:28.386258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:29.386382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:30.386495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:31.386736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:32.387688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:33.388811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:34.388884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:35.388973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:36.389935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:37.390354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:38.390435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:39.390543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:40.390652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:41.390793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:42.391218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:43.391625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:44.391856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:45.392078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:46.392175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:47.392290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:48.392765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:49.392861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:50.392980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:51.393490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:52.394156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:53.394276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:54.394379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:55.394526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:56.395527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:57.396298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:58.397308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:37:59.397422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:00.397519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:01.397627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:02.398658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:03.399466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:04.400325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:05.400894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:06.401013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:07.401105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:08.401513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:09.401622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:10.401729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:11.401843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:12.401944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:13.402070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:14.402176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:15.402698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:16.402790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:17.403502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:18.404434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:19.405372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:20.405474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:21.405592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:22.405856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:23.406227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:24.406817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:25.406922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:26.407527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:27.407678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:28.407718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:29.407830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:30.407935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:31.408052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:32.408762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:33.409052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:34.409186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:35.409284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:36.409352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:37.410169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:38.410278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:39.410514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:40.410644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:41.410826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:42.411277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:43.411376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:44.411689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:45.412772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:46.413368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:47.413448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:48.413552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:49.413658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:50.414586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:51.414696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:52.414876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:53.415090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:54.415209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:55.416028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:56.416884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:57.416995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:58.417094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:38:59.417208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:00.418022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:01.418352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:02.418466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:03.418653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:04.418745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:05.418874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 06/24/23 13:39:05.549
  Jun 24 13:39:06.065: INFO: Successfully updated pod "var-expansion-4eb06075-77c8-4d66-9929-df46c379e5c5"
  STEP: waiting for pod running @ 06/24/23 13:39:06.065
  E0624 13:39:06.419760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:07.420533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 06/24/23 13:39:08.073
  Jun 24 13:39:08.073: INFO: Deleting pod "var-expansion-4eb06075-77c8-4d66-9929-df46c379e5c5" in namespace "var-expansion-3694"
  Jun 24 13:39:08.082: INFO: Wait up to 5m0s for pod "var-expansion-4eb06075-77c8-4d66-9929-df46c379e5c5" to be fully deleted
  E0624 13:39:08.420520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:09.420881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:10.421049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:11.421187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:12.421905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:13.422049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:14.422616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:15.422717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:16.423522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:17.424204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:18.425106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:19.425256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:20.425848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:21.426092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:22.426509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:23.426633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:24.426780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:25.427220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:26.427691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:27.428630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:28.428789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:29.428911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:30.429597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:31.429991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:32.430505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:33.431390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:34.432078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:35.432229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:36.432849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:37.433250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:38.433410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:39.433876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:39:40.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3694" for this suite. @ 06/24/23 13:39:40.17
• [154.674 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 06/24/23 13:39:40.18
  Jun 24 13:39:40.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/24/23 13:39:40.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:39:40.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:39:40.208
  STEP: Creating 50 configmaps @ 06/24/23 13:39:40.211
  E0624 13:39:40.434919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 06/24/23 13:39:40.471
  Jun 24 13:39:40.557: INFO: Pod name wrapped-volume-race-66749098-7ec0-45a8-8ab5-0e0b5a23d85c: Found 3 pods out of 5
  E0624 13:39:41.435098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:42.435221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:43.435721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:44.435689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:45.435716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:39:45.567: INFO: Pod name wrapped-volume-race-66749098-7ec0-45a8-8ab5-0e0b5a23d85c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/24/23 13:39:45.568
  STEP: Creating RC which spawns configmap-volume pods @ 06/24/23 13:39:45.603
  Jun 24 13:39:45.626: INFO: Pod name wrapped-volume-race-72502488-9022-46fd-bd61-1928d2e63f8b: Found 0 pods out of 5
  E0624 13:39:46.435852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:47.436790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:48.436906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:49.437012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:50.437161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:39:50.634: INFO: Pod name wrapped-volume-race-72502488-9022-46fd-bd61-1928d2e63f8b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/24/23 13:39:50.634
  STEP: Creating RC which spawns configmap-volume pods @ 06/24/23 13:39:50.661
  Jun 24 13:39:50.681: INFO: Pod name wrapped-volume-race-415ca58c-9797-448a-b9c6-a57648600311: Found 0 pods out of 5
  E0624 13:39:51.437284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:52.437619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:53.437665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:54.437783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:55.437894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:39:55.693: INFO: Pod name wrapped-volume-race-415ca58c-9797-448a-b9c6-a57648600311: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/24/23 13:39:55.693
  Jun 24 13:39:55.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-415ca58c-9797-448a-b9c6-a57648600311 in namespace emptydir-wrapper-5314, will wait for the garbage collector to delete the pods @ 06/24/23 13:39:55.722
  Jun 24 13:39:55.785: INFO: Deleting ReplicationController wrapped-volume-race-415ca58c-9797-448a-b9c6-a57648600311 took: 8.603093ms
  Jun 24 13:39:55.886: INFO: Terminating ReplicationController wrapped-volume-race-415ca58c-9797-448a-b9c6-a57648600311 pods took: 100.393234ms
  E0624 13:39:56.438783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:57.439771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-72502488-9022-46fd-bd61-1928d2e63f8b in namespace emptydir-wrapper-5314, will wait for the garbage collector to delete the pods @ 06/24/23 13:39:58.186
  Jun 24 13:39:58.251: INFO: Deleting ReplicationController wrapped-volume-race-72502488-9022-46fd-bd61-1928d2e63f8b took: 9.261851ms
  Jun 24 13:39:58.352: INFO: Terminating ReplicationController wrapped-volume-race-72502488-9022-46fd-bd61-1928d2e63f8b pods took: 101.079953ms
  E0624 13:39:58.440709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:39:59.441349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:00.442072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-66749098-7ec0-45a8-8ab5-0e0b5a23d85c in namespace emptydir-wrapper-5314, will wait for the garbage collector to delete the pods @ 06/24/23 13:40:00.853
  Jun 24 13:40:00.917: INFO: Deleting ReplicationController wrapped-volume-race-66749098-7ec0-45a8-8ab5-0e0b5a23d85c took: 8.900776ms
  Jun 24 13:40:01.018: INFO: Terminating ReplicationController wrapped-volume-race-66749098-7ec0-45a8-8ab5-0e0b5a23d85c pods took: 101.060563ms
  E0624 13:40:01.442127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:02.442170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 06/24/23 13:40:03.319
  E0624 13:40:03.442244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-5314" for this suite. @ 06/24/23 13:40:03.668
• [23.496 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 06/24/23 13:40:03.676
  Jun 24 13:40:03.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename resourcequota @ 06/24/23 13:40:03.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:03.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:03.712
  STEP: Creating a ResourceQuota with best effort scope @ 06/24/23 13:40:03.717
  STEP: Ensuring ResourceQuota status is calculated @ 06/24/23 13:40:03.722
  E0624 13:40:04.442404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:05.442661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 06/24/23 13:40:05.728
  STEP: Ensuring ResourceQuota status is calculated @ 06/24/23 13:40:05.733
  E0624 13:40:06.443615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:07.443757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 06/24/23 13:40:07.738
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 06/24/23 13:40:07.76
  E0624 13:40:08.443936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:09.443999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 06/24/23 13:40:09.764
  E0624 13:40:10.444793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:11.444899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/24/23 13:40:11.769
  STEP: Ensuring resource quota status released the pod usage @ 06/24/23 13:40:11.78
  E0624 13:40:12.445051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:13.445191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 06/24/23 13:40:13.786
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 06/24/23 13:40:13.797
  E0624 13:40:14.445431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:15.445868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 06/24/23 13:40:15.803
  E0624 13:40:16.446854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:17.447476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/24/23 13:40:17.808
  STEP: Ensuring resource quota status released the pod usage @ 06/24/23 13:40:17.819
  E0624 13:40:18.448347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:19.448770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:19.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-388" for this suite. @ 06/24/23 13:40:19.829
• [16.160 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 06/24/23 13:40:19.838
  Jun 24 13:40:19.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename containers @ 06/24/23 13:40:19.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:19.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:19.865
  STEP: Creating a pod to test override all @ 06/24/23 13:40:19.87
  E0624 13:40:20.448906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:21.449463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:22.450491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:23.450594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:24.451450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:25.451705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:40:25.905
  Jun 24 13:40:25.910: INFO: Trying to get logs from node ip-172-31-19-205 pod client-containers-206c22f9-160f-4fc8-909d-52feb77772c8 container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:40:25.937
  Jun 24 13:40:25.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5296" for this suite. @ 06/24/23 13:40:25.964
• [6.135 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 06/24/23 13:40:25.974
  Jun 24 13:40:25.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/24/23 13:40:25.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:26.006
  Jun 24 13:40:26.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:40:26.452821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:27.453468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 06/24/23 13:40:27.573
  Jun 24 13:40:27.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 create -f -'
  E0624 13:40:28.453587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:28.876: INFO: stderr: ""
  Jun 24 13:40:28.876: INFO: stdout: "e2e-test-crd-publish-openapi-9810-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 24 13:40:28.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 delete e2e-test-crd-publish-openapi-9810-crds test-foo'
  Jun 24 13:40:28.992: INFO: stderr: ""
  Jun 24 13:40:28.992: INFO: stdout: "e2e-test-crd-publish-openapi-9810-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jun 24 13:40:28.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 apply -f -'
  Jun 24 13:40:29.336: INFO: stderr: ""
  Jun 24 13:40:29.336: INFO: stdout: "e2e-test-crd-publish-openapi-9810-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun 24 13:40:29.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 delete e2e-test-crd-publish-openapi-9810-crds test-foo'
  Jun 24 13:40:29.432: INFO: stderr: ""
  Jun 24 13:40:29.432: INFO: stdout: "e2e-test-crd-publish-openapi-9810-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 06/24/23 13:40:29.432
  Jun 24 13:40:29.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 create -f -'
  E0624 13:40:29.454016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:30.401: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 06/24/23 13:40:30.401
  Jun 24 13:40:30.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 create -f -'
  E0624 13:40:30.454306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:30.769: INFO: rc: 1
  Jun 24 13:40:30.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 apply -f -'
  Jun 24 13:40:31.133: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 06/24/23 13:40:31.133
  Jun 24 13:40:31.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 create -f -'
  E0624 13:40:31.454760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:31.518: INFO: rc: 1
  Jun 24 13:40:31.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 --namespace=crd-publish-openapi-895 apply -f -'
  Jun 24 13:40:31.765: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 06/24/23 13:40:31.765
  Jun 24 13:40:31.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 explain e2e-test-crd-publish-openapi-9810-crds'
  Jun 24 13:40:32.000: INFO: stderr: ""
  Jun 24 13:40:32.000: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9810-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 06/24/23 13:40:32.001
  Jun 24 13:40:32.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 explain e2e-test-crd-publish-openapi-9810-crds.metadata'
  Jun 24 13:40:32.276: INFO: stderr: ""
  Jun 24 13:40:32.276: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9810-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jun 24 13:40:32.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 explain e2e-test-crd-publish-openapi-9810-crds.spec'
  E0624 13:40:32.455779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:32.510: INFO: stderr: ""
  Jun 24 13:40:32.510: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9810-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jun 24 13:40:32.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 explain e2e-test-crd-publish-openapi-9810-crds.spec.bars'
  Jun 24 13:40:32.824: INFO: stderr: ""
  Jun 24 13:40:32.824: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-9810-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 06/24/23 13:40:32.825
  Jun 24 13:40:32.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=crd-publish-openapi-895 explain e2e-test-crd-publish-openapi-9810-crds.spec.bars2'
  Jun 24 13:40:33.163: INFO: rc: 1
  E0624 13:40:33.456301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:34.456463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:35.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-895" for this suite. @ 06/24/23 13:40:35.129
• [9.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 06/24/23 13:40:35.141
  Jun 24 13:40:35.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:40:35.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:35.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:35.167
  STEP: Creating configMap with name projected-configmap-test-volume-map-8019f1bd-e952-4f53-9552-143e2221ac9a @ 06/24/23 13:40:35.171
  STEP: Creating a pod to test consume configMaps @ 06/24/23 13:40:35.177
  E0624 13:40:35.457192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:36.457622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:37.458276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:38.458390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:40:39.208
  Jun 24 13:40:39.212: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-projected-configmaps-e34d4a63-d236-4fd1-beb3-5e991704099b container agnhost-container: <nil>
  STEP: delete the pod @ 06/24/23 13:40:39.235
  Jun 24 13:40:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5588" for this suite. @ 06/24/23 13:40:39.263
• [4.132 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 06/24/23 13:40:39.275
  Jun 24 13:40:39.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:40:39.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:39.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:39.311
  STEP: creating the pod @ 06/24/23 13:40:39.316
  Jun 24 13:40:39.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 create -f -'
  E0624 13:40:39.458862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:40.459789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:40:40.698: INFO: stderr: ""
  Jun 24 13:40:40.698: INFO: stdout: "pod/pause created\n"
  E0624 13:40:41.460881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:42.460961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 06/24/23 13:40:42.717
  Jun 24 13:40:42.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 label pods pause testing-label=testing-label-value'
  Jun 24 13:40:42.815: INFO: stderr: ""
  Jun 24 13:40:42.815: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 06/24/23 13:40:42.815
  Jun 24 13:40:42.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 get pod pause -L testing-label'
  Jun 24 13:40:42.900: INFO: stderr: ""
  Jun 24 13:40:42.900: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 06/24/23 13:40:42.9
  Jun 24 13:40:42.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 label pods pause testing-label-'
  Jun 24 13:40:42.998: INFO: stderr: ""
  Jun 24 13:40:42.998: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 06/24/23 13:40:42.998
  Jun 24 13:40:42.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 get pod pause -L testing-label'
  Jun 24 13:40:43.084: INFO: stderr: ""
  Jun 24 13:40:43.084: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 06/24/23 13:40:43.084
  Jun 24 13:40:43.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 delete --grace-period=0 --force -f -'
  Jun 24 13:40:43.180: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:40:43.180: INFO: stdout: "pod \"pause\" force deleted\n"
  Jun 24 13:40:43.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 get rc,svc -l name=pause --no-headers'
  Jun 24 13:40:43.271: INFO: stderr: "No resources found in kubectl-2774 namespace.\n"
  Jun 24 13:40:43.271: INFO: stdout: ""
  Jun 24 13:40:43.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2774 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun 24 13:40:43.352: INFO: stderr: ""
  Jun 24 13:40:43.352: INFO: stdout: ""
  Jun 24 13:40:43.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2774" for this suite. @ 06/24/23 13:40:43.357
• [4.091 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 06/24/23 13:40:43.366
  Jun 24 13:40:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 13:40:43.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:43.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:43.391
  STEP: Creating secret with name secret-test-7609538f-7090-4025-b09b-e41464338d52 @ 06/24/23 13:40:43.421
  STEP: Creating a pod to test consume secrets @ 06/24/23 13:40:43.427
  E0624 13:40:43.461191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:44.461372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:45.461869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:46.462047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:40:47.452
  Jun 24 13:40:47.457: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-2c9de11f-f2d0-4d00-a2d7-1c1c63c30948 container secret-volume-test: <nil>
  E0624 13:40:47.462151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 06/24/23 13:40:47.465
  Jun 24 13:40:47.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8463" for this suite. @ 06/24/23 13:40:47.49
  STEP: Destroying namespace "secret-namespace-4741" for this suite. @ 06/24/23 13:40:47.499
• [4.139 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 06/24/23 13:40:47.506
  Jun 24 13:40:47.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 13:40:47.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:47.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:47.531
  STEP: create the rc @ 06/24/23 13:40:47.54
  W0624 13:40:47.546476      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0624 13:40:48.462403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:49.462492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:50.462608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:51.462628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:52.462677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 06/24/23 13:40:52.552
  STEP: wait for all pods to be garbage collected @ 06/24/23 13:40:52.566
  E0624 13:40:53.463080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:54.463192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:55.463296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:56.463427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:57.464107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/24/23 13:40:57.574
  W0624 13:40:57.579981      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 13:40:57.580: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 13:40:57.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8463" for this suite. @ 06/24/23 13:40:57.587
• [10.096 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 06/24/23 13:40:57.602
  Jun 24 13:40:57.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/24/23 13:40:57.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:57.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:57.63
  STEP: creating @ 06/24/23 13:40:57.634
  STEP: getting @ 06/24/23 13:40:57.656
  STEP: listing in namespace @ 06/24/23 13:40:57.66
  STEP: patching @ 06/24/23 13:40:57.665
  STEP: deleting @ 06/24/23 13:40:57.685
  Jun 24 13:40:57.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3410" for this suite. @ 06/24/23 13:40:57.704
• [0.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 06/24/23 13:40:57.715
  Jun 24 13:40:57.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context-test @ 06/24/23 13:40:57.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:40:57.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:40:57.742
  E0624 13:40:58.464743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:40:59.464879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:00.465085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:01.465217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:02.466099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:03.466220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:41:03.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-345" for this suite. @ 06/24/23 13:41:03.786
• [6.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 06/24/23 13:41:03.806
  Jun 24 13:41:03.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename crd-watch @ 06/24/23 13:41:03.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:41:03.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:41:03.83
  Jun 24 13:41:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  E0624 13:41:04.467221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:05.467532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 06/24/23 13:41:06.403
  Jun 24 13:41:06.412: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:06Z]] name:name1 resourceVersion:41369 uid:63625b5b-0d04-4c2d-a9af-0fee7fad25ef] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:06.468235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:07.468357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:08.468478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:09.468654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:10.468890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:11.469308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:12.469418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:13.470066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:14.470744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:15.470867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 06/24/23 13:41:16.412
  Jun 24 13:41:16.421: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:16Z]] name:name2 resourceVersion:41402 uid:e446ecc0-13c5-4558-9fc8-6ade785f58e1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:16.471255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:17.471357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:18.471686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:19.471794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:20.472861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:21.473718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:22.473829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:23.473940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:24.474264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:25.474374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 06/24/23 13:41:26.421
  Jun 24 13:41:26.429: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:26Z]] name:name1 resourceVersion:41423 uid:63625b5b-0d04-4c2d-a9af-0fee7fad25ef] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:26.474405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:27.474489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:28.474600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:29.474705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:30.474821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:31.474922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:32.475000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:33.475107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:34.475684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:35.475774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 06/24/23 13:41:36.429
  Jun 24 13:41:36.438: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:36Z]] name:name2 resourceVersion:41444 uid:e446ecc0-13c5-4558-9fc8-6ade785f58e1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:36.475822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:37.476435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:38.476873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:39.477001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:40.477228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:41.478278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:42.478649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:43.478896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:44.479242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:45.479414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 06/24/23 13:41:46.439
  Jun 24 13:41:46.448: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:26Z]] name:name1 resourceVersion:41466 uid:63625b5b-0d04-4c2d-a9af-0fee7fad25ef] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:46.479850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:47.479960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:48.480064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:49.480174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:50.480296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:51.480399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:52.480503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:53.480927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:54.481245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:55.481351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 06/24/23 13:41:56.449
  Jun 24 13:41:56.460: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-24T13:41:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-24T13:41:36Z]] name:name2 resourceVersion:41486 uid:e446ecc0-13c5-4558-9fc8-6ade785f58e1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0624 13:41:56.482083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:57.482250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:58.482641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:41:59.482793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:00.482898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:01.483063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:02.483614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:03.483716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:04.484883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:05.485613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:06.486254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:42:06.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-6784" for this suite. @ 06/24/23 13:42:06.985
• [63.187 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 06/24/23 13:42:06.995
  Jun 24 13:42:06.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename svcaccounts @ 06/24/23 13:42:06.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:42:07.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:42:07.021
  E0624 13:42:07.486537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:08.486969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 06/24/23 13:42:09.058
  Jun 24 13:42:09.059: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3409 pod-service-account-0f1ffc32-bb0c-4b08-8913-4caeef584a53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 06/24/23 13:42:09.212
  Jun 24 13:42:09.212: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3409 pod-service-account-0f1ffc32-bb0c-4b08-8913-4caeef584a53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 06/24/23 13:42:09.377
  Jun 24 13:42:09.377: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3409 pod-service-account-0f1ffc32-bb0c-4b08-8913-4caeef584a53 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  E0624 13:42:09.488025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:42:09.543: INFO: Got root ca configmap in namespace "svcaccounts-3409"
  Jun 24 13:42:09.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3409" for this suite. @ 06/24/23 13:42:09.551
• [2.565 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 06/24/23 13:42:09.56
  Jun 24 13:42:09.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:42:09.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:42:09.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:42:09.584
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/24/23 13:42:09.591
  E0624 13:42:10.488146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:11.488289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:12.488348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:13.488804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:42:13.635
  Jun 24 13:42:13.640: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-01a948e2-2a24-4448-b00c-9ab6028e0e56 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:42:13.649
  Jun 24 13:42:13.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3482" for this suite. @ 06/24/23 13:42:13.674
• [4.122 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 06/24/23 13:42:13.683
  Jun 24 13:42:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption @ 06/24/23 13:42:13.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:42:13.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:42:13.707
  STEP: Creating a pdb that targets all three pods in a test replica set @ 06/24/23 13:42:13.712
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:42:13.718
  E0624 13:42:14.488973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:15.490028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 06/24/23 13:42:15.734
  STEP: Waiting for all pods to be running @ 06/24/23 13:42:15.734
  Jun 24 13:42:15.740: INFO: pods: 0 < 3
  E0624 13:42:16.491126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:17.491816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/24/23 13:42:17.746
  STEP: Updating the pdb to allow a pod to be evicted @ 06/24/23 13:42:17.759
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:42:17.769
  E0624 13:42:18.492879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:19.492953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/24/23 13:42:19.78
  STEP: Waiting for all pods to be running @ 06/24/23 13:42:19.78
  STEP: Waiting for the pdb to observed all healthy pods @ 06/24/23 13:42:19.785
  STEP: Patching the pdb to disallow a pod to be evicted @ 06/24/23 13:42:19.818
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:42:19.853
  STEP: Waiting for all pods to be running @ 06/24/23 13:42:19.861
  Jun 24 13:42:19.869: INFO: running pods: 2 < 3
  E0624 13:42:20.494001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:21.494021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/24/23 13:42:21.873
  STEP: Deleting the pdb to allow a pod to be evicted @ 06/24/23 13:42:21.885
  STEP: Waiting for the pdb to be deleted @ 06/24/23 13:42:21.893
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/24/23 13:42:21.898
  STEP: Waiting for all pods to be running @ 06/24/23 13:42:21.898
  Jun 24 13:42:21.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6421" for this suite. @ 06/24/23 13:42:21.929
• [8.265 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 06/24/23 13:42:21.949
  Jun 24 13:42:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename dns @ 06/24/23 13:42:21.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:42:21.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:42:21.98
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/24/23 13:42:21.984
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/24/23 13:42:21.984
  STEP: creating a pod to probe DNS @ 06/24/23 13:42:21.985
  STEP: submitting the pod to kubernetes @ 06/24/23 13:42:21.985
  E0624 13:42:22.494228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:23.494639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/24/23 13:42:24.011
  STEP: looking for the results for each expected name from probers @ 06/24/23 13:42:24.015
  Jun 24 13:42:24.034: INFO: DNS probes using dns-9373/dns-test-2a236fe2-63c3-49f9-a106-59c4cd277ae1 succeeded

  Jun 24 13:42:24.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:42:24.039
  STEP: Destroying namespace "dns-9373" for this suite. @ 06/24/23 13:42:24.054
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 06/24/23 13:42:24.071
  Jun 24 13:42:24.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename projected @ 06/24/23 13:42:24.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:42:24.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:42:24.091
  STEP: Creating configMap with name cm-test-opt-del-0aeac945-d541-4a4d-a374-0c32050a11c0 @ 06/24/23 13:42:24.108
  STEP: Creating configMap with name cm-test-opt-upd-17c08681-ddc8-4d69-89bf-297254c16253 @ 06/24/23 13:42:24.113
  STEP: Creating the pod @ 06/24/23 13:42:24.12
  E0624 13:42:24.495678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:25.495890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-0aeac945-d541-4a4d-a374-0c32050a11c0 @ 06/24/23 13:42:26.178
  STEP: Updating configmap cm-test-opt-upd-17c08681-ddc8-4d69-89bf-297254c16253 @ 06/24/23 13:42:26.187
  STEP: Creating configMap with name cm-test-opt-create-1628663a-02d7-48c1-9adf-e7eebf83e9e0 @ 06/24/23 13:42:26.193
  STEP: waiting to observe update in volume @ 06/24/23 13:42:26.199
  E0624 13:42:26.496312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:27.496833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:28.497248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:29.497370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:30.498015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:31.498570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:32.499421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:33.499534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:34.499923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:35.499797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:36.500296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:37.500418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:38.501136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:39.501271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:40.502018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:41.502133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:42.502691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:43.502813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:44.503447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:45.503697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:46.503934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:47.504263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:48.504629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:49.504917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:50.506020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:51.506294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:52.507267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:53.507681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:54.508192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:55.508931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:56.509263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:57.509743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:58.510342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:42:59.510428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:00.511369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:01.511473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:02.512261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:03.512380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:04.513423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:05.513571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:06.514512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:07.514913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:08.515545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:09.515693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:10.516533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:11.516766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:12.516870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:13.516987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:14.517822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:15.518024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:16.518996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:17.519436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:18.520223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:19.520279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:20.520941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:21.521080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:22.522108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:23.522469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:24.522699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:25.523024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:26.523906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:27.523979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:28.525017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:29.525143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:30.525433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:31.526184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:32.526367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:33.526957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:34.527085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:35.527198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:36.527307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:37.527404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:38.527526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:39.527537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:40.527713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:41.528989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:42.529068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:43.529195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:44.529276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:45.529396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:46.529700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:47.529723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:48.529819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:49.530617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:50.530718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:51.530843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:52.531351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:53.531677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:54.531779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:55.531907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:43:56.532008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:43:56.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5496" for this suite. @ 06/24/23 13:43:56.702
• [92.640 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 06/24/23 13:43:56.712
  Jun 24 13:43:56.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename limitrange @ 06/24/23 13:43:56.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:43:56.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:43:56.735
  STEP: Creating LimitRange "e2e-limitrange-8qcn6" in namespace "limitrange-8942" @ 06/24/23 13:43:56.741
  STEP: Creating another limitRange in another namespace @ 06/24/23 13:43:56.747
  Jun 24 13:43:56.764: INFO: Namespace "e2e-limitrange-8qcn6-3511" created
  Jun 24 13:43:56.765: INFO: Creating LimitRange "e2e-limitrange-8qcn6" in namespace "e2e-limitrange-8qcn6-3511"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-8qcn6" @ 06/24/23 13:43:56.775
  Jun 24 13:43:56.779: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-8qcn6" in "limitrange-8942" namespace @ 06/24/23 13:43:56.779
  Jun 24 13:43:56.787: INFO: LimitRange "e2e-limitrange-8qcn6" has been patched
  STEP: Delete LimitRange "e2e-limitrange-8qcn6" by Collection with labelSelector: "e2e-limitrange-8qcn6=patched" @ 06/24/23 13:43:56.788
  STEP: Confirm that the limitRange "e2e-limitrange-8qcn6" has been deleted @ 06/24/23 13:43:56.799
  Jun 24 13:43:56.799: INFO: Requesting list of LimitRange to confirm quantity
  Jun 24 13:43:56.804: INFO: Found 0 LimitRange with label "e2e-limitrange-8qcn6=patched"
  Jun 24 13:43:56.804: INFO: LimitRange "e2e-limitrange-8qcn6" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-8qcn6" @ 06/24/23 13:43:56.804
  Jun 24 13:43:56.809: INFO: Found 1 limitRange
  Jun 24 13:43:56.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8942" for this suite. @ 06/24/23 13:43:56.813
  STEP: Destroying namespace "e2e-limitrange-8qcn6-3511" for this suite. @ 06/24/23 13:43:56.821
• [0.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 06/24/23 13:43:56.833
  Jun 24 13:43:56.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename cronjob @ 06/24/23 13:43:56.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:43:56.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:43:56.858
  STEP: Creating a cronjob @ 06/24/23 13:43:56.867
  STEP: creating @ 06/24/23 13:43:56.867
  STEP: getting @ 06/24/23 13:43:56.874
  STEP: listing @ 06/24/23 13:43:56.877
  STEP: watching @ 06/24/23 13:43:56.882
  Jun 24 13:43:56.882: INFO: starting watch
  STEP: cluster-wide listing @ 06/24/23 13:43:56.883
  STEP: cluster-wide watching @ 06/24/23 13:43:56.888
  Jun 24 13:43:56.888: INFO: starting watch
  STEP: patching @ 06/24/23 13:43:56.889
  STEP: updating @ 06/24/23 13:43:56.897
  Jun 24 13:43:56.910: INFO: waiting for watch events with expected annotations
  Jun 24 13:43:56.910: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/24/23 13:43:56.91
  STEP: updating /status @ 06/24/23 13:43:56.918
  STEP: get /status @ 06/24/23 13:43:56.928
  STEP: deleting @ 06/24/23 13:43:56.932
  STEP: deleting a collection @ 06/24/23 13:43:56.951
  Jun 24 13:43:56.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4305" for this suite. @ 06/24/23 13:43:56.971
• [0.146 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 06/24/23 13:43:56.98
  Jun 24 13:43:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename daemonsets @ 06/24/23 13:43:56.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:43:56.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:43:57.004
  STEP: Creating simple DaemonSet "daemon-set" @ 06/24/23 13:43:57.034
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/24/23 13:43:57.043
  Jun 24 13:43:57.048: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:57.048: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:57.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:43:57.054: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:43:57.532290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:43:58.060: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:58.060: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:58.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun 24 13:43:58.065: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:43:58.532738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:43:59.059: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:59.060: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:43:59.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:43:59.064: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:43:59.533053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:00.060: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:00.060: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:00.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:44:00.067: INFO: Node ip-172-31-15-136 is running 0 daemon pod, expected 1
  E0624 13:44:00.533154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:01.059: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:01.060: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:01.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 13:44:01.064: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 06/24/23 13:44:01.068
  Jun 24 13:44:01.087: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:01.087: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:01.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:44:01.093: INFO: Node ip-172-31-19-205 is running 0 daemon pod, expected 1
  E0624 13:44:01.533759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:02.100: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:02.100: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:02.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:44:02.107: INFO: Node ip-172-31-19-205 is running 0 daemon pod, expected 1
  E0624 13:44:02.534595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:03.099: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:03.100: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:03.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun 24 13:44:03.104: INFO: Node ip-172-31-19-205 is running 0 daemon pod, expected 1
  E0624 13:44:03.535351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:04.099: INFO: DaemonSet pods can't tolerate node ip-172-31-15-72 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:04.099: INFO: DaemonSet pods can't tolerate node ip-172-31-26-147 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun 24 13:44:04.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun 24 13:44:04.104: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/24/23 13:44:04.109
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9924, will wait for the garbage collector to delete the pods @ 06/24/23 13:44:04.109
  Jun 24 13:44:04.173: INFO: Deleting DaemonSet.extensions daemon-set took: 9.566393ms
  Jun 24 13:44:04.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.632161ms
  E0624 13:44:04.536751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:05.536992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:06.537417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:06.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun 24 13:44:06.978: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun 24 13:44:06.982: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42225"},"items":null}

  Jun 24 13:44:06.986: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42225"},"items":null}

  Jun 24 13:44:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9924" for this suite. @ 06/24/23 13:44:07.008
• [10.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 06/24/23 13:44:07.019
  Jun 24 13:44:07.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:44:07.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:07.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:07.053
  STEP: creating an Endpoint @ 06/24/23 13:44:07.062
  STEP: waiting for available Endpoint @ 06/24/23 13:44:07.069
  STEP: listing all Endpoints @ 06/24/23 13:44:07.071
  STEP: updating the Endpoint @ 06/24/23 13:44:07.075
  STEP: fetching the Endpoint @ 06/24/23 13:44:07.084
  STEP: patching the Endpoint @ 06/24/23 13:44:07.088
  STEP: fetching the Endpoint @ 06/24/23 13:44:07.1
  STEP: deleting the Endpoint by Collection @ 06/24/23 13:44:07.104
  STEP: waiting for Endpoint deletion @ 06/24/23 13:44:07.115
  STEP: fetching the Endpoint @ 06/24/23 13:44:07.117
  Jun 24 13:44:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2340" for this suite. @ 06/24/23 13:44:07.126
• [0.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 06/24/23 13:44:07.138
  Jun 24 13:44:07.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:44:07.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:07.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:07.166
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6796 @ 06/24/23 13:44:07.171
  STEP: changing the ExternalName service to type=ClusterIP @ 06/24/23 13:44:07.176
  STEP: creating replication controller externalname-service in namespace services-6796 @ 06/24/23 13:44:07.193
  I0624 13:44:07.202052      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6796, replica count: 2
  E0624 13:44:07.537846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:08.538371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:09.538578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 13:44:10.253119      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 13:44:10.253: INFO: Creating new exec pod
  E0624 13:44:10.538625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:11.538932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:12.539680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:13.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-6796 exec execpodth95b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun 24 13:44:13.457: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 24 13:44:13.457: INFO: stdout: ""
  E0624 13:44:13.540702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:14.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-6796 exec execpodth95b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0624 13:44:14.541252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:14.612: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun 24 13:44:14.612: INFO: stdout: "externalname-service-k5nx5"
  Jun 24 13:44:14.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-6796 exec execpodth95b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.34 80'
  Jun 24 13:44:14.771: INFO: stderr: "+ nc -v -t -w 2 10.152.183.34 80\nConnection to 10.152.183.34 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun 24 13:44:14.771: INFO: stdout: "externalname-service-k5nx5"
  Jun 24 13:44:14.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 13:44:14.776: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-6796" for this suite. @ 06/24/23 13:44:14.802
• [7.675 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 06/24/23 13:44:14.814
  Jun 24 13:44:14.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename field-validation @ 06/24/23 13:44:14.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:14.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:14.843
  STEP: apply creating a deployment @ 06/24/23 13:44:14.848
  Jun 24 13:44:14.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8952" for this suite. @ 06/24/23 13:44:14.873
• [0.067 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 06/24/23 13:44:14.883
  Jun 24 13:44:14.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename services @ 06/24/23 13:44:14.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:14.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:14.913
  STEP: creating service in namespace services-3943 @ 06/24/23 13:44:14.92
  STEP: creating service affinity-clusterip in namespace services-3943 @ 06/24/23 13:44:14.92
  STEP: creating replication controller affinity-clusterip in namespace services-3943 @ 06/24/23 13:44:14.933
  I0624 13:44:14.945321      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-3943, replica count: 3
  E0624 13:44:15.541985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:16.542412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:17.543002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 13:44:17.996721      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0624 13:44:18.543694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:19.543854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:20.543971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0624 13:44:20.997791      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun 24 13:44:21.008: INFO: Creating new exec pod
  E0624 13:44:21.545013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:22.545580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:23.546250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:24.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-3943 exec execpod-affinityg7nkj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jun 24 13:44:24.185: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jun 24 13:44:24.185: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:44:24.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-3943 exec execpod-affinityg7nkj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.139 80'
  Jun 24 13:44:24.361: INFO: stderr: "+ nc -v -t -w 2 10.152.183.139 80\n+ echo hostName\nConnection to 10.152.183.139 80 port [tcp/http] succeeded!\n"
  Jun 24 13:44:24.361: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun 24 13:44:24.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=services-3943 exec execpod-affinityg7nkj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.139:80/ ; done'
  E0624 13:44:24.546792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:44:24.605: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.139:80/\n"
  Jun 24 13:44:24.605: INFO: stdout: "\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng\naffinity-clusterip-5hqng"
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Received response from host: affinity-clusterip-5hqng
  Jun 24 13:44:24.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun 24 13:44:24.611: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-3943, will wait for the garbage collector to delete the pods @ 06/24/23 13:44:24.626
  Jun 24 13:44:24.688: INFO: Deleting ReplicationController affinity-clusterip took: 7.117064ms
  Jun 24 13:44:24.789: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.058161ms
  E0624 13:44:25.547671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:26.548512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3943" for this suite. @ 06/24/23 13:44:27.312
• [12.440 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 06/24/23 13:44:27.326
  Jun 24 13:44:27.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename gc @ 06/24/23 13:44:27.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:27.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:27.351
  STEP: create the deployment @ 06/24/23 13:44:27.355
  W0624 13:44:27.361650      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/24/23 13:44:27.361
  E0624 13:44:27.549508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 06/24/23 13:44:27.872
  STEP: wait for all rs to be garbage collected @ 06/24/23 13:44:27.88
  STEP: expected 0 rs, got 1 rs @ 06/24/23 13:44:27.89
  STEP: expected 0 pods, got 2 pods @ 06/24/23 13:44:27.899
  STEP: Gathering metrics @ 06/24/23 13:44:28.412
  W0624 13:44:28.416993      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun 24 13:44:28.417: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun 24 13:44:28.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8098" for this suite. @ 06/24/23 13:44:28.422
• [1.103 seconds]
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 06/24/23 13:44:28.43
  Jun 24 13:44:28.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename configmap @ 06/24/23 13:44:28.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:44:28.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:44:28.455
  STEP: Creating configMap with name cm-test-opt-del-7d60408d-7fd7-4292-a79e-9b68a1f07930 @ 06/24/23 13:44:28.464
  STEP: Creating configMap with name cm-test-opt-upd-1f82b4af-fd27-46f2-ab41-8b3b1ae8b390 @ 06/24/23 13:44:28.471
  STEP: Creating the pod @ 06/24/23 13:44:28.476
  E0624 13:44:28.550125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:29.550199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-7d60408d-7fd7-4292-a79e-9b68a1f07930 @ 06/24/23 13:44:30.53
  STEP: Updating configmap cm-test-opt-upd-1f82b4af-fd27-46f2-ab41-8b3b1ae8b390 @ 06/24/23 13:44:30.538
  STEP: Creating configMap with name cm-test-opt-create-bd414735-8244-4df0-a6b6-2b71ce87b03c @ 06/24/23 13:44:30.544
  E0624 13:44:30.550619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting to observe update in volume @ 06/24/23 13:44:30.553
  E0624 13:44:31.550750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:32.550878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:33.551188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:34.551481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:35.551657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:36.552451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:37.552540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:38.552638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:39.552762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:40.552931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:41.553822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:42.554163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:43.554266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:44.554376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:45.554494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:46.554745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:47.554894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:48.555198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:49.555260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:50.555703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:51.555823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:52.556777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:53.557105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:54.557222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:55.557278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:56.557462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:57.557495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:58.557722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:44:59.557865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:00.557965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:01.558081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:02.558421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:03.558484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:04.558619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:05.559612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:06.559720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:07.560214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:08.560942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:09.561114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:10.561432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:11.561537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:12.561670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:13.561811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:14.561906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:15.562440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:16.562596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:17.562625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:18.562748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:19.562885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:20.563310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:21.563466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:22.563744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:23.564626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:24.564840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:25.564965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:26.565064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:27.565196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:28.565355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:29.565325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:30.565443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:31.566409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:32.566526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:33.567629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:34.567728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:35.567813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:36.567923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:37.568745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:38.568889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:39.568972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:40.569085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:40.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1762" for this suite. @ 06/24/23 13:45:40.944
• [72.524 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 06/24/23 13:45:40.956
  Jun 24 13:45:40.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename replicaset @ 06/24/23 13:45:40.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:45:40.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:45:40.999
  Jun 24 13:45:41.019: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0624 13:45:41.569348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:42.569463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:43.569734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:44.569861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:45.570037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:46.024: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/24/23 13:45:46.024
  STEP: Scaling up "test-rs" replicaset  @ 06/24/23 13:45:46.024
  Jun 24 13:45:46.037: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 06/24/23 13:45:46.037
  W0624 13:45:46.049573      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun 24 13:45:46.052: INFO: observed ReplicaSet test-rs in namespace replicaset-6908 with ReadyReplicas 1, AvailableReplicas 1
  Jun 24 13:45:46.074: INFO: observed ReplicaSet test-rs in namespace replicaset-6908 with ReadyReplicas 1, AvailableReplicas 1
  Jun 24 13:45:46.101: INFO: observed ReplicaSet test-rs in namespace replicaset-6908 with ReadyReplicas 1, AvailableReplicas 1
  Jun 24 13:45:46.112: INFO: observed ReplicaSet test-rs in namespace replicaset-6908 with ReadyReplicas 1, AvailableReplicas 1
  E0624 13:45:46.571355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:47.244: INFO: observed ReplicaSet test-rs in namespace replicaset-6908 with ReadyReplicas 2, AvailableReplicas 2
  Jun 24 13:45:47.353: INFO: observed Replicaset test-rs in namespace replicaset-6908 with ReadyReplicas 3 found true
  Jun 24 13:45:47.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6908" for this suite. @ 06/24/23 13:45:47.359
• [6.411 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 06/24/23 13:45:47.372
  Jun 24 13:45:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename security-context-test @ 06/24/23 13:45:47.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:45:47.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:45:47.402
  E0624 13:45:47.571820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:48.571918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:49.572095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:50.572208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:51.440: INFO: Got logs for pod "busybox-privileged-false-0be66b7f-4d9b-449a-8a64-1a113768f237": "ip: RTNETLINK answers: Operation not permitted\n"
  Jun 24 13:45:51.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8824" for this suite. @ 06/24/23 13:45:51.447
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 06/24/23 13:45:51.46
  Jun 24 13:45:51.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:45:51.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:45:51.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:45:51.492
  STEP: creating all guestbook components @ 06/24/23 13:45:51.497
  Jun 24 13:45:51.497: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jun 24 13:45:51.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  E0624 13:45:51.573108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:51.963: INFO: stderr: ""
  Jun 24 13:45:51.963: INFO: stdout: "service/agnhost-replica created\n"
  Jun 24 13:45:51.963: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jun 24 13:45:51.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  Jun 24 13:45:52.289: INFO: stderr: ""
  Jun 24 13:45:52.289: INFO: stdout: "service/agnhost-primary created\n"
  Jun 24 13:45:52.289: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jun 24 13:45:52.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  E0624 13:45:52.573714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:52.601: INFO: stderr: ""
  Jun 24 13:45:52.601: INFO: stdout: "service/frontend created\n"
  Jun 24 13:45:52.601: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jun 24 13:45:52.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  Jun 24 13:45:53.052: INFO: stderr: ""
  Jun 24 13:45:53.052: INFO: stdout: "deployment.apps/frontend created\n"
  Jun 24 13:45:53.052: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 24 13:45:53.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  Jun 24 13:45:53.521: INFO: stderr: ""
  Jun 24 13:45:53.521: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jun 24 13:45:53.521: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun 24 13:45:53.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 create -f -'
  E0624 13:45:53.573769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:54.004: INFO: stderr: ""
  Jun 24 13:45:54.004: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 06/24/23 13:45:54.004
  Jun 24 13:45:54.005: INFO: Waiting for all frontend pods to be Running.
  E0624 13:45:54.574320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:55.574463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:56.574526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:57.574581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:45:58.574775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:59.056: INFO: Waiting for frontend to serve content.
  Jun 24 13:45:59.067: INFO: Trying to add a new entry to the guestbook.
  Jun 24 13:45:59.080: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.093
  Jun 24 13:45:59.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  Jun 24 13:45:59.198: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.198: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.198
  Jun 24 13:45:59.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  Jun 24 13:45:59.302: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.302: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.303
  Jun 24 13:45:59.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  Jun 24 13:45:59.398: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.398: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.398
  Jun 24 13:45:59.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  Jun 24 13:45:59.482: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.483: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.483
  Jun 24 13:45:59.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  E0624 13:45:59.575845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:45:59.597: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.597: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/24/23 13:45:59.597
  Jun 24 13:45:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-2477 delete --grace-period=0 --force -f -'
  Jun 24 13:45:59.703: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun 24 13:45:59.703: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jun 24 13:45:59.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2477" for this suite. @ 06/24/23 13:45:59.71
• [8.268 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 06/24/23 13:45:59.728
  Jun 24 13:45:59.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename webhook @ 06/24/23 13:45:59.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:45:59.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:45:59.755
  STEP: Setting up server cert @ 06/24/23 13:45:59.793
  E0624 13:46:00.576880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/24/23 13:46:00.771
  STEP: Deploying the webhook pod @ 06/24/23 13:46:00.783
  STEP: Wait for the deployment to be ready @ 06/24/23 13:46:00.801
  Jun 24 13:46:00.813: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0624 13:46:01.577671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:02.577776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:46:02.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 24, 13, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 24, 13, 46, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0624 13:46:03.578499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:04.579266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/24/23 13:46:04.832
  STEP: Verifying the service has paired with the endpoint @ 06/24/23 13:46:04.848
  E0624 13:46:05.579699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:46:05.849: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun 24 13:46:05.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2445-crds.webhook.example.com via the AdmissionRegistration API @ 06/24/23 13:46:06.368
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/24/23 13:46:06.386
  E0624 13:46:06.580316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:07.580496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:46:08.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0624 13:46:08.581135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1530" for this suite. @ 06/24/23 13:46:09.026
  STEP: Destroying namespace "webhook-markers-4990" for this suite. @ 06/24/23 13:46:09.034
• [9.315 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 06/24/23 13:46:09.044
  Jun 24 13:46:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename emptydir @ 06/24/23 13:46:09.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:46:09.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:46:09.073
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/24/23 13:46:09.078
  E0624 13:46:09.581492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:10.581642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:11.582468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:12.582623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:46:13.107
  Jun 24 13:46:13.111: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-d456bce1-5522-484b-8e8e-2d9b7fe3f419 container test-container: <nil>
  STEP: delete the pod @ 06/24/23 13:46:13.12
  Jun 24 13:46:13.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1688" for this suite. @ 06/24/23 13:46:13.144
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 06/24/23 13:46:13.157
  Jun 24 13:46:13.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename kubectl @ 06/24/23 13:46:13.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:46:13.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:46:13.178
  STEP: starting the proxy server @ 06/24/23 13:46:13.183
  Jun 24 13:46:13.183: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2038643291 --namespace=kubectl-8449 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 06/24/23 13:46:13.245
  Jun 24 13:46:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8449" for this suite. @ 06/24/23 13:46:13.26
• [0.112 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 06/24/23 13:46:13.269
  Jun 24 13:46:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename init-container @ 06/24/23 13:46:13.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:46:13.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:46:13.292
  STEP: creating the pod @ 06/24/23 13:46:13.297
  Jun 24 13:46:13.297: INFO: PodSpec: initContainers in spec.initContainers
  E0624 13:46:13.583403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:14.583670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:15.583747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:16.584799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:17.585091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:18.585238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:19.585716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:20.585843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:21.586053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:22.586421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:23.586597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:24.586899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:25.587202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:26.587611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:27.588267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:28.588601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:29.589501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:30.589736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:31.590036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:32.590348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:33.590602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:34.590899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:35.590978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:36.591034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:37.591180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:38.591631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:39.591710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:40.591906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:41.591962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:42.593002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:43.593135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:44.593317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:45.593526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:46.593720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:47.593846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:48.594011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:49.594361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:50.594665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:51.594906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:52.595024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:53.595134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:54.595242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:55.595344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:56.595440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:57.595670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:46:58.595773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:46:59.500: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-61f337f2-1d24-4b7a-9eb7-da5dfce2764d", GenerateName:"", Namespace:"init-container-3038", SelfLink:"", UID:"5a793916-5af3-41d0-8eb4-b91de1fcd64c", ResourceVersion:"43540", Generation:0, CreationTimestamp:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"297696995"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005c143d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 24, 13, 46, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005c14438), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-v24sl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0058b1800), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-v24sl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-v24sl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-v24sl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004370a00), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-19-205", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a8a1c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004370a90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004370ab0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004370ab8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004370abc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc005e4b610), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.19.205", PodIP:"192.168.150.247", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.150.247"}}, StartTime:time.Date(2023, time.June, 24, 13, 46, 13, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a8a380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a8a3f0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ea6ea88eeb6412b0b37ff6e7a385a6a4c1bbc63b63daff4d8f397d66c9b74d27", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0058b1880), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0058b1860), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004370b34), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jun 24 13:46:59.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3038" for this suite. @ 06/24/23 13:46:59.507
• [46.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 06/24/23 13:46:59.517
  Jun 24 13:46:59.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename container-probe @ 06/24/23 13:46:59.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:46:59.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:46:59.541
  STEP: Creating pod test-grpc-de41620d-d0a7-4d66-b09a-520b6899819f in namespace container-probe-4634 @ 06/24/23 13:46:59.545
  E0624 13:46:59.595874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:00.596216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:47:01.570: INFO: Started pod test-grpc-de41620d-d0a7-4d66-b09a-520b6899819f in namespace container-probe-4634
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/24/23 13:47:01.57
  Jun 24 13:47:01.573: INFO: Initial restart count of pod test-grpc-de41620d-d0a7-4d66-b09a-520b6899819f is 0
  E0624 13:47:01.596461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:02.596569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:03.597435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:04.597575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:05.597727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:06.597895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:07.598806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:08.598963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:09.599434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:10.599699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:11.599857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:12.600253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:13.600485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:14.601169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:15.601327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:16.601437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:17.601560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:18.601675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:19.601790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:20.601911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:21.602326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:22.602881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:23.603114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:24.603707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:25.604787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:26.605029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:27.605199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:28.605332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:29.605458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:30.605528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:31.605634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:32.606637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:33.606753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:34.606863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:35.606931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:36.607084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:37.607509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:38.608362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:39.608429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:40.608555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:41.608924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:42.608962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:43.609077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:44.609228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:45.609336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:46.609463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:47.609561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:48.609676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:49.609799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:50.609939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:51.610006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:52.610143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:53.610257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:54.610420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:55.610469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:56.610610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:57.611632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:58.611737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:47:59.612794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:00.612916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:01.613800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:02.613920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:03.614017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:04.614626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:05.615109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:06.615222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:07.615332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:08.615370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:09.616102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:10.616226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:11.616329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:12.616480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:13.616561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:14.616739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:15.616975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:16.617184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:17.617659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:18.617796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:19.618554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:20.618649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:21.618873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:22.618856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:23.618942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:24.618994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:25.619092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:26.619413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:27.619689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:28.619841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:29.619917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:30.620513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:31.620640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:32.620918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:33.620968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:34.620977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:35.621184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:36.622243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:37.622800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:38.623011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:39.623124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:40.624049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:41.624146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:42.624250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:43.624363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:44.624470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:45.624578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:46.624673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:47.625281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:48.625399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:49.625788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:50.625907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:51.626323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:52.626442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:53.627413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:54.628443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:55.629324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:56.630062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:57.630167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:58.630271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:48:59.630388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:00.630940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:01.631035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:02.631685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:03.631791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:04.632105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:05.632197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:06.632309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:07.632777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:08.632881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:09.632999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:10.633636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:11.633734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:12.633827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:13.634569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:14.634685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:15.634806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:16.634893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:17.635000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:18.635119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:19.635230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:20.635339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:21.635437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:22.635682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:23.635798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:24.636604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:25.636719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:26.636891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:27.636910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:28.637586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:29.637677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:30.638514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:31.638616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:32.638724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:33.638864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:34.638948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:35.639052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:36.639170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:37.639307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:38.639390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:39.639687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:40.640282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:41.640488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:42.640706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:43.641314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:44.641442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:45.641588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:46.641765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:47.641881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:48.642965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:49.643070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:50.643228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:51.643292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:52.643442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:53.643797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:54.644725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:55.644843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:56.645067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:57.645175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:58.645375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:49:59.645654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:00.646554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:01.646839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:02.646965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:03.648022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:04.648819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:05.648947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:06.650019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:07.650540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:08.650724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:09.651000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:10.651142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:11.651261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:12.651497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:13.651746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:14.651878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:15.652007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:16.652334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:17.652793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:18.652936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:19.653470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:20.653624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:21.653921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:22.654968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:23.655202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:24.655686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:25.655715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:26.655811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:27.655920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:28.656304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:29.656419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:30.656790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:31.656900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:32.657074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:33.657451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:34.658417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:35.658524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:36.658655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:37.659059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:38.660084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:39.660210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:40.660876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:41.660907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:42.661049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:43.661273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:44.661443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:45.661627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:46.661710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:47.661767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:48.662306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:49.662371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:50.663010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:51.663132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:52.663185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:53.663305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:54.663920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:55.664767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:56.664913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:57.664996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:58.665652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:50:59.665853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:00.666616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:01.667014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun 24 13:51:02.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/24/23 13:51:02.226
  STEP: Destroying namespace "container-probe-4634" for this suite. @ 06/24/23 13:51:02.243
• [242.735 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 06/24/23 13:51:02.253
  Jun 24 13:51:02.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename disruption @ 06/24/23 13:51:02.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:51:02.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:51:02.284
  STEP: creating the pdb @ 06/24/23 13:51:02.291
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:51:02.297
  E0624 13:51:02.667682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:03.667809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 06/24/23 13:51:04.306
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:51:04.318
  E0624 13:51:04.668775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:05.668879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 06/24/23 13:51:06.327
  STEP: Waiting for the pdb to be processed @ 06/24/23 13:51:06.338
  STEP: Waiting for the pdb to be deleted @ 06/24/23 13:51:06.354
  Jun 24 13:51:06.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-848" for this suite. @ 06/24/23 13:51:06.363
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 06/24/23 13:51:06.377
  Jun 24 13:51:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename subjectreview @ 06/24/23 13:51:06.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:51:06.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:51:06.401
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-564" @ 06/24/23 13:51:06.406
  Jun 24 13:51:06.413: INFO: saUsername: "system:serviceaccount:subjectreview-564:e2e"
  Jun 24 13:51:06.413: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-564"}
  Jun 24 13:51:06.413: INFO: saUID: "45b5f6a3-6aa1-4c6a-83a1-26a16e531976"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-564:e2e" @ 06/24/23 13:51:06.413
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-564:e2e" @ 06/24/23 13:51:06.414
  Jun 24 13:51:06.417: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-564:e2e" api 'list' configmaps in "subjectreview-564" namespace @ 06/24/23 13:51:06.418
  Jun 24 13:51:06.421: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-564:e2e" @ 06/24/23 13:51:06.421
  Jun 24 13:51:06.424: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jun 24 13:51:06.425: INFO: LocalSubjectAccessReview has been verified
  Jun 24 13:51:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-564" for this suite. @ 06/24/23 13:51:06.431
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 06/24/23 13:51:06.441
  Jun 24 13:51:06.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2038643291
  STEP: Building a namespace api object, basename secrets @ 06/24/23 13:51:06.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/24/23 13:51:06.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/24/23 13:51:06.466
  STEP: Creating secret with name secret-test-9b2a8812-9643-4610-9e8c-462ab4d4d5f5 @ 06/24/23 13:51:06.481
  STEP: Creating a pod to test consume secrets @ 06/24/23 13:51:06.488
  E0624 13:51:06.669097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:07.669488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:08.669999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0624 13:51:09.670130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/24/23 13:51:10.525
  Jun 24 13:51:10.529: INFO: Trying to get logs from node ip-172-31-19-205 pod pod-secrets-e99b962e-6401-48fd-bcf9-240a83688b9a container secret-env-test: <nil>
  STEP: delete the pod @ 06/24/23 13:51:10.553
  Jun 24 13:51:10.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-991" for this suite. @ 06/24/23 13:51:10.576
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jun 24 13:51:10.587: INFO: Running AfterSuite actions on node 1
  Jun 24 13:51:10.588: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.062 seconds]
------------------------------

Ran 378 of 7207 Specs in 6123.326 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h42m3.898853485s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

