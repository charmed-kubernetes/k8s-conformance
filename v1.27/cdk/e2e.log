  I0722 12:05:20.315941      19 e2e.go:117] Starting e2e run "c04793bf-29a9-4416-8432-0365d1fcd818" on Ginkgo node 1
  Jul 22 12:05:20.360: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1690027520 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul 22 12:05:20.614: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:05:20.615: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul 22 12:05:20.671: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul 22 12:05:20.678: INFO: e2e test version: v1.27.4
  Jul 22 12:05:20.681: INFO: kube-apiserver version: v1.27.4
  Jul 22 12:05:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:05:20.689: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/22/23 12:05:21.124
  Jul 22 12:05:21.125: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:05:21.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:05:21.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:05:21.157
  STEP: Creating configMap with name configmap-test-upd-44a287e2-98f9-479c-942c-7a96dc7b1069 @ 07/22/23 12:05:21.168
  STEP: Creating the pod @ 07/22/23 12:05:21.174
  STEP: Updating configmap configmap-test-upd-44a287e2-98f9-479c-942c-7a96dc7b1069 @ 07/22/23 12:05:27.255
  STEP: waiting to observe update in volume @ 07/22/23 12:05:27.266
  Jul 22 12:05:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4302" for this suite. @ 07/22/23 12:05:29.301
• [8.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/22/23 12:05:29.321
  Jul 22 12:05:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 12:05:29.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:05:29.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:05:29.362
  STEP: Creating pod test-grpc-cdc6f427-ec21-4fa7-9844-800e1b2cc3a6 in namespace container-probe-7466 @ 07/22/23 12:05:29.368
  Jul 22 12:05:31.400: INFO: Started pod test-grpc-cdc6f427-ec21-4fa7-9844-800e1b2cc3a6 in namespace container-probe-7466
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 12:05:31.4
  Jul 22 12:05:31.405: INFO: Initial restart count of pod test-grpc-cdc6f427-ec21-4fa7-9844-800e1b2cc3a6 is 0
  Jul 22 12:06:45.698: INFO: Restart count of pod container-probe-7466/test-grpc-cdc6f427-ec21-4fa7-9844-800e1b2cc3a6 is now 1 (1m14.29218099s elapsed)
  Jul 22 12:06:45.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:06:45.704
  STEP: Destroying namespace "container-probe-7466" for this suite. @ 07/22/23 12:06:45.721
• [76.411 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/22/23 12:06:45.732
  Jul 22 12:06:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:06:45.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:06:45.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:06:45.776
  STEP: Creating projection with secret that has name projected-secret-test-map-e4a41366-ef54-4c36-9b92-3a0c24bcff89 @ 07/22/23 12:06:45.779
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:06:45.785
  STEP: Saw pod success @ 07/22/23 12:06:49.827
  Jul 22 12:06:49.836: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-047dd020-c5ea-4d3e-9446-0007f8b165fb container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:06:49.847
  Jul 22 12:06:49.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7244" for this suite. @ 07/22/23 12:06:49.888
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/22/23 12:06:49.903
  Jul 22 12:06:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-webhook @ 07/22/23 12:06:49.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:06:49.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:06:49.939
  STEP: Setting up server cert @ 07/22/23 12:06:49.944
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/22/23 12:06:50.691
  STEP: Deploying the custom resource conversion webhook pod @ 07/22/23 12:06:50.705
  STEP: Wait for the deployment to be ready @ 07/22/23 12:06:50.726
  Jul 22 12:06:50.738: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/22/23 12:06:52.757
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:06:52.775
  Jul 22 12:06:53.775: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 22 12:06:53.781: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Creating a v1 custom resource @ 07/22/23 12:06:56.416
  STEP: Create a v2 custom resource @ 07/22/23 12:06:56.453
  STEP: List CRs in v1 @ 07/22/23 12:06:56.47
  STEP: List CRs in v2 @ 07/22/23 12:06:56.539
  Jul 22 12:06:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4956" for this suite. @ 07/22/23 12:06:57.148
• [7.257 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/22/23 12:06:57.163
  Jul 22 12:06:57.163: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename runtimeclass @ 07/22/23 12:06:57.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:06:57.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:06:57.214
  STEP: Deleting RuntimeClass runtimeclass-6816-delete-me @ 07/22/23 12:06:57.232
  STEP: Waiting for the RuntimeClass to disappear @ 07/22/23 12:06:57.242
  Jul 22 12:06:57.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6816" for this suite. @ 07/22/23 12:06:57.295
• [0.147 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/22/23 12:06:57.311
  Jul 22 12:06:57.311: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/22/23 12:06:57.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:06:57.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:06:57.344
  STEP: create the container to handle the HTTPGet hook request. @ 07/22/23 12:06:57.354
  STEP: create the pod with lifecycle hook @ 07/22/23 12:07:03.401
  STEP: check poststart hook @ 07/22/23 12:07:05.432
  STEP: delete the pod with lifecycle hook @ 07/22/23 12:07:05.461
  Jul 22 12:07:07.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3464" for this suite. @ 07/22/23 12:07:07.498
• [10.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 07/22/23 12:07:07.515
  Jul 22 12:07:07.515: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 12:07:07.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:07.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:07.56
  Jul 22 12:07:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:07:10.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3915" for this suite. @ 07/22/23 12:07:10.77
• [3.265 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/22/23 12:07:10.781
  Jul 22 12:07:10.781: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 12:07:10.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:10.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:10.813
  STEP: Creating service test in namespace statefulset-2470 @ 07/22/23 12:07:10.817
  Jul 22 12:07:10.846: INFO: Found 0 stateful pods, waiting for 1
  Jul 22 12:07:20.859: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/22/23 12:07:20.868
  W0722 12:07:20.882020      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 22 12:07:20.893: INFO: Found 1 stateful pods, waiting for 2
  Jul 22 12:07:30.901: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Jul 22 12:07:40.899: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:07:40.899: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/22/23 12:07:40.911
  STEP: Delete all of the StatefulSets @ 07/22/23 12:07:40.919
  STEP: Verify that StatefulSets have been deleted @ 07/22/23 12:07:40.931
  Jul 22 12:07:40.936: INFO: Deleting all statefulset in ns statefulset-2470
  Jul 22 12:07:40.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2470" for this suite. @ 07/22/23 12:07:40.972
• [30.200 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/22/23 12:07:40.981
  Jul 22 12:07:40.981: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 12:07:40.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:41.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:41.022
  STEP: Creating a pod to test service account token:  @ 07/22/23 12:07:41.028
  STEP: Saw pod success @ 07/22/23 12:07:45.066
  Jul 22 12:07:45.071: INFO: Trying to get logs from node ip-172-31-15-55 pod test-pod-8bffa863-13a9-4c49-804b-c48c179f441d container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:07:45.082
  Jul 22 12:07:45.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5843" for this suite. @ 07/22/23 12:07:45.114
• [4.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/22/23 12:07:45.127
  Jul 22 12:07:45.127: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 12:07:45.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:45.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:45.158
  STEP: Creating service test in namespace statefulset-4100 @ 07/22/23 12:07:45.164
  STEP: Looking for a node to schedule stateful set and pod @ 07/22/23 12:07:45.171
  STEP: Creating pod with conflicting port in namespace statefulset-4100 @ 07/22/23 12:07:45.178
  STEP: Waiting until pod test-pod will start running in namespace statefulset-4100 @ 07/22/23 12:07:45.197
  STEP: Creating statefulset with conflicting port in namespace statefulset-4100 @ 07/22/23 12:07:47.208
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4100 @ 07/22/23 12:07:47.225
  Jul 22 12:07:47.244: INFO: Observed stateful pod in namespace: statefulset-4100, name: ss-0, uid: 5218dbd3-ac01-47c5-ad0e-2daf0cc368db, status phase: Pending. Waiting for statefulset controller to delete.
  Jul 22 12:07:47.271: INFO: Observed stateful pod in namespace: statefulset-4100, name: ss-0, uid: 5218dbd3-ac01-47c5-ad0e-2daf0cc368db, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 22 12:07:47.310: INFO: Observed stateful pod in namespace: statefulset-4100, name: ss-0, uid: 5218dbd3-ac01-47c5-ad0e-2daf0cc368db, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 22 12:07:47.316: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4100
  STEP: Removing pod with conflicting port in namespace statefulset-4100 @ 07/22/23 12:07:47.317
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4100 and will be in running state @ 07/22/23 12:07:47.351
  Jul 22 12:07:49.370: INFO: Deleting all statefulset in ns statefulset-4100
  Jul 22 12:07:49.377: INFO: Scaling statefulset ss to 0
  Jul 22 12:07:59.412: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 12:07:59.418: INFO: Deleting statefulset ss
  Jul 22 12:07:59.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4100" for this suite. @ 07/22/23 12:07:59.453
• [14.340 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/22/23 12:07:59.468
  Jul 22 12:07:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename ingress @ 07/22/23 12:07:59.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:59.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:59.547
  STEP: getting /apis @ 07/22/23 12:07:59.552
  STEP: getting /apis/networking.k8s.io @ 07/22/23 12:07:59.56
  STEP: getting /apis/networking.k8s.iov1 @ 07/22/23 12:07:59.567
  STEP: creating @ 07/22/23 12:07:59.569
  STEP: getting @ 07/22/23 12:07:59.595
  STEP: listing @ 07/22/23 12:07:59.602
  STEP: watching @ 07/22/23 12:07:59.608
  Jul 22 12:07:59.609: INFO: starting watch
  STEP: cluster-wide listing @ 07/22/23 12:07:59.611
  STEP: cluster-wide watching @ 07/22/23 12:07:59.618
  Jul 22 12:07:59.618: INFO: starting watch
  STEP: patching @ 07/22/23 12:07:59.621
  STEP: updating @ 07/22/23 12:07:59.631
  Jul 22 12:07:59.644: INFO: waiting for watch events with expected annotations
  Jul 22 12:07:59.646: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/22/23 12:07:59.646
  STEP: updating /status @ 07/22/23 12:07:59.659
  STEP: get /status @ 07/22/23 12:07:59.675
  STEP: deleting @ 07/22/23 12:07:59.682
  STEP: deleting a collection @ 07/22/23 12:07:59.711
  Jul 22 12:07:59.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-3310" for this suite. @ 07/22/23 12:07:59.753
• [0.297 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/22/23 12:07:59.81
  Jul 22 12:07:59.811: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:07:59.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:07:59.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:07:59.842
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-8236 @ 07/22/23 12:07:59.846
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/22/23 12:07:59.874
  STEP: creating service externalsvc in namespace services-8236 @ 07/22/23 12:07:59.874
  STEP: creating replication controller externalsvc in namespace services-8236 @ 07/22/23 12:07:59.898
  I0722 12:07:59.909796      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8236, replica count: 2
  I0722 12:08:02.961958      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0722 12:08:05.962834      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/22/23 12:08:05.967
  Jul 22 12:08:05.999: INFO: Creating new exec pod
  Jul 22 12:08:08.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8236 exec execpodwr2r7 -- /bin/sh -x -c nslookup nodeport-service.services-8236.svc.cluster.local'
  Jul 22 12:08:08.266: INFO: stderr: "+ nslookup nodeport-service.services-8236.svc.cluster.local\n"
  Jul 22 12:08:08.266: INFO: stdout: "Server:\t\t10.152.183.138\nAddress:\t10.152.183.138#53\n\nnodeport-service.services-8236.svc.cluster.local\tcanonical name = externalsvc.services-8236.svc.cluster.local.\nName:\texternalsvc.services-8236.svc.cluster.local\nAddress: 10.152.183.62\n\n"
  Jul 22 12:08:08.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8236, will wait for the garbage collector to delete the pods @ 07/22/23 12:08:08.274
  Jul 22 12:08:08.341: INFO: Deleting ReplicationController externalsvc took: 9.569013ms
  Jul 22 12:08:08.442: INFO: Terminating ReplicationController externalsvc pods took: 100.79746ms
  Jul 22 12:08:12.373: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-8236" for this suite. @ 07/22/23 12:08:12.389
• [12.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/22/23 12:08:12.406
  Jul 22 12:08:12.406: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename hostport @ 07/22/23 12:08:12.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:08:12.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:08:12.441
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/22/23 12:08:12.451
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.81.237 on the node which pod1 resides and expect scheduled @ 07/22/23 12:08:14.497
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.81.237 but use UDP protocol on the node which pod2 resides @ 07/22/23 12:08:16.516
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/22/23 12:08:20.562
  Jul 22 12:08:20.562: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.81.237 http://127.0.0.1:54323/hostname] Namespace:hostport-3014 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:08:20.562: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:08:20.563: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:08:20.563: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3014/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.81.237+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.81.237, port: 54323 @ 07/22/23 12:08:20.656
  Jul 22 12:08:20.656: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.81.237:54323/hostname] Namespace:hostport-3014 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:08:20.657: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:08:20.657: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:08:20.658: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3014/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.81.237%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.81.237, port: 54323 UDP @ 07/22/23 12:08:20.759
  Jul 22 12:08:20.759: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.81.237 54323] Namespace:hostport-3014 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:08:20.759: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:08:20.760: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:08:20.760: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-3014/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.81.237+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jul 22 12:08:25.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3014" for this suite. @ 07/22/23 12:08:25.86
• [13.463 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/22/23 12:08:25.871
  Jul 22 12:08:25.871: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:08:25.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:08:25.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:08:25.905
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:08:25.909
  STEP: Saw pod success @ 07/22/23 12:08:29.988
  Jul 22 12:08:29.998: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-9400764e-24f1-4ba7-91aa-786183fbd9a4 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:08:30.01
  Jul 22 12:08:30.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-524" for this suite. @ 07/22/23 12:08:30.039
• [4.186 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/22/23 12:08:30.057
  Jul 22 12:08:30.057: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename cronjob @ 07/22/23 12:08:30.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:08:30.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:08:30.088
  STEP: Creating a suspended cronjob @ 07/22/23 12:08:30.093
  STEP: Ensuring no jobs are scheduled @ 07/22/23 12:08:30.103
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/22/23 12:13:30.117
  STEP: Removing cronjob @ 07/22/23 12:13:30.127
  Jul 22 12:13:30.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3909" for this suite. @ 07/22/23 12:13:30.147
• [300.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/22/23 12:13:30.171
  Jul 22 12:13:30.172: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 12:13:30.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:30.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:30.215
  Jul 22 12:13:30.220: INFO: Creating deployment "test-recreate-deployment"
  Jul 22 12:13:30.227: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul 22 12:13:30.240: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Jul 22 12:13:32.254: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul 22 12:13:32.260: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul 22 12:13:32.274: INFO: Updating deployment test-recreate-deployment
  Jul 22 12:13:32.274: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul 22 12:13:32.397: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4456  20cca35e-fd45-4af6-ae04-72815c6580f6 4273 2 2023-07-22 12:13:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d1d118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-22 12:13:32 +0000 UTC,LastTransitionTime:2023-07-22 12:13:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-22 12:13:32 +0000 UTC,LastTransitionTime:2023-07-22 12:13:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 22 12:13:32.403: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-4456  7e62b100-d78a-46d6-9b5d-b5b34ec3f6f6 4270 1 2023-07-22 12:13:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 20cca35e-fd45-4af6-ae04-72815c6580f6 0xc003d1d4c7 0xc003d1d4c8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20cca35e-fd45-4af6-ae04-72815c6580f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d1d568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:13:32.403: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul 22 12:13:32.403: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-4456  3072fcff-946e-4dea-93b1-e63bf2b5e7ee 4260 2 2023-07-22 12:13:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 20cca35e-fd45-4af6-ae04-72815c6580f6 0xc003d1d5c7 0xc003d1d5c8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20cca35e-fd45-4af6-ae04-72815c6580f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d1d678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:13:32.410: INFO: Pod "test-recreate-deployment-54757ffd6c-rvnzt" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-rvnzt test-recreate-deployment-54757ffd6c- deployment-4456  a481b33e-7606-4629-9d6b-3ec4f044e257 4272 0 2023-07-22 12:13:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 7e62b100-d78a-46d6-9b5d-b5b34ec3f6f6 0xc003d1daf7 0xc003d1daf8}] [] [{kube-controller-manager Update v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e62b100-d78a-46d6-9b5d-b5b34ec3f6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 12:13:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98dc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98dc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:13:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:13:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:,StartTime:2023-07-22 12:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 12:13:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4456" for this suite. @ 07/22/23 12:13:32.418
• [2.260 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/22/23 12:13:32.433
  Jul 22 12:13:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:13:32.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:32.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:32.466
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/22/23 12:13:32.472
  STEP: Saw pod success @ 07/22/23 12:13:36.505
  Jul 22 12:13:36.513: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-0eb801b0-ee19-4dd0-8a19-51ec2ee15486 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:13:36.542
  Jul 22 12:13:36.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4922" for this suite. @ 07/22/23 12:13:36.574
• [4.156 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 07/22/23 12:13:36.591
  Jul 22 12:13:36.591: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 12:13:36.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:36.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:36.628
  Jul 22 12:13:36.632: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  W0722 12:13:39.216573      19 warnings.go:70] unknown field "alpha"
  W0722 12:13:39.216613      19 warnings.go:70] unknown field "beta"
  W0722 12:13:39.216619      19 warnings.go:70] unknown field "delta"
  W0722 12:13:39.216625      19 warnings.go:70] unknown field "epsilon"
  W0722 12:13:39.216630      19 warnings.go:70] unknown field "gamma"
  Jul 22 12:13:39.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4797" for this suite. @ 07/22/23 12:13:39.817
• [3.236 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/22/23 12:13:39.828
  Jul 22 12:13:39.828: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:13:39.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:39.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:39.868
  STEP: Setting up server cert @ 07/22/23 12:13:39.913
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:13:41.003
  STEP: Deploying the webhook pod @ 07/22/23 12:13:41.018
  STEP: Wait for the deployment to be ready @ 07/22/23 12:13:41.04
  Jul 22 12:13:41.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/22/23 12:13:43.089
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:13:43.11
  Jul 22 12:13:44.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/22/23 12:13:44.115
  STEP: create a pod @ 07/22/23 12:13:44.141
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/22/23 12:13:46.173
  Jul 22 12:13:46.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=webhook-4277 attach --namespace=webhook-4277 to-be-attached-pod -i -c=container1'
  Jul 22 12:13:46.323: INFO: rc: 1
  Jul 22 12:13:46.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4277" for this suite. @ 07/22/23 12:13:46.405
  STEP: Destroying namespace "webhook-markers-1506" for this suite. @ 07/22/23 12:13:46.419
• [6.602 seconds]
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/22/23 12:13:46.435
  Jul 22 12:13:46.436: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename watch @ 07/22/23 12:13:46.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:46.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:46.48
  STEP: creating a watch on configmaps @ 07/22/23 12:13:46.483
  STEP: creating a new configmap @ 07/22/23 12:13:46.485
  STEP: modifying the configmap once @ 07/22/23 12:13:46.495
  STEP: closing the watch once it receives two notifications @ 07/22/23 12:13:46.506
  Jul 22 12:13:46.507: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8678  3ee9aa9c-deff-4d01-97d4-f06f78212396 4464 0 2023-07-22 12:13:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-22 12:13:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:13:46.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8678  3ee9aa9c-deff-4d01-97d4-f06f78212396 4465 0 2023-07-22 12:13:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-22 12:13:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/22/23 12:13:46.508
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/22/23 12:13:46.519
  STEP: deleting the configmap @ 07/22/23 12:13:46.521
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/22/23 12:13:46.531
  Jul 22 12:13:46.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8678  3ee9aa9c-deff-4d01-97d4-f06f78212396 4466 0 2023-07-22 12:13:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-22 12:13:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:13:46.532: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8678  3ee9aa9c-deff-4d01-97d4-f06f78212396 4467 0 2023-07-22 12:13:46 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-22 12:13:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:13:46.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8678" for this suite. @ 07/22/23 12:13:46.54
• [0.119 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/22/23 12:13:46.555
  Jul 22 12:13:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:13:46.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:13:46.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:13:46.583
  STEP: Creating a ResourceQuota with best effort scope @ 07/22/23 12:13:46.588
  STEP: Ensuring ResourceQuota status is calculated @ 07/22/23 12:13:46.596
  STEP: Creating a ResourceQuota with not best effort scope @ 07/22/23 12:13:48.601
  STEP: Ensuring ResourceQuota status is calculated @ 07/22/23 12:13:48.609
  STEP: Creating a best-effort pod @ 07/22/23 12:13:50.616
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/22/23 12:13:50.64
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/22/23 12:13:52.647
  STEP: Deleting the pod @ 07/22/23 12:13:54.655
  STEP: Ensuring resource quota status released the pod usage @ 07/22/23 12:13:54.672
  STEP: Creating a not best-effort pod @ 07/22/23 12:13:56.678
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/22/23 12:13:56.703
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/22/23 12:13:58.71
  STEP: Deleting the pod @ 07/22/23 12:14:00.716
  STEP: Ensuring resource quota status released the pod usage @ 07/22/23 12:14:00.736
  Jul 22 12:14:02.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9088" for this suite. @ 07/22/23 12:14:02.755
• [16.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/22/23 12:14:02.766
  Jul 22 12:14:02.766: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:14:02.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:14:02.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:14:02.8
  STEP: creating the pod @ 07/22/23 12:14:02.808
  Jul 22 12:14:02.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 create -f -'
  Jul 22 12:14:03.599: INFO: stderr: ""
  Jul 22 12:14:03.599: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/22/23 12:14:05.613
  Jul 22 12:14:05.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 label pods pause testing-label=testing-label-value'
  Jul 22 12:14:05.724: INFO: stderr: ""
  Jul 22 12:14:05.724: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/22/23 12:14:05.724
  Jul 22 12:14:05.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 get pod pause -L testing-label'
  Jul 22 12:14:05.825: INFO: stderr: ""
  Jul 22 12:14:05.825: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/22/23 12:14:05.825
  Jul 22 12:14:05.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 label pods pause testing-label-'
  Jul 22 12:14:05.936: INFO: stderr: ""
  Jul 22 12:14:05.936: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/22/23 12:14:05.937
  Jul 22 12:14:05.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 get pod pause -L testing-label'
  Jul 22 12:14:06.049: INFO: stderr: ""
  Jul 22 12:14:06.049: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 07/22/23 12:14:06.049
  Jul 22 12:14:06.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Jul 22 12:14:06.181: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:14:06.181: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul 22 12:14:06.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 get rc,svc -l name=pause --no-headers'
  Jul 22 12:14:06.307: INFO: stderr: "No resources found in kubectl-9197 namespace.\n"
  Jul 22 12:14:06.307: INFO: stdout: ""
  Jul 22 12:14:06.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9197 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 22 12:14:06.414: INFO: stderr: ""
  Jul 22 12:14:06.414: INFO: stdout: ""
  Jul 22 12:14:06.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9197" for this suite. @ 07/22/23 12:14:06.42
• [3.663 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/22/23 12:14:06.431
  Jul 22 12:14:06.431: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 12:14:06.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:14:06.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:14:06.465
  STEP: creating a Deployment @ 07/22/23 12:14:06.476
  Jul 22 12:14:06.476: INFO: Creating simple deployment test-deployment-p574b
  Jul 22 12:14:06.498: INFO: deployment "test-deployment-p574b" doesn't have the required revision set
  STEP: Getting /status @ 07/22/23 12:14:08.521
  Jul 22 12:14:08.534: INFO: Deployment test-deployment-p574b has Conditions: [{Available True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-p574b-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/22/23 12:14:08.534
  Jul 22 12:14:08.551: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 14, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 14, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 14, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 14, 6, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-p574b-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/22/23 12:14:08.552
  Jul 22 12:14:08.555: INFO: Observed &Deployment event: ADDED
  Jul 22 12:14:08.555: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-p574b-5994cf9475"}
  Jul 22 12:14:08.555: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.560: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-p574b-5994cf9475"}
  Jul 22 12:14:08.561: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 22 12:14:08.561: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.561: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 22 12:14:08.561: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-p574b-5994cf9475" is progressing.}
  Jul 22 12:14:08.562: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.562: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 22 12:14:08.562: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-p574b-5994cf9475" has successfully progressed.}
  Jul 22 12:14:08.562: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.562: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 22 12:14:08.562: INFO: Observed Deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-p574b-5994cf9475" has successfully progressed.}
  Jul 22 12:14:08.562: INFO: Found Deployment test-deployment-p574b in namespace deployment-3830 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 12:14:08.562: INFO: Deployment test-deployment-p574b has an updated status
  STEP: patching the Statefulset Status @ 07/22/23 12:14:08.562
  Jul 22 12:14:08.562: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 22 12:14:08.585: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/22/23 12:14:08.585
  Jul 22 12:14:08.591: INFO: Observed &Deployment event: ADDED
  Jul 22 12:14:08.591: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-p574b-5994cf9475"}
  Jul 22 12:14:08.592: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.592: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-p574b-5994cf9475"}
  Jul 22 12:14:08.592: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 22 12:14:08.593: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.594: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 22 12:14:08.595: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:06 +0000 UTC 2023-07-22 12:14:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-p574b-5994cf9475" is progressing.}
  Jul 22 12:14:08.596: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.597: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 22 12:14:08.597: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-p574b-5994cf9475" has successfully progressed.}
  Jul 22 12:14:08.598: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.599: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 22 12:14:08.599: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-22 12:14:07 +0000 UTC 2023-07-22 12:14:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-p574b-5994cf9475" has successfully progressed.}
  Jul 22 12:14:08.599: INFO: Observed deployment test-deployment-p574b in namespace deployment-3830 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 12:14:08.600: INFO: Observed &Deployment event: MODIFIED
  Jul 22 12:14:08.601: INFO: Found deployment test-deployment-p574b in namespace deployment-3830 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul 22 12:14:08.601: INFO: Deployment test-deployment-p574b has a patched status
  Jul 22 12:14:08.607: INFO: Deployment "test-deployment-p574b":
  &Deployment{ObjectMeta:{test-deployment-p574b  deployment-3830  9656c591-2eea-41df-be24-ea63eb77fe46 4644 1 2023-07-22 12:14:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-22 12:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-22 12:14:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-22 12:14:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee5f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-p574b-5994cf9475",LastUpdateTime:2023-07-22 12:14:08 +0000 UTC,LastTransitionTime:2023-07-22 12:14:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 22 12:14:08.614: INFO: New ReplicaSet "test-deployment-p574b-5994cf9475" of Deployment "test-deployment-p574b":
  &ReplicaSet{ObjectMeta:{test-deployment-p574b-5994cf9475  deployment-3830  cd7c3dc8-0997-4f5e-a27c-7683a2aee464 4622 1 2023-07-22 12:14:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-p574b 9656c591-2eea-41df-be24-ea63eb77fe46 0xc003b49427 0xc003b49428}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9656c591-2eea-41df-be24-ea63eb77fe46\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:14:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b494d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:14:08.621: INFO: Pod "test-deployment-p574b-5994cf9475-4bcgj" is available:
  &Pod{ObjectMeta:{test-deployment-p574b-5994cf9475-4bcgj test-deployment-p574b-5994cf9475- deployment-3830  12ca38df-db2b-4ad6-8f31-44960b71f03c 4621 0 2023-07-22 12:14:06 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-p574b-5994cf9475 cd7c3dc8-0997-4f5e-a27c-7683a2aee464 0xc003fce307 0xc003fce308}] [] [{kube-controller-manager Update v1 2023-07-22 12:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd7c3dc8-0997-4f5e-a27c-7683a2aee464\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 12:14:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4xz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4xz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:14:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:14:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:14:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:14:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.148,StartTime:2023-07-22 12:14:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 12:14:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://844b9337515d2092443fd12a5804e171a40d725a84fd000ceb40d304dd7bb870,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.148,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 12:14:08.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3830" for this suite. @ 07/22/23 12:14:08.629
• [2.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/22/23 12:14:08.653
  Jul 22 12:14:08.653: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:14:08.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:14:08.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:14:08.71
  STEP: Creating secret with name secret-test-1ce69539-7682-4f32-a632-1366ee9b3485 @ 07/22/23 12:14:08.714
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:14:08.723
  STEP: Saw pod success @ 07/22/23 12:14:12.762
  Jul 22 12:14:12.769: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-5c9863ba-34ee-458f-a181-c107381dccb3 container secret-env-test: <nil>
  STEP: delete the pod @ 07/22/23 12:14:12.781
  Jul 22 12:14:12.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7331" for this suite. @ 07/22/23 12:14:12.808
• [4.166 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/22/23 12:14:12.822
  Jul 22 12:14:12.822: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename cronjob @ 07/22/23 12:14:12.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:14:12.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:14:12.853
  STEP: Creating a ForbidConcurrent cronjob @ 07/22/23 12:14:12.858
  STEP: Ensuring a job is scheduled @ 07/22/23 12:14:12.865
  STEP: Ensuring exactly one is scheduled @ 07/22/23 12:15:00.874
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/22/23 12:15:00.882
  STEP: Ensuring no more jobs are scheduled @ 07/22/23 12:15:00.893
  STEP: Removing cronjob @ 07/22/23 12:20:00.904
  Jul 22 12:20:00.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6458" for this suite. @ 07/22/23 12:20:00.922
• [348.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/22/23 12:20:00.935
  Jul 22 12:20:00.935: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename cronjob @ 07/22/23 12:20:00.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:00.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:00.976
  STEP: Creating a cronjob @ 07/22/23 12:20:00.981
  STEP: creating @ 07/22/23 12:20:00.981
  STEP: getting @ 07/22/23 12:20:00.989
  STEP: listing @ 07/22/23 12:20:00.995
  STEP: watching @ 07/22/23 12:20:00.999
  Jul 22 12:20:01.000: INFO: starting watch
  STEP: cluster-wide listing @ 07/22/23 12:20:01.001
  STEP: cluster-wide watching @ 07/22/23 12:20:01.006
  Jul 22 12:20:01.006: INFO: starting watch
  STEP: patching @ 07/22/23 12:20:01.009
  STEP: updating @ 07/22/23 12:20:01.02
  Jul 22 12:20:01.035: INFO: waiting for watch events with expected annotations
  Jul 22 12:20:01.035: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/22/23 12:20:01.035
  STEP: updating /status @ 07/22/23 12:20:01.045
  STEP: get /status @ 07/22/23 12:20:01.057
  STEP: deleting @ 07/22/23 12:20:01.065
  STEP: deleting a collection @ 07/22/23 12:20:01.087
  Jul 22 12:20:01.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5539" for this suite. @ 07/22/23 12:20:01.118
• [0.197 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/22/23 12:20:01.132
  Jul 22 12:20:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:20:01.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:01.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:01.221
  STEP: Read namespace status @ 07/22/23 12:20:01.226
  Jul 22 12:20:01.239: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/22/23 12:20:01.239
  Jul 22 12:20:01.249: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/22/23 12:20:01.249
  Jul 22 12:20:01.265: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul 22 12:20:01.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8358" for this suite. @ 07/22/23 12:20:01.272
• [0.160 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/22/23 12:20:01.293
  Jul 22 12:20:01.293: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:20:01.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:01.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:01.337
  STEP: Creating a pod to test downward api env vars @ 07/22/23 12:20:01.345
  STEP: Saw pod success @ 07/22/23 12:20:05.393
  Jul 22 12:20:05.399: INFO: Trying to get logs from node ip-172-31-15-55 pod downward-api-7dd345aa-0460-4cdd-bb4d-3673e9eda25d container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 12:20:05.432
  Jul 22 12:20:05.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9606" for this suite. @ 07/22/23 12:20:05.46
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 07/22/23 12:20:05.475
  Jul 22 12:20:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 12:20:05.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:05.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:05.51
  STEP: Creating simple DaemonSet "daemon-set" @ 07/22/23 12:20:05.563
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 12:20:05.571
  Jul 22 12:20:05.576: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:05.576: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:05.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:20:05.581: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:20:06.588: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:06.588: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:06.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:20:06.595: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:20:07.588: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:07.589: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:07.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 22 12:20:07.594: INFO: Node ip-172-31-26-93 is running 0 daemon pod, expected 1
  Jul 22 12:20:08.590: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:08.590: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:08.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 22 12:20:08.597: INFO: Node ip-172-31-26-93 is running 0 daemon pod, expected 1
  Jul 22 12:20:09.588: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:09.588: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:09.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 12:20:09.596: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  Jul 22 12:20:10.589: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:10.589: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:10.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 12:20:10.593: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  Jul 22 12:20:11.588: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:11.588: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:20:11.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 12:20:11.595: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/22/23 12:20:11.601
  STEP: DeleteCollection of the DaemonSets @ 07/22/23 12:20:11.607
  STEP: Verify that ReplicaSets have been deleted @ 07/22/23 12:20:11.626
  Jul 22 12:20:11.648: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5579"},"items":null}

  Jul 22 12:20:11.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5583"},"items":[{"metadata":{"name":"daemon-set-2vvbz","generateName":"daemon-set-","namespace":"daemonsets-1771","uid":"19620f07-8589-457e-a2c7-1e121c1e698a","resourceVersion":"5580","creationTimestamp":"2023-07-22T12:20:05Z","deletionTimestamp":"2023-07-22T12:20:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1763c70a-bc1a-48be-9226-78eca9be0e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1763c70a-bc1a-48be-9226-78eca9be0e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zl727","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zl727","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-26-93","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-26-93"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"}],"hostIP":"172.31.26.93","podIP":"192.168.120.5","podIPs":[{"ip":"192.168.120.5"}],"startTime":"2023-07-22T12:20:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-22T12:20:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d945a92294d78c56ec20836bad0911add67cc2c168830ce142084faf6bff8dd2","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5z8hl","generateName":"daemon-set-","namespace":"daemonsets-1771","uid":"3fd9c371-74f3-432f-93ed-e20bea1b12f6","resourceVersion":"5579","creationTimestamp":"2023-07-22T12:20:05Z","deletionTimestamp":"2023-07-22T12:20:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1763c70a-bc1a-48be-9226-78eca9be0e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1763c70a-bc1a-48be-9226-78eca9be0e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.121.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lv65t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lv65t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-81-237","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-81-237"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"}],"hostIP":"172.31.81.237","podIP":"192.168.121.202","podIPs":[{"ip":"192.168.121.202"}],"startTime":"2023-07-22T12:20:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-22T12:20:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://3db05d38736c727b14516e138214e703e1f170fb1b080a23b704f84a451908be","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-b6xsw","generateName":"daemon-set-","namespace":"daemonsets-1771","uid":"abc1e818-f344-4dac-a7f7-ca860bf007c8","resourceVersion":"5581","creationTimestamp":"2023-07-22T12:20:05Z","deletionTimestamp":"2023-07-22T12:20:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1763c70a-bc1a-48be-9226-78eca9be0e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1763c70a-bc1a-48be-9226-78eca9be0e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-22T12:20:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z9fbn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z9fbn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-15-55","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-15-55"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-22T12:20:05Z"}],"hostIP":"172.31.15.55","podIP":"192.168.196.152","podIPs":[{"ip":"192.168.196.152"}],"startTime":"2023-07-22T12:20:05Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-22T12:20:06Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://943f5d56fee2c7e209be37cd332d2dbacc40f1e7347c1a9de4db49dde889c9aa","started":true}],"qosClass":"BestEffort"}}]}

  Jul 22 12:20:11.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1771" for this suite. @ 07/22/23 12:20:11.685
• [6.221 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/22/23 12:20:11.698
  Jul 22 12:20:11.698: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:20:11.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:11.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:11.729
  STEP: Updating Namespace "namespaces-5356" @ 07/22/23 12:20:11.734
  Jul 22 12:20:11.748: INFO: Namespace "namespaces-5356" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"c04793bf-29a9-4416-8432-0365d1fcd818", "kubernetes.io/metadata.name":"namespaces-5356", "namespaces-5356":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul 22 12:20:11.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5356" for this suite. @ 07/22/23 12:20:11.759
• [0.072 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/22/23 12:20:11.77
  Jul 22 12:20:11.770: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:20:11.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:11.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:11.801
  STEP: creating a Namespace @ 07/22/23 12:20:11.805
  STEP: patching the Namespace @ 07/22/23 12:20:11.833
  STEP: get the Namespace and ensuring it has the label @ 07/22/23 12:20:11.843
  Jul 22 12:20:11.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9870" for this suite. @ 07/22/23 12:20:11.929
  STEP: Destroying namespace "nspatchtest-1842558a-7b2b-4c2c-952f-8488d762f36c-5549" for this suite. @ 07/22/23 12:20:11.947
• [0.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/22/23 12:20:11.96
  Jul 22 12:20:11.960: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:20:11.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:11.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:11.988
  STEP: Creating secret with name secret-test-d814f5b9-83ae-40d9-89f4-9dae5dbbc64d @ 07/22/23 12:20:11.993
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:20:12.001
  STEP: Saw pod success @ 07/22/23 12:20:16.038
  Jul 22 12:20:16.043: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-b1855a48-607c-4e24-a491-6e30e191b02a container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:20:16.055
  Jul 22 12:20:16.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2941" for this suite. @ 07/22/23 12:20:16.086
• [4.139 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/22/23 12:20:16.1
  Jul 22 12:20:16.100: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 12:20:16.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:16.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:16.139
  STEP: create the rc @ 07/22/23 12:20:16.153
  W0722 12:20:16.161468      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/22/23 12:20:22.168
  STEP: wait for the rc to be deleted @ 07/22/23 12:20:22.182
  Jul 22 12:20:23.209: INFO: 80 pods remaining
  Jul 22 12:20:23.209: INFO: 80 pods has nil DeletionTimestamp
  Jul 22 12:20:23.209: INFO: 
  Jul 22 12:20:24.209: INFO: 71 pods remaining
  Jul 22 12:20:24.209: INFO: 71 pods has nil DeletionTimestamp
  Jul 22 12:20:24.209: INFO: 
  Jul 22 12:20:25.222: INFO: 60 pods remaining
  Jul 22 12:20:25.222: INFO: 59 pods has nil DeletionTimestamp
  Jul 22 12:20:25.223: INFO: 
  Jul 22 12:20:26.199: INFO: 40 pods remaining
  Jul 22 12:20:26.199: INFO: 40 pods has nil DeletionTimestamp
  Jul 22 12:20:26.199: INFO: 
  Jul 22 12:20:27.201: INFO: 32 pods remaining
  Jul 22 12:20:27.202: INFO: 32 pods has nil DeletionTimestamp
  Jul 22 12:20:27.202: INFO: 
  Jul 22 12:20:28.258: INFO: 19 pods remaining
  Jul 22 12:20:28.271: INFO: 19 pods has nil DeletionTimestamp
  Jul 22 12:20:28.271: INFO: 
  STEP: Gathering metrics @ 07/22/23 12:20:29.193
  W0722 12:20:29.199188      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 12:20:29.199: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 12:20:29.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3821" for this suite. @ 07/22/23 12:20:29.208
• [13.117 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/22/23 12:20:29.218
  Jul 22 12:20:29.218: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:20:29.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:29.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:29.266
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:20:29.272
  STEP: Saw pod success @ 07/22/23 12:20:39.333
  Jul 22 12:20:39.338: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-81a78b4a-1f8d-4e72-9d27-1af0f920315a container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:20:39.359
  Jul 22 12:20:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6743" for this suite. @ 07/22/23 12:20:39.394
• [10.195 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/22/23 12:20:39.415
  Jul 22 12:20:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:20:39.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:39.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:39.503
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:20:39.509
  STEP: Saw pod success @ 07/22/23 12:20:47.583
  Jul 22 12:20:47.589: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-6f9cd2eb-5afc-4a39-9ab1-194879f2c0b4 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:20:47.605
  Jul 22 12:20:47.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1323" for this suite. @ 07/22/23 12:20:47.646
• [8.242 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/22/23 12:20:47.657
  Jul 22 12:20:47.657: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:20:47.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:47.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:47.699
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/22/23 12:20:47.705
  Jul 22 12:20:47.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-8680 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 22 12:20:47.826: INFO: stderr: ""
  Jul 22 12:20:47.826: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/22/23 12:20:47.826
  Jul 22 12:20:47.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-8680 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul 22 12:20:47.957: INFO: stderr: ""
  Jul 22 12:20:47.957: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/22/23 12:20:47.958
  Jul 22 12:20:47.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-8680 delete pods e2e-test-httpd-pod'
  Jul 22 12:20:50.577: INFO: stderr: ""
  Jul 22 12:20:50.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 22 12:20:50.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8680" for this suite. @ 07/22/23 12:20:50.585
• [2.937 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/22/23 12:20:50.595
  Jul 22 12:20:50.595: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:20:50.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:50.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:50.637
  STEP: creating Agnhost RC @ 07/22/23 12:20:50.644
  Jul 22 12:20:50.644: INFO: namespace kubectl-2726
  Jul 22 12:20:50.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-2726 create -f -'
  Jul 22 12:20:51.118: INFO: stderr: ""
  Jul 22 12:20:51.118: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/22/23 12:20:51.118
  Jul 22 12:20:52.125: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 12:20:52.126: INFO: Found 0 / 1
  Jul 22 12:20:53.130: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 12:20:53.130: INFO: Found 1 / 1
  Jul 22 12:20:53.130: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 22 12:20:53.136: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 12:20:53.136: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 22 12:20:53.136: INFO: wait on agnhost-primary startup in kubectl-2726 
  Jul 22 12:20:53.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-2726 logs agnhost-primary-lkcg8 agnhost-primary'
  Jul 22 12:20:53.260: INFO: stderr: ""
  Jul 22 12:20:53.260: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/22/23 12:20:53.26
  Jul 22 12:20:53.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-2726 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul 22 12:20:53.379: INFO: stderr: ""
  Jul 22 12:20:53.380: INFO: stdout: "service/rm2 exposed\n"
  Jul 22 12:20:53.391: INFO: Service rm2 in namespace kubectl-2726 found.
  STEP: exposing service @ 07/22/23 12:20:55.402
  Jul 22 12:20:55.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-2726 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul 22 12:20:55.529: INFO: stderr: ""
  Jul 22 12:20:55.529: INFO: stdout: "service/rm3 exposed\n"
  Jul 22 12:20:55.536: INFO: Service rm3 in namespace kubectl-2726 found.
  Jul 22 12:20:57.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2726" for this suite. @ 07/22/23 12:20:57.558
• [6.974 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/22/23 12:20:57.57
  Jul 22 12:20:57.570: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:20:57.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:20:57.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:20:57.656
  STEP: Setting up server cert @ 07/22/23 12:20:57.687
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:20:57.983
  STEP: Deploying the webhook pod @ 07/22/23 12:20:57.997
  STEP: Wait for the deployment to be ready @ 07/22/23 12:20:58.017
  Jul 22 12:20:58.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/22/23 12:21:00.046
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:21:00.063
  Jul 22 12:21:01.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/22/23 12:21:01.072
  STEP: create a configmap that should be updated by the webhook @ 07/22/23 12:21:01.095
  Jul 22 12:21:01.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8768" for this suite. @ 07/22/23 12:21:01.192
  STEP: Destroying namespace "webhook-markers-7880" for this suite. @ 07/22/23 12:21:01.204
• [3.646 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/22/23 12:21:01.218
  Jul 22 12:21:01.218: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 12:21:01.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:01.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:01.255
  STEP: creating the pod @ 07/22/23 12:21:01.261
  STEP: setting up watch @ 07/22/23 12:21:01.262
  STEP: submitting the pod to kubernetes @ 07/22/23 12:21:01.369
  STEP: verifying the pod is in kubernetes @ 07/22/23 12:21:01.386
  STEP: verifying pod creation was observed @ 07/22/23 12:21:01.393
  STEP: deleting the pod gracefully @ 07/22/23 12:21:03.415
  STEP: verifying pod deletion was observed @ 07/22/23 12:21:03.433
  Jul 22 12:21:04.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3524" for this suite. @ 07/22/23 12:21:04.712
• [3.506 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/22/23 12:21:04.726
  Jul 22 12:21:04.726: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/22/23 12:21:04.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:04.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:04.754
  Jul 22 12:21:04.760: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:21:08.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2560" for this suite. @ 07/22/23 12:21:08.168
• [3.451 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/22/23 12:21:08.179
  Jul 22 12:21:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename proxy @ 07/22/23 12:21:08.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:08.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:08.214
  Jul 22 12:21:08.222: INFO: Creating pod...
  Jul 22 12:21:10.253: INFO: Creating service...
  Jul 22 12:21:10.268: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=DELETE
  Jul 22 12:21:10.281: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 22 12:21:10.281: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=OPTIONS
  Jul 22 12:21:10.298: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 22 12:21:10.298: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=PATCH
  Jul 22 12:21:10.314: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 22 12:21:10.314: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=POST
  Jul 22 12:21:10.322: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 22 12:21:10.323: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=PUT
  Jul 22 12:21:10.333: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 22 12:21:10.334: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul 22 12:21:10.349: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 22 12:21:10.349: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul 22 12:21:10.369: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 22 12:21:10.369: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul 22 12:21:10.377: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 22 12:21:10.377: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=POST
  Jul 22 12:21:10.388: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 22 12:21:10.388: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=PUT
  Jul 22 12:21:10.397: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 22 12:21:10.397: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=GET
  Jul 22 12:21:10.403: INFO: http.Client request:GET StatusCode:301
  Jul 22 12:21:10.403: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=GET
  Jul 22 12:21:10.411: INFO: http.Client request:GET StatusCode:301
  Jul 22 12:21:10.411: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/pods/agnhost/proxy?method=HEAD
  Jul 22 12:21:10.416: INFO: http.Client request:HEAD StatusCode:301
  Jul 22 12:21:10.416: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-9594/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul 22 12:21:10.424: INFO: http.Client request:HEAD StatusCode:301
  Jul 22 12:21:10.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9594" for this suite. @ 07/22/23 12:21:10.43
• [2.262 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/22/23 12:21:10.446
  Jul 22 12:21:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl-logs @ 07/22/23 12:21:10.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:10.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:10.485
  STEP: creating an pod @ 07/22/23 12:21:10.491
  Jul 22 12:21:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul 22 12:21:10.607: INFO: stderr: ""
  Jul 22 12:21:10.607: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/22/23 12:21:10.607
  Jul 22 12:21:10.607: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jul 22 12:21:12.629: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/22/23 12:21:12.629
  Jul 22 12:21:12.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator'
  Jul 22 12:21:12.740: INFO: stderr: ""
  Jul 22 12:21:12.740: INFO: stdout: "I0722 12:21:11.455662       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/v8qs 515\nI0722 12:21:11.655790       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/kcc 384\nI0722 12:21:11.856391       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/dwlt 213\nI0722 12:21:12.056756       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/r5hh 255\nI0722 12:21:12.256126       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/ff6 211\nI0722 12:21:12.456436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/p9f5 367\nI0722 12:21:12.655725       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vgc 481\n"
  STEP: limiting log lines @ 07/22/23 12:21:12.74
  Jul 22 12:21:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator --tail=1'
  Jul 22 12:21:12.852: INFO: stderr: ""
  Jul 22 12:21:12.852: INFO: stdout: "I0722 12:21:12.655725       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vgc 481\n"
  Jul 22 12:21:12.852: INFO: got output "I0722 12:21:12.655725       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vgc 481\n"
  STEP: limiting log bytes @ 07/22/23 12:21:12.852
  Jul 22 12:21:12.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator --limit-bytes=1'
  Jul 22 12:21:12.951: INFO: stderr: ""
  Jul 22 12:21:12.951: INFO: stdout: "I"
  Jul 22 12:21:12.951: INFO: got output "I"
  STEP: exposing timestamps @ 07/22/23 12:21:12.951
  Jul 22 12:21:12.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul 22 12:21:13.066: INFO: stderr: ""
  Jul 22 12:21:13.066: INFO: stdout: "2023-07-22T12:21:13.056592278Z I0722 12:21:13.056443       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dlv 299\n"
  Jul 22 12:21:13.066: INFO: got output "2023-07-22T12:21:13.056592278Z I0722 12:21:13.056443       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dlv 299\n"
  STEP: restricting to a time range @ 07/22/23 12:21:13.066
  Jul 22 12:21:15.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator --since=1s'
  Jul 22 12:21:15.667: INFO: stderr: ""
  Jul 22 12:21:15.667: INFO: stdout: "I0722 12:21:14.856459       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/ktv4 426\nI0722 12:21:15.055937       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/hgv 303\nI0722 12:21:15.256188       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/lkg6 477\nI0722 12:21:15.456695       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/f8kg 340\nI0722 12:21:15.656078       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/9lcb 326\n"
  Jul 22 12:21:15.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 logs logs-generator logs-generator --since=24h'
  Jul 22 12:21:15.795: INFO: stderr: ""
  Jul 22 12:21:15.795: INFO: stdout: "I0722 12:21:11.455662       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/v8qs 515\nI0722 12:21:11.655790       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/kcc 384\nI0722 12:21:11.856391       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/dwlt 213\nI0722 12:21:12.056756       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/r5hh 255\nI0722 12:21:12.256126       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/ff6 211\nI0722 12:21:12.456436       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/p9f5 367\nI0722 12:21:12.655725       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vgc 481\nI0722 12:21:12.856094       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/h5g 399\nI0722 12:21:13.056443       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dlv 299\nI0722 12:21:13.255734       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/ct74 347\nI0722 12:21:13.456122       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/rkh 332\nI0722 12:21:13.656445       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/74m 222\nI0722 12:21:13.855704       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/cpbr 428\nI0722 12:21:14.056036       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/xfp 361\nI0722 12:21:14.256391       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/kpss 237\nI0722 12:21:14.455780       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/qfrl 550\nI0722 12:21:14.656096       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4tk 595\nI0722 12:21:14.856459       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/ktv4 426\nI0722 12:21:15.055937       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/hgv 303\nI0722 12:21:15.256188       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/lkg6 477\nI0722 12:21:15.456695       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/f8kg 340\nI0722 12:21:15.656078       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/9lcb 326\n"
  Jul 22 12:21:15.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-logs-7085 delete pod logs-generator'
  Jul 22 12:21:16.705: INFO: stderr: ""
  Jul 22 12:21:16.705: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul 22 12:21:16.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7085" for this suite. @ 07/22/23 12:21:16.713
• [6.281 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/22/23 12:21:16.725
  Jul 22 12:21:16.725: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 12:21:16.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:16.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:16.767
  Jul 22 12:21:16.773: INFO: Creating ReplicaSet my-hostname-basic-b286524e-5019-476f-9ed5-fe03cd9201b4
  Jul 22 12:21:16.810: INFO: Pod name my-hostname-basic-b286524e-5019-476f-9ed5-fe03cd9201b4: Found 1 pods out of 1
  Jul 22 12:21:16.810: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b286524e-5019-476f-9ed5-fe03cd9201b4" is running
  Jul 22 12:21:18.833: INFO: Pod "my-hostname-basic-b286524e-5019-476f-9ed5-fe03cd9201b4-q4b2r" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-22 12:21:16 +0000 UTC Reason: Message:}])
  Jul 22 12:21:18.833: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/22/23 12:21:18.833
  Jul 22 12:21:18.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8045" for this suite. @ 07/22/23 12:21:18.859
• [2.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/22/23 12:21:18.875
  Jul 22 12:21:18.875: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:21:18.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:18.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:18.91
  STEP: creating service in namespace services-3824 @ 07/22/23 12:21:18.913
  STEP: creating service affinity-clusterip in namespace services-3824 @ 07/22/23 12:21:18.913
  STEP: creating replication controller affinity-clusterip in namespace services-3824 @ 07/22/23 12:21:18.932
  I0722 12:21:18.944662      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-3824, replica count: 3
  I0722 12:21:21.997385      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 12:21:22.008: INFO: Creating new exec pod
  Jul 22 12:21:25.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-3824 exec execpod-affinityz8msr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul 22 12:21:25.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul 22 12:21:25.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:21:25.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-3824 exec execpod-affinityz8msr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.65 80'
  Jul 22 12:21:25.503: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.65 80\nConnection to 10.152.183.65 80 port [tcp/http] succeeded!\n"
  Jul 22 12:21:25.503: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:21:25.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-3824 exec execpod-affinityz8msr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.65:80/ ; done'
  Jul 22 12:21:25.778: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.65:80/\n"
  Jul 22 12:21:25.778: INFO: stdout: "\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g\naffinity-clusterip-jvc5g"
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Received response from host: affinity-clusterip-jvc5g
  Jul 22 12:21:25.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:21:25.786: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-3824, will wait for the garbage collector to delete the pods @ 07/22/23 12:21:25.803
  Jul 22 12:21:25.870: INFO: Deleting ReplicationController affinity-clusterip took: 9.257013ms
  Jul 22 12:21:25.970: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.339084ms
  STEP: Destroying namespace "services-3824" for this suite. @ 07/22/23 12:21:28.2
• [9.335 seconds]
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/22/23 12:21:28.211
  Jul 22 12:21:28.211: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 12:21:28.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:28.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:28.256
  STEP: Creating ReplicationController "e2e-rc-lm5m5" @ 07/22/23 12:21:28.261
  Jul 22 12:21:28.271: INFO: Get Replication Controller "e2e-rc-lm5m5" to confirm replicas
  Jul 22 12:21:29.282: INFO: Get Replication Controller "e2e-rc-lm5m5" to confirm replicas
  Jul 22 12:21:29.290: INFO: Found 1 replicas for "e2e-rc-lm5m5" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-lm5m5" @ 07/22/23 12:21:29.29
  STEP: Updating a scale subresource @ 07/22/23 12:21:29.299
  STEP: Verifying replicas where modified for replication controller "e2e-rc-lm5m5" @ 07/22/23 12:21:29.31
  Jul 22 12:21:29.310: INFO: Get Replication Controller "e2e-rc-lm5m5" to confirm replicas
  Jul 22 12:21:30.321: INFO: Get Replication Controller "e2e-rc-lm5m5" to confirm replicas
  Jul 22 12:21:30.327: INFO: Found 2 replicas for "e2e-rc-lm5m5" replication controller
  Jul 22 12:21:30.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3528" for this suite. @ 07/22/23 12:21:30.34
• [2.139 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/22/23 12:21:30.352
  Jul 22 12:21:30.352: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:21:30.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:30.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:30.406
  STEP: Creating a test headless service @ 07/22/23 12:21:30.412
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4433.svc.cluster.local;sleep 1; done
   @ 07/22/23 12:21:30.421
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4433.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4433.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4433.svc.cluster.local;sleep 1; done
   @ 07/22/23 12:21:30.421
  STEP: creating a pod to probe DNS @ 07/22/23 12:21:30.421
  STEP: submitting the pod to kubernetes @ 07/22/23 12:21:30.421
  STEP: retrieving the pod @ 07/22/23 12:21:44.493
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:21:44.499
  Jul 22 12:21:44.505: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.512: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.521: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.533: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.545: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.553: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.561: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.570: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4433.svc.cluster.local from pod dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb: the server could not find the requested resource (get pods dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb)
  Jul 22 12:21:44.570: INFO: Lookups using dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4433.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4433.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4433.svc.cluster.local jessie_udp@dns-test-service-2.dns-4433.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4433.svc.cluster.local]

  Jul 22 12:21:49.632: INFO: DNS probes using dns-4433/dns-test-82f51075-8dd1-400d-9117-86bdb3a4cfcb succeeded

  Jul 22 12:21:49.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:21:49.638
  STEP: deleting the test headless service @ 07/22/23 12:21:49.656
  STEP: Destroying namespace "dns-4433" for this suite. @ 07/22/23 12:21:49.674
• [19.335 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/22/23 12:21:49.691
  Jul 22 12:21:49.691: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:21:49.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:49.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:49.732
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:21:49.738
  STEP: Saw pod success @ 07/22/23 12:21:57.792
  Jul 22 12:21:57.796: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-6dce48b6-73c9-47b0-b0b9-d4a15c6eb02c container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:21:57.813
  Jul 22 12:21:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2692" for this suite. @ 07/22/23 12:21:57.85
• [8.167 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/22/23 12:21:57.858
  Jul 22 12:21:57.858: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:21:57.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:21:57.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:21:57.888
  STEP: Counting existing ResourceQuota @ 07/22/23 12:21:57.895
  STEP: Creating a ResourceQuota @ 07/22/23 12:22:02.903
  STEP: Ensuring resource quota status is calculated @ 07/22/23 12:22:02.91
  STEP: Creating a ReplicaSet @ 07/22/23 12:22:04.924
  STEP: Ensuring resource quota status captures replicaset creation @ 07/22/23 12:22:04.951
  STEP: Deleting a ReplicaSet @ 07/22/23 12:22:06.972
  STEP: Ensuring resource quota status released usage @ 07/22/23 12:22:06.984
  Jul 22 12:22:08.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4166" for this suite. @ 07/22/23 12:22:09.001
• [11.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/22/23 12:22:09.022
  Jul 22 12:22:09.022: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:22:09.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:09.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:09.063
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/22/23 12:22:09.07
  Jul 22 12:22:09.082: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8159  7e536cd4-495b-4c5f-a70d-e8c65b966ead 8303 0 2023-07-22 12:22:09 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-22 12:22:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4thgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4thgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/22/23 12:22:11.098
  Jul 22 12:22:11.098: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8159 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:22:11.098: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:22:11.099: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:22:11.099: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-8159/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/22/23 12:22:11.2
  Jul 22 12:22:11.200: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8159 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:22:11.200: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:22:11.201: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:22:11.201: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-8159/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 22 12:22:11.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:22:11.300: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-8159" for this suite. @ 07/22/23 12:22:11.322
• [2.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/22/23 12:22:11.336
  Jul 22 12:22:11.336: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:22:11.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:11.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:11.371
  STEP: creating all guestbook components @ 07/22/23 12:22:11.376
  Jul 22 12:22:11.377: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul 22 12:22:11.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:11.895: INFO: stderr: ""
  Jul 22 12:22:11.895: INFO: stdout: "service/agnhost-replica created\n"
  Jul 22 12:22:11.895: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul 22 12:22:11.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:12.287: INFO: stderr: ""
  Jul 22 12:22:12.287: INFO: stdout: "service/agnhost-primary created\n"
  Jul 22 12:22:12.287: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul 22 12:22:12.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:12.912: INFO: stderr: ""
  Jul 22 12:22:12.912: INFO: stdout: "service/frontend created\n"
  Jul 22 12:22:12.912: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul 22 12:22:12.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:13.280: INFO: stderr: ""
  Jul 22 12:22:13.280: INFO: stdout: "deployment.apps/frontend created\n"
  Jul 22 12:22:13.280: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 22 12:22:13.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:13.674: INFO: stderr: ""
  Jul 22 12:22:13.674: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul 22 12:22:13.674: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 22 12:22:13.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 create -f -'
  Jul 22 12:22:14.251: INFO: stderr: ""
  Jul 22 12:22:14.251: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/22/23 12:22:14.252
  Jul 22 12:22:14.252: INFO: Waiting for all frontend pods to be Running.
  Jul 22 12:22:19.305: INFO: Waiting for frontend to serve content.
  Jul 22 12:22:19.325: INFO: Trying to add a new entry to the guestbook.
  Jul 22 12:22:19.341: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/22/23 12:22:19.365
  Jul 22 12:22:19.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:19.507: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:19.507: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/22/23 12:22:19.507
  Jul 22 12:22:19.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:19.676: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:19.676: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/22/23 12:22:19.677
  Jul 22 12:22:19.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:19.806: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:19.806: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/22/23 12:22:19.807
  Jul 22 12:22:19.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:19.911: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:19.911: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/22/23 12:22:19.911
  Jul 22 12:22:19.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:20.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:20.028: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/22/23 12:22:20.028
  Jul 22 12:22:20.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6457 delete --grace-period=0 --force -f -'
  Jul 22 12:22:20.169: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:22:20.169: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul 22 12:22:20.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6457" for this suite. @ 07/22/23 12:22:20.175
• [8.847 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/22/23 12:22:20.184
  Jul 22 12:22:20.184: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 12:22:20.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:20.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:20.216
  STEP: Creating a pod to test env composition @ 07/22/23 12:22:20.222
  STEP: Saw pod success @ 07/22/23 12:22:24.261
  Jul 22 12:22:24.271: INFO: Trying to get logs from node ip-172-31-15-55 pod var-expansion-4a5d3d9e-88c1-40b5-ab64-2c0f41fdd6ec container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 12:22:24.286
  Jul 22 12:22:24.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2079" for this suite. @ 07/22/23 12:22:24.335
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/22/23 12:22:24.355
  Jul 22 12:22:24.355: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 12:22:24.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:24.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:24.392
  STEP: create the rc1 @ 07/22/23 12:22:24.413
  STEP: create the rc2 @ 07/22/23 12:22:24.425
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/22/23 12:22:30.447
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/22/23 12:22:31.411
  STEP: wait for the rc to be deleted @ 07/22/23 12:22:31.423
  Jul 22 12:22:36.448: INFO: 66 pods remaining
  Jul 22 12:22:36.448: INFO: 66 pods has nil DeletionTimestamp
  Jul 22 12:22:36.448: INFO: 
  STEP: Gathering metrics @ 07/22/23 12:22:41.442
  W0722 12:22:41.450050      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 12:22:41.450: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 12:22:41.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-24pmc" in namespace "gc-8603"
  Jul 22 12:22:41.465: INFO: Deleting pod "simpletest-rc-to-be-deleted-27gl5" in namespace "gc-8603"
  Jul 22 12:22:41.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b79p" in namespace "gc-8603"
  Jul 22 12:22:41.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c5cq" in namespace "gc-8603"
  Jul 22 12:22:41.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-2frds" in namespace "gc-8603"
  Jul 22 12:22:41.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-42fhf" in namespace "gc-8603"
  Jul 22 12:22:41.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k8sd" in namespace "gc-8603"
  Jul 22 12:22:41.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v2nm" in namespace "gc-8603"
  Jul 22 12:22:41.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v7mf" in namespace "gc-8603"
  Jul 22 12:22:41.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mbbt" in namespace "gc-8603"
  Jul 22 12:22:41.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r9kw" in namespace "gc-8603"
  Jul 22 12:22:41.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zn27" in namespace "gc-8603"
  Jul 22 12:22:41.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zw2j" in namespace "gc-8603"
  Jul 22 12:22:41.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-69dsz" in namespace "gc-8603"
  Jul 22 12:22:41.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fhfb" in namespace "gc-8603"
  Jul 22 12:22:41.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qlxq" in namespace "gc-8603"
  Jul 22 12:22:41.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h8h2" in namespace "gc-8603"
  Jul 22 12:22:41.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jsn5" in namespace "gc-8603"
  Jul 22 12:22:41.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xkcd" in namespace "gc-8603"
  Jul 22 12:22:41.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zbr7" in namespace "gc-8603"
  Jul 22 12:22:42.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dbvx" in namespace "gc-8603"
  Jul 22 12:22:42.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qwlz" in namespace "gc-8603"
  Jul 22 12:22:42.054: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vbdh" in namespace "gc-8603"
  Jul 22 12:22:42.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bczk" in namespace "gc-8603"
  Jul 22 12:22:42.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bp5l" in namespace "gc-8603"
  Jul 22 12:22:42.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ts8r" in namespace "gc-8603"
  Jul 22 12:22:42.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4bwx" in namespace "gc-8603"
  Jul 22 12:22:42.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6tp2" in namespace "gc-8603"
  Jul 22 12:22:42.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6vw6" in namespace "gc-8603"
  Jul 22 12:22:42.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-b87tg" in namespace "gc-8603"
  Jul 22 12:22:42.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc6ls" in namespace "gc-8603"
  Jul 22 12:22:42.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs6sf" in namespace "gc-8603"
  Jul 22 12:22:42.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz4cx" in namespace "gc-8603"
  Jul 22 12:22:42.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw8hj" in namespace "gc-8603"
  Jul 22 12:22:42.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcth6" in namespace "gc-8603"
  Jul 22 12:22:42.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpzx7" in namespace "gc-8603"
  Jul 22 12:22:42.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz6ht" in namespace "gc-8603"
  Jul 22 12:22:42.388: INFO: Deleting pod "simpletest-rc-to-be-deleted-f42sh" in namespace "gc-8603"
  Jul 22 12:22:42.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-f457b" in namespace "gc-8603"
  Jul 22 12:22:42.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7zh2" in namespace "gc-8603"
  Jul 22 12:22:42.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj8gm" in namespace "gc-8603"
  Jul 22 12:22:42.465: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwlw2" in namespace "gc-8603"
  Jul 22 12:22:42.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-htqwd" in namespace "gc-8603"
  Jul 22 12:22:42.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhm4c" in namespace "gc-8603"
  Jul 22 12:22:42.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk2ws" in namespace "gc-8603"
  Jul 22 12:22:42.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnnfh" in namespace "gc-8603"
  Jul 22 12:22:42.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnrxq" in namespace "gc-8603"
  Jul 22 12:22:42.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-js7nb" in namespace "gc-8603"
  Jul 22 12:22:42.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-kh5ch" in namespace "gc-8603"
  Jul 22 12:22:42.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjf7x" in namespace "gc-8603"
  Jul 22 12:22:42.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8603" for this suite. @ 07/22/23 12:22:42.627
• [18.284 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/22/23 12:22:42.641
  Jul 22 12:22:42.641: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:22:42.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:42.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:42.697
  Jul 22 12:22:42.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9874" for this suite. @ 07/22/23 12:22:42.774
• [0.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/22/23 12:22:42.8
  Jul 22 12:22:42.800: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-pred @ 07/22/23 12:22:42.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:42.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:42.834
  Jul 22 12:22:42.843: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 22 12:22:42.871: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 12:22:42.879: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-55 before test
  Jul 22 12:22:42.894: INFO: default-http-backend-kubernetes-worker-65fc475d49-slc26 from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:29 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.894: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 22 12:22:42.894: INFO: nginx-ingress-controller-kubernetes-worker-m59lf from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:31 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.894: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:22:42.895: INFO: calico-kube-controllers-79b76dbbcc-9vwj9 from kube-system started at 2023-07-22 11:54:47 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.895: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 22 12:22:42.895: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:22:42.895: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:22:42.895: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 12:22:42.895: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-26-93 before test
  Jul 22 12:22:42.907: INFO: nginx-ingress-controller-kubernetes-worker-9f8fn from ingress-nginx-kubernetes-worker started at 2023-07-22 12:03:49 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.907: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:22:42.907: INFO: sonobuoy from sonobuoy started at 2023-07-22 12:04:59 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.907: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 22 12:22:42.907: INFO: sonobuoy-e2e-job-09ff55b4a2944177 from sonobuoy started at 2023-07-22 12:05:02 +0000 UTC (2 container statuses recorded)
  Jul 22 12:22:42.907: INFO: 	Container e2e ready: true, restart count 0
  Jul 22 12:22:42.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:22:42.907: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-l8p6k from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:22:42.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:22:42.907: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 12:22:42.908: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-81-237 before test
  Jul 22 12:22:42.917: INFO: nginx-ingress-controller-kubernetes-worker-r6kjp from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:28 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.917: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:22:42.917: INFO: coredns-5c7f76ccb8-cvjf4 from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.917: INFO: 	Container coredns ready: true, restart count 0
  Jul 22 12:22:42.917: INFO: kube-state-metrics-5b95b4459c-pp6nn from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.917: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 22 12:22:42.917: INFO: metrics-server-v0.5.2-6cf8c8b69c-lv5pw from kube-system started at 2023-07-22 11:54:21 +0000 UTC (2 container statuses recorded)
  Jul 22 12:22:42.917: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 22 12:22:42.918: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 22 12:22:42.918: INFO: dashboard-metrics-scraper-6b8586b5c9-29rq6 from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.918: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 22 12:22:42.918: INFO: kubernetes-dashboard-6869f4cd5f-5xvbw from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:22:42.918: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 22 12:22:42.918: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-s457l from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:22:42.918: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:22:42.918: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-15-55 @ 07/22/23 12:22:42.944
  STEP: verifying the node has the label node ip-172-31-26-93 @ 07/22/23 12:22:42.961
  STEP: verifying the node has the label node ip-172-31-81-237 @ 07/22/23 12:22:42.981
  Jul 22 12:22:43.009: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-slc26 requesting resource cpu=10m on Node ip-172-31-15-55
  Jul 22 12:22:43.009: INFO: Pod nginx-ingress-controller-kubernetes-worker-9f8fn requesting resource cpu=0m on Node ip-172-31-26-93
  Jul 22 12:22:43.009: INFO: Pod nginx-ingress-controller-kubernetes-worker-m59lf requesting resource cpu=0m on Node ip-172-31-15-55
  Jul 22 12:22:43.009: INFO: Pod nginx-ingress-controller-kubernetes-worker-r6kjp requesting resource cpu=0m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod calico-kube-controllers-79b76dbbcc-9vwj9 requesting resource cpu=0m on Node ip-172-31-15-55
  Jul 22 12:22:43.010: INFO: Pod coredns-5c7f76ccb8-cvjf4 requesting resource cpu=100m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod kube-state-metrics-5b95b4459c-pp6nn requesting resource cpu=0m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-lv5pw requesting resource cpu=5m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-29rq6 requesting resource cpu=0m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod kubernetes-dashboard-6869f4cd5f-5xvbw requesting resource cpu=0m on Node ip-172-31-81-237
  Jul 22 12:22:43.010: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-26-93
  Jul 22 12:22:43.010: INFO: Pod sonobuoy-e2e-job-09ff55b4a2944177 requesting resource cpu=0m on Node ip-172-31-26-93
  Jul 22 12:22:43.010: INFO: Pod sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-l8p6k requesting resource cpu=0m on Node ip-172-31-26-93
  Jul 22 12:22:43.010: INFO: Pod sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-s457l requesting resource cpu=0m on Node ip-172-31-81-237
  Jul 22 12:22:43.011: INFO: Pod sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j requesting resource cpu=0m on Node ip-172-31-15-55
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/22/23 12:22:43.011
  Jul 22 12:22:43.011: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-15-55
  Jul 22 12:22:43.023: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-26-93
  Jul 22 12:22:43.036: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-81-237
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/22/23 12:22:53.123
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-439f3939-498a-48dd-811e-36bae0ca0732.177430062024d9d7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8955/filler-pod-439f3939-498a-48dd-811e-36bae0ca0732 to ip-172-31-15-55] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-439f3939-498a-48dd-811e-36bae0ca0732.1774300792d3229a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-439f3939-498a-48dd-811e-36bae0ca0732.1774300795918389], Reason = [Created], Message = [Created container filler-pod-439f3939-498a-48dd-811e-36bae0ca0732] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-439f3939-498a-48dd-811e-36bae0ca0732.17743007c12eef33], Reason = [Started], Message = [Started container filler-pod-439f3939-498a-48dd-811e-36bae0ca0732] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5.1774300622267ab8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8955/filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5 to ip-172-31-81-237] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5.177430077b8317d3], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5.17743007a9c75f7a], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 776.198907ms (776.207497ms including waiting)] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5.17743007ad45ce95], Reason = [Created], Message = [Created container filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5.17743007eb47ad60], Reason = [Started], Message = [Started container filler-pod-78b3982e-cfe5-48ef-9d82-dd73a043b0d5] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca.177430062096b9d8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8955/filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca to ip-172-31-26-93] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca.1774300785e95bf0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca.177430078b528350], Reason = [Created], Message = [Created container filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca.17743007b96721b3], Reason = [Started], Message = [Started container filler-pod-fc4eb43f-b70e-491f-b021-96b238f917ca] @ 07/22/23 12:22:53.129
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177430087a4f4bb8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 07/22/23 12:22:53.154
  STEP: removing the label node off the node ip-172-31-15-55 @ 07/22/23 12:22:54.148
  STEP: verifying the node doesn't have the label node @ 07/22/23 12:22:54.165
  STEP: removing the label node off the node ip-172-31-26-93 @ 07/22/23 12:22:54.169
  STEP: verifying the node doesn't have the label node @ 07/22/23 12:22:54.189
  STEP: removing the label node off the node ip-172-31-81-237 @ 07/22/23 12:22:54.196
  STEP: verifying the node doesn't have the label node @ 07/22/23 12:22:54.217
  Jul 22 12:22:54.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8955" for this suite. @ 07/22/23 12:22:54.233
• [11.442 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/22/23 12:22:54.248
  Jul 22 12:22:54.248: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 12:22:54.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:22:54.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:22:54.293
  STEP: Creating a job @ 07/22/23 12:22:54.297
  STEP: Ensuring active pods == parallelism @ 07/22/23 12:22:54.309
  STEP: Orphaning one of the Job's Pods @ 07/22/23 12:22:56.319
  Jul 22 12:22:56.846: INFO: Successfully updated pod "adopt-release-r6rl5"
  STEP: Checking that the Job readopts the Pod @ 07/22/23 12:22:56.846
  STEP: Removing the labels from the Job's Pod @ 07/22/23 12:22:58.858
  Jul 22 12:22:59.376: INFO: Successfully updated pod "adopt-release-r6rl5"
  STEP: Checking that the Job releases the Pod @ 07/22/23 12:22:59.376
  Jul 22 12:23:01.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5079" for this suite. @ 07/22/23 12:23:01.392
• [7.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/22/23 12:23:01.403
  Jul 22 12:23:01.403: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 12:23:01.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:01.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:01.437
  Jul 22 12:23:01.469: INFO: created pod pod-service-account-defaultsa
  Jul 22 12:23:01.469: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul 22 12:23:01.480: INFO: created pod pod-service-account-mountsa
  Jul 22 12:23:01.480: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul 22 12:23:01.493: INFO: created pod pod-service-account-nomountsa
  Jul 22 12:23:01.494: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul 22 12:23:01.506: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul 22 12:23:01.506: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul 22 12:23:01.515: INFO: created pod pod-service-account-mountsa-mountspec
  Jul 22 12:23:01.516: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul 22 12:23:01.525: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul 22 12:23:01.525: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul 22 12:23:01.537: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul 22 12:23:01.537: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul 22 12:23:01.547: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul 22 12:23:01.547: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul 22 12:23:01.560: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul 22 12:23:01.560: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul 22 12:23:01.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3389" for this suite. @ 07/22/23 12:23:01.571
• [0.182 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/22/23 12:23:01.586
  Jul 22 12:23:01.586: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:23:01.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:01.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:01.62
  STEP: Creating a ResourceQuota @ 07/22/23 12:23:01.627
  STEP: Getting a ResourceQuota @ 07/22/23 12:23:01.635
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/22/23 12:23:01.651
  STEP: Patching the ResourceQuota @ 07/22/23 12:23:01.658
  STEP: Deleting a Collection of ResourceQuotas @ 07/22/23 12:23:01.668
  STEP: Verifying the deleted ResourceQuota @ 07/22/23 12:23:01.688
  Jul 22 12:23:01.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-336" for this suite. @ 07/22/23 12:23:01.706
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/22/23 12:23:01.724
  Jul 22 12:23:01.724: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption @ 07/22/23 12:23:01.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:01.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:01.765
  STEP: Creating a kubernetes client @ 07/22/23 12:23:01.77
  Jul 22 12:23:01.770: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption-2 @ 07/22/23 12:23:01.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:01.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:01.81
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:23:01.824
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:23:03.846
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:23:05.867
  STEP: listing a collection of PDBs across all namespaces @ 07/22/23 12:23:07.882
  STEP: listing a collection of PDBs in namespace disruption-926 @ 07/22/23 12:23:07.888
  STEP: deleting a collection of PDBs @ 07/22/23 12:23:07.904
  STEP: Waiting for the PDB collection to be deleted @ 07/22/23 12:23:07.924
  Jul 22 12:23:07.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:23:07.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5783" for this suite. @ 07/22/23 12:23:07.948
  STEP: Destroying namespace "disruption-926" for this suite. @ 07/22/23 12:23:07.96
• [6.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/22/23 12:23:07.983
  Jul 22 12:23:07.983: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:23:07.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:08.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:08.016
  STEP: Counting existing ResourceQuota @ 07/22/23 12:23:08.022
  STEP: Creating a ResourceQuota @ 07/22/23 12:23:13.025
  STEP: Ensuring resource quota status is calculated @ 07/22/23 12:23:13.035
  STEP: Creating a ReplicationController @ 07/22/23 12:23:15.041
  STEP: Ensuring resource quota status captures replication controller creation @ 07/22/23 12:23:15.058
  STEP: Deleting a ReplicationController @ 07/22/23 12:23:17.064
  STEP: Ensuring resource quota status released usage @ 07/22/23 12:23:17.076
  Jul 22 12:23:19.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-629" for this suite. @ 07/22/23 12:23:19.089
• [11.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/22/23 12:23:19.115
  Jul 22 12:23:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:23:19.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:19.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:19.167
  STEP: Setting up server cert @ 07/22/23 12:23:19.27
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:23:19.991
  STEP: Deploying the webhook pod @ 07/22/23 12:23:20.019
  STEP: Wait for the deployment to be ready @ 07/22/23 12:23:20.037
  Jul 22 12:23:20.047: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/22/23 12:23:22.068
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:23:22.079
  Jul 22 12:23:23.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/22/23 12:23:23.231
  STEP: Creating a configMap that should be mutated @ 07/22/23 12:23:23.273
  STEP: Deleting the collection of validation webhooks @ 07/22/23 12:23:23.332
  STEP: Creating a configMap that should not be mutated @ 07/22/23 12:23:23.415
  Jul 22 12:23:23.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6572" for this suite. @ 07/22/23 12:23:23.535
  STEP: Destroying namespace "webhook-markers-8492" for this suite. @ 07/22/23 12:23:23.549
• [4.444 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/22/23 12:23:23.559
  Jul 22 12:23:23.559: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/22/23 12:23:23.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:23.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:23.615
  STEP: creating @ 07/22/23 12:23:23.623
  STEP: getting @ 07/22/23 12:23:23.659
  STEP: listing @ 07/22/23 12:23:23.671
  STEP: deleting @ 07/22/23 12:23:23.677
  Jul 22 12:23:23.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-523" for this suite. @ 07/22/23 12:23:23.719
• [0.169 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/22/23 12:23:23.73
  Jul 22 12:23:23.730: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 12:23:23.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:23.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:23.782
  STEP: create the rc @ 07/22/23 12:23:23.789
  W0722 12:23:23.804587      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/22/23 12:23:28.815
  STEP: wait for all pods to be garbage collected @ 07/22/23 12:23:28.827
  STEP: Gathering metrics @ 07/22/23 12:23:33.84
  W0722 12:23:33.848067      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 12:23:33.848: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 12:23:33.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8757" for this suite. @ 07/22/23 12:23:33.854
• [10.134 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/22/23 12:23:33.866
  Jul 22 12:23:33.866: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 12:23:33.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:33.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:33.922
  STEP: apply creating a deployment @ 07/22/23 12:23:33.927
  Jul 22 12:23:33.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-679" for this suite. @ 07/22/23 12:23:33.958
• [0.105 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/22/23 12:23:33.972
  Jul 22 12:23:33.972: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:23:33.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:34.004
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:23:34.009
  STEP: Saw pod success @ 07/22/23 12:23:38.045
  Jul 22 12:23:38.050: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-15427019-b20d-41ff-a851-2a4cacbd3136 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:23:38.06
  Jul 22 12:23:38.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2175" for this suite. @ 07/22/23 12:23:38.086
• [4.123 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/22/23 12:23:38.097
  Jul 22 12:23:38.097: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:23:38.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:38.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:38.126
  STEP: creating service in namespace services-598 @ 07/22/23 12:23:38.132
  STEP: creating service affinity-clusterip-transition in namespace services-598 @ 07/22/23 12:23:38.133
  STEP: creating replication controller affinity-clusterip-transition in namespace services-598 @ 07/22/23 12:23:38.149
  I0722 12:23:38.175054      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-598, replica count: 3
  I0722 12:23:41.226147      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 12:23:41.239: INFO: Creating new exec pod
  Jul 22 12:23:44.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-598 exec execpod-affinitym92ld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul 22 12:23:44.484: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul 22 12:23:44.485: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:23:44.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-598 exec execpod-affinitym92ld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.231 80'
  Jul 22 12:23:44.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.231 80\nConnection to 10.152.183.231 80 port [tcp/http] succeeded!\n"
  Jul 22 12:23:44.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:23:44.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-598 exec execpod-affinitym92ld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.231:80/ ; done'
  Jul 22 12:23:45.023: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n"
  Jul 22 12:23:45.023: INFO: stdout: "\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-nmt6j\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-nmt6j\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-nmt6j\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-nmt6j\naffinity-clusterip-transition-7v2jt\naffinity-clusterip-transition-nmt6j"
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-nmt6j
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-nmt6j
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-nmt6j
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-nmt6j
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-7v2jt
  Jul 22 12:23:45.023: INFO: Received response from host: affinity-clusterip-transition-nmt6j
  Jul 22 12:23:45.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-598 exec execpod-affinitym92ld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.231:80/ ; done'
  Jul 22 12:23:45.328: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.231:80/\n"
  Jul 22 12:23:45.328: INFO: stdout: "\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd\naffinity-clusterip-transition-hsdmd"
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Received response from host: affinity-clusterip-transition-hsdmd
  Jul 22 12:23:45.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:23:45.334: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-598, will wait for the garbage collector to delete the pods @ 07/22/23 12:23:45.354
  Jul 22 12:23:45.424: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.689426ms
  Jul 22 12:23:45.525: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.713223ms
  STEP: Destroying namespace "services-598" for this suite. @ 07/22/23 12:23:48.067
• [9.995 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/22/23 12:23:48.101
  Jul 22 12:23:48.101: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 12:23:48.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:23:48.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:23:48.136
  STEP: Creating pod liveness-1063f489-acf1-410c-909f-7f78db18d6ce in namespace container-probe-2083 @ 07/22/23 12:23:48.14
  Jul 22 12:23:50.166: INFO: Started pod liveness-1063f489-acf1-410c-909f-7f78db18d6ce in namespace container-probe-2083
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 12:23:50.166
  Jul 22 12:23:50.170: INFO: Initial restart count of pod liveness-1063f489-acf1-410c-909f-7f78db18d6ce is 0
  Jul 22 12:27:51.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:27:51.011
  STEP: Destroying namespace "container-probe-2083" for this suite. @ 07/22/23 12:27:51.033
• [242.945 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/22/23 12:27:51.048
  Jul 22 12:27:51.048: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 12:27:51.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:27:51.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:27:51.082
  Jul 22 12:27:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  W0722 12:27:53.801478      19 warnings.go:70] unknown field "alpha"
  W0722 12:27:53.801620      19 warnings.go:70] unknown field "beta"
  W0722 12:27:53.801705      19 warnings.go:70] unknown field "delta"
  W0722 12:27:53.801780      19 warnings.go:70] unknown field "epsilon"
  W0722 12:27:53.801838      19 warnings.go:70] unknown field "gamma"
  Jul 22 12:27:54.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-521" for this suite. @ 07/22/23 12:27:54.408
• [3.371 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/22/23 12:27:54.42
  Jul 22 12:27:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subpath @ 07/22/23 12:27:54.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:27:54.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:27:54.463
  STEP: Setting up data @ 07/22/23 12:27:54.471
  STEP: Creating pod pod-subpath-test-secret-w8r8 @ 07/22/23 12:27:54.488
  STEP: Creating a pod to test atomic-volume-subpath @ 07/22/23 12:27:54.488
  STEP: Saw pod success @ 07/22/23 12:28:18.588
  Jul 22 12:28:18.601: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-subpath-test-secret-w8r8 container test-container-subpath-secret-w8r8: <nil>
  STEP: delete the pod @ 07/22/23 12:28:18.63
  STEP: Deleting pod pod-subpath-test-secret-w8r8 @ 07/22/23 12:28:18.653
  Jul 22 12:28:18.653: INFO: Deleting pod "pod-subpath-test-secret-w8r8" in namespace "subpath-2539"
  Jul 22 12:28:18.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2539" for this suite. @ 07/22/23 12:28:18.667
• [24.258 seconds]
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/22/23 12:28:18.678
  Jul 22 12:28:18.678: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:28:18.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:28:18.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:28:18.71
  STEP: Creating secret with name s-test-opt-del-21a5088f-b740-42aa-9fd8-7f50427bd197 @ 07/22/23 12:28:18.723
  STEP: Creating secret with name s-test-opt-upd-161cf491-b5f9-490d-986a-787ce04dc873 @ 07/22/23 12:28:18.731
  STEP: Creating the pod @ 07/22/23 12:28:18.737
  STEP: Deleting secret s-test-opt-del-21a5088f-b740-42aa-9fd8-7f50427bd197 @ 07/22/23 12:28:20.806
  STEP: Updating secret s-test-opt-upd-161cf491-b5f9-490d-986a-787ce04dc873 @ 07/22/23 12:28:20.815
  STEP: Creating secret with name s-test-opt-create-ea6c5171-5953-4e91-838a-7215a575c1cc @ 07/22/23 12:28:20.823
  STEP: waiting to observe update in volume @ 07/22/23 12:28:20.83
  Jul 22 12:29:33.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7624" for this suite. @ 07/22/23 12:29:33.351
• [74.683 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/22/23 12:29:33.362
  Jul 22 12:29:33.362: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:29:33.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:33.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:33.4
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:29:33.405
  STEP: Saw pod success @ 07/22/23 12:29:37.437
  Jul 22 12:29:37.448: INFO: Trying to get logs from node ip-172-31-26-93 pod downwardapi-volume-92fa92d8-bdb3-47aa-ad05-344212eb35c9 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:29:37.473
  Jul 22 12:29:37.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7773" for this suite. @ 07/22/23 12:29:37.5
• [4.147 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/22/23 12:29:37.512
  Jul 22 12:29:37.512: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/22/23 12:29:37.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:37.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:37.549
  STEP: mirroring a new custom Endpoint @ 07/22/23 12:29:37.573
  Jul 22 12:29:37.588: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 07/22/23 12:29:39.594
  STEP: mirroring deletion of a custom Endpoint @ 07/22/23 12:29:39.618
  Jul 22 12:29:39.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7795" for this suite. @ 07/22/23 12:29:39.65
• [2.150 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/22/23 12:29:39.661
  Jul 22 12:29:39.661: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:29:39.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:39.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:39.697
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/22/23 12:29:39.702
  STEP: Saw pod success @ 07/22/23 12:29:43.737
  Jul 22 12:29:43.743: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-8c81fd14-744c-49fd-a005-aa98ce51501b container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:29:43.756
  Jul 22 12:29:43.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5613" for this suite. @ 07/22/23 12:29:43.788
• [4.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/22/23 12:29:43.808
  Jul 22 12:29:43.808: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption @ 07/22/23 12:29:43.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:43.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:43.847
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/22/23 12:29:43.852
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:29:43.861
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/22/23 12:29:45.878
  STEP: Waiting for all pods to be running @ 07/22/23 12:29:45.878
  Jul 22 12:29:45.884: INFO: pods: 0 < 3
  Jul 22 12:29:47.891: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 07/22/23 12:29:49.893
  STEP: Updating the pdb to allow a pod to be evicted @ 07/22/23 12:29:49.908
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:29:49.92
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/22/23 12:29:51.938
  STEP: Waiting for all pods to be running @ 07/22/23 12:29:51.938
  STEP: Waiting for the pdb to observed all healthy pods @ 07/22/23 12:29:51.946
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/22/23 12:29:51.992
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:29:52.026
  STEP: Waiting for all pods to be running @ 07/22/23 12:29:54.038
  STEP: locating a running pod @ 07/22/23 12:29:54.047
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/22/23 12:29:54.063
  STEP: Waiting for the pdb to be deleted @ 07/22/23 12:29:54.074
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/22/23 12:29:54.079
  STEP: Waiting for all pods to be running @ 07/22/23 12:29:54.079
  Jul 22 12:29:54.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5569" for this suite. @ 07/22/23 12:29:54.116
• [10.319 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/22/23 12:29:54.129
  Jul 22 12:29:54.129: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 12:29:54.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:54.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:54.167
  Jul 22 12:29:54.173: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/22/23 12:29:55.783
  Jul 22 12:29:55.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-2057 --namespace=crd-publish-openapi-2057 create -f -'
  Jul 22 12:29:56.717: INFO: stderr: ""
  Jul 22 12:29:56.717: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 22 12:29:56.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-2057 --namespace=crd-publish-openapi-2057 delete e2e-test-crd-publish-openapi-3470-crds test-cr'
  Jul 22 12:29:56.819: INFO: stderr: ""
  Jul 22 12:29:56.819: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul 22 12:29:56.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-2057 --namespace=crd-publish-openapi-2057 apply -f -'
  Jul 22 12:29:57.631: INFO: stderr: ""
  Jul 22 12:29:57.631: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 22 12:29:57.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-2057 --namespace=crd-publish-openapi-2057 delete e2e-test-crd-publish-openapi-3470-crds test-cr'
  Jul 22 12:29:57.731: INFO: stderr: ""
  Jul 22 12:29:57.731: INFO: stdout: "e2e-test-crd-publish-openapi-3470-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/22/23 12:29:57.731
  Jul 22 12:29:57.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-2057 explain e2e-test-crd-publish-openapi-3470-crds'
  Jul 22 12:29:57.968: INFO: stderr: ""
  Jul 22 12:29:57.968: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-3470-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 22 12:29:59.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2057" for this suite. @ 07/22/23 12:29:59.502
• [5.384 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/22/23 12:29:59.514
  Jul 22 12:29:59.514: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:29:59.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:29:59.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:29:59.552
  STEP: Creating configMap configmap-1585/configmap-test-a938378a-8ca6-4352-9e09-40ccce964f58 @ 07/22/23 12:29:59.556
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:29:59.565
  STEP: Saw pod success @ 07/22/23 12:30:03.609
  Jul 22 12:30:03.619: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-b0860632-3c3d-4883-a0f0-8a89b5ec64f9 container env-test: <nil>
  STEP: delete the pod @ 07/22/23 12:30:03.636
  Jul 22 12:30:03.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1585" for this suite. @ 07/22/23 12:30:03.679
• [4.179 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/22/23 12:30:03.693
  Jul 22 12:30:03.693: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/22/23 12:30:03.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:03.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:03.745
  Jul 22 12:30:03.750: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:30:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9077" for this suite. @ 07/22/23 12:30:10.17
• [6.488 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/22/23 12:30:10.184
  Jul 22 12:30:10.184: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 12:30:10.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:10.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:10.218
  STEP: Creating a suspended job @ 07/22/23 12:30:10.23
  STEP: Patching the Job @ 07/22/23 12:30:10.238
  STEP: Watching for Job to be patched @ 07/22/23 12:30:10.261
  Jul 22 12:30:10.264: INFO: Event ADDED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 22 12:30:10.264: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 22 12:30:10.264: INFO: Event MODIFIED found for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/22/23 12:30:10.264
  STEP: Watching for Job to be updated @ 07/22/23 12:30:10.28
  Jul 22 12:30:10.282: INFO: Event MODIFIED found for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:10.282: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/22/23 12:30:10.282
  Jul 22 12:30:10.289: INFO: Job: e2e-w9jzj as labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched]
  STEP: Waiting for job to complete @ 07/22/23 12:30:10.289
  STEP: Delete a job collection with a labelselector @ 07/22/23 12:30:20.296
  STEP: Watching for Job to be deleted @ 07/22/23 12:30:20.321
  Jul 22 12:30:20.325: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:20.325: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:20.326: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:20.326: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:20.326: INFO: Event MODIFIED observed for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 22 12:30:20.326: INFO: Event DELETED found for Job e2e-w9jzj in namespace job-9178 with labels: map[e2e-job-label:e2e-w9jzj e2e-w9jzj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/22/23 12:30:20.326
  Jul 22 12:30:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9178" for this suite. @ 07/22/23 12:30:20.343
• [10.236 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/22/23 12:30:20.422
  Jul 22 12:30:20.422: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:30:20.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:20.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:20.456
  STEP: Counting existing ResourceQuota @ 07/22/23 12:30:20.461
  STEP: Creating a ResourceQuota @ 07/22/23 12:30:25.472
  STEP: Ensuring resource quota status is calculated @ 07/22/23 12:30:25.481
  Jul 22 12:30:27.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4840" for this suite. @ 07/22/23 12:30:27.499
• [7.096 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/22/23 12:30:27.518
  Jul 22 12:30:27.518: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 12:30:27.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:27.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:27.57
  STEP: Saw pod success @ 07/22/23 12:30:33.703
  Jul 22 12:30:33.708: INFO: Trying to get logs from node ip-172-31-15-55 pod client-envvars-69bfd124-978f-471a-a3f4-81fed70704f3 container env3cont: <nil>
  STEP: delete the pod @ 07/22/23 12:30:33.719
  Jul 22 12:30:33.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9528" for this suite. @ 07/22/23 12:30:33.771
• [6.263 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/22/23 12:30:33.783
  Jul 22 12:30:33.783: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename conformance-tests @ 07/22/23 12:30:33.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:33.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:33.815
  STEP: Getting node addresses @ 07/22/23 12:30:33.82
  Jul 22 12:30:33.820: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul 22 12:30:33.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-490" for this suite. @ 07/22/23 12:30:33.839
• [0.069 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/22/23 12:30:33.855
  Jul 22 12:30:33.855: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:30:33.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:30:33.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:30:33.889
  STEP: Creating configMap with name cm-test-opt-del-e945ef8b-1f09-4245-b3ac-675495c03cb6 @ 07/22/23 12:30:33.907
  STEP: Creating configMap with name cm-test-opt-upd-83328fa6-6fc9-4d2f-9630-32bf994bce84 @ 07/22/23 12:30:33.914
  STEP: Creating the pod @ 07/22/23 12:30:33.923
  STEP: Deleting configmap cm-test-opt-del-e945ef8b-1f09-4245-b3ac-675495c03cb6 @ 07/22/23 12:30:38.007
  STEP: Updating configmap cm-test-opt-upd-83328fa6-6fc9-4d2f-9630-32bf994bce84 @ 07/22/23 12:30:38.017
  STEP: Creating configMap with name cm-test-opt-create-ac88763d-b7ac-4159-baa1-b9bddd534202 @ 07/22/23 12:30:38.027
  STEP: waiting to observe update in volume @ 07/22/23 12:30:38.035
  Jul 22 12:31:46.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7944" for this suite. @ 07/22/23 12:31:46.555
• [72.712 seconds]
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/22/23 12:31:46.566
  Jul 22 12:31:46.566: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:31:46.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:31:46.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:31:46.61
  STEP: creating a secret @ 07/22/23 12:31:46.615
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/22/23 12:31:46.625
  STEP: patching the secret @ 07/22/23 12:31:46.63
  STEP: deleting the secret using a LabelSelector @ 07/22/23 12:31:46.646
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/22/23 12:31:46.663
  Jul 22 12:31:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6132" for this suite. @ 07/22/23 12:31:46.676
• [0.121 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/22/23 12:31:46.688
  Jul 22 12:31:46.689: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:31:46.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:31:46.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:31:46.719
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8401 @ 07/22/23 12:31:46.724
  STEP: changing the ExternalName service to type=NodePort @ 07/22/23 12:31:46.733
  STEP: creating replication controller externalname-service in namespace services-8401 @ 07/22/23 12:31:46.765
  I0722 12:31:46.779142      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8401, replica count: 2
  I0722 12:31:49.830444      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 12:31:49.830: INFO: Creating new exec pod
  Jul 22 12:31:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8401 exec execpodtbprr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 22 12:31:53.061: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 22 12:31:53.061: INFO: stdout: ""
  Jul 22 12:31:54.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8401 exec execpodtbprr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 22 12:31:54.278: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 22 12:31:54.278: INFO: stdout: "externalname-service-dk4lj"
  Jul 22 12:31:54.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8401 exec execpodtbprr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.181 80'
  Jul 22 12:31:54.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.181 80\nConnection to 10.152.183.181 80 port [tcp/http] succeeded!\n"
  Jul 22 12:31:54.497: INFO: stdout: "externalname-service-dk4lj"
  Jul 22 12:31:54.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8401 exec execpodtbprr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.55 32549'
  Jul 22 12:31:54.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.55 32549\nConnection to 172.31.15.55 32549 port [tcp/*] succeeded!\n"
  Jul 22 12:31:54.767: INFO: stdout: "externalname-service-7qr2l"
  Jul 22 12:31:54.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8401 exec execpodtbprr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.93 32549'
  Jul 22 12:31:54.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.26.93 32549\nConnection to 172.31.26.93 32549 port [tcp/*] succeeded!\n"
  Jul 22 12:31:54.962: INFO: stdout: "externalname-service-dk4lj"
  Jul 22 12:31:54.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:31:54.970: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-8401" for this suite. @ 07/22/23 12:31:55.014
• [8.338 seconds]
------------------------------
SSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/22/23 12:31:55.027
  Jul 22 12:31:55.027: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:31:55.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:31:55.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:31:55.057
  STEP: Creating configMap configmap-4645/configmap-test-e22b1696-a341-4195-9293-430fe209099a @ 07/22/23 12:31:55.062
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:31:55.071
  STEP: Saw pod success @ 07/22/23 12:31:59.102
  Jul 22 12:31:59.107: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-a37fb7e5-9eb5-4af0-ad9c-42e275f9a543 container env-test: <nil>
  STEP: delete the pod @ 07/22/23 12:31:59.117
  Jul 22 12:31:59.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4645" for this suite. @ 07/22/23 12:31:59.15
• [4.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/22/23 12:31:59.165
  Jul 22 12:31:59.165: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 12:31:59.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:31:59.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:31:59.201
  STEP: creating the pod with failed condition @ 07/22/23 12:31:59.204
  STEP: updating the pod @ 07/22/23 12:33:59.217
  Jul 22 12:33:59.733: INFO: Successfully updated pod "var-expansion-2ffb7986-700a-441b-9a8a-852f3b908dd4"
  STEP: waiting for pod running @ 07/22/23 12:33:59.733
  STEP: deleting the pod gracefully @ 07/22/23 12:34:01.751
  Jul 22 12:34:01.751: INFO: Deleting pod "var-expansion-2ffb7986-700a-441b-9a8a-852f3b908dd4" in namespace "var-expansion-9524"
  Jul 22 12:34:01.767: INFO: Wait up to 5m0s for pod "var-expansion-2ffb7986-700a-441b-9a8a-852f3b908dd4" to be fully deleted
  Jul 22 12:34:33.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9524" for this suite. @ 07/22/23 12:34:33.893
• [154.743 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/22/23 12:34:33.91
  Jul 22 12:34:33.911: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:34:33.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:34:33.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:34:33.943
  STEP: Creating a test externalName service @ 07/22/23 12:34:33.95
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:33.958
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:33.958
  STEP: creating a pod to probe DNS @ 07/22/23 12:34:33.958
  STEP: submitting the pod to kubernetes @ 07/22/23 12:34:33.958
  STEP: retrieving the pod @ 07/22/23 12:34:35.995
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:34:36
  Jul 22 12:34:36.018: INFO: DNS probes using dns-test-58272f6f-19ce-4de0-86b7-53f7776df4d0 succeeded

  STEP: changing the externalName to bar.example.com @ 07/22/23 12:34:36.018
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:36.035
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:36.035
  STEP: creating a second pod to probe DNS @ 07/22/23 12:34:36.035
  STEP: submitting the pod to kubernetes @ 07/22/23 12:34:36.035
  STEP: retrieving the pod @ 07/22/23 12:34:46.097
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:34:46.104
  Jul 22 12:34:46.115: INFO: DNS probes using dns-test-90568de2-155d-41e4-8d40-3ffce4591788 succeeded

  STEP: changing the service to type=ClusterIP @ 07/22/23 12:34:46.115
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:46.134
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4010.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4010.svc.cluster.local; sleep 1; done
   @ 07/22/23 12:34:46.134
  STEP: creating a third pod to probe DNS @ 07/22/23 12:34:46.134
  STEP: submitting the pod to kubernetes @ 07/22/23 12:34:46.143
  STEP: retrieving the pod @ 07/22/23 12:34:48.185
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:34:48.193
  Jul 22 12:34:48.213: INFO: DNS probes using dns-test-721259c9-7aa7-4645-af4c-3bce3459998f succeeded

  Jul 22 12:34:48.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:34:48.22
  STEP: deleting the pod @ 07/22/23 12:34:48.245
  STEP: deleting the pod @ 07/22/23 12:34:48.264
  STEP: deleting the test externalName service @ 07/22/23 12:34:48.281
  STEP: Destroying namespace "dns-4010" for this suite. @ 07/22/23 12:34:48.316
• [14.420 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/22/23 12:34:48.333
  Jul 22 12:34:48.333: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 12:34:48.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:34:48.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:34:48.371
  STEP: reading a file in the container @ 07/22/23 12:34:50.416
  Jul 22 12:34:50.416: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9133 pod-service-account-eca37897-9e1a-49c0-b611-6d60ccfb1f24 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 07/22/23 12:34:50.609
  Jul 22 12:34:50.609: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9133 pod-service-account-eca37897-9e1a-49c0-b611-6d60ccfb1f24 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/22/23 12:34:50.806
  Jul 22 12:34:50.806: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9133 pod-service-account-eca37897-9e1a-49c0-b611-6d60ccfb1f24 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul 22 12:34:51.001: INFO: Got root ca configmap in namespace "svcaccounts-9133"
  Jul 22 12:34:51.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9133" for this suite. @ 07/22/23 12:34:51.011
• [2.690 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/22/23 12:34:51.023
  Jul 22 12:34:51.023: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename ingressclass @ 07/22/23 12:34:51.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:34:51.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:34:51.054
  STEP: getting /apis @ 07/22/23 12:34:51.058
  STEP: getting /apis/networking.k8s.io @ 07/22/23 12:34:51.065
  STEP: getting /apis/networking.k8s.iov1 @ 07/22/23 12:34:51.066
  STEP: creating @ 07/22/23 12:34:51.068
  STEP: getting @ 07/22/23 12:34:51.092
  STEP: listing @ 07/22/23 12:34:51.096
  STEP: watching @ 07/22/23 12:34:51.105
  Jul 22 12:34:51.105: INFO: starting watch
  STEP: patching @ 07/22/23 12:34:51.107
  STEP: updating @ 07/22/23 12:34:51.116
  Jul 22 12:34:51.122: INFO: waiting for watch events with expected annotations
  Jul 22 12:34:51.122: INFO: saw patched and updated annotations
  STEP: deleting @ 07/22/23 12:34:51.122
  STEP: deleting a collection @ 07/22/23 12:34:51.143
  Jul 22 12:34:51.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3444" for this suite. @ 07/22/23 12:34:51.178
• [0.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/22/23 12:34:51.19
  Jul 22 12:34:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 12:34:51.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:34:51.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:34:51.22
  Jul 22 12:34:53.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:34:53.270: INFO: Deleting pod "var-expansion-410384a0-6ae4-4668-b28d-fe1ed1efdab9" in namespace "var-expansion-7404"
  Jul 22 12:34:53.284: INFO: Wait up to 5m0s for pod "var-expansion-410384a0-6ae4-4668-b28d-fe1ed1efdab9" to be fully deleted
  STEP: Destroying namespace "var-expansion-7404" for this suite. @ 07/22/23 12:34:55.306
• [4.128 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/22/23 12:34:55.318
  Jul 22 12:34:55.318: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:34:55.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:34:55.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:34:55.348
  STEP: Setting up server cert @ 07/22/23 12:34:55.398
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:34:56.53
  STEP: Deploying the webhook pod @ 07/22/23 12:34:56.544
  STEP: Wait for the deployment to be ready @ 07/22/23 12:34:56.566
  Jul 22 12:34:56.576: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Jul 22 12:34:58.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 12, 34, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 34, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 34, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 34, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/22/23 12:35:00.601
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:35:00.62
  Jul 22 12:35:01.621: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/22/23 12:35:01.626
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/22/23 12:35:01.658
  STEP: Creating a dummy validating-webhook-configuration object @ 07/22/23 12:35:01.688
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/22/23 12:35:01.711
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/22/23 12:35:01.725
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/22/23 12:35:01.741
  Jul 22 12:35:01.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9491" for this suite. @ 07/22/23 12:35:01.857
  STEP: Destroying namespace "webhook-markers-6705" for this suite. @ 07/22/23 12:35:01.871
• [6.564 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/22/23 12:35:01.882
  Jul 22 12:35:01.882: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-runtime @ 07/22/23 12:35:01.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:01.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:01.922
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/22/23 12:35:01.946
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/22/23 12:35:19.079
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/22/23 12:35:19.084
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/22/23 12:35:19.094
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/22/23 12:35:19.094
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/22/23 12:35:19.123
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/22/23 12:35:22.147
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/22/23 12:35:23.159
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/22/23 12:35:23.17
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/22/23 12:35:23.17
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/22/23 12:35:23.213
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/22/23 12:35:24.228
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/22/23 12:35:26.248
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/22/23 12:35:26.26
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/22/23 12:35:26.26
  Jul 22 12:35:26.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6806" for this suite. @ 07/22/23 12:35:26.328
• [24.455 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/22/23 12:35:26.338
  Jul 22 12:35:26.338: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 12:35:26.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:26.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:26.395
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/22/23 12:35:26.404
  STEP: When a replication controller with a matching selector is created @ 07/22/23 12:35:28.444
  STEP: Then the orphan pod is adopted @ 07/22/23 12:35:28.455
  Jul 22 12:35:29.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7320" for this suite. @ 07/22/23 12:35:29.48
• [3.156 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/22/23 12:35:29.494
  Jul 22 12:35:29.494: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:35:29.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:29.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:29.532
  STEP: Creating configMap with name configmap-test-volume-f65f3b70-c5e6-4fc3-be36-26bd519951b7 @ 07/22/23 12:35:29.544
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:35:29.555
  STEP: Saw pod success @ 07/22/23 12:35:33.596
  Jul 22 12:35:33.604: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-6af4dc8d-8d86-4bbb-85a3-ff935abd31bd container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:35:33.641
  Jul 22 12:35:33.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1877" for this suite. @ 07/22/23 12:35:33.676
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/22/23 12:35:33.692
  Jul 22 12:35:33.692: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:35:33.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:33.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:33.74
  STEP: Creating configMap with name configmap-test-upd-7c69f589-df83-400c-9a87-019639e99a64 @ 07/22/23 12:35:33.755
  STEP: Creating the pod @ 07/22/23 12:35:33.764
  STEP: Waiting for pod with text data @ 07/22/23 12:35:35.794
  STEP: Waiting for pod with binary data @ 07/22/23 12:35:35.804
  Jul 22 12:35:35.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9081" for this suite. @ 07/22/23 12:35:35.821
• [2.140 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/22/23 12:35:35.833
  Jul 22 12:35:35.833: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:35:35.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:35.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:35.865
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:35:35.869
  STEP: Saw pod success @ 07/22/23 12:35:39.912
  Jul 22 12:35:39.918: INFO: Trying to get logs from node ip-172-31-26-93 pod downwardapi-volume-9bd0c058-b7ca-48b3-a1ef-d819f247de3f container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:35:39.95
  Jul 22 12:35:39.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7980" for this suite. @ 07/22/23 12:35:39.991
• [4.172 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/22/23 12:35:40.007
  Jul 22 12:35:40.007: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subpath @ 07/22/23 12:35:40.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:35:40.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:35:40.06
  STEP: Setting up data @ 07/22/23 12:35:40.069
  STEP: Creating pod pod-subpath-test-downwardapi-pxzd @ 07/22/23 12:35:40.093
  STEP: Creating a pod to test atomic-volume-subpath @ 07/22/23 12:35:40.093
  STEP: Saw pod success @ 07/22/23 12:36:04.218
  Jul 22 12:36:04.223: INFO: Trying to get logs from node ip-172-31-26-93 pod pod-subpath-test-downwardapi-pxzd container test-container-subpath-downwardapi-pxzd: <nil>
  STEP: delete the pod @ 07/22/23 12:36:04.234
  STEP: Deleting pod pod-subpath-test-downwardapi-pxzd @ 07/22/23 12:36:04.258
  Jul 22 12:36:04.258: INFO: Deleting pod "pod-subpath-test-downwardapi-pxzd" in namespace "subpath-8692"
  Jul 22 12:36:04.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8692" for this suite. @ 07/22/23 12:36:04.277
• [24.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 07/22/23 12:36:04.289
  Jul 22 12:36:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 12:36:04.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:04.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:04.325
  Jul 22 12:36:04.364: INFO: Create a RollingUpdate DaemonSet
  Jul 22 12:36:04.372: INFO: Check that daemon pods launch on every node of the cluster
  Jul 22 12:36:04.378: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:04.378: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:04.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:36:04.384: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:36:05.390: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:05.390: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:05.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:36:05.397: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:36:06.391: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:06.391: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:06.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 12:36:06.396: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul 22 12:36:06.396: INFO: Update the DaemonSet to trigger a rollout
  Jul 22 12:36:06.411: INFO: Updating DaemonSet daemon-set
  Jul 22 12:36:07.440: INFO: Roll back the DaemonSet before rollout is complete
  Jul 22 12:36:07.454: INFO: Updating DaemonSet daemon-set
  Jul 22 12:36:07.454: INFO: Make sure DaemonSet rollback is complete
  Jul 22 12:36:07.460: INFO: Wrong image for pod: daemon-set-n2l6w. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul 22 12:36:07.460: INFO: Pod daemon-set-n2l6w is not available
  Jul 22 12:36:07.465: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:07.465: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:08.478: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:08.479: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:09.482: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:09.483: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:10.472: INFO: Pod daemon-set-cnlfb is not available
  Jul 22 12:36:10.477: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:36:10.478: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 12:36:10.489
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9504, will wait for the garbage collector to delete the pods @ 07/22/23 12:36:10.489
  Jul 22 12:36:10.559: INFO: Deleting DaemonSet.extensions daemon-set took: 14.725703ms
  Jul 22 12:36:10.659: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.746553ms
  Jul 22 12:36:12.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:36:12.067: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 12:36:12.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15127"},"items":null}

  Jul 22 12:36:12.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15127"},"items":null}

  Jul 22 12:36:12.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9504" for this suite. @ 07/22/23 12:36:12.106
• [7.827 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/22/23 12:36:12.117
  Jul 22 12:36:12.117: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubelet-test @ 07/22/23 12:36:12.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:12.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:12.144
  Jul 22 12:36:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4451" for this suite. @ 07/22/23 12:36:14.199
• [2.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/22/23 12:36:14.217
  Jul 22 12:36:14.217: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:36:14.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:14.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:14.3
  STEP: creating secret secrets-9805/secret-test-82e22f96-e22a-4ee7-9d0f-2ad2e7467fb4 @ 07/22/23 12:36:14.31
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:36:14.333
  STEP: Saw pod success @ 07/22/23 12:36:18.384
  Jul 22 12:36:18.390: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-8bcc2c23-584e-425f-9ff3-8653e3c05266 container env-test: <nil>
  STEP: delete the pod @ 07/22/23 12:36:18.402
  Jul 22 12:36:18.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9805" for this suite. @ 07/22/23 12:36:18.432
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/22/23 12:36:18.447
  Jul 22 12:36:18.447: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename watch @ 07/22/23 12:36:18.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:18.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:18.479
  STEP: creating a watch on configmaps with label A @ 07/22/23 12:36:18.484
  STEP: creating a watch on configmaps with label B @ 07/22/23 12:36:18.485
  STEP: creating a watch on configmaps with label A or B @ 07/22/23 12:36:18.486
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/22/23 12:36:18.488
  Jul 22 12:36:18.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15227 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:18.497: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15227 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/22/23 12:36:18.497
  Jul 22 12:36:18.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15228 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:18.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15228 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/22/23 12:36:18.515
  Jul 22 12:36:18.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15229 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:18.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15229 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/22/23 12:36:18.531
  Jul 22 12:36:18.543: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15230 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:18.544: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9754  14e06cfd-323a-42be-8451-d0f531767b47 15230 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/22/23 12:36:18.544
  Jul 22 12:36:18.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9754  d7b1f1b4-cecb-4a7c-8556-4be62e91af29 15231 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:18.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9754  d7b1f1b4-cecb-4a7c-8556-4be62e91af29 15231 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/22/23 12:36:28.553
  Jul 22 12:36:28.566: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9754  d7b1f1b4-cecb-4a7c-8556-4be62e91af29 15271 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:28.566: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9754  d7b1f1b4-cecb-4a7c-8556-4be62e91af29 15271 0 2023-07-22 12:36:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-22 12:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:36:38.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9754" for this suite. @ 07/22/23 12:36:38.58
• [20.143 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/22/23 12:36:38.591
  Jul 22 12:36:38.591: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:36:38.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:38.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:38.627
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:36:38.632
  STEP: Saw pod success @ 07/22/23 12:36:42.665
  Jul 22 12:36:42.670: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-0100d485-80ba-4cae-873c-c21b3dc1c5db container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:36:42.686
  Jul 22 12:36:42.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9249" for this suite. @ 07/22/23 12:36:42.722
• [4.141 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/22/23 12:36:42.735
  Jul 22 12:36:42.735: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 12:36:42.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:42.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:42.767
  STEP: creating a Pod with a static label @ 07/22/23 12:36:42.781
  STEP: watching for Pod to be ready @ 07/22/23 12:36:42.794
  Jul 22 12:36:42.796: INFO: observed Pod pod-test in namespace pods-7432 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul 22 12:36:42.798: INFO: observed Pod pod-test in namespace pods-7432 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC  }]
  Jul 22 12:36:42.819: INFO: observed Pod pod-test in namespace pods-7432 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC  }]
  Jul 22 12:36:43.824: INFO: Found Pod pod-test in namespace pods-7432 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 12:36:42 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/22/23 12:36:43.834
  STEP: getting the Pod and ensuring that it's patched @ 07/22/23 12:36:43.849
  STEP: replacing the Pod's status Ready condition to False @ 07/22/23 12:36:43.856
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/22/23 12:36:43.875
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/22/23 12:36:43.875
  STEP: watching for the Pod to be deleted @ 07/22/23 12:36:43.96
  Jul 22 12:36:43.964: INFO: observed event type MODIFIED
  Jul 22 12:36:45.836: INFO: observed event type MODIFIED
  Jul 22 12:36:46.263: INFO: observed event type MODIFIED
  Jul 22 12:36:46.850: INFO: observed event type MODIFIED
  Jul 22 12:36:46.872: INFO: observed event type MODIFIED
  Jul 22 12:36:46.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7432" for this suite. @ 07/22/23 12:36:46.894
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/22/23 12:36:46.91
  Jul 22 12:36:46.910: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/22/23 12:36:46.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:46.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:46.946
  STEP: getting /apis @ 07/22/23 12:36:46.952
  STEP: getting /apis/storage.k8s.io @ 07/22/23 12:36:46.958
  STEP: getting /apis/storage.k8s.io/v1 @ 07/22/23 12:36:46.96
  STEP: creating @ 07/22/23 12:36:46.963
  STEP: watching @ 07/22/23 12:36:46.989
  Jul 22 12:36:46.989: INFO: starting watch
  STEP: getting @ 07/22/23 12:36:47.002
  STEP: listing in namespace @ 07/22/23 12:36:47.014
  STEP: listing across namespaces @ 07/22/23 12:36:47.025
  STEP: patching @ 07/22/23 12:36:47.032
  STEP: updating @ 07/22/23 12:36:47.04
  Jul 22 12:36:47.052: INFO: waiting for watch events with expected annotations in namespace
  Jul 22 12:36:47.052: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/22/23 12:36:47.053
  STEP: deleting a collection @ 07/22/23 12:36:47.075
  Jul 22 12:36:47.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5378" for this suite. @ 07/22/23 12:36:47.108
• [0.212 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/22/23 12:36:47.122
  Jul 22 12:36:47.122: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename podtemplate @ 07/22/23 12:36:47.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:47.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:47.158
  Jul 22 12:36:47.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7110" for this suite. @ 07/22/23 12:36:47.222
• [0.110 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/22/23 12:36:47.235
  Jul 22 12:36:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-webhook @ 07/22/23 12:36:47.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:47.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:47.27
  STEP: Setting up server cert @ 07/22/23 12:36:47.275
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/22/23 12:36:48.007
  STEP: Deploying the custom resource conversion webhook pod @ 07/22/23 12:36:48.022
  STEP: Wait for the deployment to be ready @ 07/22/23 12:36:48.04
  Jul 22 12:36:48.051: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  Jul 22 12:36:50.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 12, 36, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 36, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 36, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 36, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/22/23 12:36:52.079
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:36:52.098
  Jul 22 12:36:53.098: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 22 12:36:53.104: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Creating a v1 custom resource @ 07/22/23 12:36:55.72
  STEP: v2 custom resource should be converted @ 07/22/23 12:36:55.727
  Jul 22 12:36:55.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6092" for this suite. @ 07/22/23 12:36:56.345
• [9.122 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/22/23 12:36:56.358
  Jul 22 12:36:56.358: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context-test @ 07/22/23 12:36:56.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:36:56.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:36:56.399
  Jul 22 12:37:00.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1848" for this suite. @ 07/22/23 12:37:00.445
• [4.098 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/22/23 12:37:00.457
  Jul 22 12:37:00.457: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 12:37:00.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:37:00.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:37:00.497
  STEP: Creating pod liveness-6f928938-d782-4ab6-b1cf-3259658a25cc in namespace container-probe-350 @ 07/22/23 12:37:00.503
  Jul 22 12:37:02.533: INFO: Started pod liveness-6f928938-d782-4ab6-b1cf-3259658a25cc in namespace container-probe-350
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 12:37:02.533
  Jul 22 12:37:02.540: INFO: Initial restart count of pod liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is 0
  Jul 22 12:37:22.617: INFO: Restart count of pod container-probe-350/liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is now 1 (20.07701772s elapsed)
  Jul 22 12:37:42.688: INFO: Restart count of pod container-probe-350/liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is now 2 (40.147778479s elapsed)
  Jul 22 12:38:02.760: INFO: Restart count of pod container-probe-350/liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is now 3 (1m0.219976195s elapsed)
  Jul 22 12:38:22.825: INFO: Restart count of pod container-probe-350/liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is now 4 (1m20.284760853s elapsed)
  Jul 22 12:39:35.089: INFO: Restart count of pod container-probe-350/liveness-6f928938-d782-4ab6-b1cf-3259658a25cc is now 5 (2m32.549057712s elapsed)
  Jul 22 12:39:35.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:39:35.1
  STEP: Destroying namespace "container-probe-350" for this suite. @ 07/22/23 12:39:35.122
• [154.678 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/22/23 12:39:35.141
  Jul 22 12:39:35.141: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:39:35.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:39:35.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:39:35.19
  STEP: Counting existing ResourceQuota @ 07/22/23 12:39:35.2
  STEP: Creating a ResourceQuota @ 07/22/23 12:39:40.205
  STEP: Ensuring resource quota status is calculated @ 07/22/23 12:39:40.213
  STEP: Creating a Pod that fits quota @ 07/22/23 12:39:42.221
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/22/23 12:39:42.243
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/22/23 12:39:44.251
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/22/23 12:39:44.254
  STEP: Ensuring a pod cannot update its resource requirements @ 07/22/23 12:39:44.258
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/22/23 12:39:44.265
  STEP: Deleting the pod @ 07/22/23 12:39:46.277
  STEP: Ensuring resource quota status released the pod usage @ 07/22/23 12:39:46.292
  Jul 22 12:39:48.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3610" for this suite. @ 07/22/23 12:39:48.306
• [13.177 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 07/22/23 12:39:48.319
  Jul 22 12:39:48.319: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 12:39:48.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:39:48.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:39:48.353
  STEP: Creating simple DaemonSet "daemon-set" @ 07/22/23 12:39:48.399
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 12:39:48.407
  Jul 22 12:39:48.412: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:48.413: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:48.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:39:48.421: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:39:49.430: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:49.430: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:49.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:39:49.439: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  Jul 22 12:39:50.430: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:50.430: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:39:50.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 12:39:50.436: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/22/23 12:39:50.442
  Jul 22 12:39:50.447: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/22/23 12:39:50.447
  Jul 22 12:39:50.462: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/22/23 12:39:50.462
  Jul 22 12:39:50.471: INFO: Observed &DaemonSet event: ADDED
  Jul 22 12:39:50.471: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.471: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.472: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.472: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.472: INFO: Found daemon set daemon-set in namespace daemonsets-8461 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 22 12:39:50.472: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/22/23 12:39:50.473
  STEP: watching for the daemon set status to be patched @ 07/22/23 12:39:50.487
  Jul 22 12:39:50.491: INFO: Observed &DaemonSet event: ADDED
  Jul 22 12:39:50.491: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.491: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.492: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.492: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.492: INFO: Observed daemon set daemon-set in namespace daemonsets-8461 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 22 12:39:50.493: INFO: Observed &DaemonSet event: MODIFIED
  Jul 22 12:39:50.493: INFO: Found daemon set daemon-set in namespace daemonsets-8461 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul 22 12:39:50.493: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 12:39:50.501
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods @ 07/22/23 12:39:50.501
  Jul 22 12:39:50.567: INFO: Deleting DaemonSet.extensions daemon-set took: 9.016009ms
  Jul 22 12:39:50.668: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.599192ms
  Jul 22 12:39:53.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 12:39:53.374: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 12:39:53.381: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16059"},"items":null}

  Jul 22 12:39:53.386: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16059"},"items":null}

  Jul 22 12:39:53.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8461" for this suite. @ 07/22/23 12:39:53.415
• [5.113 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/22/23 12:39:53.432
  Jul 22 12:39:53.432: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 12:39:53.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:39:53.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:39:53.471
  STEP: creating a ReplicationController @ 07/22/23 12:39:53.48
  STEP: waiting for RC to be added @ 07/22/23 12:39:53.492
  STEP: waiting for available Replicas @ 07/22/23 12:39:53.492
  STEP: patching ReplicationController @ 07/22/23 12:39:55.3
  STEP: waiting for RC to be modified @ 07/22/23 12:39:55.317
  STEP: patching ReplicationController status @ 07/22/23 12:39:55.317
  STEP: waiting for RC to be modified @ 07/22/23 12:39:55.326
  STEP: waiting for available Replicas @ 07/22/23 12:39:55.328
  STEP: fetching ReplicationController status @ 07/22/23 12:39:55.333
  STEP: patching ReplicationController scale @ 07/22/23 12:39:55.344
  STEP: waiting for RC to be modified @ 07/22/23 12:39:55.361
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/22/23 12:39:55.362
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/22/23 12:39:56.804
  STEP: updating ReplicationController status @ 07/22/23 12:39:56.809
  STEP: waiting for RC to be modified @ 07/22/23 12:39:56.827
  STEP: listing all ReplicationControllers @ 07/22/23 12:39:56.827
  STEP: checking that ReplicationController has expected values @ 07/22/23 12:39:56.835
  STEP: deleting ReplicationControllers by collection @ 07/22/23 12:39:56.835
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/22/23 12:39:56.854
  Jul 22 12:39:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 12:39:56.922547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5479" for this suite. @ 07/22/23 12:39:56.932
• [3.518 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/22/23 12:39:56.952
  Jul 22 12:39:56.952: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:39:56.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:39:56.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:39:56.999
  STEP: Setting up server cert @ 07/22/23 12:39:57.049
  E0722 12:39:57.923011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:39:58.213
  STEP: Deploying the webhook pod @ 07/22/23 12:39:58.225
  STEP: Wait for the deployment to be ready @ 07/22/23 12:39:58.243
  Jul 22 12:39:58.251: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0722 12:39:58.923142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:39:59.923326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:00.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 12, 39, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 39, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 39, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 39, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:40:00.923373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:01.923554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 12:40:02.283
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:40:02.325
  E0722 12:40:02.924178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:03.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/22/23 12:40:03.333
  STEP: create a namespace for the webhook @ 07/22/23 12:40:03.36
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/22/23 12:40:03.389
  Jul 22 12:40:03.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6797" for this suite. @ 07/22/23 12:40:03.577
  STEP: Destroying namespace "webhook-markers-6064" for this suite. @ 07/22/23 12:40:03.591
  STEP: Destroying namespace "fail-closed-namespace-869" for this suite. @ 07/22/23 12:40:03.605
• [6.664 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/22/23 12:40:03.618
  Jul 22 12:40:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:40:03.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:40:03.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:40:03.666
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/22/23 12:40:03.671
  E0722 12:40:03.924879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:04.925546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:05.925705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:06.925777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:40:07.706
  Jul 22 12:40:07.712: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-2018aa9f-2b5b-4d3d-8c72-36f51b15d838 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:40:07.738
  Jul 22 12:40:07.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4278" for this suite. @ 07/22/23 12:40:07.77
• [4.162 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/22/23 12:40:07.781
  Jul 22 12:40:07.781: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 12:40:07.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:40:07.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:40:07.865
  STEP: creating the pod @ 07/22/23 12:40:07.872
  STEP: submitting the pod to kubernetes @ 07/22/23 12:40:07.872
  W0722 12:40:07.887440      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0722 12:40:07.931338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:08.931528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:09.932533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:10.932728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/22/23 12:40:11.916
  STEP: updating the pod @ 07/22/23 12:40:11.921
  E0722 12:40:11.933599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:12.441: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5655425f-d14f-4b7a-8b69-3be8cd51c2dd"
  E0722 12:40:12.934257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:13.934674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2552" for this suite. @ 07/22/23 12:40:14.459
• [6.690 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/22/23 12:40:14.473
  Jul 22 12:40:14.473: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 12:40:14.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:40:14.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:40:14.514
  STEP: Creating service test in namespace statefulset-7354 @ 07/22/23 12:40:14.527
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/22/23 12:40:14.536
  STEP: Creating stateful set ss in namespace statefulset-7354 @ 07/22/23 12:40:14.544
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7354 @ 07/22/23 12:40:14.555
  Jul 22 12:40:14.565: INFO: Found 0 stateful pods, waiting for 1
  E0722 12:40:14.935223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:15.935977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:16.936726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:17.937024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:18.937306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:19.938068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:20.938475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:21.938580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:22.939005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:23.939113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:24.573: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/22/23 12:40:24.573
  Jul 22 12:40:24.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:40:24.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:40:24.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:40:24.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 12:40:24.780: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0722 12:40:24.940069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:25.940271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:26.940369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:27.940484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:28.940604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:29.940836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:30.941078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:31.941581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:32.942481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:33.942593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:34.786: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 12:40:34.786: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 12:40:34.814: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
  E0722 12:40:34.942871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:35.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993491937s
  E0722 12:40:35.943610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:36.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989064114s
  E0722 12:40:36.943883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:37.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982565982s
  E0722 12:40:37.944191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:38.841: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973350903s
  E0722 12:40:38.945242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:39.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.966011063s
  E0722 12:40:39.946028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:40.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.95853478s
  E0722 12:40:40.947021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:41.864: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950692335s
  E0722 12:40:41.947531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:42.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.943771119s
  E0722 12:40:42.948546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:43.877: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.71275ms
  E0722 12:40:43.949434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7354 @ 07/22/23 12:40:44.877
  Jul 22 12:40:44.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0722 12:40:44.949680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:45.104: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:40:45.104: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:40:45.104: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 12:40:45.109: INFO: Found 1 stateful pods, waiting for 3
  E0722 12:40:45.950713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:46.951648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:47.951752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:48.951873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:49.952027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:50.952167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:51.952259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:52.952376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:53.952559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:54.952896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:40:55.118: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:40:55.118: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:40:55.118: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/22/23 12:40:55.118
  STEP: Scale down will halt with unhealthy stateful pod @ 07/22/23 12:40:55.118
  Jul 22 12:40:55.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:40:55.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:40:55.320: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:40:55.320: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 12:40:55.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:40:55.549: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:40:55.550: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:40:55.550: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 12:40:55.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:40:55.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:40:55.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:40:55.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 12:40:55.763: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 12:40:55.772: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0722 12:40:55.953225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:56.953274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:57.954176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:58.955244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:40:59.955462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:00.956131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:01.956271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:02.956803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:03.957152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:04.957107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:05.784: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 12:41:05.784: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 12:41:05.784: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 12:41:05.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999964s
  E0722 12:41:05.957518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:06.811: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995030634s
  E0722 12:41:06.957716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:07.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988916602s
  E0722 12:41:07.957962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:08.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98146612s
  E0722 12:41:08.959009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:09.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975621011s
  E0722 12:41:09.959167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:10.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96947067s
  E0722 12:41:10.959728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:11.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962073528s
  E0722 12:41:11.960237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:12.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955880205s
  E0722 12:41:12.961064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:13.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948503339s
  E0722 12:41:13.961591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:14.869: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.781602ms
  E0722 12:41:14.961720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7354 @ 07/22/23 12:41:15.87
  Jul 22 12:41:15.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0722 12:41:15.962724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:16.056: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:41:16.056: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:41:16.056: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 12:41:16.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 12:41:16.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:41:16.263: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:41:16.263: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 12:41:16.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-7354 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 12:41:16.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:41:16.464: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:41:16.464: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 12:41:16.464: INFO: Scaling statefulset ss to 0
  E0722 12:41:16.963345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:17.964358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:18.965063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:19.965997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:20.967045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:21.967410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:22.967832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:23.968257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:24.968467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:25.969041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/22/23 12:41:26.494
  Jul 22 12:41:26.494: INFO: Deleting all statefulset in ns statefulset-7354
  Jul 22 12:41:26.501: INFO: Scaling statefulset ss to 0
  Jul 22 12:41:26.519: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 12:41:26.524: INFO: Deleting statefulset ss
  Jul 22 12:41:26.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7354" for this suite. @ 07/22/23 12:41:26.551
• [72.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/22/23 12:41:26.567
  Jul 22 12:41:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 12:41:26.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:41:26.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:41:26.613
  STEP: Creating pod busybox-1ca49196-cee7-4b23-8f89-7143594cc906 in namespace container-probe-5765 @ 07/22/23 12:41:26.619
  E0722 12:41:26.969079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:27.969172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:41:28.763: INFO: Started pod busybox-1ca49196-cee7-4b23-8f89-7143594cc906 in namespace container-probe-5765
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 12:41:28.763
  Jul 22 12:41:28.769: INFO: Initial restart count of pod busybox-1ca49196-cee7-4b23-8f89-7143594cc906 is 0
  E0722 12:41:28.970119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:29.970481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:30.971487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:31.971492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:32.971991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:33.972137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:34.972816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:35.973062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:36.974121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:37.974771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:38.975805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:39.976033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:40.976639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:41.977619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:42.978014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:43.978126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:44.979100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:45.979214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:46.980812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:47.981104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:48.981615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:49.981729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:50.982548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:51.982759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:52.983081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:53.983255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:54.983632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:55.983677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:56.984753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:57.985118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:58.985704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:41:59.985831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:00.985902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:01.986018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:02.986367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:03.986693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:04.986744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:05.987098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:06.987663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:07.987963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:08.988288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:09.988654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:10.989375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:11.989821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:12.989983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:13.990797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:14.991800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:15.992778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:16.993792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:17.993902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:18.943: INFO: Restart count of pod container-probe-5765/busybox-1ca49196-cee7-4b23-8f89-7143594cc906 is now 1 (50.173837099s elapsed)
  Jul 22 12:42:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:42:18.948
  STEP: Destroying namespace "container-probe-5765" for this suite. @ 07/22/23 12:42:18.965
• [52.413 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/22/23 12:42:18.98
  Jul 22 12:42:18.980: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:42:18.981
  E0722 12:42:18.994356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:19.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:19.013
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:42:19.017
  E0722 12:42:19.995335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:20.996252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:21.996484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:22.996496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:42:23.052
  Jul 22 12:42:23.058: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-bf6e2213-4aac-43dc-b5bd-b284e67dc69a container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:42:23.088
  Jul 22 12:42:23.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6234" for this suite. @ 07/22/23 12:42:23.116
• [4.146 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/22/23 12:42:23.127
  Jul 22 12:42:23.127: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:42:23.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:23.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:23.159
  STEP: Creating a test headless service @ 07/22/23 12:42:23.163
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8392 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8392;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8392 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8392;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8392.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8392.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8392.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8392.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8392.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8392.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8392.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8392.svc;check="$$(dig +notcp +noall +answer +search 24.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.24_udp@PTR;check="$$(dig +tcp +noall +answer +search 24.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.24_tcp@PTR;sleep 1; done
   @ 07/22/23 12:42:23.198
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8392 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8392;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8392 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8392;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8392.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8392.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8392.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8392.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8392.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8392.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8392.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8392.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8392.svc;check="$$(dig +notcp +noall +answer +search 24.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.24_udp@PTR;check="$$(dig +tcp +noall +answer +search 24.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.24_tcp@PTR;sleep 1; done
   @ 07/22/23 12:42:23.198
  STEP: creating a pod to probe DNS @ 07/22/23 12:42:23.199
  STEP: submitting the pod to kubernetes @ 07/22/23 12:42:23.199
  E0722 12:42:23.997134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:24.997344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:25.998224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:26.998913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/22/23 12:42:27.242
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:42:27.248
  Jul 22 12:42:27.255: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.261: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.268: INFO: Unable to read wheezy_udp@dns-test-service.dns-8392 from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.274: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8392 from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.280: INFO: Unable to read wheezy_udp@dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.287: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.328: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.337: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.343: INFO: Unable to read jessie_udp@dns-test-service.dns-8392 from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-8392 from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.359: INFO: Unable to read jessie_udp@dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.366: INFO: Unable to read jessie_tcp@dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.373: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.379: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8392.svc from pod dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282: the server could not find the requested resource (get pods dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282)
  Jul 22 12:42:27.406: INFO: Lookups using dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8392 wheezy_tcp@dns-test-service.dns-8392 wheezy_udp@dns-test-service.dns-8392.svc wheezy_tcp@dns-test-service.dns-8392.svc wheezy_udp@_http._tcp.dns-test-service.dns-8392.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8392.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8392 jessie_tcp@dns-test-service.dns-8392 jessie_udp@dns-test-service.dns-8392.svc jessie_tcp@dns-test-service.dns-8392.svc jessie_udp@_http._tcp.dns-test-service.dns-8392.svc jessie_tcp@_http._tcp.dns-test-service.dns-8392.svc]

  E0722 12:42:27.999922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:29.000748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:30.001123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:31.001284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:32.002147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:32.569: INFO: DNS probes using dns-8392/dns-test-20c8b8fd-ba5c-4402-873c-61447e8c6282 succeeded

  Jul 22 12:42:32.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:42:32.574
  STEP: deleting the test service @ 07/22/23 12:42:32.598
  STEP: deleting the test headless service @ 07/22/23 12:42:32.65
  STEP: Destroying namespace "dns-8392" for this suite. @ 07/22/23 12:42:32.676
• [9.570 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/22/23 12:42:32.698
  Jul 22 12:42:32.698: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-pred @ 07/22/23 12:42:32.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:32.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:32.73
  Jul 22 12:42:32.736: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 22 12:42:32.750: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 12:42:32.757: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-55 before test
  Jul 22 12:42:32.765: INFO: default-http-backend-kubernetes-worker-65fc475d49-slc26 from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:29 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.765: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 22 12:42:32.765: INFO: nginx-ingress-controller-kubernetes-worker-m59lf from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:31 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.765: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:42:32.765: INFO: calico-kube-controllers-79b76dbbcc-9vwj9 from kube-system started at 2023-07-22 11:54:47 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.765: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 22 12:42:32.765: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:42:32.765: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:42:32.765: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 12:42:32.765: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-26-93 before test
  Jul 22 12:42:32.777: INFO: nginx-ingress-controller-kubernetes-worker-9f8fn from ingress-nginx-kubernetes-worker started at 2023-07-22 12:03:49 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.777: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: sonobuoy from sonobuoy started at 2023-07-22 12:04:59 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.777: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: sonobuoy-e2e-job-09ff55b4a2944177 from sonobuoy started at 2023-07-22 12:05:02 +0000 UTC (2 container statuses recorded)
  Jul 22 12:42:32.777: INFO: 	Container e2e ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-l8p6k from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:42:32.777: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 12:42:32.777: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-81-237 before test
  Jul 22 12:42:32.784: INFO: nginx-ingress-controller-kubernetes-worker-r6kjp from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:28 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.784: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 12:42:32.784: INFO: coredns-5c7f76ccb8-cvjf4 from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.784: INFO: 	Container coredns ready: true, restart count 0
  Jul 22 12:42:32.785: INFO: kube-state-metrics-5b95b4459c-pp6nn from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.785: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 22 12:42:32.785: INFO: metrics-server-v0.5.2-6cf8c8b69c-lv5pw from kube-system started at 2023-07-22 11:54:21 +0000 UTC (2 container statuses recorded)
  Jul 22 12:42:32.785: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 22 12:42:32.785: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 22 12:42:32.785: INFO: dashboard-metrics-scraper-6b8586b5c9-29rq6 from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.785: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 22 12:42:32.786: INFO: kubernetes-dashboard-6869f4cd5f-5xvbw from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 12:42:32.786: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 22 12:42:32.786: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-s457l from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 12:42:32.786: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 12:42:32.786: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/22/23 12:42:32.787
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1774311b247dedba], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 07/22/23 12:42:32.829
  E0722 12:42:33.003114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:33.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4029" for this suite. @ 07/22/23 12:42:33.828
• [1.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/22/23 12:42:33.838
  Jul 22 12:42:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 12:42:33.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:33.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:33.868
  Jul 22 12:42:33.871: INFO: Creating simple deployment test-new-deployment
  Jul 22 12:42:33.888: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0722 12:42:34.003162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:35.003326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 07/22/23 12:42:35.915
  STEP: updating a scale subresource @ 07/22/23 12:42:35.926
  STEP: verifying the deployment Spec.Replicas was modified @ 07/22/23 12:42:35.948
  STEP: Patch a scale subresource @ 07/22/23 12:42:35.954
  Jul 22 12:42:35.991: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-5860  a47adfcb-e93c-4bd9-9453-b3113652db43 17068 3 2023-07-22 12:42:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-22 12:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b31ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-22 12:42:35 +0000 UTC,LastTransitionTime:2023-07-22 12:42:33 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-22 12:42:35 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 22 12:42:35.996: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-5860  d2e07f84-544b-4de7-842d-bd0bf0cb04b9 17071 2 2023-07-22 12:42:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a47adfcb-e93c-4bd9-9453-b3113652db43 0xc0036f4cf7 0xc0036f4cf8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a47adfcb-e93c-4bd9-9453-b3113652db43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036f4e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:42:36.003: INFO: Pod "test-new-deployment-67bd4bf6dc-d6skn" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-d6skn test-new-deployment-67bd4bf6dc- deployment-5860  68aa0aaf-444b-46b3-848a-b057157486ee 17074 0 2023-07-22 12:42:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d2e07f84-544b-4de7-842d-bd0bf0cb04b9 0xc0036f56f7 0xc0036f56f8}] [] [{kube-controller-manager Update v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2e07f84-544b-4de7-842d-bd0bf0cb04b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-677vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-677vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:,StartTime:2023-07-22 12:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 12:42:36.004: INFO: Pod "test-new-deployment-67bd4bf6dc-db9cx" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-db9cx test-new-deployment-67bd4bf6dc- deployment-5860  4e609dfa-0540-4e5c-aa3d-82a4e47707b2 17060 0 2023-07-22 12:42:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d2e07f84-544b-4de7-842d-bd0bf0cb04b9 0xc0036f5997 0xc0036f5998}] [] [{kube-controller-manager Update v1 2023-07-22 12:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2e07f84-544b-4de7-842d-bd0bf0cb04b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 12:42:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbkx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbkx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:42:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.147,StartTime:2023-07-22 12:42:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 12:42:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d7d3c6d1c8578c6b9962dc6b1b3d83b17cfdeb89d87975375cfd75284a27403c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.147,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0722 12:42:36.004571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:36.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5860" for this suite. @ 07/22/23 12:42:36.01
• [2.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/22/23 12:42:36.025
  Jul 22 12:42:36.026: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:42:36.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:36.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:36.078
  STEP: Creating Pod @ 07/22/23 12:42:36.081
  E0722 12:42:37.005051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:38.005112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/22/23 12:42:38.123
  Jul 22 12:42:38.123: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-161 PodName:pod-sharedvolume-78af3d19-591b-4588-9105-1802f70ec656 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:42:38.123: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:42:38.124: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:42:38.124: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-161/pods/pod-sharedvolume-78af3d19-591b-4588-9105-1802f70ec656/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jul 22 12:42:38.209: INFO: Exec stderr: ""
  Jul 22 12:42:38.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-161" for this suite. @ 07/22/23 12:42:38.218
• [2.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/22/23 12:42:38.232
  Jul 22 12:42:38.232: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 12:42:38.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:38.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:38.259
  Jul 22 12:42:38.265: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:42:39.006194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/22/23 12:42:39.756
  Jul 22 12:42:39.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-9443 --namespace=crd-publish-openapi-9443 create -f -'
  E0722 12:42:40.007184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:40.712: INFO: stderr: ""
  Jul 22 12:42:40.712: INFO: stdout: "e2e-test-crd-publish-openapi-4772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 22 12:42:40.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-9443 --namespace=crd-publish-openapi-9443 delete e2e-test-crd-publish-openapi-4772-crds test-cr'
  Jul 22 12:42:40.808: INFO: stderr: ""
  Jul 22 12:42:40.808: INFO: stdout: "e2e-test-crd-publish-openapi-4772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul 22 12:42:40.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-9443 --namespace=crd-publish-openapi-9443 apply -f -'
  E0722 12:42:41.007498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:41.214: INFO: stderr: ""
  Jul 22 12:42:41.214: INFO: stdout: "e2e-test-crd-publish-openapi-4772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 22 12:42:41.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-9443 --namespace=crd-publish-openapi-9443 delete e2e-test-crd-publish-openapi-4772-crds test-cr'
  Jul 22 12:42:41.319: INFO: stderr: ""
  Jul 22 12:42:41.319: INFO: stdout: "e2e-test-crd-publish-openapi-4772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/22/23 12:42:41.319
  Jul 22 12:42:41.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-9443 explain e2e-test-crd-publish-openapi-4772-crds'
  Jul 22 12:42:41.638: INFO: stderr: ""
  Jul 22 12:42:41.638: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-4772-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0722 12:42:42.008273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:43.009340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:43.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9443" for this suite. @ 07/22/23 12:42:43.308
• [5.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/22/23 12:42:43.321
  Jul 22 12:42:43.321: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:42:43.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:43.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:43.355
  STEP: Creating the pod @ 07/22/23 12:42:43.36
  E0722 12:42:44.010231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:45.010487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:45.941: INFO: Successfully updated pod "annotationupdatea3669374-9953-4db9-9b22-110a47781377"
  E0722 12:42:46.011232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:47.011623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:48.012450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:49.012641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9587" for this suite. @ 07/22/23 12:42:49.983
• [6.673 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/22/23 12:42:49.996
  Jul 22 12:42:49.996: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename events @ 07/22/23 12:42:49.997
  E0722 12:42:50.013366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:50.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:50.034
  STEP: creating a test event @ 07/22/23 12:42:50.041
  STEP: listing all events in all namespaces @ 07/22/23 12:42:50.052
  STEP: patching the test event @ 07/22/23 12:42:50.063
  STEP: fetching the test event @ 07/22/23 12:42:50.073
  STEP: updating the test event @ 07/22/23 12:42:50.078
  STEP: getting the test event @ 07/22/23 12:42:50.094
  STEP: deleting the test event @ 07/22/23 12:42:50.1
  STEP: listing all events in all namespaces @ 07/22/23 12:42:50.112
  Jul 22 12:42:50.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5689" for this suite. @ 07/22/23 12:42:50.127
• [0.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/22/23 12:42:50.147
  Jul 22 12:42:50.147: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:42:50.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:50.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:50.174
  STEP: Setting up server cert @ 07/22/23 12:42:50.207
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:42:50.739
  STEP: Deploying the webhook pod @ 07/22/23 12:42:50.75
  STEP: Wait for the deployment to be ready @ 07/22/23 12:42:50.772
  Jul 22 12:42:50.783: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 12:42:51.014158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:52.014554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 12:42:52.805
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:42:52.822
  E0722 12:42:53.015272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:53.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 22 12:42:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:42:54.015857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5330-crds.webhook.example.com via the AdmissionRegistration API @ 07/22/23 12:42:54.358
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/22/23 12:42:54.383
  E0722 12:42:55.015958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:56.016235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:42:56.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 12:42:57.016396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9376" for this suite. @ 07/22/23 12:42:57.122
  STEP: Destroying namespace "webhook-markers-6908" for this suite. @ 07/22/23 12:42:57.132
• [6.995 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/22/23 12:42:57.142
  Jul 22 12:42:57.143: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 12:42:57.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:42:57.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:42:57.171
  STEP: Create a Replicaset @ 07/22/23 12:42:57.181
  STEP: Verify that the required pods have come up. @ 07/22/23 12:42:57.192
  Jul 22 12:42:57.196: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0722 12:42:58.017240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:42:59.017745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:00.017890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:01.018238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:02.018444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:43:02.205: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 12:43:02.206
  STEP: Getting /status @ 07/22/23 12:43:02.206
  Jul 22 12:43:02.213: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/22/23 12:43:02.213
  Jul 22 12:43:02.232: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/22/23 12:43:02.232
  Jul 22 12:43:02.238: INFO: Observed &ReplicaSet event: ADDED
  Jul 22 12:43:02.239: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.239: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.240: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.240: INFO: Found replicaset test-rs in namespace replicaset-7651 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 22 12:43:02.240: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/22/23 12:43:02.242
  Jul 22 12:43:02.243: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 22 12:43:02.260: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/22/23 12:43:02.26
  Jul 22 12:43:02.263: INFO: Observed &ReplicaSet event: ADDED
  Jul 22 12:43:02.264: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.264: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.265: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.265: INFO: Observed replicaset test-rs in namespace replicaset-7651 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 12:43:02.266: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 22 12:43:02.266: INFO: Found replicaset test-rs in namespace replicaset-7651 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul 22 12:43:02.266: INFO: Replicaset test-rs has a patched status
  Jul 22 12:43:02.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7651" for this suite. @ 07/22/23 12:43:02.285
• [5.152 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/22/23 12:43:02.295
  Jul 22 12:43:02.295: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-runtime @ 07/22/23 12:43:02.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:43:02.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:43:02.389
  STEP: create the container @ 07/22/23 12:43:02.398
  W0722 12:43:02.451350      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/22/23 12:43:02.451
  E0722 12:43:03.019011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:04.019236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:05.019290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/22/23 12:43:05.496
  STEP: the container should be terminated @ 07/22/23 12:43:05.502
  STEP: the termination message should be set @ 07/22/23 12:43:05.502
  Jul 22 12:43:05.502: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/22/23 12:43:05.503
  Jul 22 12:43:05.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7740" for this suite. @ 07/22/23 12:43:05.536
• [3.251 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/22/23 12:43:05.548
  Jul 22 12:43:05.548: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:43:05.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:43:05.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:43:05.591
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/22/23 12:43:05.597
  Jul 22 12:43:05.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9006 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul 22 12:43:05.705: INFO: stderr: ""
  Jul 22 12:43:05.705: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/22/23 12:43:05.705
  Jul 22 12:43:05.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9006 delete pods e2e-test-httpd-pod'
  E0722 12:43:06.020398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:07.021228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:43:07.851: INFO: stderr: ""
  Jul 22 12:43:07.851: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 22 12:43:07.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9006" for this suite. @ 07/22/23 12:43:07.858
• [2.320 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/22/23 12:43:07.869
  Jul 22 12:43:07.869: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subpath @ 07/22/23 12:43:07.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:43:07.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:43:07.902
  STEP: Setting up data @ 07/22/23 12:43:07.906
  STEP: Creating pod pod-subpath-test-configmap-pbbv @ 07/22/23 12:43:07.921
  STEP: Creating a pod to test atomic-volume-subpath @ 07/22/23 12:43:07.921
  E0722 12:43:08.021916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:09.022150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:10.022630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:11.022762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:12.023692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:13.024081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:14.024503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:15.024813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:16.025824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:17.026355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:18.026941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:19.027103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:20.028010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:21.028725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:22.028898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:23.029103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:24.030224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:25.030694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:26.031649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:27.031787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:28.032561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:29.032755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:30.033577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:31.034145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:32.035053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:43:32.038
  Jul 22 12:43:32.044: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-subpath-test-configmap-pbbv container test-container-subpath-configmap-pbbv: <nil>
  STEP: delete the pod @ 07/22/23 12:43:32.068
  STEP: Deleting pod pod-subpath-test-configmap-pbbv @ 07/22/23 12:43:32.097
  Jul 22 12:43:32.097: INFO: Deleting pod "pod-subpath-test-configmap-pbbv" in namespace "subpath-9068"
  Jul 22 12:43:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9068" for this suite. @ 07/22/23 12:43:32.109
• [24.251 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/22/23 12:43:32.12
  Jul 22 12:43:32.120: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pod-network-test @ 07/22/23 12:43:32.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:43:32.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:43:32.159
  STEP: Performing setup for networking test in namespace pod-network-test-5182 @ 07/22/23 12:43:32.164
  STEP: creating a selector @ 07/22/23 12:43:32.164
  STEP: Creating the service pods in kubernetes @ 07/22/23 12:43:32.164
  Jul 22 12:43:32.164: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0722 12:43:33.035538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:34.035766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:35.035805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:36.036435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:37.036692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:38.036761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:39.037215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:40.037309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:41.037950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:42.038088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:43.038548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:44.038687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:45.038750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:46.039106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:47.039262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:48.039440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:49.039628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:50.039727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:51.039837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:52.040313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:53.041252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:54.042195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/22/23 12:43:54.332
  E0722 12:43:55.043057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:56.043192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:43:56.364: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 22 12:43:56.364: INFO: Breadth first check of 192.168.196.179 on host 172.31.15.55...
  Jul 22 12:43:56.369: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.175:9080/dial?request=hostname&protocol=udp&host=192.168.196.179&port=8081&tries=1'] Namespace:pod-network-test-5182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:43:56.369: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:43:56.370: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:43:56.370: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.196.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 22 12:43:56.469: INFO: Waiting for responses: map[]
  Jul 22 12:43:56.469: INFO: reached 192.168.196.179 after 0/1 tries
  Jul 22 12:43:56.469: INFO: Breadth first check of 192.168.120.31 on host 172.31.26.93...
  Jul 22 12:43:56.476: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.175:9080/dial?request=hostname&protocol=udp&host=192.168.120.31&port=8081&tries=1'] Namespace:pod-network-test-5182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:43:56.476: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:43:56.477: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:43:56.477: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.120.31%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 22 12:43:56.560: INFO: Waiting for responses: map[]
  Jul 22 12:43:56.560: INFO: reached 192.168.120.31 after 0/1 tries
  Jul 22 12:43:56.561: INFO: Breadth first check of 192.168.121.219 on host 172.31.81.237...
  Jul 22 12:43:56.567: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.175:9080/dial?request=hostname&protocol=udp&host=192.168.121.219&port=8081&tries=1'] Namespace:pod-network-test-5182 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:43:56.567: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:43:56.568: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:43:56.569: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5182/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.121.219%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 22 12:43:56.656: INFO: Waiting for responses: map[]
  Jul 22 12:43:56.657: INFO: reached 192.168.121.219 after 0/1 tries
  Jul 22 12:43:56.657: INFO: Going to retry 0 out of 3 pods....
  Jul 22 12:43:56.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5182" for this suite. @ 07/22/23 12:43:56.664
• [24.553 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/22/23 12:43:56.675
  Jul 22 12:43:56.675: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:43:56.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:43:56.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:43:56.707
  STEP: Creating configMap with name projected-configmap-test-volume-map-63bed07c-990e-402e-8075-7cc7a3ef1b47 @ 07/22/23 12:43:56.712
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:43:56.719
  E0722 12:43:57.044096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:58.044814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:43:59.045028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:00.045136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:44:00.748
  Jul 22 12:44:00.754: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-a7681568-2890-424b-b86b-2b84218b5119 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:44:00.765
  Jul 22 12:44:00.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6165" for this suite. @ 07/22/23 12:44:00.805
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/22/23 12:44:00.821
  Jul 22 12:44:00.821: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename init-container @ 07/22/23 12:44:00.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:00.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:00.923
  STEP: creating the pod @ 07/22/23 12:44:00.933
  Jul 22 12:44:00.934: INFO: PodSpec: initContainers in spec.initContainers
  E0722 12:44:01.045893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:02.046611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:03.046588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:04.047384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:05.048094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:06.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3863" for this suite. @ 07/22/23 12:44:06.043
  E0722 12:44:06.049054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [5.237 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/22/23 12:44:06.059
  Jul 22 12:44:06.060: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:44:06.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:06.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:06.092
  STEP: creating service multi-endpoint-test in namespace services-7666 @ 07/22/23 12:44:06.099
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7666 to expose endpoints map[] @ 07/22/23 12:44:06.113
  Jul 22 12:44:06.150: INFO: successfully validated that service multi-endpoint-test in namespace services-7666 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7666 @ 07/22/23 12:44:06.15
  E0722 12:44:07.049276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:08.050172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7666 to expose endpoints map[pod1:[100]] @ 07/22/23 12:44:08.179
  Jul 22 12:44:08.198: INFO: successfully validated that service multi-endpoint-test in namespace services-7666 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-7666 @ 07/22/23 12:44:08.198
  E0722 12:44:09.050300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:10.050427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7666 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/22/23 12:44:10.226
  Jul 22 12:44:10.247: INFO: successfully validated that service multi-endpoint-test in namespace services-7666 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/22/23 12:44:10.247
  Jul 22 12:44:10.247: INFO: Creating new exec pod
  E0722 12:44:11.050486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:12.050772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:13.051951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:13.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7666 exec execpod5r4hg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul 22 12:44:13.486: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul 22 12:44:13.486: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:44:13.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7666 exec execpod5r4hg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.76 80'
  Jul 22 12:44:13.686: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.76 80\nConnection to 10.152.183.76 80 port [tcp/http] succeeded!\n"
  Jul 22 12:44:13.686: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:44:13.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7666 exec execpod5r4hg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul 22 12:44:13.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul 22 12:44:13.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 12:44:13.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7666 exec execpod5r4hg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.76 81'
  E0722 12:44:14.052350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:14.083: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.76 81\nConnection to 10.152.183.76 81 port [tcp/*] succeeded!\n"
  Jul 22 12:44:14.083: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7666 @ 07/22/23 12:44:14.083
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7666 to expose endpoints map[pod2:[101]] @ 07/22/23 12:44:14.102
  E0722 12:44:15.052701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:15.146: INFO: successfully validated that service multi-endpoint-test in namespace services-7666 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-7666 @ 07/22/23 12:44:15.146
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7666 to expose endpoints map[] @ 07/22/23 12:44:15.173
  E0722 12:44:16.052741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:16.205: INFO: successfully validated that service multi-endpoint-test in namespace services-7666 exposes endpoints map[]
  Jul 22 12:44:16.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7666" for this suite. @ 07/22/23 12:44:16.238
• [10.198 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/22/23 12:44:16.258
  Jul 22 12:44:16.258: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:44:16.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:16.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:16.29
  STEP: creating a replication controller @ 07/22/23 12:44:16.296
  Jul 22 12:44:16.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 create -f -'
  E0722 12:44:17.053122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:17.091: INFO: stderr: ""
  Jul 22 12:44:17.091: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/22/23 12:44:17.091
  Jul 22 12:44:17.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:44:17.191: INFO: stderr: ""
  Jul 22 12:44:17.191: INFO: stdout: "update-demo-nautilus-2k2zc update-demo-nautilus-dmt8g "
  Jul 22 12:44:17.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods update-demo-nautilus-2k2zc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:44:17.281: INFO: stderr: ""
  Jul 22 12:44:17.281: INFO: stdout: ""
  Jul 22 12:44:17.281: INFO: update-demo-nautilus-2k2zc is created but not running
  E0722 12:44:18.053669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:19.053703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:20.053797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:21.053941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:22.054241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:22.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:44:22.402: INFO: stderr: ""
  Jul 22 12:44:22.402: INFO: stdout: "update-demo-nautilus-2k2zc update-demo-nautilus-dmt8g "
  Jul 22 12:44:22.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods update-demo-nautilus-2k2zc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:44:22.536: INFO: stderr: ""
  Jul 22 12:44:22.536: INFO: stdout: "true"
  Jul 22 12:44:22.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods update-demo-nautilus-2k2zc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:44:22.672: INFO: stderr: ""
  Jul 22 12:44:22.672: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:44:22.672: INFO: validating pod update-demo-nautilus-2k2zc
  Jul 22 12:44:22.679: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:44:22.680: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:44:22.680: INFO: update-demo-nautilus-2k2zc is verified up and running
  Jul 22 12:44:22.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods update-demo-nautilus-dmt8g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:44:22.807: INFO: stderr: ""
  Jul 22 12:44:22.807: INFO: stdout: "true"
  Jul 22 12:44:22.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods update-demo-nautilus-dmt8g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:44:22.938: INFO: stderr: ""
  Jul 22 12:44:22.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:44:22.938: INFO: validating pod update-demo-nautilus-dmt8g
  Jul 22 12:44:22.947: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:44:22.947: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:44:22.947: INFO: update-demo-nautilus-dmt8g is verified up and running
  STEP: using delete to clean up resources @ 07/22/23 12:44:22.947
  Jul 22 12:44:22.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 delete --grace-period=0 --force -f -'
  Jul 22 12:44:23.044: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:44:23.044: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 22 12:44:23.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get rc,svc -l name=update-demo --no-headers'
  E0722 12:44:23.054721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:23.146: INFO: stderr: "No resources found in kubectl-7995 namespace.\n"
  Jul 22 12:44:23.146: INFO: stdout: ""
  Jul 22 12:44:23.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7995 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 22 12:44:23.257: INFO: stderr: ""
  Jul 22 12:44:23.257: INFO: stdout: ""
  Jul 22 12:44:23.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7995" for this suite. @ 07/22/23 12:44:23.263
• [7.016 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/22/23 12:44:23.275
  Jul 22 12:44:23.275: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:44:23.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:23.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:23.353
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:44:23.36
  E0722 12:44:24.055135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:25.055130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:26.055278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:27.055466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:44:27.401
  Jul 22 12:44:27.407: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-3ff6afe4-0157-4654-bd1c-16532fccc9ef container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:44:27.416
  Jul 22 12:44:27.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5064" for this suite. @ 07/22/23 12:44:27.457
• [4.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/22/23 12:44:27.472
  Jul 22 12:44:27.472: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:44:27.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:27.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:27.51
  STEP: creating a ConfigMap @ 07/22/23 12:44:27.515
  STEP: fetching the ConfigMap @ 07/22/23 12:44:27.523
  STEP: patching the ConfigMap @ 07/22/23 12:44:27.53
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/22/23 12:44:27.538
  STEP: deleting the ConfigMap by collection with a label selector @ 07/22/23 12:44:27.547
  STEP: listing all ConfigMaps in test namespace @ 07/22/23 12:44:27.562
  Jul 22 12:44:27.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7083" for this suite. @ 07/22/23 12:44:27.575
• [0.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/22/23 12:44:27.59
  Jul 22 12:44:27.590: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 12:44:27.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:44:27.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:44:27.636
  STEP: Creating service test in namespace statefulset-3993 @ 07/22/23 12:44:27.641
  STEP: Creating a new StatefulSet @ 07/22/23 12:44:27.654
  Jul 22 12:44:27.677: INFO: Found 0 stateful pods, waiting for 3
  E0722 12:44:28.055583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:29.055806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:30.055868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:31.056209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:32.056764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:33.057053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:34.057081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:35.057191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:36.057318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:37.058163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:37.685: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:44:37.685: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:44:37.685: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 12:44:37.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-3993 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:44:37.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:44:37.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:44:37.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0722 12:44:38.058590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:39.058852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:40.058890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:41.059024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:42.059185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:43.059428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:44.059938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:45.060057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:46.061223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:47.061047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/22/23 12:44:47.914
  Jul 22 12:44:47.950: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/22/23 12:44:47.951
  E0722 12:44:48.061430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:49.061848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:50.061940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:51.062077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:52.062225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:53.062486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:54.062745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:55.062954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:56.064029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:44:57.064440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 07/22/23 12:44:57.984
  Jul 22 12:44:57.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-3993 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0722 12:44:58.067117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:44:58.189: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:44:58.189: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:44:58.189: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0722 12:44:59.067356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:00.067488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:01.067631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:02.067701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:03.067840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:04.067954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:05.068069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:06.068221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:07.068357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:08.069230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:09.069353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:10.069486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:11.069598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:12.069949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:13.070220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:14.070555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:15.071455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:16.071582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:17.072380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:18.072693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 07/22/23 12:45:18.227
  Jul 22 12:45:18.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-3993 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 12:45:18.534: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 12:45:18.534: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 12:45:18.534: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0722 12:45:19.072725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:20.072868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:21.073093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:22.073261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:23.073358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:24.073471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:25.073586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:26.073704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:27.073840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:28.073973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:45:28.589: INFO: Updating stateful set ss2
  E0722 12:45:29.074873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:30.075063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:31.075156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:32.075283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:33.075535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:34.075656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:35.075777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:36.075839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:37.076142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:38.076425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 07/22/23 12:45:38.615
  Jul 22 12:45:38.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-3993 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 12:45:38.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 12:45:38.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 12:45:38.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0722 12:45:39.077221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:40.078246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:41.078341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:42.078719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:43.078779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:44.079361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:45.079228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:46.080117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:47.080828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:48.081192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:45:48.843: INFO: Waiting for StatefulSet statefulset-3993/ss2 to complete update
  Jul 22 12:45:48.843: INFO: Waiting for Pod statefulset-3993/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
  E0722 12:45:49.082110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:50.082338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:51.083082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:52.083620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:53.084590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:54.084700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:55.084806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:56.085120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:57.086170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:45:58.086384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:45:58.856: INFO: Deleting all statefulset in ns statefulset-3993
  Jul 22 12:45:58.861: INFO: Scaling statefulset ss2 to 0
  E0722 12:45:59.086567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:00.086598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:01.087000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:02.087037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:03.087130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:04.087358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:05.087561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:06.088045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:07.088477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:08.088832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:08.892: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 12:46:08.897: INFO: Deleting statefulset ss2
  Jul 22 12:46:08.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3993" for this suite. @ 07/22/23 12:46:08.933
• [101.358 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/22/23 12:46:08.953
  Jul 22 12:46:08.953: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:46:08.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:08.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:08.999
  STEP: Creating configMap with name configmap-test-volume-map-304f716e-6078-444d-9f90-ecf83dae671d @ 07/22/23 12:46:09.005
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:46:09.015
  E0722 12:46:09.089880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:10.089994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:11.090307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:12.090662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:46:13.073
  Jul 22 12:46:13.079: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-7ae7502d-ffdb-4873-897d-170ba3591ece container agnhost-container: <nil>
  E0722 12:46:13.091626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 07/22/23 12:46:13.1
  Jul 22 12:46:13.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8235" for this suite. @ 07/22/23 12:46:13.128
• [4.187 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/22/23 12:46:13.141
  Jul 22 12:46:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:46:13.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:13.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:13.176
  STEP: creating a Service @ 07/22/23 12:46:13.188
  STEP: watching for the Service to be added @ 07/22/23 12:46:13.212
  Jul 22 12:46:13.218: INFO: Found Service test-service-m7fsw in namespace services-6224 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul 22 12:46:13.218: INFO: Service test-service-m7fsw created
  STEP: Getting /status @ 07/22/23 12:46:13.218
  Jul 22 12:46:13.229: INFO: Service test-service-m7fsw has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/22/23 12:46:13.229
  STEP: watching for the Service to be patched @ 07/22/23 12:46:13.256
  Jul 22 12:46:13.262: INFO: observed Service test-service-m7fsw in namespace services-6224 with annotations: map[] & LoadBalancer: {[]}
  Jul 22 12:46:13.263: INFO: Found Service test-service-m7fsw in namespace services-6224 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul 22 12:46:13.263: INFO: Service test-service-m7fsw has service status patched
  STEP: updating the ServiceStatus @ 07/22/23 12:46:13.263
  Jul 22 12:46:13.277: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/22/23 12:46:13.277
  Jul 22 12:46:13.283: INFO: Observed Service test-service-m7fsw in namespace services-6224 with annotations: map[] & Conditions: {[]}
  Jul 22 12:46:13.283: INFO: Observed event: &Service{ObjectMeta:{test-service-m7fsw  services-6224  3e7bb025-81ab-4edc-9255-b8ae626c3b32 18762 0 2023-07-22 12:46:13 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-22 12:46:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-22 12:46:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.203,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.203],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul 22 12:46:13.284: INFO: Found Service test-service-m7fsw in namespace services-6224 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 22 12:46:13.284: INFO: Service test-service-m7fsw has service status updated
  STEP: patching the service @ 07/22/23 12:46:13.284
  STEP: watching for the Service to be patched @ 07/22/23 12:46:13.322
  Jul 22 12:46:13.326: INFO: observed Service test-service-m7fsw in namespace services-6224 with labels: map[test-service-static:true]
  Jul 22 12:46:13.327: INFO: observed Service test-service-m7fsw in namespace services-6224 with labels: map[test-service-static:true]
  Jul 22 12:46:13.327: INFO: observed Service test-service-m7fsw in namespace services-6224 with labels: map[test-service-static:true]
  Jul 22 12:46:13.327: INFO: Found Service test-service-m7fsw in namespace services-6224 with labels: map[test-service:patched test-service-static:true]
  Jul 22 12:46:13.327: INFO: Service test-service-m7fsw patched
  STEP: deleting the service @ 07/22/23 12:46:13.327
  STEP: watching for the Service to be deleted @ 07/22/23 12:46:13.353
  Jul 22 12:46:13.356: INFO: Observed event: ADDED
  Jul 22 12:46:13.356: INFO: Observed event: MODIFIED
  Jul 22 12:46:13.356: INFO: Observed event: MODIFIED
  Jul 22 12:46:13.357: INFO: Observed event: MODIFIED
  Jul 22 12:46:13.357: INFO: Found Service test-service-m7fsw in namespace services-6224 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul 22 12:46:13.357: INFO: Service test-service-m7fsw deleted
  Jul 22 12:46:13.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6224" for this suite. @ 07/22/23 12:46:13.371
• [0.242 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/22/23 12:46:13.385
  Jul 22 12:46:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/22/23 12:46:13.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:13.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:13.42
  STEP: create the container to handle the HTTPGet hook request. @ 07/22/23 12:46:13.438
  E0722 12:46:14.091725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:15.091838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/22/23 12:46:15.478
  E0722 12:46:16.091949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:17.092238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 07/22/23 12:46:17.503
  E0722 12:46:18.092530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:19.092991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 07/22/23 12:46:19.526
  Jul 22 12:46:19.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8076" for this suite. @ 07/22/23 12:46:19.542
• [6.166 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/22/23 12:46:19.551
  Jul 22 12:46:19.551: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:46:19.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:19.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:19.581
  STEP: creating a replication controller @ 07/22/23 12:46:19.586
  Jul 22 12:46:19.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 create -f -'
  E0722 12:46:20.093108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:20.120: INFO: stderr: ""
  Jul 22 12:46:20.120: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/22/23 12:46:20.12
  Jul 22 12:46:20.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:46:20.231: INFO: stderr: ""
  Jul 22 12:46:20.231: INFO: stdout: "update-demo-nautilus-kxjf2 update-demo-nautilus-slw84 "
  Jul 22 12:46:20.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-kxjf2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:20.333: INFO: stderr: ""
  Jul 22 12:46:20.333: INFO: stdout: ""
  Jul 22 12:46:20.333: INFO: update-demo-nautilus-kxjf2 is created but not running
  E0722 12:46:21.093298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:22.093308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:23.093472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:24.108235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:25.108599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:25.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:46:25.472: INFO: stderr: ""
  Jul 22 12:46:25.472: INFO: stdout: "update-demo-nautilus-kxjf2 update-demo-nautilus-slw84 "
  Jul 22 12:46:25.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-kxjf2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:25.603: INFO: stderr: ""
  Jul 22 12:46:25.603: INFO: stdout: "true"
  Jul 22 12:46:25.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-kxjf2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:46:25.734: INFO: stderr: ""
  Jul 22 12:46:25.734: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:46:25.734: INFO: validating pod update-demo-nautilus-kxjf2
  Jul 22 12:46:25.741: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:46:25.741: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:46:25.741: INFO: update-demo-nautilus-kxjf2 is verified up and running
  Jul 22 12:46:25.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:25.874: INFO: stderr: ""
  Jul 22 12:46:25.874: INFO: stdout: "true"
  Jul 22 12:46:25.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:46:25.994: INFO: stderr: ""
  Jul 22 12:46:25.994: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:46:25.994: INFO: validating pod update-demo-nautilus-slw84
  Jul 22 12:46:26.000: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:46:26.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:46:26.001: INFO: update-demo-nautilus-slw84 is verified up and running
  STEP: scaling down the replication controller @ 07/22/23 12:46:26.001
  Jul 22 12:46:26.002: INFO: scanned /root for discovery docs: <nil>
  Jul 22 12:46:26.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0722 12:46:26.109638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:27.110266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:27.117: INFO: stderr: ""
  Jul 22 12:46:27.117: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/22/23 12:46:27.117
  Jul 22 12:46:27.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:46:27.211: INFO: stderr: ""
  Jul 22 12:46:27.211: INFO: stdout: "update-demo-nautilus-slw84 "
  Jul 22 12:46:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:27.302: INFO: stderr: ""
  Jul 22 12:46:27.302: INFO: stdout: "true"
  Jul 22 12:46:27.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:46:27.393: INFO: stderr: ""
  Jul 22 12:46:27.393: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:46:27.393: INFO: validating pod update-demo-nautilus-slw84
  Jul 22 12:46:27.399: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:46:27.399: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:46:27.399: INFO: update-demo-nautilus-slw84 is verified up and running
  STEP: scaling up the replication controller @ 07/22/23 12:46:27.399
  Jul 22 12:46:27.401: INFO: scanned /root for discovery docs: <nil>
  Jul 22 12:46:27.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0722 12:46:28.110160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:28.527: INFO: stderr: ""
  Jul 22 12:46:28.527: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/22/23 12:46:28.527
  Jul 22 12:46:28.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:46:28.625: INFO: stderr: ""
  Jul 22 12:46:28.625: INFO: stdout: "update-demo-nautilus-86k6m update-demo-nautilus-slw84 "
  Jul 22 12:46:28.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-86k6m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:28.717: INFO: stderr: ""
  Jul 22 12:46:28.717: INFO: stdout: ""
  Jul 22 12:46:28.717: INFO: update-demo-nautilus-86k6m is created but not running
  E0722 12:46:29.110517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:30.110610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:31.110673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:32.111419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:33.111513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:33.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 22 12:46:33.830: INFO: stderr: ""
  Jul 22 12:46:33.830: INFO: stdout: "update-demo-nautilus-86k6m update-demo-nautilus-slw84 "
  Jul 22 12:46:33.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-86k6m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 22 12:46:33.926: INFO: stderr: ""
  Jul 22 12:46:33.926: INFO: stdout: "true"
  Jul 22 12:46:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-86k6m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:46:34.023: INFO: stderr: ""
  Jul 22 12:46:34.023: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:46:34.023: INFO: validating pod update-demo-nautilus-86k6m
  Jul 22 12:46:34.032: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:46:34.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:46:34.032: INFO: update-demo-nautilus-86k6m is verified up and running
  Jul 22 12:46:34.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0722 12:46:34.113239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:34.146: INFO: stderr: ""
  Jul 22 12:46:34.146: INFO: stdout: "true"
  Jul 22 12:46:34.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods update-demo-nautilus-slw84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 22 12:46:34.241: INFO: stderr: ""
  Jul 22 12:46:34.241: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 22 12:46:34.241: INFO: validating pod update-demo-nautilus-slw84
  Jul 22 12:46:34.248: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 22 12:46:34.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 22 12:46:34.248: INFO: update-demo-nautilus-slw84 is verified up and running
  STEP: using delete to clean up resources @ 07/22/23 12:46:34.248
  Jul 22 12:46:34.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 delete --grace-period=0 --force -f -'
  Jul 22 12:46:34.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 22 12:46:34.356: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 22 12:46:34.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get rc,svc -l name=update-demo --no-headers'
  Jul 22 12:46:34.494: INFO: stderr: "No resources found in kubectl-940 namespace.\n"
  Jul 22 12:46:34.494: INFO: stdout: ""
  Jul 22 12:46:34.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-940 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 22 12:46:34.601: INFO: stderr: ""
  Jul 22 12:46:34.601: INFO: stdout: ""
  Jul 22 12:46:34.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-940" for this suite. @ 07/22/23 12:46:34.608
• [15.065 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/22/23 12:46:34.617
  Jul 22 12:46:34.617: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:46:34.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:34.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:34.646
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:46:34.649
  E0722 12:46:35.113879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:36.114001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:37.114715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:38.114831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:46:38.685
  Jul 22 12:46:38.690: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-01eec05b-d140-4c8f-a44e-5815a55dfb7d container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:46:38.702
  Jul 22 12:46:38.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4981" for this suite. @ 07/22/23 12:46:38.737
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/22/23 12:46:38.748
  Jul 22 12:46:38.748: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:46:38.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:38.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:38.787
  STEP: Creating projection with secret that has name projected-secret-test-map-16407039-d8e6-4a09-8fa0-acfea99e4fc8 @ 07/22/23 12:46:38.792
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:46:38.801
  E0722 12:46:39.114878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:40.115084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:41.115278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:42.115584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:46:42.834
  Jul 22 12:46:42.840: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-29c25535-da03-48bf-b854-925ae946d4a4 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:46:42.851
  Jul 22 12:46:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3777" for this suite. @ 07/22/23 12:46:42.882
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/22/23 12:46:42.897
  Jul 22 12:46:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sysctl @ 07/22/23 12:46:42.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:42.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:42.926
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/22/23 12:46:42.931
  Jul 22 12:46:42.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1389" for this suite. @ 07/22/23 12:46:42.947
• [0.061 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/22/23 12:46:42.96
  Jul 22 12:46:42.960: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:46:42.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:42.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:42.986
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8367 @ 07/22/23 12:46:42.993
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/22/23 12:46:43.007
  STEP: creating service externalsvc in namespace services-8367 @ 07/22/23 12:46:43.008
  STEP: creating replication controller externalsvc in namespace services-8367 @ 07/22/23 12:46:43.027
  I0722 12:46:43.040898      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8367, replica count: 2
  E0722 12:46:43.116581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:44.116738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:45.116741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 12:46:46.091951      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/22/23 12:46:46.099
  E0722 12:46:46.117164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:46.121: INFO: Creating new exec pod
  E0722 12:46:47.117225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:48.118203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:48.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-8367 exec execpods67bl -- /bin/sh -x -c nslookup clusterip-service.services-8367.svc.cluster.local'
  Jul 22 12:46:48.412: INFO: stderr: "+ nslookup clusterip-service.services-8367.svc.cluster.local\n"
  Jul 22 12:46:48.412: INFO: stdout: "Server:\t\t10.152.183.138\nAddress:\t10.152.183.138#53\n\nclusterip-service.services-8367.svc.cluster.local\tcanonical name = externalsvc.services-8367.svc.cluster.local.\nName:\texternalsvc.services-8367.svc.cluster.local\nAddress: 10.152.183.76\n\n"
  Jul 22 12:46:48.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8367, will wait for the garbage collector to delete the pods @ 07/22/23 12:46:48.419
  Jul 22 12:46:48.484: INFO: Deleting ReplicationController externalsvc took: 8.980077ms
  Jul 22 12:46:48.584: INFO: Terminating ReplicationController externalsvc pods took: 100.72007ms
  E0722 12:46:49.118739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:50.118809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:46:51.010: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-8367" for this suite. @ 07/22/23 12:46:51.032
• [8.085 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/22/23 12:46:51.046
  Jul 22 12:46:51.046: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:46:51.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:46:51.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:46:51.077
  STEP: Creating configMap with name cm-test-opt-del-20afd8d6-3249-405f-bf90-cada16f37446 @ 07/22/23 12:46:51.089
  STEP: Creating configMap with name cm-test-opt-upd-7557be53-49bf-4f86-baee-b6a5ee29512c @ 07/22/23 12:46:51.097
  STEP: Creating the pod @ 07/22/23 12:46:51.103
  E0722 12:46:51.119189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:52.119302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:53.120281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-20afd8d6-3249-405f-bf90-cada16f37446 @ 07/22/23 12:46:53.171
  STEP: Updating configmap cm-test-opt-upd-7557be53-49bf-4f86-baee-b6a5ee29512c @ 07/22/23 12:46:53.183
  STEP: Creating configMap with name cm-test-opt-create-083849bd-ffe9-4be5-813a-916c81641c53 @ 07/22/23 12:46:53.192
  STEP: waiting to observe update in volume @ 07/22/23 12:46:53.2
  E0722 12:46:54.120638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:55.121081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:56.121190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:57.122188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:58.122906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:46:59.123264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:00.124199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:01.124308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:02.124496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:03.124736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:04.125068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:05.125399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:06.126201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:07.126342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:08.127282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:09.131520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:10.131593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:11.132421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:12.133082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:13.134170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:14.134288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:15.134528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:16.134652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:17.135646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:18.135769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:19.136730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:20.136887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:21.137161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:22.137275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:23.137434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:24.138199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:25.138567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:26.138675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:27.138812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:28.138891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:29.139093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:30.139522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:31.139870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:32.140009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:33.140126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:34.141016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:35.141101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:36.141623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:37.141762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:38.141877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:39.141961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:40.142978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:41.143711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:42.143804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:43.144248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:44.145280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:45.145376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:46.146253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:47.146289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:48.146353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:49.146472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:50.146893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:51.147011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:52.147040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:53.147498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:54.148301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:55.149012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:56.149794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:57.149890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:58.150627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:47:59.150771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:00.151157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:01.151297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:02.151726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:03.151818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:04.152636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:05.152791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:06.153828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:07.154198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:08.155177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:09.155657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:09.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8965" for this suite. @ 07/22/23 12:48:09.714
• [78.677 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/22/23 12:48:09.723
  Jul 22 12:48:09.723: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename containers @ 07/22/23 12:48:09.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:48:09.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:48:09.756
  E0722 12:48:10.156167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:11.156421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:11.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7682" for this suite. @ 07/22/23 12:48:11.831
• [2.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/22/23 12:48:11.841
  Jul 22 12:48:11.841: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 12:48:11.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:48:11.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:48:11.871
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/22/23 12:48:11.881
  Jul 22 12:48:11.882: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:48:12.156472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:13.172103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:48:14.172498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:15.173209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:16.173892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:17.174036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:18.174464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:19.175066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:20.175804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:20.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7500" for this suite. @ 07/22/23 12:48:20.84
• [9.013 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/22/23 12:48:20.856
  Jul 22 12:48:20.856: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 12:48:20.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:48:20.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:48:20.903
  STEP: creating the pod @ 07/22/23 12:48:20.909
  STEP: waiting for pod running @ 07/22/23 12:48:20.93
  E0722 12:48:21.176786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:22.177813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 07/22/23 12:48:22.953
  Jul 22 12:48:22.959: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1652 PodName:var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:48:22.959: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:48:22.960: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:48:22.960: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-1652/pods/var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/22/23 12:48:23.038
  Jul 22 12:48:23.043: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1652 PodName:var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:48:23.043: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:48:23.044: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:48:23.044: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-1652/pods/var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/22/23 12:48:23.133
  E0722 12:48:23.178006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:23.651: INFO: Successfully updated pod "var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4"
  STEP: waiting for annotated pod running @ 07/22/23 12:48:23.651
  STEP: deleting the pod gracefully @ 07/22/23 12:48:23.657
  Jul 22 12:48:23.658: INFO: Deleting pod "var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4" in namespace "var-expansion-1652"
  Jul 22 12:48:23.669: INFO: Wait up to 5m0s for pod "var-expansion-8481532a-417a-4a3c-a3ed-b3c51d8b2be4" to be fully deleted
  E0722 12:48:24.179080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:25.179228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:26.180306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:27.180801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:28.180813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:29.180967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:30.181227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:31.181339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:32.182189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:33.182319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:34.182502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:35.182576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:36.183489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:37.183880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:38.184000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:39.184646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:40.185329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:41.186186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:42.187299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:43.187396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:44.190149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:45.190428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:46.190542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:47.190619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:48.190713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:49.190739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:50.191641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:51.191820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:52.193097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:53.193755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:54.193869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:55.194007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:56.194202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:57.194344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:48:57.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1652" for this suite. @ 07/22/23 12:48:57.792
• [36.947 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/22/23 12:48:57.804
  Jul 22 12:48:57.804: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename runtimeclass @ 07/22/23 12:48:57.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:48:57.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:48:57.854
  STEP: getting /apis @ 07/22/23 12:48:57.86
  STEP: getting /apis/node.k8s.io @ 07/22/23 12:48:57.867
  STEP: getting /apis/node.k8s.io/v1 @ 07/22/23 12:48:57.87
  STEP: creating @ 07/22/23 12:48:57.873
  STEP: watching @ 07/22/23 12:48:57.927
  Jul 22 12:48:57.927: INFO: starting watch
  STEP: getting @ 07/22/23 12:48:57.943
  STEP: listing @ 07/22/23 12:48:57.954
  STEP: patching @ 07/22/23 12:48:57.96
  STEP: updating @ 07/22/23 12:48:57.97
  Jul 22 12:48:57.977: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/22/23 12:48:57.978
  STEP: deleting a collection @ 07/22/23 12:48:58.005
  Jul 22 12:48:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2026" for this suite. @ 07/22/23 12:48:58.044
• [0.258 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/22/23 12:48:58.064
  Jul 22 12:48:58.064: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename init-container @ 07/22/23 12:48:58.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:48:58.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:48:58.119
  STEP: creating the pod @ 07/22/23 12:48:58.141
  Jul 22 12:48:58.141: INFO: PodSpec: initContainers in spec.initContainers
  E0722 12:48:58.195314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:48:59.196087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:00.196724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:01.197314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:49:02.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4611" for this suite. @ 07/22/23 12:49:02.164
• [4.112 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/22/23 12:49:02.176
  Jul 22 12:49:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename cronjob @ 07/22/23 12:49:02.179
  E0722 12:49:02.197757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:49:02.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:49:02.211
  STEP: Creating a ReplaceConcurrent cronjob @ 07/22/23 12:49:02.217
  STEP: Ensuring a job is scheduled @ 07/22/23 12:49:02.235
  E0722 12:49:03.197894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:04.198034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:05.198162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:06.199035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:07.199209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:08.199310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:09.199451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:10.199838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:11.200231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:12.200680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:13.201311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:14.201702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:15.201838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:16.201963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:17.202153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:18.203428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:19.203843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:20.203989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:21.204144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:22.204270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:23.204424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:24.204534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:25.205169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:26.205231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:27.205327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:28.205707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:29.205833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:30.206095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:31.206237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:32.206356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:33.206576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:34.207538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:35.207672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:36.208608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:37.208689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:38.209041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:39.209873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:40.209994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:41.210423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:42.210506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:43.211308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:44.211536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:45.212119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:46.213183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:47.213269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:48.213382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:49.214133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:50.214551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:51.215353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:52.215442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:53.215572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:54.215757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:55.215932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:56.216059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:57.216176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:58.216298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:49:59.216461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:00.216721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/22/23 12:50:00.248
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/22/23 12:50:00.268
  STEP: Ensuring the job is replaced with a new one @ 07/22/23 12:50:00.278
  E0722 12:50:01.217042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:02.217171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:03.217279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:04.217425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:05.217563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:06.217665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:07.217900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:08.218992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:09.219097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:10.219231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:11.219533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:12.219685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:13.219861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:14.220217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:15.221128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:16.221202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:17.221509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:18.221489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:19.221612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:20.221751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:21.222273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:22.223379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:23.223541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:24.223789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:25.223882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:26.224024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:27.224118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:28.224371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:29.225098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:30.225527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:31.226498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:32.226628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:33.226976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:34.227140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:35.227230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:36.227343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:37.227532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:38.227687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:39.227840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:40.228767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:41.228492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:42.228673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:43.229268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:44.229519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:45.230178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:46.230282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:47.230414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:48.230542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:49.230692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:50.230932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:51.231206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:52.231425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:53.232231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:54.232421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:55.232569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:56.232722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:57.232838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:58.233171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:50:59.233326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:00.234221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/22/23 12:51:00.285
  Jul 22 12:51:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6363" for this suite. @ 07/22/23 12:51:00.301
• [118.138 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/22/23 12:51:00.315
  Jul 22 12:51:00.315: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename endpointslice @ 07/22/23 12:51:00.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:00.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:00.36
  Jul 22 12:51:00.385: INFO: Endpoints addresses: [172.31.24.255 172.31.8.181] , ports: [6443]
  Jul 22 12:51:00.385: INFO: EndpointSlices addresses: [172.31.24.255 172.31.8.181] , ports: [6443]
  Jul 22 12:51:00.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4150" for this suite. @ 07/22/23 12:51:00.391
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/22/23 12:51:00.403
  Jul 22 12:51:00.403: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:51:00.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:00.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:00.439
  STEP: Creating a pod to test downward api env vars @ 07/22/23 12:51:00.451
  E0722 12:51:01.235303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:02.235455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:03.236292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:04.236446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:51:04.487
  Jul 22 12:51:04.493: INFO: Trying to get logs from node ip-172-31-15-55 pod downward-api-be24015d-c92e-406e-a6f8-b84bb679287d container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 12:51:04.517
  Jul 22 12:51:04.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2867" for this suite. @ 07/22/23 12:51:04.549
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/22/23 12:51:04.563
  Jul 22 12:51:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 12:51:04.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:04.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:04.596
  STEP: Creating secret with name secret-test-56dfccdb-e5d4-4680-a804-3b80521ff8a1 @ 07/22/23 12:51:04.633
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:51:04.641
  E0722 12:51:05.238046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:06.238404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:07.238442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:08.238743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:51:08.675
  Jul 22 12:51:08.681: INFO: Trying to get logs from node ip-172-31-26-93 pod pod-secrets-d80cf977-f7d5-47ad-b159-3c9a2a9e38f8 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:51:08.707
  Jul 22 12:51:08.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5397" for this suite. @ 07/22/23 12:51:08.75
  STEP: Destroying namespace "secret-namespace-8776" for this suite. @ 07/22/23 12:51:08.76
• [4.208 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/22/23 12:51:08.773
  Jul 22 12:51:08.773: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/22/23 12:51:08.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:08.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:08.807
  Jul 22 12:51:08.814: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:51:09.239524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6567" for this suite. @ 07/22/23 12:51:09.388
• [0.629 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/22/23 12:51:09.403
  Jul 22 12:51:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 12:51:09.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:09.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:09.43
  Jul 22 12:51:09.453: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0722 12:51:10.239620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:11.240125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:12.240318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:13.240678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:14.241166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:14.460: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 12:51:14.46
  Jul 22 12:51:14.460: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0722 12:51:15.242191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:16.242305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:16.465: INFO: Creating deployment "test-rollover-deployment"
  Jul 22 12:51:16.499: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0722 12:51:17.242465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:18.242591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:18.519: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul 22 12:51:18.535: INFO: Ensure that both replica sets have 1 created replica
  Jul 22 12:51:18.550: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul 22 12:51:18.572: INFO: Updating deployment test-rollover-deployment
  Jul 22 12:51:18.572: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0722 12:51:19.242987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:20.243300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:20.584: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul 22 12:51:20.597: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul 22 12:51:20.609: INFO: all replica sets need to contain the pod-template-hash label
  Jul 22 12:51:20.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:51:21.244121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:22.244503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:22.621: INFO: all replica sets need to contain the pod-template-hash label
  Jul 22 12:51:22.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:51:23.245336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:24.245511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:24.622: INFO: all replica sets need to contain the pod-template-hash label
  Jul 22 12:51:24.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:51:25.245641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:26.245750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:26.622: INFO: all replica sets need to contain the pod-template-hash label
  Jul 22 12:51:26.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:51:27.246097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:28.246248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:28.623: INFO: all replica sets need to contain the pod-template-hash label
  Jul 22 12:51:28.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 12, 51, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 12, 51, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 12:51:29.246337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:30.246492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:30.622: INFO: 
  Jul 22 12:51:30.622: INFO: Ensure that both old replica sets have no replicas
  Jul 22 12:51:30.643: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4892  baae8c33-294c-4726-8bc2-8796096e285a 20310 2 2023-07-22 12:51:16 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-22 12:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e7f238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-22 12:51:16 +0000 UTC,LastTransitionTime:2023-07-22 12:51:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-22 12:51:30 +0000 UTC,LastTransitionTime:2023-07-22 12:51:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 22 12:51:30.653: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-4892  f04a02d2-ee62-43f9-b26a-cb57df1a0fdc 20300 2 2023-07-22 12:51:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment baae8c33-294c-4726-8bc2-8796096e285a 0xc00392a0a7 0xc00392a0a8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baae8c33-294c-4726-8bc2-8796096e285a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:51:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00392a158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:51:30.653: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul 22 12:51:30.654: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4892  c8e858d6-7175-4c8b-a3b3-0d0ffa33dab5 20309 2 2023-07-22 12:51:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment baae8c33-294c-4726-8bc2-8796096e285a 0xc001e7ff77 0xc001e7ff78}] [] [{e2e.test Update apps/v1 2023-07-22 12:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baae8c33-294c-4726-8bc2-8796096e285a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:51:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00392a038 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:51:30.654: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-4892  bb6af127-2a42-4ee0-958d-9802d9c609ba 20261 2 2023-07-22 12:51:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment baae8c33-294c-4726-8bc2-8796096e285a 0xc00392a1c7 0xc00392a1c8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 12:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baae8c33-294c-4726-8bc2-8796096e285a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 12:51:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00392a278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 12:51:30.663: INFO: Pod "test-rollover-deployment-57777854c9-t8zm6" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-t8zm6 test-rollover-deployment-57777854c9- deployment-4892  794ca923-7257-450e-9650-c3fd179e0bdc 20278 0 2023-07-22 12:51:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 f04a02d2-ee62-43f9-b26a-cb57df1a0fdc 0xc00392a847 0xc00392a848}] [] [{kube-controller-manager Update v1 2023-07-22 12:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f04a02d2-ee62-43f9-b26a-cb57df1a0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 12:51:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9848r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9848r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:51:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:51:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:51:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 12:51:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.136,StartTime:2023-07-22 12:51:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 12:51:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0bcfa187efe4213e6ddedb6d9daef739078e7cd3a5bef0d5e14302f0b18bdbd6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.136,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 12:51:30.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4892" for this suite. @ 07/22/23 12:51:30.671
• [21.276 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/22/23 12:51:30.686
  Jul 22 12:51:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename proxy @ 07/22/23 12:51:30.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:30.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:30.721
  Jul 22 12:51:30.727: INFO: Creating pod...
  E0722 12:51:31.246688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:32.247171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:32.761: INFO: Creating service...
  Jul 22 12:51:32.775: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/DELETE
  Jul 22 12:51:32.796: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 22 12:51:32.796: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/GET
  Jul 22 12:51:32.809: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 22 12:51:32.809: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/HEAD
  Jul 22 12:51:32.815: INFO: http.Client request:HEAD | StatusCode:200
  Jul 22 12:51:32.815: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul 22 12:51:32.822: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 22 12:51:32.822: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/PATCH
  Jul 22 12:51:32.829: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 22 12:51:32.829: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/POST
  Jul 22 12:51:32.834: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 22 12:51:32.834: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/pods/agnhost/proxy/some/path/with/PUT
  Jul 22 12:51:32.847: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 22 12:51:32.847: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/DELETE
  Jul 22 12:51:32.854: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 22 12:51:32.854: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/GET
  Jul 22 12:51:32.861: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 22 12:51:32.862: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/HEAD
  Jul 22 12:51:32.871: INFO: http.Client request:HEAD | StatusCode:200
  Jul 22 12:51:32.871: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/OPTIONS
  Jul 22 12:51:32.878: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 22 12:51:32.878: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/PATCH
  Jul 22 12:51:32.890: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 22 12:51:32.890: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/POST
  Jul 22 12:51:32.900: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 22 12:51:32.900: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6109/services/test-service/proxy/some/path/with/PUT
  Jul 22 12:51:32.907: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 22 12:51:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6109" for this suite. @ 07/22/23 12:51:32.914
• [2.239 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/22/23 12:51:32.926
  Jul 22 12:51:32.926: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pod-network-test @ 07/22/23 12:51:32.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:32.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:32.964
  STEP: Performing setup for networking test in namespace pod-network-test-1190 @ 07/22/23 12:51:32.97
  STEP: creating a selector @ 07/22/23 12:51:32.97
  STEP: Creating the service pods in kubernetes @ 07/22/23 12:51:32.97
  Jul 22 12:51:32.971: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0722 12:51:33.247152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:34.247404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:35.247440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:36.247514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:37.247628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:38.247741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:39.248264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:40.248369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:41.248716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:42.249453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:43.250547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:44.250662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/22/23 12:51:45.097
  E0722 12:51:45.250941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:46.251129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:47.153: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 22 12:51:47.154: INFO: Going to poll 192.168.196.137 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 12:51:47.160: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.196.137 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1190 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:51:47.160: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:51:47.161: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:51:47.161: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1190/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.196.137+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0722 12:51:47.251802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:48.252248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:48.275: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 22 12:51:48.275: INFO: Going to poll 192.168.120.56 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 12:51:48.281: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.120.56 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1190 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:51:48.281: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:51:48.283: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:51:48.283: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1190/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.120.56+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0722 12:51:49.252891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:49.377: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 22 12:51:49.377: INFO: Going to poll 192.168.121.221 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 12:51:49.383: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.121.221 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1190 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 12:51:49.383: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 12:51:49.384: INFO: ExecWithOptions: Clientset creation
  Jul 22 12:51:49.384: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1190/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.121.221+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0722 12:51:50.252994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:51:50.471: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 22 12:51:50.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1190" for this suite. @ 07/22/23 12:51:50.479
• [17.564 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/22/23 12:51:50.491
  Jul 22 12:51:50.491: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 12:51:50.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:50.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:50.522
  STEP: Creating a pod to test substitution in container's args @ 07/22/23 12:51:50.528
  E0722 12:51:51.253407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:52.253528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:53.254183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:54.254318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:51:54.562
  Jul 22 12:51:54.567: INFO: Trying to get logs from node ip-172-31-15-55 pod var-expansion-de780606-2384-4213-9d41-173211e7a421 container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 12:51:54.579
  Jul 22 12:51:54.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3350" for this suite. @ 07/22/23 12:51:54.611
• [4.130 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/22/23 12:51:54.622
  Jul 22 12:51:54.622: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption @ 07/22/23 12:51:54.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:54.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:54.656
  STEP: creating the pdb @ 07/22/23 12:51:54.663
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:51:54.673
  STEP: updating the pdb @ 07/22/23 12:51:54.688
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:51:54.704
  E0722 12:51:55.255254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:56.255552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 07/22/23 12:51:56.714
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:51:56.728
  E0722 12:51:57.255599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:51:58.255700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 07/22/23 12:51:58.749
  Jul 22 12:51:58.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9323" for this suite. @ 07/22/23 12:51:58.759
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/22/23 12:51:58.777
  Jul 22 12:51:58.777: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:51:58.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:51:58.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:51:58.807
  STEP: Creating a pod to test downward api env vars @ 07/22/23 12:51:58.813
  E0722 12:51:59.256057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:00.257100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:01.257847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:02.258885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:02.848
  Jul 22 12:52:02.853: INFO: Trying to get logs from node ip-172-31-15-55 pod downward-api-b1ad8122-e0c9-4c10-9830-6b7369be7c15 container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 12:52:02.865
  Jul 22 12:52:02.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2779" for this suite. @ 07/22/23 12:52:02.893
• [4.126 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/22/23 12:52:02.905
  Jul 22 12:52:02.905: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 12:52:02.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:02.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:02.943
  Jul 22 12:52:02.949: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0722 12:52:03.259157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/22/23 12:52:03.972
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/22/23 12:52:03.979
  E0722 12:52:04.260118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/22/23 12:52:04.995
  Jul 22 12:52:05.011: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/22/23 12:52:05.011
  E0722 12:52:05.260252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:52:06.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3275" for this suite. @ 07/22/23 12:52:06.028
• [3.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/22/23 12:52:06.041
  Jul 22 12:52:06.041: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:52:06.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:06.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:06.071
  STEP: creating service nodeport-test with type=NodePort in namespace services-2890 @ 07/22/23 12:52:06.076
  STEP: creating replication controller nodeport-test in namespace services-2890 @ 07/22/23 12:52:06.103
  I0722 12:52:06.116816      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2890, replica count: 2
  E0722 12:52:06.261297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:07.262173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:08.262943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 12:52:09.168223      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 12:52:09.168: INFO: Creating new exec pod
  E0722 12:52:09.263618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:10.263772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:11.264059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:52:12.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-2890 exec execpod977bz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0722 12:52:12.264887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:52:12.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 22 12:52:12.382: INFO: stdout: "nodeport-test-5rw76"
  Jul 22 12:52:12.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-2890 exec execpod977bz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.34 80'
  Jul 22 12:52:12.549: INFO: stderr: "+ nc -v -t -w 2 10.152.183.34 80\n+ echo hostName\nConnection to 10.152.183.34 80 port [tcp/http] succeeded!\n"
  Jul 22 12:52:12.549: INFO: stdout: "nodeport-test-5rw76"
  Jul 22 12:52:12.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-2890 exec execpod977bz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.81.237 30061'
  Jul 22 12:52:12.746: INFO: stderr: "+ nc -v -t -w 2 172.31.81.237 30061\n+ echo hostName\nConnection to 172.31.81.237 30061 port [tcp/*] succeeded!\n"
  Jul 22 12:52:12.746: INFO: stdout: "nodeport-test-5rw76"
  Jul 22 12:52:12.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-2890 exec execpod977bz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.26.93 30061'
  Jul 22 12:52:12.915: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.26.93 30061\nConnection to 172.31.26.93 30061 port [tcp/*] succeeded!\n"
  Jul 22 12:52:12.915: INFO: stdout: "nodeport-test-7ddhs"
  Jul 22 12:52:12.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2890" for this suite. @ 07/22/23 12:52:12.92
• [6.889 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/22/23 12:52:12.931
  Jul 22 12:52:12.931: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:52:12.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:12.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:12.958
  STEP: Creating configMap with name projected-configmap-test-volume-map-84475e23-fe3b-42f9-b538-8c6b29fb889e @ 07/22/23 12:52:12.967
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:52:12.976
  E0722 12:52:13.265017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:14.265276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:15.265285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:16.266363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:17.006
  Jul 22 12:52:17.013: INFO: Trying to get logs from node ip-172-31-26-93 pod pod-projected-configmaps-1a385fb6-1950-4a84-a305-c373268521f5 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:52:17.026
  Jul 22 12:52:17.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1343" for this suite. @ 07/22/23 12:52:17.054
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/22/23 12:52:17.068
  Jul 22 12:52:17.068: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:52:17.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:17.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:17.102
  Jul 22 12:52:17.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8674" for this suite. @ 07/22/23 12:52:17.118
• [0.059 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/22/23 12:52:17.127
  Jul 22 12:52:17.128: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:52:17.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:17.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:17.157
  STEP: Creating a test headless service @ 07/22/23 12:52:17.162
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9945.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9945.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/22/23 12:52:17.175
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9945.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9945.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/22/23 12:52:17.175
  STEP: creating a pod to probe DNS @ 07/22/23 12:52:17.175
  STEP: submitting the pod to kubernetes @ 07/22/23 12:52:17.176
  E0722 12:52:17.266474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:18.266587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/22/23 12:52:19.218
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:52:19.223
  Jul 22 12:52:19.253: INFO: DNS probes using dns-9945/dns-test-3b409406-97c2-49b7-9d87-8008579d93ba succeeded

  Jul 22 12:52:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:52:19.259
  E0722 12:52:19.267066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test headless service @ 07/22/23 12:52:19.283
  STEP: Destroying namespace "dns-9945" for this suite. @ 07/22/23 12:52:19.304
• [2.193 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/22/23 12:52:19.321
  Jul 22 12:52:19.321: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:52:19.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:19.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:19.359
  STEP: Creating secret with name projected-secret-test-823d34df-d100-4d8f-bc17-826af517808a @ 07/22/23 12:52:19.365
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:52:19.379
  E0722 12:52:20.267268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:21.267633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:22.267784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:23.267910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:23.412
  Jul 22 12:52:23.417: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-e1464c22-4214-4948-ae07-2fb0b82aa7e6 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:52:23.432
  Jul 22 12:52:23.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4822" for this suite. @ 07/22/23 12:52:23.463
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/22/23 12:52:23.478
  Jul 22 12:52:23.478: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:52:23.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:23.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:23.509
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:52:23.514
  E0722 12:52:24.268067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:25.268581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:26.269116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:27.269284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:27.547
  Jul 22 12:52:27.552: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-83b6bb68-e859-4b2b-a557-6593f3c7cc30 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:52:27.562
  Jul 22 12:52:27.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9426" for this suite. @ 07/22/23 12:52:27.595
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/22/23 12:52:27.609
  Jul 22 12:52:27.609: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 12:52:27.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:27.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:27.638
  STEP: Starting the proxy @ 07/22/23 12:52:27.644
  Jul 22 12:52:27.645: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7920 proxy --unix-socket=/tmp/kubectl-proxy-unix4245425297/test'
  STEP: retrieving proxy /api/ output @ 07/22/23 12:52:27.718
  Jul 22 12:52:27.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7920" for this suite. @ 07/22/23 12:52:27.726
• [0.128 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/22/23 12:52:27.737
  Jul 22 12:52:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:52:27.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:27.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:27.769
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/22/23 12:52:27.78
  E0722 12:52:28.269421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:29.269481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:30.270201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:31.270311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:31.814
  Jul 22 12:52:31.820: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-71394908-6c81-4659-a7df-776420d2d3b0 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:52:31.83
  Jul 22 12:52:31.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-447" for this suite. @ 07/22/23 12:52:31.871
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/22/23 12:52:31.897
  Jul 22 12:52:31.897: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/22/23 12:52:31.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:31.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:31.932
  STEP: creating @ 07/22/23 12:52:31.938
  STEP: getting @ 07/22/23 12:52:31.982
  STEP: listing in namespace @ 07/22/23 12:52:31.991
  STEP: patching @ 07/22/23 12:52:31.997
  STEP: deleting @ 07/22/23 12:52:32.03
  Jul 22 12:52:32.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3152" for this suite. @ 07/22/23 12:52:32.064
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/22/23 12:52:32.08
  Jul 22 12:52:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 12:52:32.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:32.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:32.114
  STEP: set up a multi version CRD @ 07/22/23 12:52:32.121
  Jul 22 12:52:32.122: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:52:32.270408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:33.271258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:34.272397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:35.272654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:36.273307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 07/22/23 12:52:36.668
  STEP: check the unserved version gets removed @ 07/22/23 12:52:36.699
  E0722 12:52:37.274790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/22/23 12:52:38.234
  E0722 12:52:38.275457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:39.275612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:40.276359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:41.277216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:52:41.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-797" for this suite. @ 07/22/23 12:52:41.451
• [9.383 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/22/23 12:52:41.463
  Jul 22 12:52:41.463: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:52:41.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:41.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:41.493
  STEP: Creating configMap with name configmap-test-volume-map-abe06690-7dd4-4775-ab42-03846e548498 @ 07/22/23 12:52:41.497
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:52:41.505
  E0722 12:52:42.278649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:43.279095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:44.279764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:45.280667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:45.542
  Jul 22 12:52:45.548: INFO: Trying to get logs from node ip-172-31-26-93 pod pod-configmaps-95225d62-d814-48f9-8d35-89ff643b5f50 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:52:45.572
  Jul 22 12:52:45.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7358" for this suite. @ 07/22/23 12:52:45.609
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/22/23 12:52:45.628
  Jul 22 12:52:45.628: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:52:45.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:45.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:45.668
  STEP: Creating configMap with name configmap-projected-all-test-volume-701c6abe-f819-4b87-a286-f5de21ebe9b7 @ 07/22/23 12:52:45.675
  STEP: Creating secret with name secret-projected-all-test-volume-38b208c8-5dce-4ca9-9251-ae498fac46f0 @ 07/22/23 12:52:45.682
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/22/23 12:52:45.688
  E0722 12:52:46.280721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:47.281049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:48.282024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:49.282025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:52:49.732
  Jul 22 12:52:49.737: INFO: Trying to get logs from node ip-172-31-26-93 pod projected-volume-26fb51e7-547e-4516-b572-e677575acde9 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:52:49.747
  Jul 22 12:52:49.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4111" for this suite. @ 07/22/23 12:52:49.786
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/22/23 12:52:49.798
  Jul 22 12:52:49.798: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:52:49.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:52:49.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:52:49.831
  STEP: Setting up server cert @ 07/22/23 12:52:49.877
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:52:50.258
  STEP: Deploying the webhook pod @ 07/22/23 12:52:50.269
  E0722 12:52:50.282187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 07/22/23 12:52:50.295
  Jul 22 12:52:50.308: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 12:52:51.282908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:52.282947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 12:52:52.325
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:52:52.34
  E0722 12:52:53.282964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:52:53.340: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/22/23 12:52:53.345
  STEP: create a pod that should be denied by the webhook @ 07/22/23 12:52:53.37
  STEP: create a pod that causes the webhook to hang @ 07/22/23 12:52:53.393
  E0722 12:52:54.283250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:55.283245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:56.283384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:57.283538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:58.283798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:52:59.283751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:00.283855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:01.284060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:02.284221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:03.284346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 07/22/23 12:53:03.416
  STEP: create a configmap that should be admitted by the webhook @ 07/22/23 12:53:03.436
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/22/23 12:53:03.458
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/22/23 12:53:03.477
  STEP: create a namespace that bypass the webhook @ 07/22/23 12:53:03.491
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/22/23 12:53:03.543
  Jul 22 12:53:03.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4080" for this suite. @ 07/22/23 12:53:03.69
  STEP: Destroying namespace "webhook-markers-7761" for this suite. @ 07/22/23 12:53:03.706
  STEP: Destroying namespace "exempted-namespace-3660" for this suite. @ 07/22/23 12:53:03.719
• [13.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/22/23 12:53:03.742
  Jul 22 12:53:03.742: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 12:53:03.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:03.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:03.785
  Jul 22 12:53:03.803: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: creating the pod @ 07/22/23 12:53:03.804
  STEP: submitting the pod to kubernetes @ 07/22/23 12:53:03.804
  E0722 12:53:04.284625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:05.284846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:05.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2054" for this suite. @ 07/22/23 12:53:05.861
• [2.129 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/22/23 12:53:05.874
  Jul 22 12:53:05.874: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 12:53:05.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:05.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:05.905
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/22/23 12:53:05.914
  E0722 12:53:06.285375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:07.285952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:08.286927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:09.286984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:53:09.946
  Jul 22 12:53:09.952: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-5d0557ed-3c58-4371-913b-f5ed81c307ae container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:53:09.977
  Jul 22 12:53:10.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7593" for this suite. @ 07/22/23 12:53:10.078
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/22/23 12:53:10.094
  Jul 22 12:53:10.094: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:53:10.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:10.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:10.127
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:53:10.132
  E0722 12:53:10.287688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:11.288149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:12.289165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:13.289563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:53:14.175
  Jul 22 12:53:14.181: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-bab674b8-24ca-4eec-89be-547aa0253e84 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:53:14.189
  Jul 22 12:53:14.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7631" for this suite. @ 07/22/23 12:53:14.237
• [4.157 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/22/23 12:53:14.252
  Jul 22 12:53:14.252: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename podtemplate @ 07/22/23 12:53:14.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:14.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:14.288
  E0722 12:53:14.289564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create set of pod templates @ 07/22/23 12:53:14.295
  Jul 22 12:53:14.306: INFO: created test-podtemplate-1
  Jul 22 12:53:14.314: INFO: created test-podtemplate-2
  Jul 22 12:53:14.322: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/22/23 12:53:14.322
  STEP: delete collection of pod templates @ 07/22/23 12:53:14.333
  Jul 22 12:53:14.333: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/22/23 12:53:14.366
  Jul 22 12:53:14.366: INFO: requesting list of pod templates to confirm quantity
  Jul 22 12:53:14.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1484" for this suite. @ 07/22/23 12:53:14.379
• [0.141 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/22/23 12:53:14.393
  Jul 22 12:53:14.393: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 12:53:14.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:14.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:14.433
  STEP: Creating configMap with name configmap-test-volume-ca8fb685-4984-4c97-a188-7b4da3741c74 @ 07/22/23 12:53:14.438
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:53:14.446
  E0722 12:53:15.290181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:16.290795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:17.290920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:18.291136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:53:18.48
  Jul 22 12:53:18.487: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-1332d240-2b2b-48c9-9bbf-ebc4966edee8 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:53:18.496
  Jul 22 12:53:18.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7263" for this suite. @ 07/22/23 12:53:18.532
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/22/23 12:53:18.55
  Jul 22 12:53:18.551: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/22/23 12:53:18.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:18.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:18.588
  STEP: Creating 50 configmaps @ 07/22/23 12:53:18.594
  STEP: Creating RC which spawns configmap-volume pods @ 07/22/23 12:53:19.008
  Jul 22 12:53:19.030: INFO: Pod name wrapped-volume-race-1fc28b98-1589-442e-80ec-5dbf945ed739: Found 0 pods out of 5
  E0722 12:53:19.292033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:20.292141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:21.298128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:22.293252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:23.300515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:24.039: INFO: Pod name wrapped-volume-race-1fc28b98-1589-442e-80ec-5dbf945ed739: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/22/23 12:53:24.039
  STEP: Creating RC which spawns configmap-volume pods @ 07/22/23 12:53:24.094
  Jul 22 12:53:24.117: INFO: Pod name wrapped-volume-race-b7afdd98-b769-4645-a04b-a477c5cd8a0e: Found 0 pods out of 5
  E0722 12:53:24.301429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:25.303587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:26.304348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:27.304867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:28.305133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:29.132: INFO: Pod name wrapped-volume-race-b7afdd98-b769-4645-a04b-a477c5cd8a0e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/22/23 12:53:29.132
  STEP: Creating RC which spawns configmap-volume pods @ 07/22/23 12:53:29.186
  Jul 22 12:53:29.223: INFO: Pod name wrapped-volume-race-77dd5028-822e-42b3-a26a-7ae76650f2ef: Found 0 pods out of 5
  E0722 12:53:29.305699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:30.306567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:31.306716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:32.307456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:33.308035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:34.237: INFO: Pod name wrapped-volume-race-77dd5028-822e-42b3-a26a-7ae76650f2ef: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/22/23 12:53:34.237
  Jul 22 12:53:34.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-77dd5028-822e-42b3-a26a-7ae76650f2ef in namespace emptydir-wrapper-7569, will wait for the garbage collector to delete the pods @ 07/22/23 12:53:34.272
  E0722 12:53:34.308565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:34.344: INFO: Deleting ReplicationController wrapped-volume-race-77dd5028-822e-42b3-a26a-7ae76650f2ef took: 15.588019ms
  Jul 22 12:53:34.445: INFO: Terminating ReplicationController wrapped-volume-race-77dd5028-822e-42b3-a26a-7ae76650f2ef pods took: 100.48639ms
  E0722 12:53:35.309271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:36.310285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-b7afdd98-b769-4645-a04b-a477c5cd8a0e in namespace emptydir-wrapper-7569, will wait for the garbage collector to delete the pods @ 07/22/23 12:53:36.745
  Jul 22 12:53:36.812: INFO: Deleting ReplicationController wrapped-volume-race-b7afdd98-b769-4645-a04b-a477c5cd8a0e took: 10.403137ms
  Jul 22 12:53:36.913: INFO: Terminating ReplicationController wrapped-volume-race-b7afdd98-b769-4645-a04b-a477c5cd8a0e pods took: 100.52877ms
  E0722 12:53:37.310989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:38.311881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:39.312684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-1fc28b98-1589-442e-80ec-5dbf945ed739 in namespace emptydir-wrapper-7569, will wait for the garbage collector to delete the pods @ 07/22/23 12:53:39.613
  Jul 22 12:53:39.682: INFO: Deleting ReplicationController wrapped-volume-race-1fc28b98-1589-442e-80ec-5dbf945ed739 took: 11.50918ms
  Jul 22 12:53:39.782: INFO: Terminating ReplicationController wrapped-volume-race-1fc28b98-1589-442e-80ec-5dbf945ed739 pods took: 100.387109ms
  E0722 12:53:40.313675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:41.314415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:42.314933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 07/22/23 12:53:42.783
  E0722 12:53:43.315816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-7569" for this suite. @ 07/22/23 12:53:43.377
• [24.836 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/22/23 12:53:43.387
  Jul 22 12:53:43.387: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:53:43.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:43.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:43.422
  STEP: Creating configMap with name projected-configmap-test-volume-3088b564-59b7-4a88-a303-d3131862a866 @ 07/22/23 12:53:43.427
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:53:43.437
  E0722 12:53:44.315851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:45.315918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:46.316997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:47.317283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:53:47.473
  Jul 22 12:53:47.479: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-899cb875-dae1-4f3b-96a0-1e386e51615c container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:53:47.489
  Jul 22 12:53:47.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1822" for this suite. @ 07/22/23 12:53:47.519
• [4.144 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/22/23 12:53:47.533
  Jul 22 12:53:47.533: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:53:47.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:47.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:47.575
  STEP: Creating namespace "e2e-ns-qhdsm" @ 07/22/23 12:53:47.581
  Jul 22 12:53:47.609: INFO: Namespace "e2e-ns-qhdsm-9425" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-qhdsm-9425" @ 07/22/23 12:53:47.609
  Jul 22 12:53:47.625: INFO: Namespace "e2e-ns-qhdsm-9425" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-qhdsm-9425" @ 07/22/23 12:53:47.625
  Jul 22 12:53:47.639: INFO: Namespace "e2e-ns-qhdsm-9425" has []v1.FinalizerName{"kubernetes"}
  Jul 22 12:53:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9072" for this suite. @ 07/22/23 12:53:47.646
  STEP: Destroying namespace "e2e-ns-qhdsm-9425" for this suite. @ 07/22/23 12:53:47.657
• [0.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/22/23 12:53:47.676
  Jul 22 12:53:47.676: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-runtime @ 07/22/23 12:53:47.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:47.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:47.709
  STEP: create the container @ 07/22/23 12:53:47.714
  W0722 12:53:47.728596      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/22/23 12:53:47.728
  E0722 12:53:48.317416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:49.317614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:50.317856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:51.318161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/22/23 12:53:51.761
  STEP: the container should be terminated @ 07/22/23 12:53:51.767
  STEP: the termination message should be set @ 07/22/23 12:53:51.767
  Jul 22 12:53:51.767: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/22/23 12:53:51.767
  Jul 22 12:53:51.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-560" for this suite. @ 07/22/23 12:53:51.794
• [4.132 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/22/23 12:53:51.812
  Jul 22 12:53:51.812: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename watch @ 07/22/23 12:53:51.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:51.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:51.859
  STEP: creating a new configmap @ 07/22/23 12:53:51.863
  STEP: modifying the configmap once @ 07/22/23 12:53:51.869
  STEP: modifying the configmap a second time @ 07/22/23 12:53:51.883
  STEP: deleting the configmap @ 07/22/23 12:53:51.894
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/22/23 12:53:51.905
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/22/23 12:53:51.908
  Jul 22 12:53:51.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2701  278ddf61-6392-4149-a7d9-e01220b74edb 22304 0 2023-07-22 12:53:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-22 12:53:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:53:51.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2701  278ddf61-6392-4149-a7d9-e01220b74edb 22305 0 2023-07-22 12:53:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-22 12:53:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 12:53:51.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2701" for this suite. @ 07/22/23 12:53:51.916
• [0.117 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/22/23 12:53:51.927
  Jul 22 12:53:51.927: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 12:53:51.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:51.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:51.958
  STEP: Setting up server cert @ 07/22/23 12:53:52.003
  E0722 12:53:52.318479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 12:53:53.058
  STEP: Deploying the webhook pod @ 07/22/23 12:53:53.071
  STEP: Wait for the deployment to be ready @ 07/22/23 12:53:53.089
  Jul 22 12:53:53.100: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0722 12:53:53.319362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:54.319499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 12:53:55.117
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 12:53:55.135
  E0722 12:53:55.319913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:56.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 22 12:53:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:53:56.320840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3221-crds.webhook.example.com via the AdmissionRegistration API @ 07/22/23 12:53:56.675
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/22/23 12:53:56.702
  E0722 12:53:57.321829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:53:58.321989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:53:58.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 12:53:59.322128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2582" for this suite. @ 07/22/23 12:53:59.392
  STEP: Destroying namespace "webhook-markers-8854" for this suite. @ 07/22/23 12:53:59.402
• [7.490 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/22/23 12:53:59.42
  Jul 22 12:53:59.420: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:53:59.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:53:59.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:53:59.48
  STEP: Creating projection with secret that has name projected-secret-test-414eb59c-f771-457c-ab35-c20677db6005 @ 07/22/23 12:53:59.485
  STEP: Creating a pod to test consume secrets @ 07/22/23 12:53:59.492
  E0722 12:54:00.323101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:01.323364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:02.323556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:03.324063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:04.324264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:05.324716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:54:05.536
  Jul 22 12:54:05.542: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-87a57416-3731-4f5c-8a86-5f85a5fb7e79 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 12:54:05.565
  Jul 22 12:54:05.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4780" for this suite. @ 07/22/23 12:54:05.594
• [6.186 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/22/23 12:54:05.608
  Jul 22 12:54:05.608: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename tables @ 07/22/23 12:54:05.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:05.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:05.639
  Jul 22 12:54:05.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-905" for this suite. @ 07/22/23 12:54:05.662
• [0.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/22/23 12:54:05.673
  Jul 22 12:54:05.673: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/22/23 12:54:05.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:05.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:05.712
  STEP: create the container to handle the HTTPGet hook request. @ 07/22/23 12:54:05.732
  E0722 12:54:06.325528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:07.325599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/22/23 12:54:07.774
  E0722 12:54:08.326242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:09.326545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/22/23 12:54:09.81
  STEP: delete the pod with lifecycle hook @ 07/22/23 12:54:09.823
  E0722 12:54:10.327253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:11.327670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:11.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-189" for this suite. @ 07/22/23 12:54:11.858
• [6.196 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/22/23 12:54:11.871
  Jul 22 12:54:11.871: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename controllerrevisions @ 07/22/23 12:54:11.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:11.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:11.905
  STEP: Creating DaemonSet "e2e-jkfjn-daemon-set" @ 07/22/23 12:54:11.944
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 12:54:11.953
  Jul 22 12:54:11.964: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:11.965: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:11.971: INFO: Number of nodes with available pods controlled by daemonset e2e-jkfjn-daemon-set: 0
  Jul 22 12:54:11.971: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 12:54:12.328692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:12.979: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:12.980: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:12.984: INFO: Number of nodes with available pods controlled by daemonset e2e-jkfjn-daemon-set: 0
  Jul 22 12:54:12.985: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 12:54:13.329372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:13.978: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:13.978: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 12:54:13.985: INFO: Number of nodes with available pods controlled by daemonset e2e-jkfjn-daemon-set: 3
  Jul 22 12:54:13.985: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-jkfjn-daemon-set
  STEP: Confirm DaemonSet "e2e-jkfjn-daemon-set" successfully created with "daemonset-name=e2e-jkfjn-daemon-set" label @ 07/22/23 12:54:13.991
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-jkfjn-daemon-set" @ 07/22/23 12:54:14.002
  Jul 22 12:54:14.006: INFO: Located ControllerRevision: "e2e-jkfjn-daemon-set-788d69ddbb"
  STEP: Patching ControllerRevision "e2e-jkfjn-daemon-set-788d69ddbb" @ 07/22/23 12:54:14.014
  Jul 22 12:54:14.025: INFO: e2e-jkfjn-daemon-set-788d69ddbb has been patched
  STEP: Create a new ControllerRevision @ 07/22/23 12:54:14.025
  Jul 22 12:54:14.032: INFO: Created ControllerRevision: e2e-jkfjn-daemon-set-554dcc99bd
  STEP: Confirm that there are two ControllerRevisions @ 07/22/23 12:54:14.032
  Jul 22 12:54:14.032: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 22 12:54:14.037: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-jkfjn-daemon-set-788d69ddbb" @ 07/22/23 12:54:14.038
  STEP: Confirm that there is only one ControllerRevision @ 07/22/23 12:54:14.047
  Jul 22 12:54:14.047: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 22 12:54:14.052: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-jkfjn-daemon-set-554dcc99bd" @ 07/22/23 12:54:14.058
  Jul 22 12:54:14.071: INFO: e2e-jkfjn-daemon-set-554dcc99bd has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/22/23 12:54:14.071
  W0722 12:54:14.083285      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/22/23 12:54:14.083
  Jul 22 12:54:14.083: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0722 12:54:14.329517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:15.091: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 22 12:54:15.096: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-jkfjn-daemon-set-554dcc99bd=updated" @ 07/22/23 12:54:15.097
  STEP: Confirm that there is only one ControllerRevision @ 07/22/23 12:54:15.11
  Jul 22 12:54:15.110: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 22 12:54:15.116: INFO: Found 1 ControllerRevisions
  Jul 22 12:54:15.123: INFO: ControllerRevision "e2e-jkfjn-daemon-set-89c46d96f" has revision 3
  STEP: Deleting DaemonSet "e2e-jkfjn-daemon-set" @ 07/22/23 12:54:15.13
  STEP: deleting DaemonSet.extensions e2e-jkfjn-daemon-set in namespace controllerrevisions-2803, will wait for the garbage collector to delete the pods @ 07/22/23 12:54:15.13
  Jul 22 12:54:15.199: INFO: Deleting DaemonSet.extensions e2e-jkfjn-daemon-set took: 12.663015ms
  Jul 22 12:54:15.300: INFO: Terminating DaemonSet.extensions e2e-jkfjn-daemon-set pods took: 100.753357ms
  E0722 12:54:15.330793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:16.331401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:16.607: INFO: Number of nodes with available pods controlled by daemonset e2e-jkfjn-daemon-set: 0
  Jul 22 12:54:16.607: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-jkfjn-daemon-set
  Jul 22 12:54:16.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22654"},"items":null}

  Jul 22 12:54:16.624: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22654"},"items":null}

  Jul 22 12:54:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2803" for this suite. @ 07/22/23 12:54:16.686
• [4.826 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/22/23 12:54:16.701
  Jul 22 12:54:16.701: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename events @ 07/22/23 12:54:16.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:16.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:16.784
  STEP: Create set of events @ 07/22/23 12:54:16.791
  STEP: get a list of Events with a label in the current namespace @ 07/22/23 12:54:16.828
  STEP: delete a list of events @ 07/22/23 12:54:16.837
  Jul 22 12:54:16.837: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/22/23 12:54:16.881
  Jul 22 12:54:16.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-956" for this suite. @ 07/22/23 12:54:16.901
• [0.214 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/22/23 12:54:16.916
  Jul 22 12:54:16.916: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption @ 07/22/23 12:54:16.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:16.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:16.962
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:54:16.972
  E0722 12:54:17.331403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:18.331638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 07/22/23 12:54:18.989
  STEP: Waiting for all pods to be running @ 07/22/23 12:54:19.012
  Jul 22 12:54:19.024: INFO: running pods: 0 < 1
  E0722 12:54:19.332297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:20.332771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/22/23 12:54:21.03
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:54:21.053
  STEP: Patching PodDisruptionBudget status @ 07/22/23 12:54:21.064
  STEP: Waiting for the pdb to be processed @ 07/22/23 12:54:21.079
  Jul 22 12:54:21.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7538" for this suite. @ 07/22/23 12:54:21.097
• [4.194 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 07/22/23 12:54:21.111
  Jul 22 12:54:21.111: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 12:54:21.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:21.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:21.151
  Jul 22 12:54:21.157: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 12:54:21.333233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:22.333339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:23.333429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0722 12:54:23.774090      19 warnings.go:70] unknown field "alpha"
  W0722 12:54:23.774111      19 warnings.go:70] unknown field "beta"
  W0722 12:54:23.774117      19 warnings.go:70] unknown field "delta"
  W0722 12:54:23.774122      19 warnings.go:70] unknown field "epsilon"
  W0722 12:54:23.774127      19 warnings.go:70] unknown field "gamma"
  Jul 22 12:54:24.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 12:54:24.334376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-9730" for this suite. @ 07/22/23 12:54:24.359
• [3.262 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/22/23 12:54:24.374
  Jul 22 12:54:24.374: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename containers @ 07/22/23 12:54:24.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:24.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:24.42
  STEP: Creating a pod to test override arguments @ 07/22/23 12:54:24.434
  E0722 12:54:25.336053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:26.335781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:27.335821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:28.336081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:54:28.472
  Jul 22 12:54:28.478: INFO: Trying to get logs from node ip-172-31-26-93 pod client-containers-5245be59-c0c2-4d07-a901-8148d5c6fbdd container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:54:28.507
  Jul 22 12:54:28.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5692" for this suite. @ 07/22/23 12:54:28.541
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/22/23 12:54:28.553
  Jul 22 12:54:28.553: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:54:28.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:28.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:28.599
  STEP: Creating a test namespace @ 07/22/23 12:54:28.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:28.635
  STEP: Creating a pod in the namespace @ 07/22/23 12:54:28.645
  STEP: Waiting for the pod to have running status @ 07/22/23 12:54:28.662
  E0722 12:54:29.336203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:30.336992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 07/22/23 12:54:30.672
  STEP: Waiting for the namespace to be removed. @ 07/22/23 12:54:30.684
  E0722 12:54:31.337853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:32.337985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:33.338114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:34.338298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:35.339074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:36.339282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:37.339383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:38.339846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:39.339955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:40.340426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:41.340618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/22/23 12:54:41.69
  STEP: Verifying there are no pods in the namespace @ 07/22/23 12:54:41.722
  Jul 22 12:54:41.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-545" for this suite. @ 07/22/23 12:54:41.735
  STEP: Destroying namespace "nsdeletetest-3784" for this suite. @ 07/22/23 12:54:41.744
  Jul 22 12:54:41.754: INFO: Namespace nsdeletetest-3784 was already deleted
  STEP: Destroying namespace "nsdeletetest-3307" for this suite. @ 07/22/23 12:54:41.755
• [13.218 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/22/23 12:54:41.773
  Jul 22 12:54:41.774: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename namespaces @ 07/22/23 12:54:41.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:41.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:41.81
  STEP: Creating a test namespace @ 07/22/23 12:54:41.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:41.884
  STEP: Creating a service in the namespace @ 07/22/23 12:54:41.891
  STEP: Deleting the namespace @ 07/22/23 12:54:41.915
  STEP: Waiting for the namespace to be removed. @ 07/22/23 12:54:41.932
  E0722 12:54:42.341249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:43.342159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:44.342261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:45.342304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:46.342459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:47.343389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/22/23 12:54:47.94
  STEP: Verifying there is no service in the namespace @ 07/22/23 12:54:47.975
  Jul 22 12:54:47.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7265" for this suite. @ 07/22/23 12:54:48.009
  STEP: Destroying namespace "nsdeletetest-8410" for this suite. @ 07/22/23 12:54:48.023
  Jul 22 12:54:48.028: INFO: Namespace nsdeletetest-8410 was already deleted
  STEP: Destroying namespace "nsdeletetest-6309" for this suite. @ 07/22/23 12:54:48.028
• [6.270 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/22/23 12:54:48.044
  Jul 22 12:54:48.044: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 12:54:48.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:48.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:48.076
  STEP: Creating ServiceAccount "e2e-sa-2f687"  @ 07/22/23 12:54:48.081
  Jul 22 12:54:48.086: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-2f687"  @ 07/22/23 12:54:48.087
  Jul 22 12:54:48.102: INFO: AutomountServiceAccountToken: true
  Jul 22 12:54:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8831" for this suite. @ 07/22/23 12:54:48.107
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/22/23 12:54:48.123
  Jul 22 12:54:48.123: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 12:54:48.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:48.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:48.165
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1460 @ 07/22/23 12:54:48.172
  STEP: changing the ExternalName service to type=ClusterIP @ 07/22/23 12:54:48.178
  STEP: creating replication controller externalname-service in namespace services-1460 @ 07/22/23 12:54:48.201
  I0722 12:54:48.214532      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1460, replica count: 2
  E0722 12:54:48.344030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:49.344524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:50.344739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 12:54:51.268701      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 12:54:51.268: INFO: Creating new exec pod
  E0722 12:54:51.345491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:52.345674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:53.346790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:54.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-1460 exec execpod8k8h8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0722 12:54:54.347716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:54.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 22 12:54:54.499: INFO: stdout: "externalname-service-kqp66"
  Jul 22 12:54:54.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-1460 exec execpod8k8h8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.190 80'
  Jul 22 12:54:54.751: INFO: stderr: "+ echo+  hostNamenc -v -t -w 2 10.152.183.190 80\n\nConnection to 10.152.183.190 80 port [tcp/http] succeeded!\n"
  Jul 22 12:54:54.751: INFO: stdout: ""
  E0722 12:54:55.348103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:54:55.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-1460 exec execpod8k8h8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.190 80'
  Jul 22 12:54:55.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.190 80\nConnection to 10.152.183.190 80 port [tcp/http] succeeded!\n"
  Jul 22 12:54:55.942: INFO: stdout: "externalname-service-kqp66"
  Jul 22 12:54:55.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 12:54:55.950: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-1460" for this suite. @ 07/22/23 12:54:55.981
• [7.870 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/22/23 12:54:55.994
  Jul 22 12:54:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-runtime @ 07/22/23 12:54:55.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:56.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:56.107
  STEP: create the container @ 07/22/23 12:54:56.11
  W0722 12:54:56.125402      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/22/23 12:54:56.125
  E0722 12:54:56.348165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:57.349032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:54:58.349792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/22/23 12:54:59.15
  STEP: the container should be terminated @ 07/22/23 12:54:59.154
  STEP: the termination message should be set @ 07/22/23 12:54:59.154
  Jul 22 12:54:59.154: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/22/23 12:54:59.154
  Jul 22 12:54:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8251" for this suite. @ 07/22/23 12:54:59.18
• [3.197 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/22/23 12:54:59.191
  Jul 22 12:54:59.191: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:54:59.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:54:59.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:54:59.233
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 12:54:59.239
  E0722 12:54:59.350160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:00.350696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:01.355876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:02.356560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:55:03.272
  Jul 22 12:55:03.279: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-6879da71-8416-4ed4-afac-b16d06580c87 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 12:55:03.291
  Jul 22 12:55:03.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2693" for this suite. @ 07/22/23 12:55:03.351
  E0722 12:55:03.357046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/22/23 12:55:03.367
  Jul 22 12:55:03.367: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context @ 07/22/23 12:55:03.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:55:03.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:55:03.413
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/22/23 12:55:03.419
  E0722 12:55:04.357551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:05.360792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:06.361113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:07.361338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:55:07.452
  Jul 22 12:55:07.458: INFO: Trying to get logs from node ip-172-31-15-55 pod security-context-84d1b9dc-388e-49db-96bb-b93dbeb37b81 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 12:55:07.468
  Jul 22 12:55:07.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-2905" for this suite. @ 07/22/23 12:55:07.495
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/22/23 12:55:07.511
  Jul 22 12:55:07.511: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:55:07.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:55:07.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:55:07.546
  STEP: Counting existing ResourceQuota @ 07/22/23 12:55:07.551
  E0722 12:55:08.361441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:09.361567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:10.362129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:11.362262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:12.362372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/22/23 12:55:12.557
  STEP: Ensuring resource quota status is calculated @ 07/22/23 12:55:12.564
  E0722 12:55:13.363145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:14.363481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 07/22/23 12:55:14.572
  STEP: Creating a NodePort Service @ 07/22/23 12:55:14.603
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/22/23 12:55:14.642
  STEP: Ensuring resource quota status captures service creation @ 07/22/23 12:55:14.678
  E0722 12:55:15.364421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:16.365121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 07/22/23 12:55:16.685
  STEP: Ensuring resource quota status released usage @ 07/22/23 12:55:16.741
  E0722 12:55:17.366227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:18.366340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:55:18.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7831" for this suite. @ 07/22/23 12:55:18.758
• [11.257 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/22/23 12:55:18.77
  Jul 22 12:55:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 12:55:18.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:55:18.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:55:18.805
  STEP: Creating resourceQuota "e2e-rq-status-7w6dr" @ 07/22/23 12:55:18.818
  Jul 22 12:55:18.828: INFO: Resource quota "e2e-rq-status-7w6dr" reports spec: hard cpu limit of 500m
  Jul 22 12:55:18.828: INFO: Resource quota "e2e-rq-status-7w6dr" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-7w6dr" /status @ 07/22/23 12:55:18.828
  STEP: Confirm /status for "e2e-rq-status-7w6dr" resourceQuota via watch @ 07/22/23 12:55:18.843
  Jul 22 12:55:18.845: INFO: observed resourceQuota "e2e-rq-status-7w6dr" in namespace "resourcequota-6854" with hard status: v1.ResourceList(nil)
  Jul 22 12:55:18.845: INFO: Found resourceQuota "e2e-rq-status-7w6dr" in namespace "resourcequota-6854" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 22 12:55:18.845: INFO: ResourceQuota "e2e-rq-status-7w6dr" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/22/23 12:55:18.851
  Jul 22 12:55:18.870: INFO: Resource quota "e2e-rq-status-7w6dr" reports spec: hard cpu limit of 1
  Jul 22 12:55:18.870: INFO: Resource quota "e2e-rq-status-7w6dr" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-7w6dr" /status @ 07/22/23 12:55:18.87
  STEP: Confirm /status for "e2e-rq-status-7w6dr" resourceQuota via watch @ 07/22/23 12:55:18.881
  Jul 22 12:55:18.884: INFO: observed resourceQuota "e2e-rq-status-7w6dr" in namespace "resourcequota-6854" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 22 12:55:18.884: INFO: Found resourceQuota "e2e-rq-status-7w6dr" in namespace "resourcequota-6854" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 22 12:55:18.884: INFO: ResourceQuota "e2e-rq-status-7w6dr" /status was patched
  STEP: Get "e2e-rq-status-7w6dr" /status @ 07/22/23 12:55:18.884
  Jul 22 12:55:18.892: INFO: Resourcequota "e2e-rq-status-7w6dr" reports status: hard cpu of 1
  Jul 22 12:55:18.892: INFO: Resourcequota "e2e-rq-status-7w6dr" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-7w6dr" /status before checking Spec is unchanged @ 07/22/23 12:55:18.903
  Jul 22 12:55:18.915: INFO: Resourcequota "e2e-rq-status-7w6dr" reports status: hard cpu of 2
  Jul 22 12:55:18.915: INFO: Resourcequota "e2e-rq-status-7w6dr" reports status: hard memory of 2Gi
  Jul 22 12:55:18.921: INFO: Found resourceQuota "e2e-rq-status-7w6dr" in namespace "resourcequota-6854" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0722 12:55:19.367041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:20.367113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:21.367323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:22.368387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:23.368787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:24.368959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:25.369317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:26.369503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:27.370216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:28.370360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:29.370543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:30.370916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:31.371079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:32.371219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:33.371349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:34.371473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:35.371511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:36.371736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:37.371917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:38.372059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:39.373048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:40.373202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:41.373268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:42.374171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:43.374409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:44.374566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:45.375209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:46.375540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:47.375952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:48.376347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:49.376639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:50.377569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:51.377693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:52.377838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:53.377984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:54.378705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:55.379293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:56.379476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:57.379712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:58.380634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:55:59.380721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:00.381052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:01.381217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:02.381322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:03.381471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:04.381796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:05.382122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:06.382193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:07.382284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:08.382632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:09.382764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:10.383640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:11.383864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:12.384311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:13.384740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:14.385096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:15.385717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:16.386223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:17.386412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:18.386575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:19.387701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:20.388461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:21.388576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:22.388712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:23.389142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:24.389942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:25.390410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:26.390701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:27.390846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:28.391157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:29.392253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:30.392882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:31.393082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:32.393419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:33.393530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:34.394154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:35.395045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:36.395289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:37.395489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:38.395601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:39.396002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:40.396289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:41.396536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:42.396848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:43.397154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:44.398180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:45.398387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:46.398795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:47.399587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:48.399848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:49.400727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:50.401222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:51.401418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:52.402184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:53.402277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:54.402413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:55.403320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:56.403744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:57.403981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:58.404103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:56:59.404828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:00.405480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:01.405881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:02.406170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:03.406690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:04.406800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:05.407388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:06.407533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:07.407631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:08.407835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:09.408321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:10.408607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:11.408757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:12.409252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:13.409895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:14.410007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:15.410365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:16.410402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:17.410522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:18.410594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:19.411066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:20.411832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:21.412011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:22.412098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:23.412513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:24.412643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:25.413337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:26.414192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:27.414293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:28.414473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:29.414712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:30.415794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:31.415985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:32.416111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:33.416451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:34.417191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:35.417316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:36.422842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:37.423408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:38.423491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:39.424441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:40.424975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:41.425103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:42.426117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:43.426219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:44.426354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:45.428464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:46.431775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:47.431905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:48.432145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:49.433074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:50.433811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:51.433922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:52.434053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:53.435054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:54.435866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:55.436288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:56.436413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:57.436529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:58.436745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:57:59.436854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:00.437872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:01.438062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:02.438727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:03.438821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:04.439750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:05.440394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:06.441577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:07.441651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:08.442095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:09.443056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:10.443141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:11.443423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:12.443875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:13.444744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:14.445477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:15.445847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:16.446421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:17.446712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:18.447642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:19.447781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:20.448760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:21.449115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:22.449284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:23.450133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:24.450256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:25.450748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:26.450801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:27.451751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:28.451993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:29.452181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:30.453074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:31.454166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:32.454370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:33.454594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:34.455108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:35.455499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:36.455758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:37.455972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:38.456188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:39.456778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:40.457316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:41.457490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:42.457666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:43.458308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:44.458502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:45.459511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:46.459634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:47.459766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:48.459921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:49.459987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:50.460417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:51.461493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:52.462150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:53.462263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:54.462402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:55.463397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:56.463506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:57.463606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:58.464156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:58:59.465184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:00.465300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:01.465412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:02.465620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:03.465811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:04.465943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:05.466164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:06.467101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:07.467927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:08.468431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:09.469478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:10.470017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:11.470162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:12.470242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:13.470395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:14.470567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:15.471352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:16.472222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:17.472435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:18.472623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:19.472779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:20.473875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:21.473980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:22.474126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:23.474262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:24.474899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:25.475331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:26.475539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:27.476425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:28.476639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:29.477054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:30.477098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:31.477200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:32.477347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:33.477565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:34.478182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:35.478342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:36.478645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:37.478849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:38.479694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:39.480461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:40.481066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:41.481180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:42.482172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:43.482372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:59:43.932: INFO: ResourceQuota "e2e-rq-status-7w6dr" Spec was unchanged and /status reset
  Jul 22 12:59:43.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6854" for this suite. @ 07/22/23 12:59:43.941
• [265.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/22/23 12:59:43.959
  Jul 22 12:59:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 12:59:43.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:59:43.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:59:43.994
  STEP: Creating a test headless service @ 07/22/23 12:59:43.999
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 38.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.38_tcp@PTR;sleep 1; done
   @ 07/22/23 12:59:44.033
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5654.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 38.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.38_udp@PTR;check="$$(dig +tcp +noall +answer +search 38.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.38_tcp@PTR;sleep 1; done
   @ 07/22/23 12:59:44.033
  STEP: creating a pod to probe DNS @ 07/22/23 12:59:44.033
  STEP: submitting the pod to kubernetes @ 07/22/23 12:59:44.033
  E0722 12:59:44.482723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:45.483500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/22/23 12:59:46.074
  STEP: looking for the results for each expected name from probers @ 07/22/23 12:59:46.085
  Jul 22 12:59:46.093: INFO: Unable to read wheezy_udp@dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.099: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.107: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.113: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.148: INFO: Unable to read jessie_udp@dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.161: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.168: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local from pod dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a: the server could not find the requested resource (get pods dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a)
  Jul 22 12:59:46.199: INFO: Lookups using dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a failed for: [wheezy_udp@dns-test-service.dns-5654.svc.cluster.local wheezy_tcp@dns-test-service.dns-5654.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local jessie_udp@dns-test-service.dns-5654.svc.cluster.local jessie_tcp@dns-test-service.dns-5654.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5654.svc.cluster.local]

  E0722 12:59:46.483714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:47.483974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:48.484327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:49.484740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:50.485244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 12:59:51.315: INFO: DNS probes using dns-5654/dns-test-3a7bf4e9-0e73-4ae0-8d46-1fa287f6d11a succeeded

  Jul 22 12:59:51.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 12:59:51.325
  STEP: deleting the test service @ 07/22/23 12:59:51.349
  STEP: deleting the test headless service @ 07/22/23 12:59:51.39
  STEP: Destroying namespace "dns-5654" for this suite. @ 07/22/23 12:59:51.416
• [7.468 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/22/23 12:59:51.428
  Jul 22 12:59:51.428: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename events @ 07/22/23 12:59:51.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:59:51.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:59:51.47
  STEP: creating a test event @ 07/22/23 12:59:51.476
  E0722 12:59:51.485704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing events in all namespaces @ 07/22/23 12:59:51.488
  STEP: listing events in test namespace @ 07/22/23 12:59:51.499
  STEP: listing events with field selection filtering on source @ 07/22/23 12:59:51.507
  STEP: listing events with field selection filtering on reportingController @ 07/22/23 12:59:51.514
  STEP: getting the test event @ 07/22/23 12:59:51.519
  STEP: patching the test event @ 07/22/23 12:59:51.525
  STEP: getting the test event @ 07/22/23 12:59:51.545
  STEP: updating the test event @ 07/22/23 12:59:51.55
  STEP: getting the test event @ 07/22/23 12:59:51.565
  STEP: deleting the test event @ 07/22/23 12:59:51.571
  STEP: listing events in all namespaces @ 07/22/23 12:59:51.584
  STEP: listing events in test namespace @ 07/22/23 12:59:51.595
  Jul 22 12:59:51.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6424" for this suite. @ 07/22/23 12:59:51.607
• [0.200 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/22/23 12:59:51.629
  Jul 22 12:59:51.629: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 12:59:51.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:59:51.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:59:51.673
  STEP: Creating configMap with name projected-configmap-test-volume-map-2e5dd015-64ef-4823-9664-355b55cb26c0 @ 07/22/23 12:59:51.681
  STEP: Creating a pod to test consume configMaps @ 07/22/23 12:59:51.689
  E0722 12:59:52.486428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:53.487411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:54.487533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:55.488409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 12:59:55.723
  Jul 22 12:59:55.729: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-41b5207e-eee5-49ba-bae2-0debe66fc7f0 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 12:59:55.759
  Jul 22 12:59:55.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6057" for this suite. @ 07/22/23 12:59:55.787
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/22/23 12:59:55.805
  Jul 22 12:59:55.806: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 12:59:55.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:59:55.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:59:55.842
  STEP: create the deployment @ 07/22/23 12:59:55.859
  W0722 12:59:55.869871      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/22/23 12:59:55.87
  STEP: delete the deployment @ 07/22/23 12:59:56.384
  STEP: wait for all rs to be garbage collected @ 07/22/23 12:59:56.395
  STEP: expected 0 pods, got 2 pods @ 07/22/23 12:59:56.419
  E0722 12:59:56.488481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/22/23 12:59:56.94
  W0722 12:59:56.948703      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 12:59:56.949: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 12:59:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1767" for this suite. @ 07/22/23 12:59:56.956
• [1.163 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/22/23 12:59:56.969
  Jul 22 12:59:56.970: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 12:59:56.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 12:59:57.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 12:59:57.011
  STEP: Creating a pod to test downward api env vars @ 07/22/23 12:59:57.018
  E0722 12:59:57.488693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:58.488716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 12:59:59.489179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:00.489342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:00:01.052
  Jul 22 13:00:01.057: INFO: Trying to get logs from node ip-172-31-15-55 pod downward-api-c79eb954-637e-4305-b6ab-d4dad410541b container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 13:00:01.074
  Jul 22 13:00:01.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8947" for this suite. @ 07/22/23 13:00:01.103
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/22/23 13:00:01.125
  Jul 22 13:00:01.125: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 13:00:01.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:01.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:01.168
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/22/23 13:00:01.173
  Jul 22 13:00:01.188: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0722 13:00:01.490192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:02.490649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:03.491421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:04.491514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:05.492434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:06.194: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 13:00:06.194
  STEP: getting scale subresource @ 07/22/23 13:00:06.194
  STEP: updating a scale subresource @ 07/22/23 13:00:06.199
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/22/23 13:00:06.208
  STEP: Patch a scale subresource @ 07/22/23 13:00:06.214
  Jul 22 13:00:06.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4077" for this suite. @ 07/22/23 13:00:06.237
• [5.134 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/22/23 13:00:06.26
  Jul 22 13:00:06.260: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-pred @ 07/22/23 13:00:06.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:06.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:06.338
  Jul 22 13:00:06.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 22 13:00:06.363: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 13:00:06.370: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-55 before test
  Jul 22 13:00:06.384: INFO: default-http-backend-kubernetes-worker-65fc475d49-slc26 from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:29 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.384: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: nginx-ingress-controller-kubernetes-worker-m59lf from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:31 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.384: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: calico-kube-controllers-79b76dbbcc-9vwj9 from kube-system started at 2023-07-22 11:54:47 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.384: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: test-rs-l9dgw from replicaset-4077 started at 2023-07-22 13:00:01 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.384: INFO: 	Container httpd ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:00:06.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 13:00:06.384: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-26-93 before test
  Jul 22 13:00:06.393: INFO: nginx-ingress-controller-kubernetes-worker-9f8fn from ingress-nginx-kubernetes-worker started at 2023-07-22 12:03:49 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.393: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: test-rs-dgtwq from replicaset-4077 started at 2023-07-22 13:00:06 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.393: INFO: 	Container httpd ready: false, restart count 0
  Jul 22 13:00:06.393: INFO: sonobuoy from sonobuoy started at 2023-07-22 12:04:59 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.393: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: sonobuoy-e2e-job-09ff55b4a2944177 from sonobuoy started at 2023-07-22 12:05:02 +0000 UTC (2 container statuses recorded)
  Jul 22 13:00:06.393: INFO: 	Container e2e ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-l8p6k from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:00:06.393: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 13:00:06.393: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-81-237 before test
  Jul 22 13:00:06.422: INFO: nginx-ingress-controller-kubernetes-worker-r6kjp from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:28 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: coredns-5c7f76ccb8-cvjf4 from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container coredns ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: kube-state-metrics-5b95b4459c-pp6nn from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: metrics-server-v0.5.2-6cf8c8b69c-lv5pw from kube-system started at 2023-07-22 11:54:21 +0000 UTC (2 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: dashboard-metrics-scraper-6b8586b5c9-29rq6 from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: kubernetes-dashboard-6869f4cd5f-5xvbw from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-s457l from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:00:06.422: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:00:06.422: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/22/23 13:00:06.422
  E0722 13:00:06.493265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:07.493313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/22/23 13:00:08.469
  STEP: Trying to apply a random label on the found node. @ 07/22/23 13:00:08.486
  E0722 13:00:08.493910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the label kubernetes.io/e2e-6a8b0882-7ed0-4a3b-8273-88f9171a6b9e 42 @ 07/22/23 13:00:08.501
  STEP: Trying to relaunch the pod, now with labels. @ 07/22/23 13:00:08.507
  E0722 13:00:09.494399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:10.495179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-6a8b0882-7ed0-4a3b-8273-88f9171a6b9e off the node ip-172-31-15-55 @ 07/22/23 13:00:10.536
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6a8b0882-7ed0-4a3b-8273-88f9171a6b9e @ 07/22/23 13:00:10.558
  Jul 22 13:00:10.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3388" for this suite. @ 07/22/23 13:00:10.579
• [4.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/22/23 13:00:10.598
  Jul 22 13:00:10.598: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context-test @ 07/22/23 13:00:10.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:10.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:10.639
  E0722 13:00:11.496040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:12.496398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:13.496836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:14.497111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4649" for this suite. @ 07/22/23 13:00:14.695
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/22/23 13:00:14.712
  Jul 22 13:00:14.712: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/22/23 13:00:14.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:14.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:14.754
  E0722 13:00:15.498050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:16.498196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:16.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/22/23 13:00:16.826
  STEP: Cleaning up the configmap @ 07/22/23 13:00:16.837
  STEP: Cleaning up the pod @ 07/22/23 13:00:16.851
  STEP: Destroying namespace "emptydir-wrapper-9102" for this suite. @ 07/22/23 13:00:16.912
• [2.264 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/22/23 13:00:16.977
  Jul 22 13:00:16.977: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:00:16.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:17.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:17.086
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 13:00:17.139
  E0722 13:00:17.498273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:18.498356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:19.498699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:20.500253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:00:21.209
  Jul 22 13:00:21.214: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-486f3a53-e34f-43db-b2d1-2be7bec11462 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 13:00:21.225
  Jul 22 13:00:21.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2437" for this suite. @ 07/22/23 13:00:21.253
• [4.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/22/23 13:00:21.266
  Jul 22 13:00:21.266: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:00:21.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:21.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:21.293
  STEP: Setting up server cert @ 07/22/23 13:00:21.376
  E0722 13:00:21.500474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:00:21.779
  STEP: Deploying the webhook pod @ 07/22/23 13:00:21.789
  STEP: Wait for the deployment to be ready @ 07/22/23 13:00:21.81
  Jul 22 13:00:21.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:00:22.500591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:23.500743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:00:23.839
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:00:23.854
  E0722 13:00:24.500995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:24.854: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/22/23 13:00:24.86
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/22/23 13:00:24.887
  STEP: Creating a configMap that should not be mutated @ 07/22/23 13:00:24.897
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/22/23 13:00:24.914
  STEP: Creating a configMap that should be mutated @ 07/22/23 13:00:24.926
  Jul 22 13:00:24.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5472" for this suite. @ 07/22/23 13:00:25.128
  STEP: Destroying namespace "webhook-markers-5102" for this suite. @ 07/22/23 13:00:25.14
• [3.900 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/22/23 13:00:25.169
  Jul 22 13:00:25.169: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 13:00:25.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:25.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:25.205
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/22/23 13:00:25.218
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/22/23 13:00:25.218
  STEP: creating a pod to probe DNS @ 07/22/23 13:00:25.218
  STEP: submitting the pod to kubernetes @ 07/22/23 13:00:25.218
  E0722 13:00:25.501476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:26.502188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/22/23 13:00:27.257
  STEP: looking for the results for each expected name from probers @ 07/22/23 13:00:27.265
  Jul 22 13:00:27.294: INFO: DNS probes using dns-7279/dns-test-d1daa22c-3e45-479c-94cd-234e2b44dbc4 succeeded

  Jul 22 13:00:27.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:00:27.301
  STEP: Destroying namespace "dns-7279" for this suite. @ 07/22/23 13:00:27.318
• [2.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/22/23 13:00:27.333
  Jul 22 13:00:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:00:27.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:27.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:27.372
  STEP: Creating secret with name secret-test-map-4e455087-2ab7-49be-8685-d123759da89a @ 07/22/23 13:00:27.381
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:00:27.39
  E0722 13:00:27.502292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:28.502615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:29.503506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:30.504372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:00:31.429
  Jul 22 13:00:31.435: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-11648d6f-f83b-4f5a-8576-e5c309c83f56 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:00:31.447
  Jul 22 13:00:31.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9673" for this suite. @ 07/22/23 13:00:31.483
• [4.168 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/22/23 13:00:31.502
  Jul 22 13:00:31.502: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubelet-test @ 07/22/23 13:00:31.503
  E0722 13:00:31.504684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:31.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:31.541
  STEP: Waiting for pod completion @ 07/22/23 13:00:31.572
  E0722 13:00:32.505254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:33.505270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:34.505988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:35.506361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:35.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-502" for this suite. @ 07/22/23 13:00:35.619
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/22/23 13:00:35.629
  Jul 22 13:00:35.629: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 13:00:35.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:35.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:35.672
  E0722 13:00:36.506548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:37.506863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:37.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 13:00:37.712: INFO: Deleting pod "var-expansion-6b5ecdec-fc41-42c1-8dd4-09280491ec29" in namespace "var-expansion-6210"
  Jul 22 13:00:37.724: INFO: Wait up to 5m0s for pod "var-expansion-6b5ecdec-fc41-42c1-8dd4-09280491ec29" to be fully deleted
  E0722 13:00:38.506833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:39.506926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-6210" for this suite. @ 07/22/23 13:00:39.734
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/22/23 13:00:39.75
  Jul 22 13:00:39.750: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 13:00:39.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:39.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:39.788
  STEP: Creating a ResourceQuota @ 07/22/23 13:00:39.803
  STEP: Getting a ResourceQuota @ 07/22/23 13:00:39.812
  STEP: Updating a ResourceQuota @ 07/22/23 13:00:39.818
  STEP: Verifying a ResourceQuota was modified @ 07/22/23 13:00:39.831
  STEP: Deleting a ResourceQuota @ 07/22/23 13:00:39.838
  STEP: Verifying the deleted ResourceQuota @ 07/22/23 13:00:39.847
  Jul 22 13:00:39.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5454" for this suite. @ 07/22/23 13:00:39.859
• [0.125 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/22/23 13:00:39.873
  Jul 22 13:00:39.873: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 13:00:39.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:39.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:39.905
  STEP: set up a multi version CRD @ 07/22/23 13:00:39.909
  Jul 22 13:00:39.909: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:00:40.507846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:41.508215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:42.508450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:43.509386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 07/22/23 13:00:43.984
  STEP: check the new version name is served @ 07/22/23 13:00:44.006
  E0722 13:00:44.509090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 07/22/23 13:00:44.979
  E0722 13:00:45.509566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/22/23 13:00:45.747
  E0722 13:00:46.509782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:47.509995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:48.510534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:48.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7408" for this suite. @ 07/22/23 13:00:48.986
• [9.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/22/23 13:00:48.996
  Jul 22 13:00:48.996: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 13:00:48.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:49.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:49.026
  STEP: Creating a job @ 07/22/23 13:00:49.035
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/22/23 13:00:49.046
  E0722 13:00:49.511049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:50.511871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/22/23 13:00:51.053
  STEP: updating /status @ 07/22/23 13:00:51.064
  STEP: get /status @ 07/22/23 13:00:51.102
  Jul 22 13:00:51.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5736" for this suite. @ 07/22/23 13:00:51.116
• [2.132 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/22/23 13:00:51.129
  Jul 22 13:00:51.129: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 13:00:51.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:51.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:51.156
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 13:00:51.163
  E0722 13:00:51.512691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:52.513396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:53.514411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:54.514632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:00:55.199
  Jul 22 13:00:55.204: INFO: Trying to get logs from node ip-172-31-26-93 pod downwardapi-volume-43f5d40f-5bf0-4241-8aff-032b2faecaa1 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 13:00:55.232
  Jul 22 13:00:55.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9193" for this suite. @ 07/22/23 13:00:55.265
• [4.145 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/22/23 13:00:55.275
  Jul 22 13:00:55.275: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:00:55.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:00:55.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:00:55.308
  STEP: Creating pod test-grpc-da24cafe-b2da-4a06-8e96-23780c9da49e in namespace container-probe-1822 @ 07/22/23 13:00:55.315
  E0722 13:00:55.515109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:56.515240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:00:57.350: INFO: Started pod test-grpc-da24cafe-b2da-4a06-8e96-23780c9da49e in namespace container-probe-1822
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 13:00:57.35
  Jul 22 13:00:57.355: INFO: Initial restart count of pod test-grpc-da24cafe-b2da-4a06-8e96-23780c9da49e is 0
  E0722 13:00:57.515887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:58.516522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:00:59.517554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:00.518536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:01.519152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:02.520110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:03.520171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:04.520804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:05.521543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:06.521940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:07.523046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:08.523399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:09.523759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:10.524409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:11.524699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:12.525032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:13.525207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:14.525366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:15.525946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:16.526197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:17.526753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:18.526883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:19.526964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:20.529679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:21.530356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:22.530452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:23.530674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:24.531172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:25.531465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:26.531648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:27.532247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:28.532377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:29.532402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:30.533176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:31.533271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:32.534177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:33.535280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:34.537097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:35.538246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:36.538610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:37.539055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:38.539185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:39.540035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:40.540606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:41.541588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:42.541672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:43.542606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:44.542834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:45.543431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:46.544124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:47.544552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:48.544663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:49.545574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:50.546203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:51.546812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:52.547120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:53.547230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:54.547310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:55.547903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:56.548043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:57.548273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:58.548430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:01:59.548610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:00.549285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:01.549528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:02.550600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:03.550815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:04.550936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:05.551450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:06.551818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:07.552227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:08.552643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:09.552724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:10.553417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:11.553513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:12.553809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:13.553911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:14.554089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:15.554497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:16.554626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:17.554857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:18.554974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:19.555054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:20.555259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:21.555481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:22.556184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:23.556797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:24.557102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:25.557458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:26.557580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:27.557689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:28.557973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:29.558364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:30.558482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:31.558614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:32.559508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:33.559610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:34.559993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:35.560545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:36.561685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:37.561771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:38.562426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:39.562756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:40.562813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:41.562921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:42.563150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:43.563167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:44.563296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:45.563409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:46.564714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:47.564798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:48.565250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:49.565327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:50.565427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:51.566138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:52.566182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:53.566365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:54.566884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:55.566988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:56.568093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:57.568203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:58.568360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:02:59.568493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:00.569073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:01.569216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:02.570197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:03.570627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:04.571616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:05.572200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:06.572281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:07.573096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:08.573850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:09.574108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:10.575038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:11.575433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:12.576046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:13.576431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:14.577430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:15.577948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:16.578262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:17.578424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:18.579417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:19.579621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:20.580505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:21.581045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:22.582027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:23.582086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:24.582229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:25.582429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:26.582659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:27.582720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:28.583750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:29.583773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:30.584741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:31.585506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:32.585714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:33.586403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:34.586699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:35.587490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:36.587862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:37.588042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:38.589016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:39.589101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:40.590086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:41.590181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:42.591120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:43.591277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:44.591368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:45.592152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:46.592348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:47.592880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:48.593815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:49.594029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:50.594154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:51.594250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:52.594963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:53.595095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:54.596070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:55.596864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:56.596991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:57.597106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:58.598173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:03:59.598551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:00.599589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:01.601094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:02.601080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:03.601210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:04.601870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:05.602421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:06.602512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:07.602731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:08.603486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:09.603647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:10.604632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:11.604739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:12.605004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:13.605081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:14.605199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:15.605519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:16.607205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:17.607323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:18.607448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:19.607602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:20.607948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:21.608071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:22.608202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:23.608329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:24.608465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:25.609478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:26.610424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:27.610738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:28.610886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:29.610989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:30.611742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:31.611852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:32.612657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:33.613120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:34.613175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:35.613924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:36.615196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:37.615794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:38.616796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:39.617080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:40.618125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:41.618242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:42.619022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:43.619167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:44.619954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:45.621106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:46.621796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:47.621898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:48.622033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:49.622142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:50.622588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:51.623252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:52.623381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:53.623632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:54.624415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:55.624494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:56.624791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:57.625181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:04:58.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:04:58.217
  STEP: Destroying namespace "container-probe-1822" for this suite. @ 07/22/23 13:04:58.235
• [242.974 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/22/23 13:04:58.252
  Jul 22 13:04:58.252: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename init-container @ 07/22/23 13:04:58.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:04:58.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:04:58.287
  STEP: creating the pod @ 07/22/23 13:04:58.291
  Jul 22 13:04:58.291: INFO: PodSpec: initContainers in spec.initContainers
  E0722 13:04:58.625660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:04:59.625808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:00.626510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:01.626712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:02.627031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:03.631593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:04.631712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:05.632333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:06.632489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:07.632529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:08.632971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:09.633096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:10.633864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:11.633986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:12.634115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:13.634260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:14.634614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:15.634568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:16.634796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:17.635172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:18.635309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:19.635511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:20.636450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:21.636718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:22.636842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:23.637046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:24.637139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:25.637259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:26.637458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:27.637822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:28.637959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:29.638127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:30.638764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:31.639069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:32.639172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:33.639310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:34.639511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:35.639614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:36.639744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:37.639842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:38.640086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:39.640225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:40.640699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:05:41.183: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7a648a82-fe2f-46a1-99fb-5165995dfb81", GenerateName:"", Namespace:"init-container-7677", SelfLink:"", UID:"dc1d9172-826a-4205-89d7-7fe2743496e7", ResourceVersion:"25421", Generation:0, CreationTimestamp:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"291742976"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c56018), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 22, 13, 5, 41, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c56048), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ttj5k", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00463eb00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ttj5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ttj5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ttj5k", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0038e24b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-15-55", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003d8aa10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038e2540)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038e2560)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0038e2568), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0038e256c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010f10d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.15.55", PodIP:"192.168.196.181", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.196.181"}}, StartTime:time.Date(2023, time.July, 22, 13, 4, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d8aaf0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003d8ab60)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://259aa55e8b4e76c22632865d707ba9010641f0ac8a94b29147a37345321149ac", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00463eb80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00463eb60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0038e25ef), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul 22 13:05:41.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7677" for this suite. @ 07/22/23 13:05:41.19
• [42.946 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/22/23 13:05:41.198
  Jul 22 13:05:41.198: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubelet-test @ 07/22/23 13:05:41.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:05:41.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:05:41.229
  Jul 22 13:05:41.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3830" for this suite. @ 07/22/23 13:05:41.275
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/22/23 13:05:41.291
  Jul 22 13:05:41.291: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:05:41.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:05:41.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:05:41.325
  STEP: Setting up server cert @ 07/22/23 13:05:41.378
  E0722 13:05:41.642982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:42.643424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:05:42.862
  STEP: Deploying the webhook pod @ 07/22/23 13:05:42.873
  STEP: Wait for the deployment to be ready @ 07/22/23 13:05:42.894
  Jul 22 13:05:42.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:05:43.643520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:44.643734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:05:44.921
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:05:44.938
  E0722 13:05:45.644712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:05:45.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/22/23 13:05:45.945
  STEP: create a pod that should be updated by the webhook @ 07/22/23 13:05:45.965
  Jul 22 13:05:45.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9112" for this suite. @ 07/22/23 13:05:46.085
  STEP: Destroying namespace "webhook-markers-3455" for this suite. @ 07/22/23 13:05:46.098
• [4.818 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/22/23 13:05:46.13
  Jul 22 13:05:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:05:46.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:05:46.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:05:46.162
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/22/23 13:05:46.167
  E0722 13:05:46.644862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:47.645094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:48.645744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:49.646176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:05:50.199
  Jul 22 13:05:50.205: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-2654ca57-3875-42aa-ae5d-597a6afb0b74 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:05:50.232
  Jul 22 13:05:50.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4877" for this suite. @ 07/22/23 13:05:50.257
• [4.137 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/22/23 13:05:50.268
  Jul 22 13:05:50.268: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename runtimeclass @ 07/22/23 13:05:50.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:05:50.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:05:50.301
  Jul 22 13:05:50.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6834" for this suite. @ 07/22/23 13:05:50.366
• [0.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/22/23 13:05:50.377
  Jul 22 13:05:50.377: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pod-network-test @ 07/22/23 13:05:50.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:05:50.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:05:50.407
  STEP: Performing setup for networking test in namespace pod-network-test-1183 @ 07/22/23 13:05:50.413
  STEP: creating a selector @ 07/22/23 13:05:50.413
  STEP: Creating the service pods in kubernetes @ 07/22/23 13:05:50.413
  Jul 22 13:05:50.414: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0722 13:05:50.647273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:51.647536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:52.648424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:53.648676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:54.648768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:55.649593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:56.650158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:57.650312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:58.650434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:05:59.650535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:00.651533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:01.651650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:02.651881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:03.651986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:04.652738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:05.653275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:06.653488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:07.653633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:08.653732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:09.653841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:10.654049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:11.655424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/22/23 13:06:12.578
  E0722 13:06:12.654422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:13.654926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:06:14.606: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 22 13:06:14.606: INFO: Breadth first check of 192.168.196.182 on host 172.31.15.55...
  Jul 22 13:06:14.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.185:9080/dial?request=hostname&protocol=http&host=192.168.196.182&port=8083&tries=1'] Namespace:pod-network-test-1183 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:06:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:06:14.611: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:06:14.611: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1183/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.185%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.196.182%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0722 13:06:14.655129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:06:14.702: INFO: Waiting for responses: map[]
  Jul 22 13:06:14.702: INFO: reached 192.168.196.182 after 0/1 tries
  Jul 22 13:06:14.702: INFO: Breadth first check of 192.168.120.15 on host 172.31.26.93...
  Jul 22 13:06:14.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.185:9080/dial?request=hostname&protocol=http&host=192.168.120.15&port=8083&tries=1'] Namespace:pod-network-test-1183 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:06:14.707: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:06:14.708: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:06:14.708: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1183/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.185%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.120.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 22 13:06:14.791: INFO: Waiting for responses: map[]
  Jul 22 13:06:14.791: INFO: reached 192.168.120.15 after 0/1 tries
  Jul 22 13:06:14.791: INFO: Breadth first check of 192.168.121.231 on host 172.31.81.237...
  Jul 22 13:06:14.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.196.185:9080/dial?request=hostname&protocol=http&host=192.168.121.231&port=8083&tries=1'] Namespace:pod-network-test-1183 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:06:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:06:14.797: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:06:14.797: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1183/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.196.185%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.121.231%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 22 13:06:14.888: INFO: Waiting for responses: map[]
  Jul 22 13:06:14.888: INFO: reached 192.168.121.231 after 0/1 tries
  Jul 22 13:06:14.888: INFO: Going to retry 0 out of 3 pods....
  Jul 22 13:06:14.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1183" for this suite. @ 07/22/23 13:06:14.895
• [24.529 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/22/23 13:06:14.907
  Jul 22 13:06:14.907: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:06:14.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:06:14.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:06:14.939
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/22/23 13:06:14.944
  E0722 13:06:15.655260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:16.655853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:17.655959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:18.656147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:06:18.979
  Jul 22 13:06:18.984: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-c42dab50-4265-447a-983b-8fa4f282b0cc container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:06:18.995
  Jul 22 13:06:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1983" for this suite. @ 07/22/23 13:06:19.03
• [4.134 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/22/23 13:06:19.042
  Jul 22 13:06:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:06:19.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:06:19.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:06:19.068
  STEP: starting the proxy server @ 07/22/23 13:06:19.078
  Jul 22 13:06:19.078: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-6472 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/22/23 13:06:19.146
  Jul 22 13:06:19.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6472" for this suite. @ 07/22/23 13:06:19.171
• [0.140 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/22/23 13:06:19.184
  Jul 22 13:06:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 13:06:19.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:06:19.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:06:19.236
  STEP: Creating a pod to test downward api env vars @ 07/22/23 13:06:19.246
  E0722 13:06:19.657119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:20.657765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:21.658205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:22.658475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:06:23.286
  Jul 22 13:06:23.291: INFO: Trying to get logs from node ip-172-31-15-55 pod downward-api-45b24021-59dd-4692-b3b5-59418264f91a container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 13:06:23.302
  Jul 22 13:06:23.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3475" for this suite. @ 07/22/23 13:06:23.333
• [4.160 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/22/23 13:06:23.345
  Jul 22 13:06:23.345: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:06:23.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:06:23.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:06:23.374
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 13:06:23.38
  E0722 13:06:23.659597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:24.659759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:25.660429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:26.660613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:06:27.421
  Jul 22 13:06:27.431: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-6fdb4950-c564-4479-8669-9a3a9e65fcf1 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 13:06:27.442
  Jul 22 13:06:27.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-385" for this suite. @ 07/22/23 13:06:27.47
• [4.136 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/22/23 13:06:27.482
  Jul 22 13:06:27.482: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:06:27.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:06:27.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:06:27.515
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-5f40512d-a3e4-49b8-8f47-a5097f1ae90a @ 07/22/23 13:06:27.532
  STEP: Creating the pod @ 07/22/23 13:06:27.542
  E0722 13:06:27.660841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:28.661122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-5f40512d-a3e4-49b8-8f47-a5097f1ae90a @ 07/22/23 13:06:29.586
  STEP: waiting to observe update in volume @ 07/22/23 13:06:29.596
  E0722 13:06:29.661806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:30.662150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:31.663065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:32.663227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:33.664215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:34.664227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:35.665328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:36.665472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:37.665827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:38.666015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:39.666103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:40.667067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:41.667668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:42.667813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:43.667937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:44.668514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:45.669413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:46.669558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:47.669702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:48.670164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:49.671180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:50.671663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:51.671919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:52.672195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:53.672407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:54.673065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:55.673715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:56.674456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:57.674691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:58.674874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:06:59.675032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:00.675057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:01.675102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:02.675303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:03.675572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:04.675681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:05.675781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:06.676841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:07.677001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:08.677218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:09.677374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:10.678210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:11.678326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:12.679387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:13.679509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:14.680076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:15.681047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:16.681377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:17.682248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:18.682332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:19.682364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:20.683088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:21.683441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:22.684475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:23.684555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:24.684665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:25.685552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:26.685890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:27.685971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:28.686110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:29.686282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:30.686352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:31.686469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:32.686660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:33.686833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:34.687032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:35.687613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:36.688742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:37.689048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:38.689958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:39.690305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:40.691286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:41.691507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:42.692069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:43.692372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:44.692885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:45.693530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:46.694272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:47.694616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:48.694741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:49.694983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:50.695975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:51.696087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:52.697137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:53.698268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:54.699138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:55.699226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:56.699328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:57.699453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:07:58.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8887" for this suite. @ 07/22/23 13:07:58.196
• [90.725 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/22/23 13:07:58.212
  Jul 22 13:07:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:07:58.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:07:58.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:07:58.242
  E0722 13:07:58.700256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:07:59.700750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:00.701501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:01.702381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:02.702708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:03.702698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:04.702727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:05.703093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:06.704173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:07.704633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:08.705227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:09.706129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:10.707143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:11.707697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:12.707708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:13.708266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:14.708402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:15.709531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:16.709626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:17.710531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:18.711146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:19.711636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:20.712467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:21.713272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:22.713917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:23.714041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:24.714659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:25.715764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:26.716283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:27.716386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:28.716414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:29.717436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:30.717504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:31.718530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:32.719174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:33.719832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:34.720613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:35.720834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:36.721464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:37.721827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:38.722347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:39.723687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:40.724075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:41.725096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:42.725571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:43.726337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:44.727263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:45.727900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:46.729588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:47.729845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:48.730386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:49.731946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:50.732449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:51.733032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:52.733161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:53.733537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:54.734550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:55.735362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:56.735950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:57.736487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:08:58.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9427" for this suite. @ 07/22/23 13:08:58.282
• [60.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/22/23 13:08:58.295
  Jul 22 13:08:58.295: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:08:58.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:08:58.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:08:58.333
  STEP: Creating projection with secret that has name projected-secret-test-8c86d5ff-c6d4-4501-a737-0d3a25846db5 @ 07/22/23 13:08:58.337
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:08:58.344
  E0722 13:08:58.737110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:08:59.737233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:00.738050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:01.738213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:09:02.381
  Jul 22 13:09:02.392: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-4c1587ca-7254-4b15-af83-a79021071d38 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:09:02.404
  Jul 22 13:09:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8168" for this suite. @ 07/22/23 13:09:02.444
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/22/23 13:09:02.465
  Jul 22 13:09:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 13:09:02.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:02.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:02.496
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/22/23 13:09:02.503
  E0722 13:09:02.738805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:03.738987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 07/22/23 13:09:04.538
  STEP: Then the orphan pod is adopted @ 07/22/23 13:09:04.547
  E0722 13:09:04.739155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 07/22/23 13:09:05.559
  Jul 22 13:09:05.564: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/22/23 13:09:05.586
  E0722 13:09:05.740170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:06.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-968" for this suite. @ 07/22/23 13:09:06.602
• [4.148 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/22/23 13:09:06.614
  Jul 22 13:09:06.614: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 13:09:06.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:06.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:06.645
  Jul 22 13:09:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:09:06.740703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:07.740734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/22/23 13:09:08.533
  Jul 22 13:09:08.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-8272 --namespace=crd-publish-openapi-8272 create -f -'
  E0722 13:09:08.741142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:09.587: INFO: stderr: ""
  Jul 22 13:09:09.587: INFO: stdout: "e2e-test-crd-publish-openapi-5433-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 22 13:09:09.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-8272 --namespace=crd-publish-openapi-8272 delete e2e-test-crd-publish-openapi-5433-crds test-cr'
  Jul 22 13:09:09.690: INFO: stderr: ""
  Jul 22 13:09:09.690: INFO: stdout: "e2e-test-crd-publish-openapi-5433-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul 22 13:09:09.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-8272 --namespace=crd-publish-openapi-8272 apply -f -'
  E0722 13:09:09.741383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:10.496: INFO: stderr: ""
  Jul 22 13:09:10.496: INFO: stdout: "e2e-test-crd-publish-openapi-5433-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 22 13:09:10.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-8272 --namespace=crd-publish-openapi-8272 delete e2e-test-crd-publish-openapi-5433-crds test-cr'
  Jul 22 13:09:10.602: INFO: stderr: ""
  Jul 22 13:09:10.602: INFO: stdout: "e2e-test-crd-publish-openapi-5433-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/22/23 13:09:10.602
  Jul 22 13:09:10.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-8272 explain e2e-test-crd-publish-openapi-5433-crds'
  E0722 13:09:10.742368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:10.931: INFO: stderr: ""
  Jul 22 13:09:10.931: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5433-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0722 13:09:11.742613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:12.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8272" for this suite. @ 07/22/23 13:09:12.475
• [5.871 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/22/23 13:09:12.493
  Jul 22 13:09:12.493: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename disruption @ 07/22/23 13:09:12.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:12.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:12.528
  STEP: Waiting for the pdb to be processed @ 07/22/23 13:09:12.54
  E0722 13:09:12.743082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:13.743345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 07/22/23 13:09:14.593
  Jul 22 13:09:14.600: INFO: running pods: 0 < 3
  E0722 13:09:14.744305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:15.744780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:16.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5859" for this suite. @ 07/22/23 13:09:16.625
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/22/23 13:09:16.639
  Jul 22 13:09:16.639: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:09:16.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:16.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:16.679
  STEP: Creating configMap with name projected-configmap-test-volume-894055dc-8ff3-4015-b93f-a68012910bfd @ 07/22/23 13:09:16.683
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:09:16.691
  E0722 13:09:16.745156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:17.745257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:18.745868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:19.746139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:09:20.731
  Jul 22 13:09:20.738: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-03dc36f5-17f1-4ff7-85a7-53a2b0f6c82a container agnhost-container: <nil>
  E0722 13:09:20.746730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 07/22/23 13:09:20.764
  Jul 22 13:09:20.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9327" for this suite. @ 07/22/23 13:09:20.796
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/22/23 13:09:20.813
  Jul 22 13:09:20.813: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/22/23 13:09:20.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:20.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:20.851
  STEP: Setting up the test @ 07/22/23 13:09:20.856
  STEP: Creating hostNetwork=false pod @ 07/22/23 13:09:20.857
  E0722 13:09:21.747211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:22.747762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/22/23 13:09:22.894
  E0722 13:09:23.748545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:24.748762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/22/23 13:09:24.923
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/22/23 13:09:24.923
  Jul 22 13:09:24.923: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:24.923: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:24.925: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:24.925: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 22 13:09:25.004: INFO: Exec stderr: ""
  Jul 22 13:09:25.004: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.004: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.005: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.005: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 22 13:09:25.099: INFO: Exec stderr: ""
  Jul 22 13:09:25.099: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.099: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.100: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.100: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 22 13:09:25.176: INFO: Exec stderr: ""
  Jul 22 13:09:25.176: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.176: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.176: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.176: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 22 13:09:25.255: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/22/23 13:09:25.255
  Jul 22 13:09:25.255: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.255: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.256: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.256: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 22 13:09:25.335: INFO: Exec stderr: ""
  Jul 22 13:09:25.335: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.335: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.335: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.336: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 22 13:09:25.411: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/22/23 13:09:25.412
  Jul 22 13:09:25.412: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.412: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.413: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.413: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 22 13:09:25.500: INFO: Exec stderr: ""
  Jul 22 13:09:25.501: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.501: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.502: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.502: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 22 13:09:25.605: INFO: Exec stderr: ""
  Jul 22 13:09:25.605: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.606: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.606: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 22 13:09:25.688: INFO: Exec stderr: ""
  Jul 22 13:09:25.688: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2133 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:09:25.688: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:09:25.689: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:09:25.689: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2133/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0722 13:09:25.748715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:25.770: INFO: Exec stderr: ""
  Jul 22 13:09:25.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2133" for this suite. @ 07/22/23 13:09:25.778
• [4.975 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/22/23 13:09:25.789
  Jul 22 13:09:25.789: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:09:25.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:25.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:25.819
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/22/23 13:09:25.826
  E0722 13:09:26.749693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:27.750224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:28.750430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:29.750751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:09:29.858
  Jul 22 13:09:29.862: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-ba87ddc1-1cac-4db3-8af3-367687a04488 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:09:29.872
  Jul 22 13:09:29.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3366" for this suite. @ 07/22/23 13:09:29.903
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/22/23 13:09:29.916
  Jul 22 13:09:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svc-latency @ 07/22/23 13:09:29.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:29.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:29.962
  Jul 22 13:09:29.968: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-4113 @ 07/22/23 13:09:29.969
  I0722 13:09:29.979273      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4113, replica count: 1
  E0722 13:09:30.751092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:09:31.030553      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0722 13:09:31.751347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:09:32.031756      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 13:09:32.145: INFO: Created: latency-svc-lfskz
  Jul 22 13:09:32.154: INFO: Got endpoints: latency-svc-lfskz [22.182083ms]
  Jul 22 13:09:32.178: INFO: Created: latency-svc-x7664
  Jul 22 13:09:32.184: INFO: Got endpoints: latency-svc-x7664 [30.148365ms]
  Jul 22 13:09:32.202: INFO: Created: latency-svc-wt98v
  Jul 22 13:09:32.203: INFO: Created: latency-svc-nm45j
  Jul 22 13:09:32.203: INFO: Got endpoints: latency-svc-wt98v [48.778923ms]
  Jul 22 13:09:32.208: INFO: Got endpoints: latency-svc-nm45j [53.726306ms]
  Jul 22 13:09:32.210: INFO: Created: latency-svc-zfk4d
  Jul 22 13:09:32.309: INFO: Got endpoints: latency-svc-zfk4d [154.542304ms]
  Jul 22 13:09:32.323: INFO: Created: latency-svc-qxtcc
  Jul 22 13:09:32.323: INFO: Got endpoints: latency-svc-qxtcc [167.162305ms]
  Jul 22 13:09:32.344: INFO: Created: latency-svc-7pzgz
  Jul 22 13:09:32.346: INFO: Got endpoints: latency-svc-7pzgz [189.426819ms]
  Jul 22 13:09:32.369: INFO: Created: latency-svc-4fcxg
  Jul 22 13:09:32.369: INFO: Got endpoints: latency-svc-4fcxg [214.61673ms]
  Jul 22 13:09:32.381: INFO: Created: latency-svc-cnxsq
  Jul 22 13:09:32.384: INFO: Got endpoints: latency-svc-cnxsq [226.842967ms]
  Jul 22 13:09:32.394: INFO: Created: latency-svc-xr6kd
  Jul 22 13:09:32.398: INFO: Got endpoints: latency-svc-xr6kd [236.707233ms]
  Jul 22 13:09:32.406: INFO: Created: latency-svc-zz4zb
  Jul 22 13:09:32.411: INFO: Created: latency-svc-9dsxm
  Jul 22 13:09:32.411: INFO: Got endpoints: latency-svc-zz4zb [250.730033ms]
  Jul 22 13:09:32.432: INFO: Got endpoints: latency-svc-9dsxm [275.274655ms]
  Jul 22 13:09:32.434: INFO: Created: latency-svc-sd76c
  Jul 22 13:09:32.435: INFO: Created: latency-svc-lh6xm
  Jul 22 13:09:32.436: INFO: Got endpoints: latency-svc-sd76c [275.198714ms]
  Jul 22 13:09:32.442: INFO: Created: latency-svc-b49dg
  Jul 22 13:09:32.450: INFO: Got endpoints: latency-svc-lh6xm [290.656742ms]
  Jul 22 13:09:32.457: INFO: Got endpoints: latency-svc-b49dg [297.401258ms]
  Jul 22 13:09:32.463: INFO: Created: latency-svc-cz8fx
  Jul 22 13:09:32.470: INFO: Got endpoints: latency-svc-cz8fx [311.553419ms]
  Jul 22 13:09:32.471: INFO: Created: latency-svc-pwnjt
  Jul 22 13:09:32.478: INFO: Created: latency-svc-zhjm5
  Jul 22 13:09:32.483: INFO: Got endpoints: latency-svc-pwnjt [298.775885ms]
  Jul 22 13:09:32.493: INFO: Created: latency-svc-rqr4c
  Jul 22 13:09:32.498: INFO: Got endpoints: latency-svc-zhjm5 [294.047525ms]
  Jul 22 13:09:32.503: INFO: Created: latency-svc-swl76
  Jul 22 13:09:32.507: INFO: Got endpoints: latency-svc-rqr4c [297.47127ms]
  Jul 22 13:09:32.512: INFO: Created: latency-svc-sv8vq
  Jul 22 13:09:32.513: INFO: Got endpoints: latency-svc-swl76 [202.75502ms]
  Jul 22 13:09:32.520: INFO: Got endpoints: latency-svc-sv8vq [196.753733ms]
  Jul 22 13:09:32.523: INFO: Created: latency-svc-p4zf4
  Jul 22 13:09:32.533: INFO: Got endpoints: latency-svc-p4zf4 [187.320693ms]
  Jul 22 13:09:32.537: INFO: Created: latency-svc-z529g
  Jul 22 13:09:32.545: INFO: Created: latency-svc-bngrw
  Jul 22 13:09:32.546: INFO: Got endpoints: latency-svc-z529g [176.799738ms]
  Jul 22 13:09:32.555: INFO: Got endpoints: latency-svc-bngrw [171.118135ms]
  Jul 22 13:09:32.561: INFO: Created: latency-svc-rs8ck
  Jul 22 13:09:32.569: INFO: Created: latency-svc-rbmq5
  Jul 22 13:09:32.570: INFO: Got endpoints: latency-svc-rs8ck [172.483892ms]
  Jul 22 13:09:32.576: INFO: Created: latency-svc-g4svf
  Jul 22 13:09:32.579: INFO: Got endpoints: latency-svc-rbmq5 [168.093537ms]
  Jul 22 13:09:32.584: INFO: Created: latency-svc-922st
  Jul 22 13:09:32.588: INFO: Got endpoints: latency-svc-g4svf [155.194622ms]
  Jul 22 13:09:32.596: INFO: Created: latency-svc-9ddqt
  Jul 22 13:09:32.599: INFO: Got endpoints: latency-svc-922st [163.301636ms]
  Jul 22 13:09:32.604: INFO: Got endpoints: latency-svc-9ddqt [153.669773ms]
  Jul 22 13:09:32.608: INFO: Created: latency-svc-clmwm
  Jul 22 13:09:32.620: INFO: Got endpoints: latency-svc-clmwm [163.046213ms]
  Jul 22 13:09:32.714: INFO: Created: latency-svc-k8dpc
  Jul 22 13:09:32.717: INFO: Created: latency-svc-b8ntx
  Jul 22 13:09:32.721: INFO: Created: latency-svc-gbgcl
  Jul 22 13:09:32.736: INFO: Created: latency-svc-s72wj
  Jul 22 13:09:32.736: INFO: Created: latency-svc-qgnc9
  Jul 22 13:09:32.736: INFO: Created: latency-svc-nl7r4
  Jul 22 13:09:32.736: INFO: Created: latency-svc-k9csz
  Jul 22 13:09:32.737: INFO: Created: latency-svc-vl8pf
  Jul 22 13:09:32.737: INFO: Created: latency-svc-2pqf5
  Jul 22 13:09:32.737: INFO: Created: latency-svc-mxw7f
  Jul 22 13:09:32.737: INFO: Created: latency-svc-twbgz
  Jul 22 13:09:32.737: INFO: Got endpoints: latency-svc-k8dpc [204.265769ms]
  Jul 22 13:09:32.737: INFO: Created: latency-svc-824j5
  Jul 22 13:09:32.737: INFO: Created: latency-svc-cdcsd
  Jul 22 13:09:32.738: INFO: Created: latency-svc-kkd8p
  Jul 22 13:09:32.738: INFO: Created: latency-svc-vwjsc
  Jul 22 13:09:32.749: INFO: Got endpoints: latency-svc-b8ntx [228.768242ms]
  E0722 13:09:32.752367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:32.753: INFO: Got endpoints: latency-svc-nl7r4 [197.372381ms]
  Jul 22 13:09:32.754: INFO: Got endpoints: latency-svc-2pqf5 [133.522105ms]
  Jul 22 13:09:32.754: INFO: Got endpoints: latency-svc-gbgcl [166.028841ms]
  Jul 22 13:09:32.759: INFO: Got endpoints: latency-svc-824j5 [180.372543ms]
  Jul 22 13:09:32.763: INFO: Created: latency-svc-wrbkh
  Jul 22 13:09:32.768: INFO: Got endpoints: latency-svc-kkd8p [197.154828ms]
  Jul 22 13:09:32.773: INFO: Created: latency-svc-jl5p7
  Jul 22 13:09:32.774: INFO: Got endpoints: latency-svc-vwjsc [227.340194ms]
  Jul 22 13:09:32.776: INFO: Got endpoints: latency-svc-cdcsd [176.471864ms]
  Jul 22 13:09:32.777: INFO: Got endpoints: latency-svc-mxw7f [293.996925ms]
  Jul 22 13:09:32.781: INFO: Created: latency-svc-rlc49
  Jul 22 13:09:32.788: INFO: Got endpoints: latency-svc-twbgz [317.352283ms]
  Jul 22 13:09:32.793: INFO: Created: latency-svc-fczvz
  Jul 22 13:09:32.794: INFO: Got endpoints: latency-svc-qgnc9 [296.232143ms]
  Jul 22 13:09:32.814: INFO: Got endpoints: latency-svc-vl8pf [307.276055ms]
  Jul 22 13:09:32.817: INFO: Created: latency-svc-r28gs
  Jul 22 13:09:32.828: INFO: Created: latency-svc-7vvhl
  Jul 22 13:09:32.836: INFO: Created: latency-svc-4bzq6
  Jul 22 13:09:32.849: INFO: Created: latency-svc-fbvvp
  Jul 22 13:09:32.869: INFO: Got endpoints: latency-svc-s72wj [264.984634ms]
  Jul 22 13:09:32.878: INFO: Created: latency-svc-pkq8m
  Jul 22 13:09:32.886: INFO: Created: latency-svc-4m44m
  Jul 22 13:09:32.895: INFO: Created: latency-svc-m5txz
  Jul 22 13:09:32.902: INFO: Created: latency-svc-sl5nm
  Jul 22 13:09:32.906: INFO: Got endpoints: latency-svc-k9csz [392.672735ms]
  Jul 22 13:09:32.913: INFO: Created: latency-svc-fxzf6
  Jul 22 13:09:32.921: INFO: Created: latency-svc-grc7s
  Jul 22 13:09:32.931: INFO: Created: latency-svc-fnx8c
  Jul 22 13:09:32.952: INFO: Got endpoints: latency-svc-wrbkh [211.816106ms]
  Jul 22 13:09:32.968: INFO: Created: latency-svc-5v5xk
  Jul 22 13:09:33.003: INFO: Got endpoints: latency-svc-jl5p7 [254.080916ms]
  Jul 22 13:09:33.019: INFO: Created: latency-svc-4rnw9
  Jul 22 13:09:33.053: INFO: Got endpoints: latency-svc-rlc49 [300.443207ms]
  Jul 22 13:09:33.079: INFO: Created: latency-svc-zz7g9
  Jul 22 13:09:33.101: INFO: Got endpoints: latency-svc-fczvz [347.088663ms]
  Jul 22 13:09:33.116: INFO: Created: latency-svc-bbq8s
  Jul 22 13:09:33.151: INFO: Got endpoints: latency-svc-r28gs [397.249514ms]
  Jul 22 13:09:33.170: INFO: Created: latency-svc-7fvvc
  Jul 22 13:09:33.202: INFO: Got endpoints: latency-svc-7vvhl [441.833014ms]
  Jul 22 13:09:33.218: INFO: Created: latency-svc-2zrgh
  Jul 22 13:09:33.251: INFO: Got endpoints: latency-svc-4bzq6 [483.11621ms]
  Jul 22 13:09:33.266: INFO: Created: latency-svc-df5bh
  Jul 22 13:09:33.302: INFO: Got endpoints: latency-svc-fbvvp [528.204956ms]
  Jul 22 13:09:33.318: INFO: Created: latency-svc-rlc55
  Jul 22 13:09:33.353: INFO: Got endpoints: latency-svc-pkq8m [575.755554ms]
  Jul 22 13:09:33.366: INFO: Created: latency-svc-fwc4k
  Jul 22 13:09:33.400: INFO: Got endpoints: latency-svc-4m44m [624.006231ms]
  Jul 22 13:09:33.415: INFO: Created: latency-svc-9sllx
  Jul 22 13:09:33.453: INFO: Got endpoints: latency-svc-m5txz [665.258557ms]
  Jul 22 13:09:33.470: INFO: Created: latency-svc-qq7j9
  Jul 22 13:09:33.503: INFO: Got endpoints: latency-svc-sl5nm [708.985876ms]
  Jul 22 13:09:33.516: INFO: Created: latency-svc-nqk49
  Jul 22 13:09:33.552: INFO: Got endpoints: latency-svc-fxzf6 [737.796374ms]
  Jul 22 13:09:33.565: INFO: Created: latency-svc-sgphz
  Jul 22 13:09:33.600: INFO: Got endpoints: latency-svc-grc7s [731.424272ms]
  Jul 22 13:09:33.616: INFO: Created: latency-svc-bnl7q
  Jul 22 13:09:33.652: INFO: Got endpoints: latency-svc-fnx8c [745.770925ms]
  Jul 22 13:09:33.669: INFO: Created: latency-svc-khtdm
  Jul 22 13:09:33.702: INFO: Got endpoints: latency-svc-5v5xk [749.477603ms]
  Jul 22 13:09:33.715: INFO: Created: latency-svc-bfnfd
  E0722 13:09:33.752630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:33.753: INFO: Got endpoints: latency-svc-4rnw9 [749.873238ms]
  Jul 22 13:09:33.768: INFO: Created: latency-svc-tscq8
  Jul 22 13:09:33.804: INFO: Got endpoints: latency-svc-zz7g9 [750.487656ms]
  Jul 22 13:09:33.818: INFO: Created: latency-svc-8b4hp
  Jul 22 13:09:33.852: INFO: Got endpoints: latency-svc-bbq8s [750.482115ms]
  Jul 22 13:09:33.870: INFO: Created: latency-svc-fk5ph
  Jul 22 13:09:33.902: INFO: Got endpoints: latency-svc-7fvvc [750.197853ms]
  Jul 22 13:09:33.924: INFO: Created: latency-svc-k7ffj
  Jul 22 13:09:33.955: INFO: Got endpoints: latency-svc-2zrgh [753.427553ms]
  Jul 22 13:09:33.970: INFO: Created: latency-svc-km4dw
  Jul 22 13:09:34.001: INFO: Got endpoints: latency-svc-df5bh [750.559786ms]
  Jul 22 13:09:34.015: INFO: Created: latency-svc-n6jxm
  Jul 22 13:09:34.052: INFO: Got endpoints: latency-svc-rlc55 [749.640865ms]
  Jul 22 13:09:34.068: INFO: Created: latency-svc-vjz6x
  Jul 22 13:09:34.104: INFO: Got endpoints: latency-svc-fwc4k [751.072623ms]
  Jul 22 13:09:34.125: INFO: Created: latency-svc-zktm2
  Jul 22 13:09:34.152: INFO: Got endpoints: latency-svc-9sllx [751.736641ms]
  Jul 22 13:09:34.170: INFO: Created: latency-svc-8qzkn
  Jul 22 13:09:34.204: INFO: Got endpoints: latency-svc-qq7j9 [750.744799ms]
  Jul 22 13:09:34.223: INFO: Created: latency-svc-4k27j
  Jul 22 13:09:34.257: INFO: Got endpoints: latency-svc-nqk49 [754.028391ms]
  Jul 22 13:09:34.272: INFO: Created: latency-svc-7d486
  Jul 22 13:09:34.302: INFO: Got endpoints: latency-svc-sgphz [749.954319ms]
  Jul 22 13:09:34.323: INFO: Created: latency-svc-2xct8
  Jul 22 13:09:34.353: INFO: Got endpoints: latency-svc-bnl7q [752.111347ms]
  Jul 22 13:09:34.375: INFO: Created: latency-svc-dkgtn
  Jul 22 13:09:34.401: INFO: Got endpoints: latency-svc-khtdm [749.052317ms]
  Jul 22 13:09:34.416: INFO: Created: latency-svc-kmk4g
  Jul 22 13:09:34.453: INFO: Got endpoints: latency-svc-bfnfd [750.832ms]
  Jul 22 13:09:34.467: INFO: Created: latency-svc-rlwng
  Jul 22 13:09:34.504: INFO: Got endpoints: latency-svc-tscq8 [751.169074ms]
  Jul 22 13:09:34.521: INFO: Created: latency-svc-gv92f
  Jul 22 13:09:34.555: INFO: Got endpoints: latency-svc-8b4hp [751.015112ms]
  Jul 22 13:09:34.570: INFO: Created: latency-svc-pmjfj
  Jul 22 13:09:34.604: INFO: Got endpoints: latency-svc-fk5ph [752.076286ms]
  Jul 22 13:09:34.621: INFO: Created: latency-svc-v8tzm
  Jul 22 13:09:34.661: INFO: Got endpoints: latency-svc-k7ffj [759.55088ms]
  Jul 22 13:09:34.686: INFO: Created: latency-svc-xrpnl
  Jul 22 13:09:34.703: INFO: Got endpoints: latency-svc-km4dw [746.96651ms]
  Jul 22 13:09:34.720: INFO: Created: latency-svc-bsqq4
  E0722 13:09:34.752712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:34.753: INFO: Got endpoints: latency-svc-n6jxm [750.86598ms]
  Jul 22 13:09:34.770: INFO: Created: latency-svc-s4p2x
  Jul 22 13:09:34.804: INFO: Got endpoints: latency-svc-vjz6x [752.37182ms]
  Jul 22 13:09:34.821: INFO: Created: latency-svc-pw5jm
  Jul 22 13:09:34.852: INFO: Got endpoints: latency-svc-zktm2 [747.854752ms]
  Jul 22 13:09:34.869: INFO: Created: latency-svc-fhw42
  Jul 22 13:09:34.901: INFO: Got endpoints: latency-svc-8qzkn [749.228199ms]
  Jul 22 13:09:34.915: INFO: Created: latency-svc-xh2md
  Jul 22 13:09:34.951: INFO: Got endpoints: latency-svc-4k27j [747.184444ms]
  Jul 22 13:09:34.968: INFO: Created: latency-svc-p8l49
  Jul 22 13:09:35.002: INFO: Got endpoints: latency-svc-7d486 [744.341127ms]
  Jul 22 13:09:35.020: INFO: Created: latency-svc-269ht
  Jul 22 13:09:35.052: INFO: Got endpoints: latency-svc-2xct8 [749.830847ms]
  Jul 22 13:09:35.065: INFO: Created: latency-svc-jz679
  Jul 22 13:09:35.102: INFO: Got endpoints: latency-svc-dkgtn [749.733016ms]
  Jul 22 13:09:35.118: INFO: Created: latency-svc-4ghm5
  Jul 22 13:09:35.154: INFO: Got endpoints: latency-svc-kmk4g [753.058098ms]
  Jul 22 13:09:35.170: INFO: Created: latency-svc-28msp
  Jul 22 13:09:35.203: INFO: Got endpoints: latency-svc-rlwng [750.721318ms]
  Jul 22 13:09:35.224: INFO: Created: latency-svc-k6mw2
  Jul 22 13:09:35.257: INFO: Got endpoints: latency-svc-gv92f [753.08048ms]
  Jul 22 13:09:35.276: INFO: Created: latency-svc-5l7c4
  Jul 22 13:09:35.304: INFO: Got endpoints: latency-svc-pmjfj [749.033418ms]
  Jul 22 13:09:35.323: INFO: Created: latency-svc-psckz
  Jul 22 13:09:35.354: INFO: Got endpoints: latency-svc-v8tzm [750.181933ms]
  Jul 22 13:09:35.377: INFO: Created: latency-svc-wxmpn
  Jul 22 13:09:35.403: INFO: Got endpoints: latency-svc-xrpnl [741.327019ms]
  Jul 22 13:09:35.422: INFO: Created: latency-svc-tthxg
  Jul 22 13:09:35.453: INFO: Got endpoints: latency-svc-bsqq4 [750.279563ms]
  Jul 22 13:09:35.472: INFO: Created: latency-svc-jkqb9
  Jul 22 13:09:35.502: INFO: Got endpoints: latency-svc-s4p2x [749.664405ms]
  Jul 22 13:09:35.517: INFO: Created: latency-svc-kbs4k
  Jul 22 13:09:35.560: INFO: Got endpoints: latency-svc-pw5jm [755.60461ms]
  Jul 22 13:09:35.584: INFO: Created: latency-svc-87kx9
  Jul 22 13:09:35.606: INFO: Got endpoints: latency-svc-fhw42 [753.608805ms]
  Jul 22 13:09:35.624: INFO: Created: latency-svc-6fv6h
  Jul 22 13:09:35.652: INFO: Got endpoints: latency-svc-xh2md [750.489975ms]
  Jul 22 13:09:35.667: INFO: Created: latency-svc-wrlws
  Jul 22 13:09:35.703: INFO: Got endpoints: latency-svc-p8l49 [751.206414ms]
  Jul 22 13:09:35.722: INFO: Created: latency-svc-ftkpj
  Jul 22 13:09:35.753: INFO: Got endpoints: latency-svc-269ht [751.221364ms]
  E0722 13:09:35.754428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:35.770: INFO: Created: latency-svc-gd9b6
  Jul 22 13:09:35.801: INFO: Got endpoints: latency-svc-jz679 [749.181488ms]
  Jul 22 13:09:35.819: INFO: Created: latency-svc-tz9zl
  Jul 22 13:09:35.851: INFO: Got endpoints: latency-svc-4ghm5 [748.186755ms]
  Jul 22 13:09:35.867: INFO: Created: latency-svc-cbgt8
  Jul 22 13:09:35.901: INFO: Got endpoints: latency-svc-28msp [746.783118ms]
  Jul 22 13:09:35.917: INFO: Created: latency-svc-wk9wf
  Jul 22 13:09:35.951: INFO: Got endpoints: latency-svc-k6mw2 [747.430896ms]
  Jul 22 13:09:35.966: INFO: Created: latency-svc-j544j
  Jul 22 13:09:36.002: INFO: Got endpoints: latency-svc-5l7c4 [745.014405ms]
  Jul 22 13:09:36.022: INFO: Created: latency-svc-qv855
  Jul 22 13:09:36.054: INFO: Got endpoints: latency-svc-psckz [749.482492ms]
  Jul 22 13:09:36.069: INFO: Created: latency-svc-hrhrj
  Jul 22 13:09:36.110: INFO: Got endpoints: latency-svc-wxmpn [755.774933ms]
  Jul 22 13:09:36.127: INFO: Created: latency-svc-4wl76
  Jul 22 13:09:36.152: INFO: Got endpoints: latency-svc-tthxg [749.505193ms]
  Jul 22 13:09:36.171: INFO: Created: latency-svc-zg74v
  Jul 22 13:09:36.206: INFO: Got endpoints: latency-svc-jkqb9 [752.40302ms]
  Jul 22 13:09:36.221: INFO: Created: latency-svc-cn4c5
  Jul 22 13:09:36.253: INFO: Got endpoints: latency-svc-kbs4k [750.430254ms]
  Jul 22 13:09:36.273: INFO: Created: latency-svc-zw6kt
  Jul 22 13:09:36.302: INFO: Got endpoints: latency-svc-87kx9 [738.890228ms]
  Jul 22 13:09:36.325: INFO: Created: latency-svc-6rbkz
  Jul 22 13:09:36.352: INFO: Got endpoints: latency-svc-6fv6h [745.865276ms]
  Jul 22 13:09:36.376: INFO: Created: latency-svc-lqzpp
  Jul 22 13:09:36.402: INFO: Got endpoints: latency-svc-wrlws [750.365744ms]
  Jul 22 13:09:36.417: INFO: Created: latency-svc-jdklz
  Jul 22 13:09:36.454: INFO: Got endpoints: latency-svc-ftkpj [751.212315ms]
  Jul 22 13:09:36.470: INFO: Created: latency-svc-qlv8z
  Jul 22 13:09:36.502: INFO: Got endpoints: latency-svc-gd9b6 [748.670963ms]
  Jul 22 13:09:36.518: INFO: Created: latency-svc-zwl8k
  Jul 22 13:09:36.553: INFO: Got endpoints: latency-svc-tz9zl [751.525039ms]
  Jul 22 13:09:36.567: INFO: Created: latency-svc-89xpt
  Jul 22 13:09:36.601: INFO: Got endpoints: latency-svc-cbgt8 [750.105311ms]
  Jul 22 13:09:36.627: INFO: Created: latency-svc-frw5p
  Jul 22 13:09:36.656: INFO: Got endpoints: latency-svc-wk9wf [754.329544ms]
  Jul 22 13:09:36.674: INFO: Created: latency-svc-ws7j9
  Jul 22 13:09:36.703: INFO: Got endpoints: latency-svc-j544j [751.350657ms]
  Jul 22 13:09:36.727: INFO: Created: latency-svc-bt94x
  Jul 22 13:09:36.754: INFO: Got endpoints: latency-svc-qv855 [750.981131ms]
  E0722 13:09:36.754698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:36.775: INFO: Created: latency-svc-ll2np
  Jul 22 13:09:36.802: INFO: Got endpoints: latency-svc-hrhrj [747.154524ms]
  Jul 22 13:09:36.818: INFO: Created: latency-svc-pnqmb
  Jul 22 13:09:36.853: INFO: Got endpoints: latency-svc-4wl76 [742.728387ms]
  Jul 22 13:09:36.871: INFO: Created: latency-svc-xsqtd
  Jul 22 13:09:36.902: INFO: Got endpoints: latency-svc-zg74v [748.916216ms]
  Jul 22 13:09:36.922: INFO: Created: latency-svc-7mhdm
  Jul 22 13:09:36.951: INFO: Got endpoints: latency-svc-cn4c5 [744.812913ms]
  Jul 22 13:09:36.967: INFO: Created: latency-svc-q928r
  Jul 22 13:09:37.005: INFO: Got endpoints: latency-svc-zw6kt [751.440257ms]
  Jul 22 13:09:37.021: INFO: Created: latency-svc-tqrwp
  Jul 22 13:09:37.054: INFO: Got endpoints: latency-svc-6rbkz [751.113824ms]
  Jul 22 13:09:37.074: INFO: Created: latency-svc-2rbnv
  Jul 22 13:09:37.103: INFO: Got endpoints: latency-svc-lqzpp [750.767419ms]
  Jul 22 13:09:37.120: INFO: Created: latency-svc-zlbbb
  Jul 22 13:09:37.153: INFO: Got endpoints: latency-svc-jdklz [750.296654ms]
  Jul 22 13:09:37.177: INFO: Created: latency-svc-zpcm4
  Jul 22 13:09:37.203: INFO: Got endpoints: latency-svc-qlv8z [748.005604ms]
  Jul 22 13:09:37.231: INFO: Created: latency-svc-2lxlb
  Jul 22 13:09:37.252: INFO: Got endpoints: latency-svc-zwl8k [750.048271ms]
  Jul 22 13:09:37.271: INFO: Created: latency-svc-cd7wb
  Jul 22 13:09:37.306: INFO: Got endpoints: latency-svc-89xpt [752.362849ms]
  Jul 22 13:09:37.326: INFO: Created: latency-svc-vl6kj
  Jul 22 13:09:37.350: INFO: Got endpoints: latency-svc-frw5p [749.26334ms]
  Jul 22 13:09:37.372: INFO: Created: latency-svc-n7w2h
  Jul 22 13:09:37.402: INFO: Got endpoints: latency-svc-ws7j9 [745.666715ms]
  Jul 22 13:09:37.418: INFO: Created: latency-svc-6wtjh
  Jul 22 13:09:37.452: INFO: Got endpoints: latency-svc-bt94x [749.200269ms]
  Jul 22 13:09:37.469: INFO: Created: latency-svc-hjjnx
  Jul 22 13:09:37.502: INFO: Got endpoints: latency-svc-ll2np [747.383726ms]
  Jul 22 13:09:37.519: INFO: Created: latency-svc-swft2
  Jul 22 13:09:37.552: INFO: Got endpoints: latency-svc-pnqmb [749.966409ms]
  Jul 22 13:09:37.571: INFO: Created: latency-svc-wlz9t
  Jul 22 13:09:37.601: INFO: Got endpoints: latency-svc-xsqtd [747.809402ms]
  Jul 22 13:09:37.624: INFO: Created: latency-svc-s2ng8
  Jul 22 13:09:37.653: INFO: Got endpoints: latency-svc-7mhdm [750.882441ms]
  Jul 22 13:09:37.672: INFO: Created: latency-svc-b4wbf
  Jul 22 13:09:37.702: INFO: Got endpoints: latency-svc-q928r [751.486599ms]
  Jul 22 13:09:37.717: INFO: Created: latency-svc-w9sf4
  E0722 13:09:37.755250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:37.762: INFO: Got endpoints: latency-svc-tqrwp [757.349143ms]
  Jul 22 13:09:37.783: INFO: Created: latency-svc-7w6dd
  Jul 22 13:09:37.803: INFO: Got endpoints: latency-svc-2rbnv [748.084105ms]
  Jul 22 13:09:37.829: INFO: Created: latency-svc-9fx5p
  Jul 22 13:09:37.852: INFO: Got endpoints: latency-svc-zlbbb [749.464342ms]
  Jul 22 13:09:37.871: INFO: Created: latency-svc-nqmvz
  Jul 22 13:09:37.902: INFO: Got endpoints: latency-svc-zpcm4 [749.048667ms]
  Jul 22 13:09:37.925: INFO: Created: latency-svc-jrfck
  Jul 22 13:09:37.951: INFO: Got endpoints: latency-svc-2lxlb [748.779314ms]
  Jul 22 13:09:37.985: INFO: Created: latency-svc-gtt7l
  Jul 22 13:09:38.003: INFO: Got endpoints: latency-svc-cd7wb [750.595707ms]
  Jul 22 13:09:38.033: INFO: Created: latency-svc-vvjpt
  Jul 22 13:09:38.052: INFO: Got endpoints: latency-svc-vl6kj [746.611687ms]
  Jul 22 13:09:38.076: INFO: Created: latency-svc-m6pjh
  Jul 22 13:09:38.101: INFO: Got endpoints: latency-svc-n7w2h [750.373264ms]
  Jul 22 13:09:38.126: INFO: Created: latency-svc-p26lp
  Jul 22 13:09:38.152: INFO: Got endpoints: latency-svc-6wtjh [750.04666ms]
  Jul 22 13:09:38.187: INFO: Created: latency-svc-psb7l
  Jul 22 13:09:38.212: INFO: Got endpoints: latency-svc-hjjnx [759.201995ms]
  Jul 22 13:09:38.235: INFO: Created: latency-svc-vxrkg
  Jul 22 13:09:38.253: INFO: Got endpoints: latency-svc-swft2 [751.297747ms]
  Jul 22 13:09:38.272: INFO: Created: latency-svc-mmp5x
  Jul 22 13:09:38.306: INFO: Got endpoints: latency-svc-wlz9t [753.69654ms]
  Jul 22 13:09:38.362: INFO: Got endpoints: latency-svc-s2ng8 [760.860591ms]
  Jul 22 13:09:38.402: INFO: Got endpoints: latency-svc-b4wbf [749.300408ms]
  Jul 22 13:09:38.406: INFO: Created: latency-svc-2xxkm
  Jul 22 13:09:38.415: INFO: Created: latency-svc-f6h2b
  Jul 22 13:09:38.429: INFO: Created: latency-svc-7mdmk
  Jul 22 13:09:38.451: INFO: Got endpoints: latency-svc-w9sf4 [748.883119ms]
  Jul 22 13:09:38.471: INFO: Created: latency-svc-67kt6
  Jul 22 13:09:38.503: INFO: Got endpoints: latency-svc-7w6dd [740.90013ms]
  Jul 22 13:09:38.519: INFO: Created: latency-svc-8mqmf
  Jul 22 13:09:38.552: INFO: Got endpoints: latency-svc-9fx5p [749.246373ms]
  Jul 22 13:09:38.571: INFO: Created: latency-svc-db4bl
  Jul 22 13:09:38.602: INFO: Got endpoints: latency-svc-nqmvz [750.014158ms]
  Jul 22 13:09:38.620: INFO: Created: latency-svc-rsncz
  Jul 22 13:09:38.653: INFO: Got endpoints: latency-svc-jrfck [750.833862ms]
  Jul 22 13:09:38.672: INFO: Created: latency-svc-ljtzv
  Jul 22 13:09:38.704: INFO: Got endpoints: latency-svc-gtt7l [752.310356ms]
  Jul 22 13:09:38.725: INFO: Created: latency-svc-qxs8b
  Jul 22 13:09:38.751: INFO: Got endpoints: latency-svc-vvjpt [747.267459ms]
  E0722 13:09:38.755710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:38.768: INFO: Created: latency-svc-wzflk
  Jul 22 13:09:38.802: INFO: Got endpoints: latency-svc-m6pjh [749.217028ms]
  Jul 22 13:09:38.821: INFO: Created: latency-svc-nwn75
  Jul 22 13:09:38.853: INFO: Got endpoints: latency-svc-p26lp [751.444771ms]
  Jul 22 13:09:38.870: INFO: Created: latency-svc-tfpvd
  Jul 22 13:09:38.901: INFO: Got endpoints: latency-svc-psb7l [749.172078ms]
  Jul 22 13:09:38.918: INFO: Created: latency-svc-jbn8k
  Jul 22 13:09:38.952: INFO: Got endpoints: latency-svc-vxrkg [740.096929ms]
  Jul 22 13:09:38.966: INFO: Created: latency-svc-gb88c
  Jul 22 13:09:39.001: INFO: Got endpoints: latency-svc-mmp5x [747.643878ms]
  Jul 22 13:09:39.017: INFO: Created: latency-svc-m4xsr
  Jul 22 13:09:39.053: INFO: Got endpoints: latency-svc-2xxkm [747.280424ms]
  Jul 22 13:09:39.066: INFO: Created: latency-svc-6dnpd
  Jul 22 13:09:39.103: INFO: Got endpoints: latency-svc-f6h2b [740.726736ms]
  Jul 22 13:09:39.120: INFO: Created: latency-svc-gvpww
  Jul 22 13:09:39.152: INFO: Got endpoints: latency-svc-7mdmk [749.582501ms]
  Jul 22 13:09:39.180: INFO: Created: latency-svc-5hrgq
  Jul 22 13:09:39.211: INFO: Got endpoints: latency-svc-67kt6 [760.137447ms]
  Jul 22 13:09:39.239: INFO: Created: latency-svc-5lvkz
  Jul 22 13:09:39.251: INFO: Got endpoints: latency-svc-8mqmf [748.177784ms]
  Jul 22 13:09:39.292: INFO: Created: latency-svc-d7qqb
  Jul 22 13:09:39.301: INFO: Got endpoints: latency-svc-db4bl [749.267628ms]
  Jul 22 13:09:39.319: INFO: Created: latency-svc-nsxcp
  Jul 22 13:09:39.353: INFO: Got endpoints: latency-svc-rsncz [751.034549ms]
  Jul 22 13:09:39.370: INFO: Created: latency-svc-tbb9m
  Jul 22 13:09:39.403: INFO: Got endpoints: latency-svc-ljtzv [750.077067ms]
  Jul 22 13:09:39.418: INFO: Created: latency-svc-xzzkv
  Jul 22 13:09:39.451: INFO: Got endpoints: latency-svc-qxs8b [747.239023ms]
  Jul 22 13:09:39.467: INFO: Created: latency-svc-tj6gw
  Jul 22 13:09:39.503: INFO: Got endpoints: latency-svc-wzflk [751.664755ms]
  Jul 22 13:09:39.520: INFO: Created: latency-svc-tfkq9
  Jul 22 13:09:39.553: INFO: Got endpoints: latency-svc-nwn75 [751.273552ms]
  Jul 22 13:09:39.569: INFO: Created: latency-svc-hbclg
  Jul 22 13:09:39.603: INFO: Got endpoints: latency-svc-tfpvd [750.495032ms]
  Jul 22 13:09:39.620: INFO: Created: latency-svc-tg9ss
  Jul 22 13:09:39.652: INFO: Got endpoints: latency-svc-jbn8k [750.29778ms]
  Jul 22 13:09:39.668: INFO: Created: latency-svc-2bcjq
  Jul 22 13:09:39.704: INFO: Got endpoints: latency-svc-gb88c [751.16593ms]
  Jul 22 13:09:39.721: INFO: Created: latency-svc-s6xdn
  Jul 22 13:09:39.753: INFO: Got endpoints: latency-svc-m4xsr [751.307682ms]
  E0722 13:09:39.756230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:39.770: INFO: Created: latency-svc-nnvlc
  Jul 22 13:09:39.801: INFO: Got endpoints: latency-svc-6dnpd [748.062773ms]
  Jul 22 13:09:39.818: INFO: Created: latency-svc-dkbs7
  Jul 22 13:09:39.853: INFO: Got endpoints: latency-svc-gvpww [749.632152ms]
  Jul 22 13:09:39.874: INFO: Created: latency-svc-fqmb9
  Jul 22 13:09:39.902: INFO: Got endpoints: latency-svc-5hrgq [750.585523ms]
  Jul 22 13:09:39.917: INFO: Created: latency-svc-qm6sq
  Jul 22 13:09:39.952: INFO: Got endpoints: latency-svc-5lvkz [740.663595ms]
  Jul 22 13:09:39.967: INFO: Created: latency-svc-pd2h7
  Jul 22 13:09:40.007: INFO: Got endpoints: latency-svc-d7qqb [755.38248ms]
  Jul 22 13:09:40.056: INFO: Got endpoints: latency-svc-nsxcp [753.964723ms]
  Jul 22 13:09:40.101: INFO: Got endpoints: latency-svc-tbb9m [747.992913ms]
  Jul 22 13:09:40.154: INFO: Got endpoints: latency-svc-xzzkv [750.091017ms]
  Jul 22 13:09:40.203: INFO: Got endpoints: latency-svc-tj6gw [751.432333ms]
  Jul 22 13:09:40.255: INFO: Got endpoints: latency-svc-tfkq9 [751.773057ms]
  Jul 22 13:09:40.302: INFO: Got endpoints: latency-svc-hbclg [748.129263ms]
  Jul 22 13:09:40.353: INFO: Got endpoints: latency-svc-tg9ss [749.309708ms]
  Jul 22 13:09:40.401: INFO: Got endpoints: latency-svc-2bcjq [749.431609ms]
  Jul 22 13:09:40.454: INFO: Got endpoints: latency-svc-s6xdn [750.208909ms]
  Jul 22 13:09:40.501: INFO: Got endpoints: latency-svc-nnvlc [747.965492ms]
  Jul 22 13:09:40.559: INFO: Got endpoints: latency-svc-dkbs7 [758.119623ms]
  Jul 22 13:09:40.605: INFO: Got endpoints: latency-svc-fqmb9 [751.911079ms]
  Jul 22 13:09:40.651: INFO: Got endpoints: latency-svc-qm6sq [748.664321ms]
  Jul 22 13:09:40.703: INFO: Got endpoints: latency-svc-pd2h7 [749.880715ms]
  Jul 22 13:09:40.703: INFO: Latencies: [30.148365ms 48.778923ms 53.726306ms 133.522105ms 153.669773ms 154.542304ms 155.194622ms 163.046213ms 163.301636ms 166.028841ms 167.162305ms 168.093537ms 171.118135ms 172.483892ms 176.471864ms 176.799738ms 180.372543ms 187.320693ms 189.426819ms 196.753733ms 197.154828ms 197.372381ms 202.75502ms 204.265769ms 211.816106ms 214.61673ms 226.842967ms 227.340194ms 228.768242ms 236.707233ms 250.730033ms 254.080916ms 264.984634ms 275.198714ms 275.274655ms 290.656742ms 293.996925ms 294.047525ms 296.232143ms 297.401258ms 297.47127ms 298.775885ms 300.443207ms 307.276055ms 311.553419ms 317.352283ms 347.088663ms 392.672735ms 397.249514ms 441.833014ms 483.11621ms 528.204956ms 575.755554ms 624.006231ms 665.258557ms 708.985876ms 731.424272ms 737.796374ms 738.890228ms 740.096929ms 740.663595ms 740.726736ms 740.90013ms 741.327019ms 742.728387ms 744.341127ms 744.812913ms 745.014405ms 745.666715ms 745.770925ms 745.865276ms 746.611687ms 746.783118ms 746.96651ms 747.154524ms 747.184444ms 747.239023ms 747.267459ms 747.280424ms 747.383726ms 747.430896ms 747.643878ms 747.809402ms 747.854752ms 747.965492ms 747.992913ms 748.005604ms 748.062773ms 748.084105ms 748.129263ms 748.177784ms 748.186755ms 748.664321ms 748.670963ms 748.779314ms 748.883119ms 748.916216ms 749.033418ms 749.048667ms 749.052317ms 749.172078ms 749.181488ms 749.200269ms 749.217028ms 749.228199ms 749.246373ms 749.26334ms 749.267628ms 749.300408ms 749.309708ms 749.431609ms 749.464342ms 749.477603ms 749.482492ms 749.505193ms 749.582501ms 749.632152ms 749.640865ms 749.664405ms 749.733016ms 749.830847ms 749.873238ms 749.880715ms 749.954319ms 749.966409ms 750.014158ms 750.04666ms 750.048271ms 750.077067ms 750.091017ms 750.105311ms 750.181933ms 750.197853ms 750.208909ms 750.279563ms 750.296654ms 750.29778ms 750.365744ms 750.373264ms 750.430254ms 750.482115ms 750.487656ms 750.489975ms 750.495032ms 750.559786ms 750.585523ms 750.595707ms 750.721318ms 750.744799ms 750.767419ms 750.832ms 750.833862ms 750.86598ms 750.882441ms 750.981131ms 751.015112ms 751.034549ms 751.072623ms 751.113824ms 751.16593ms 751.169074ms 751.206414ms 751.212315ms 751.221364ms 751.273552ms 751.297747ms 751.307682ms 751.350657ms 751.432333ms 751.440257ms 751.444771ms 751.486599ms 751.525039ms 751.664755ms 751.736641ms 751.773057ms 751.911079ms 752.076286ms 752.111347ms 752.310356ms 752.362849ms 752.37182ms 752.40302ms 753.058098ms 753.08048ms 753.427553ms 753.608805ms 753.69654ms 753.964723ms 754.028391ms 754.329544ms 755.38248ms 755.60461ms 755.774933ms 757.349143ms 758.119623ms 759.201995ms 759.55088ms 760.137447ms 760.860591ms]
  Jul 22 13:09:40.703: INFO: 50 %ile: 749.172078ms
  Jul 22 13:09:40.703: INFO: 90 %ile: 752.362849ms
  Jul 22 13:09:40.703: INFO: 99 %ile: 760.137447ms
  Jul 22 13:09:40.703: INFO: Total sample count: 200
  Jul 22 13:09:40.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-4113" for this suite. @ 07/22/23 13:09:40.713
• [10.808 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/22/23 13:09:40.729
  Jul 22 13:09:40.729: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:09:40.73
  E0722 13:09:40.757262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:40.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:40.789
  STEP: Creating projection with secret that has name projected-secret-test-e6400272-e2e6-4091-9b68-546a2756a23b @ 07/22/23 13:09:40.797
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:09:40.806
  E0722 13:09:41.758203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:42.758553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:43.758673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:44.758792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:09:44.857
  Jul 22 13:09:44.864: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-secrets-90fde14e-4609-4569-ad89-3aefc4a02437 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:09:44.878
  Jul 22 13:09:44.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7458" for this suite. @ 07/22/23 13:09:44.922
• [4.212 seconds]
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/22/23 13:09:44.941
  Jul 22 13:09:44.941: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 13:09:44.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:44.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:44.985
  STEP: Creating replication controller my-hostname-basic-b15e4365-f8d9-4b03-8c2f-4232ca152eee @ 07/22/23 13:09:44.996
  Jul 22 13:09:45.020: INFO: Pod name my-hostname-basic-b15e4365-f8d9-4b03-8c2f-4232ca152eee: Found 0 pods out of 1
  E0722 13:09:45.758923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:46.759189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:47.759279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:48.759414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:49.760047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:09:50.027: INFO: Pod name my-hostname-basic-b15e4365-f8d9-4b03-8c2f-4232ca152eee: Found 1 pods out of 1
  Jul 22 13:09:50.028: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b15e4365-f8d9-4b03-8c2f-4232ca152eee" are running
  Jul 22 13:09:50.034: INFO: Pod "my-hostname-basic-b15e4365-f8d9-4b03-8c2f-4232ca152eee-wvds4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-22 13:09:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-22 13:09:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-22 13:09:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-22 13:09:45 +0000 UTC Reason: Message:}])
  Jul 22 13:09:50.035: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/22/23 13:09:50.035
  Jul 22 13:09:50.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7915" for this suite. @ 07/22/23 13:09:50.06
• [5.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/22/23 13:09:50.073
  Jul 22 13:09:50.073: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename cronjob @ 07/22/23 13:09:50.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:09:50.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:09:50.108
  STEP: Creating a cronjob @ 07/22/23 13:09:50.111
  STEP: Ensuring more than one job is running at a time @ 07/22/23 13:09:50.12
  E0722 13:09:50.760707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:51.761108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:52.761750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:53.761908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:54.762028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:55.762646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:56.762780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:57.762883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:58.762985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:09:59.763246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:00.763320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:01.763534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:02.763469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:03.763575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:04.764338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:05.765415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:06.765552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:07.766245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:08.766382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:09.766650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:10.766796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:11.766974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:12.767124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:13.767406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:14.767549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:15.767641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:16.767785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:17.767907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:18.768036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:19.768133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:20.768321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:21.768552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:22.769528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:23.769598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:24.770093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:25.770303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:26.770777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:27.770873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:28.770998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:29.771730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:30.771876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:31.771961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:32.772876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:33.773224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:34.774194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:35.774724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:36.774828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:37.774918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:38.775034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:39.775186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:40.775308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:41.775404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:42.775905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:43.775983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:44.776812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:45.777765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:46.777893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:47.778044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:48.778143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:49.778234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:50.778392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:51.778510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:52.779196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:53.779541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:54.779716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:55.780696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:56.780867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:57.781070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:58.781172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:10:59.782221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:00.782335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:01.782449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/22/23 13:11:02.125
  STEP: Removing cronjob @ 07/22/23 13:11:02.131
  Jul 22 13:11:02.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9385" for this suite. @ 07/22/23 13:11:02.156
• [72.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/22/23 13:11:02.17
  Jul 22 13:11:02.170: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename limitrange @ 07/22/23 13:11:02.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:02.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:02.268
  STEP: Creating a LimitRange @ 07/22/23 13:11:02.275
  STEP: Setting up watch @ 07/22/23 13:11:02.275
  STEP: Submitting a LimitRange @ 07/22/23 13:11:02.381
  STEP: Verifying LimitRange creation was observed @ 07/22/23 13:11:02.391
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/22/23 13:11:02.391
  Jul 22 13:11:02.398: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 22 13:11:02.398: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/22/23 13:11:02.398
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/22/23 13:11:02.406
  Jul 22 13:11:02.411: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 22 13:11:02.411: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/22/23 13:11:02.411
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/22/23 13:11:02.421
  Jul 22 13:11:02.427: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul 22 13:11:02.427: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/22/23 13:11:02.427
  STEP: Failing to create a Pod with more than max resources @ 07/22/23 13:11:02.431
  STEP: Updating a LimitRange @ 07/22/23 13:11:02.434
  STEP: Verifying LimitRange updating is effective @ 07/22/23 13:11:02.45
  E0722 13:11:02.782548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:03.782688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 07/22/23 13:11:04.455
  STEP: Failing to create a Pod with more than max resources @ 07/22/23 13:11:04.463
  STEP: Deleting a LimitRange @ 07/22/23 13:11:04.466
  STEP: Verifying the LimitRange was deleted @ 07/22/23 13:11:04.477
  E0722 13:11:04.783359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:05.783487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:06.784557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:07.784707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:08.785137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:09.484: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/22/23 13:11:09.484
  Jul 22 13:11:09.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-526" for this suite. @ 07/22/23 13:11:09.515
• [7.363 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/22/23 13:11:09.536
  Jul 22 13:11:09.536: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 13:11:09.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:09.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:09.579
  STEP: creating a Deployment @ 07/22/23 13:11:09.604
  STEP: waiting for Deployment to be created @ 07/22/23 13:11:09.627
  STEP: waiting for all Replicas to be Ready @ 07/22/23 13:11:09.636
  Jul 22 13:11:09.639: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.639: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.644: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.644: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.667: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.667: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.712: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 22 13:11:09.713: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0722 13:11:09.785344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:10.683: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 22 13:11:10.683: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0722 13:11:10.786350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:11.068: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/22/23 13:11:11.068
  W0722 13:11:11.087723      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 22 13:11:11.090: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/22/23 13:11:11.09
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.098: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 0
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.099: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.103: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.104: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.156: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.157: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:11.163: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:11.164: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:11.177: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:11.177: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  E0722 13:11:11.786489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:12.698: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:12.698: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:12.754: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  STEP: listing Deployments @ 07/22/23 13:11:12.754
  Jul 22 13:11:12.763: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/22/23 13:11:12.763
  Jul 22 13:11:12.782: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/22/23 13:11:12.782
  E0722 13:11:12.787092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:12.794: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:12.802: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:12.860: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:12.885: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0722 13:11:13.787401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:14.096: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:14.130: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:14.183: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 22 13:11:14.192: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0722 13:11:14.787353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:15.708: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/22/23 13:11:15.745
  STEP: fetching the DeploymentStatus @ 07/22/23 13:11:15.756
  Jul 22 13:11:15.766: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:15.766: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:15.766: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:15.767: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 1
  Jul 22 13:11:15.767: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:15.767: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 3
  Jul 22 13:11:15.767: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:15.768: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 2
  Jul 22 13:11:15.768: INFO: observed Deployment test-deployment in namespace deployment-9557 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/22/23 13:11:15.768
  Jul 22 13:11:15.782: INFO: observed event type MODIFIED
  Jul 22 13:11:15.782: INFO: observed event type MODIFIED
  Jul 22 13:11:15.782: INFO: observed event type MODIFIED
  Jul 22 13:11:15.782: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  Jul 22 13:11:15.783: INFO: observed event type MODIFIED
  E0722 13:11:15.787669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:15.788: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul 22 13:11:15.794: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-9557  bf939d31-162b-4378-aaa5-e5ca97957a3b 28814 3 2023-07-22 13:11:09 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ebc0c890-1126-46c8-9fcd-605abf0e496f 0xc0049af5a7 0xc0049af5a8}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:11:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc0c890-1126-46c8-9fcd-605abf0e496f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:11:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049af630 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 22 13:11:15.799: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9557  e36796d3-0cfa-4ed7-bb4e-56d0086d46fc 28928 4 2023-07-22 13:11:11 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ebc0c890-1126-46c8-9fcd-605abf0e496f 0xc0049af697 0xc0049af698}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc0c890-1126-46c8-9fcd-605abf0e496f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:11:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049af720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 22 13:11:15.805: INFO: pod: "test-deployment-5b5dcbcd95-frz4p":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-frz4p test-deployment-5b5dcbcd95- deployment-9557  16d2e215-28aa-4c90-b2f8-d2aff079db19 28924 0 2023-07-22 13:11:11 +0000 UTC 2023-07-22 13:11:16 +0000 UTC 0xc004144e68 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 e36796d3-0cfa-4ed7-bb4e-56d0086d46fc 0xc004144e97 0xc004144e98}] [] [{kube-controller-manager Update v1 2023-07-22 13:11:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e36796d3-0cfa-4ed7-bb4e-56d0086d46fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:11:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5c59h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5c59h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.3,StartTime:2023-07-22 13:11:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:11:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c6bfed9c1fd5c178397d938c885712487f4ec44639e30c6e993ed510f4b7cd54,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.3,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 22 13:11:15.806: INFO: pod: "test-deployment-5b5dcbcd95-s5vs6":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-s5vs6 test-deployment-5b5dcbcd95- deployment-9557  056a31d7-e3f1-4837-89f2-408868d51b61 28919 0 2023-07-22 13:11:12 +0000 UTC 2023-07-22 13:11:15 +0000 UTC 0xc004145060 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 e36796d3-0cfa-4ed7-bb4e-56d0086d46fc 0xc004145097 0xc004145098}] [] [{kube-controller-manager Update v1 2023-07-22 13:11:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e36796d3-0cfa-4ed7-bb4e-56d0086d46fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:11:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cvsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cvsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Succeeded,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:15 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:15 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:,StartTime:2023-07-22 13:11:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-07-22 13:11:13 +0000 UTC,FinishedAt:2023-07-22 13:11:15 +0000 UTC,ContainerID:containerd://8c1c5dc070297f0cc8bb1f17a94c1cf0fa6eae2278862fba55413a5d8bcd0590,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://8c1c5dc070297f0cc8bb1f17a94c1cf0fa6eae2278862fba55413a5d8bcd0590,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 22 13:11:15.806: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-9557  3da09d8b-e1fa-4ee8-ae43-a6dbadec9f0b 28920 2 2023-07-22 13:11:12 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ebc0c890-1126-46c8-9fcd-605abf0e496f 0xc0049af787 0xc0049af788}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc0c890-1126-46c8-9fcd-605abf0e496f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:11:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049af810 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jul 22 13:11:15.813: INFO: pod: "test-deployment-6fc78d85c6-fzzwz":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-fzzwz test-deployment-6fc78d85c6- deployment-9557  2f1def80-7677-4faf-9430-c0ddf39deed7 28918 0 2023-07-22 13:11:14 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 3da09d8b-e1fa-4ee8-ae43-a6dbadec9f0b 0xc004410537 0xc004410538}] [] [{kube-controller-manager Update v1 2023-07-22 13:11:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3da09d8b-e1fa-4ee8-ae43-a6dbadec9f0b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:11:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkzrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkzrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.18,StartTime:2023-07-22 13:11:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:11:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://09ea2afd57dbddaef90f12dea4a042a938ff19fbbbd943fb9b69fa441c80916b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 22 13:11:15.813: INFO: pod: "test-deployment-6fc78d85c6-rwbvv":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-rwbvv test-deployment-6fc78d85c6- deployment-9557  2fd2259f-c007-40b0-a875-eacf0d78a81d 28866 0 2023-07-22 13:11:12 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 3da09d8b-e1fa-4ee8-ae43-a6dbadec9f0b 0xc004410727 0xc004410728}] [] [{kube-controller-manager Update v1 2023-07-22 13:11:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3da09d8b-e1fa-4ee8-ae43-a6dbadec9f0b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:11:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-46s8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-46s8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:11:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.145,StartTime:2023-07-22 13:11:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:11:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f4b425841ca9eba830609722b9d1b751b8f456099fc176f27f462be38cb346d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 22 13:11:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9557" for this suite. @ 07/22/23 13:11:15.822
• [6.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/22/23 13:11:15.839
  Jul 22 13:11:15.839: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 13:11:15.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:15.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:15.89
  STEP: Creating configMap with name configmap-test-volume-map-bc922a45-94c1-4b78-8d8e-82a138f62703 @ 07/22/23 13:11:15.896
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:11:15.905
  E0722 13:11:16.787873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:17.788986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:18.789071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:19.790131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:11:19.943
  Jul 22 13:11:19.950: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-fccae673-d190-4093-a8d1-76be3789d06a container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:11:19.976
  Jul 22 13:11:19.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6095" for this suite. @ 07/22/23 13:11:20.015
• [4.187 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/22/23 13:11:20.029
  Jul 22 13:11:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename dns @ 07/22/23 13:11:20.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:20.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:20.071
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2426.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2426.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/22/23 13:11:20.075
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2426.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2426.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/22/23 13:11:20.075
  STEP: creating a pod to probe /etc/hosts @ 07/22/23 13:11:20.075
  STEP: submitting the pod to kubernetes @ 07/22/23 13:11:20.076
  E0722 13:11:20.791264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:21.791870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/22/23 13:11:22.104
  STEP: looking for the results for each expected name from probers @ 07/22/23 13:11:22.11
  Jul 22 13:11:22.131: INFO: DNS probes using dns-2426/dns-test-31818b97-66ea-4a35-b021-8fe07f63c80e succeeded

  Jul 22 13:11:22.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:11:22.138
  STEP: Destroying namespace "dns-2426" for this suite. @ 07/22/23 13:11:22.153
• [2.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/22/23 13:11:22.169
  Jul 22 13:11:22.169: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 13:11:22.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:22.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:22.199
  STEP: Creating configMap with name configmap-test-volume-08d8bc14-779a-4aaa-8e1a-3f84a467c48b @ 07/22/23 13:11:22.205
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:11:22.213
  E0722 13:11:22.791914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:23.792676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:24.793348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:25.793675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:11:26.248
  Jul 22 13:11:26.252: INFO: Trying to get logs from node ip-172-31-26-93 pod pod-configmaps-978f88ef-1944-4f13-95db-fd0dc0d20d94 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:11:26.279
  Jul 22 13:11:26.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6397" for this suite. @ 07/22/23 13:11:26.314
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/22/23 13:11:26.327
  Jul 22 13:11:26.327: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pod-network-test @ 07/22/23 13:11:26.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:26.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:26.377
  STEP: Performing setup for networking test in namespace pod-network-test-4851 @ 07/22/23 13:11:26.389
  STEP: creating a selector @ 07/22/23 13:11:26.389
  STEP: Creating the service pods in kubernetes @ 07/22/23 13:11:26.389
  Jul 22 13:11:26.390: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0722 13:11:26.793753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:27.794371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:28.794455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:29.794757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:30.795875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:31.796138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:32.796704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:33.796751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:34.797610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:35.798014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:36.798677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:37.799093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:38.799808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:39.800112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:40.801133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:41.801142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:42.802191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:43.802463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:44.803012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:45.803246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:46.803809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:47.803961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/22/23 13:11:48.571
  E0722 13:11:48.804612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:49.805070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:50.617: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 22 13:11:50.617: INFO: Going to poll 192.168.196.155 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 13:11:50.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.196.155:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4851 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:11:50.624: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:11:50.627: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:11:50.627: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4851/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.196.155%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 22 13:11:50.726: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 22 13:11:50.726: INFO: Going to poll 192.168.120.17 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 13:11:50.735: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.120.17:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4851 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:11:50.735: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:11:50.736: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:11:50.736: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4851/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.120.17%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0722 13:11:50.806941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:50.829: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 22 13:11:50.829: INFO: Going to poll 192.168.121.232 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 22 13:11:50.842: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.121.232:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4851 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:11:50.842: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:11:50.843: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:11:50.844: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-4851/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.121.232%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 22 13:11:50.933: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 22 13:11:50.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4851" for this suite. @ 07/22/23 13:11:50.945
• [24.630 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/22/23 13:11:50.958
  Jul 22 13:11:50.958: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:11:50.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:11:50.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:11:50.994
  STEP: Creating pod busybox-aecbc119-8491-443a-a827-c6e4abb503ae in namespace container-probe-6314 @ 07/22/23 13:11:50.999
  E0722 13:11:51.809172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:52.810355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:11:53.028: INFO: Started pod busybox-aecbc119-8491-443a-a827-c6e4abb503ae in namespace container-probe-6314
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 13:11:53.028
  Jul 22 13:11:53.035: INFO: Initial restart count of pod busybox-aecbc119-8491-443a-a827-c6e4abb503ae is 0
  E0722 13:11:53.811252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:54.811370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:55.812268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:56.813401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:57.813361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:58.813479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:11:59.813578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:00.814235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:01.814376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:02.814691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:03.814814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:04.814920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:05.815060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:06.815170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:07.815234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:08.815343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:09.816272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:10.816499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:11.816607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:12.816719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:13.817151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:14.817228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:15.818146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:16.818434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:17.818551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:18.818624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:19.818792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:20.819104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:21.819222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:22.819288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:23.820258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:24.820359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:25.820445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:26.820617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:27.820805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:28.821118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:29.821309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:30.822336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:31.822438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:32.822493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:33.822651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:34.822772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:35.823533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:36.823683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:37.823837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:38.824013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:39.824651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:40.825394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:41.825468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:42.825553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:43.826126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:44.826290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:45.827273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:46.827463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:47.827557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:48.827672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:49.828662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:50.829122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:51.829226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:52.829356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:53.830275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:54.830379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:55.830605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:56.831005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:57.831137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:58.831399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:12:59.831499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:00.832567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:01.833364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:02.834182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:03.834290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:04.834512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:05.835313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:06.835415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:07.835588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:08.835743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:09.835929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:10.836048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:11.836225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:12.836511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:13.836641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:14.837080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:15.837238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:16.837337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:17.837509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:18.838463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:19.838579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:20.838693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:21.838773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:22.838878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:23.839027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:24.839137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:25.839166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:26.839315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:27.839692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:28.839773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:29.840850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:30.841680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:31.841776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:32.846657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:33.846741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:34.847032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:35.847805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:36.847952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:37.848680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:38.849156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:39.849255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:40.850143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:41.850774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:42.851088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:43.852075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:44.852264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:45.852987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:46.853101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:47.853178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:48.854213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:49.854548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:50.854676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:51.855318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:52.855366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:53.855635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:54.856002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:55.856087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:56.856413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:57.856564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:58.857165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:13:59.858251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:00.858336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:01.859212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:02.859474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:03.860097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:04.860244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:05.860520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:06.860596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:07.860735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:08.861058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:09.862175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:10.863011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:11.863472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:12.863757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:13.864573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:14.864697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:15.865643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:16.866061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:17.866872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:18.867007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:19.867527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:20.867663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:21.868726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:22.868861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:23.868884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:24.869192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:25.869598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:26.869706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:27.870911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:28.871132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:29.871600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:30.872501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:31.873232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:32.873324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:33.873473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:34.873630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:35.874691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:36.875121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:37.875982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:38.876131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:39.876346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:40.877177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:41.877329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:42.877405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:43.878279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:44.878682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:45.878784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:46.879313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:47.879488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:48.879958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:49.880657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:50.880792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:51.881351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:52.882257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:53.882827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:54.883090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:55.883586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:56.883955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:57.884665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:58.885089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:14:59.885673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:00.886416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:01.886500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:02.886888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:03.887559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:04.887693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:05.888592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:06.888723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:07.889135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:08.889238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:09.890103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:10.890804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:11.891453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:12.891727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:13.892788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:14.892964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:15.893804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:16.893969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:17.894692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:18.895118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:19.896218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:20.897052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:21.897746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:22.898023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:23.898509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:24.898611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:25.899484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:26.899897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:27.900579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:28.900665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:29.901537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:30.902098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:31.902135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:32.902233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:33.902980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:34.903071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:35.903652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:36.903793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:37.904541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:38.905055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:39.905280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:40.905481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:41.906562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:42.906711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:43.907294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:44.907423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:45.908432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:46.908527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:47.908958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:48.909091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:49.909462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:50.909581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:51.910625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:52.910786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:15:53.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:15:53.908
  E0722 13:15:53.911575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-probe-6314" for this suite. @ 07/22/23 13:15:53.926
• [242.985 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/22/23 13:15:53.946
  Jul 22 13:15:53.946: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename endpointslice @ 07/22/23 13:15:53.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:15:53.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:15:53.987
  STEP: getting /apis @ 07/22/23 13:15:53.992
  STEP: getting /apis/discovery.k8s.io @ 07/22/23 13:15:53.999
  STEP: getting /apis/discovery.k8s.iov1 @ 07/22/23 13:15:54.007
  STEP: creating @ 07/22/23 13:15:54.009
  STEP: getting @ 07/22/23 13:15:54.032
  STEP: listing @ 07/22/23 13:15:54.037
  STEP: watching @ 07/22/23 13:15:54.043
  Jul 22 13:15:54.043: INFO: starting watch
  STEP: cluster-wide listing @ 07/22/23 13:15:54.045
  STEP: cluster-wide watching @ 07/22/23 13:15:54.051
  Jul 22 13:15:54.051: INFO: starting watch
  STEP: patching @ 07/22/23 13:15:54.053
  STEP: updating @ 07/22/23 13:15:54.064
  Jul 22 13:15:54.079: INFO: waiting for watch events with expected annotations
  Jul 22 13:15:54.079: INFO: saw patched and updated annotations
  STEP: deleting @ 07/22/23 13:15:54.079
  STEP: deleting a collection @ 07/22/23 13:15:54.101
  Jul 22 13:15:54.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8043" for this suite. @ 07/22/23 13:15:54.138
• [0.203 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/22/23 13:15:54.15
  Jul 22 13:15:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename taint-single-pod @ 07/22/23 13:15:54.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:15:54.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:15:54.186
  Jul 22 13:15:54.191: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:15:54.912025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:55.912179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:56.912668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:57.912849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:58.913027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:15:59.913189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:00.913728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:01.913843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:02.914132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:03.914345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:04.914660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:05.914713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:06.914805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:07.914906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:08.915642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:09.915987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:10.916182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:11.916315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:12.916758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:13.917104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:14.918077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:15.918220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:16.918868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:17.919388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:18.919554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:19.919621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:20.919939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:21.920130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:22.921044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:23.921350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:24.921440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:25.921512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:26.922188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:27.922335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:28.923450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:29.923603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:30.923692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:31.923784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:32.923996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:33.924336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:34.925134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:35.926136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:36.926731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:37.926973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:38.927014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:39.927144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:40.928075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:41.928393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:42.929072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:43.929310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:44.930354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:45.930485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:46.930830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:47.931657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:48.931794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:49.931924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:50.932458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:51.933059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:52.933578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:53.934200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:16:54.217: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 13:16:54.224: INFO: Starting informer...
  STEP: Starting pod... @ 07/22/23 13:16:54.224
  Jul 22 13:16:54.450: INFO: Pod is running on ip-172-31-15-55. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/22/23 13:16:54.451
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/22/23 13:16:54.471
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/22/23 13:16:54.477
  Jul 22 13:16:54.477: INFO: Pod wasn't evicted. Proceeding
  Jul 22 13:16:54.477: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/22/23 13:16:54.497
  STEP: Waiting some time to make sure that toleration time passed. @ 07/22/23 13:16:54.629
  E0722 13:16:54.934270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:55.935260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:56.935382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:57.935503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:58.935626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:16:59.935971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:00.936133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:01.936344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:02.936516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:03.936611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:04.936759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:05.936867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:06.937209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:07.938344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:08.938322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:09.938416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:10.938789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:11.938934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:12.939066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:13.939294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:14.939393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:15.939557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:16.939707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:17.939816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:18.939986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:19.940079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:20.941059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:21.941220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:22.941326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:23.941547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:24.941613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:25.941714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:26.941916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:27.943457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:28.943638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:29.943734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:30.944164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:31.944774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:32.951014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:33.951074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:34.951233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:35.951932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:36.952048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:37.952111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:38.952226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:39.952354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:40.952507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:41.952662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:42.952948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:43.953084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:44.954179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:45.954316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:46.955248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:47.955686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:48.955897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:49.956657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:50.956713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:51.956830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:52.957002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:53.957994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:54.958109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:55.958593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:56.959664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:57.960563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:58.960752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:17:59.960858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:00.961093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:01.961235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:02.962173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:03.962361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:04.962801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:05.962863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:06.962978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:07.963067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:08.963183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:09.629: INFO: Pod wasn't evicted. Test successful
  Jul 22 13:18:09.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-5833" for this suite. @ 07/22/23 13:18:09.637
• [135.498 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 07/22/23 13:18:09.65
  Jul 22 13:18:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 13:18:09.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:09.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:09.691
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/22/23 13:18:09.727
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 13:18:09.736
  Jul 22 13:18:09.743: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:09.743: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:09.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:18:09.748: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:18:09.963435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:10.754: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:10.755: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:10.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:18:10.761: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:18:10.964474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:11.754: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:11.755: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:11.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:18:11.761: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/22/23 13:18:11.766
  Jul 22 13:18:11.789: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:11.789: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:11.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:18:11.795: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:18:11.964789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:12.802: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:12.802: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:12.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:18:12.809: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:18:12.965176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:13.806: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:13.806: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:18:13.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:18:13.811: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/22/23 13:18:13.811
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 13:18:13.823
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9399, will wait for the garbage collector to delete the pods @ 07/22/23 13:18:13.823
  Jul 22 13:18:13.896: INFO: Deleting DaemonSet.extensions daemon-set took: 10.257404ms
  E0722 13:18:13.965570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:13.998: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.404141ms
  E0722 13:18:14.966151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:15.966400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:16.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:18:16.703: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 13:18:16.708: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30352"},"items":null}

  Jul 22 13:18:16.713: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30352"},"items":null}

  Jul 22 13:18:16.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9399" for this suite. @ 07/22/23 13:18:16.742
• [7.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/22/23 13:18:16.761
  Jul 22 13:18:16.761: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:18:16.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:16.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:16.793
  STEP: Setting up server cert @ 07/22/23 13:18:16.839
  E0722 13:18:16.967124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:18:17.246
  STEP: Deploying the webhook pod @ 07/22/23 13:18:17.261
  STEP: Wait for the deployment to be ready @ 07/22/23 13:18:17.28
  Jul 22 13:18:17.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:18:17.967707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:18.967789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:18:19.306
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:18:19.321
  E0722 13:18:19.967833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:20.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/22/23 13:18:20.328
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/22/23 13:18:20.33
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/22/23 13:18:20.33
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/22/23 13:18:20.33
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/22/23 13:18:20.331
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/22/23 13:18:20.331
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/22/23 13:18:20.332
  Jul 22 13:18:20.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1773" for this suite. @ 07/22/23 13:18:20.39
  STEP: Destroying namespace "webhook-markers-9261" for this suite. @ 07/22/23 13:18:20.402
• [3.655 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/22/23 13:18:20.418
  Jul 22 13:18:20.418: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:18:20.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:20.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:20.449
  STEP: Creating secret with name secret-test-map-ff51114a-a7a9-41b9-a201-8a55eba8b101 @ 07/22/23 13:18:20.454
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:18:20.464
  E0722 13:18:20.968975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:21.969111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:22.969934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:23.970049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:18:24.501
  Jul 22 13:18:24.506: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-509723f7-23b4-4830-ac38-9f8a0362fdd4 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:18:24.57
  Jul 22 13:18:24.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8857" for this suite. @ 07/22/23 13:18:24.602
• [4.196 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/22/23 13:18:24.614
  Jul 22 13:18:24.614: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:18:24.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:24.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:24.647
  STEP: validating api versions @ 07/22/23 13:18:24.663
  Jul 22 13:18:24.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-729 api-versions'
  Jul 22 13:18:24.752: INFO: stderr: ""
  Jul 22 13:18:24.752: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul 22 13:18:24.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-729" for this suite. @ 07/22/23 13:18:24.758
• [0.158 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/22/23 13:18:24.772
  Jul 22 13:18:24.772: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/22/23 13:18:24.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:24.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:24.808
  STEP: fetching the /apis discovery document @ 07/22/23 13:18:24.817
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/22/23 13:18:24.821
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/22/23 13:18:24.821
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/22/23 13:18:24.821
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/22/23 13:18:24.824
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/22/23 13:18:24.824
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/22/23 13:18:24.831
  Jul 22 13:18:24.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4587" for this suite. @ 07/22/23 13:18:24.843
• [0.084 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/22/23 13:18:24.857
  Jul 22 13:18:24.857: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:18:24.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:24.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:24.891
  STEP: creating service in namespace services-4195 @ 07/22/23 13:18:24.898
  STEP: creating service affinity-nodeport-transition in namespace services-4195 @ 07/22/23 13:18:24.898
  STEP: creating replication controller affinity-nodeport-transition in namespace services-4195 @ 07/22/23 13:18:24.929
  I0722 13:18:24.942497      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-4195, replica count: 3
  E0722 13:18:24.970735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:25.971813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:26.972092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:27.972306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:18:27.993739      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0722 13:18:28.972418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:29.972591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:30.973013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:18:30.994273      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 13:18:31.013: INFO: Creating new exec pod
  E0722 13:18:31.973134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:32.974185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:33.974342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:34.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul 22 13:18:34.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul 22 13:18:34.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:34.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.73 80'
  Jul 22 13:18:34.498: INFO: stderr: "+ + nc -v -t -w 2echo hostName\n 10.152.183.73 80\nConnection to 10.152.183.73 80 port [tcp/http] succeeded!\n"
  Jul 22 13:18:34.498: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:34.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.55 31710'
  Jul 22 13:18:34.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.55 31710\nConnection to 172.31.15.55 31710 port [tcp/*] succeeded!\n"
  Jul 22 13:18:34.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:34.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.81.237 31710'
  Jul 22 13:18:34.880: INFO: stderr: "+ nc -v -t -w 2 172.31.81.237 31710\nConnection to 172.31.81.237 31710 port [tcp/*] succeeded!\n+ echo hostName\n"
  Jul 22 13:18:34.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:34.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.55:31710/ ; done'
  E0722 13:18:34.975285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:35.173: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n"
  Jul 22 13:18:35.173: INFO: stdout: "\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-bpr8t\naffinity-nodeport-transition-bpr8t\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-bpr8t\naffinity-nodeport-transition-bpr8t\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-bpr8t\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-bsfcp\naffinity-nodeport-transition-bpr8t"
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bsfcp
  Jul 22 13:18:35.173: INFO: Received response from host: affinity-nodeport-transition-bpr8t
  Jul 22 13:18:35.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4195 exec execpod-affinitytsxjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.55:31710/ ; done'
  Jul 22 13:18:35.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:31710/\n"
  Jul 22 13:18:35.460: INFO: stdout: "\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8\naffinity-nodeport-transition-552d8"
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Received response from host: affinity-nodeport-transition-552d8
  Jul 22 13:18:35.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 13:18:35.468: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4195, will wait for the garbage collector to delete the pods @ 07/22/23 13:18:35.5
  Jul 22 13:18:35.571: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.251938ms
  Jul 22 13:18:35.672: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.152124ms
  E0722 13:18:35.976068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:36.976449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4195" for this suite. @ 07/22/23 13:18:37.804
• [12.960 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/22/23 13:18:37.818
  Jul 22 13:18:37.818: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:18:37.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:37.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:37.851
  Jul 22 13:18:37.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-8338 version'
  Jul 22 13:18:37.939: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul 22 13:18:37.939: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-20T02:05:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul 22 13:18:37.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8338" for this suite. @ 07/22/23 13:18:37.947
• [0.139 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/22/23 13:18:37.957
  Jul 22 13:18:37.957: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:18:37.958
  E0722 13:18:37.976579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:37.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:37.985
  STEP: creating the pod @ 07/22/23 13:18:37.988
  STEP: submitting the pod to kubernetes @ 07/22/23 13:18:37.989
  E0722 13:18:38.977072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:39.977333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/22/23 13:18:40.022
  STEP: updating the pod @ 07/22/23 13:18:40.032
  Jul 22 13:18:40.554: INFO: Successfully updated pod "pod-update-03764c24-811b-42bb-9f38-508a9c6d3d7a"
  STEP: verifying the updated pod is in kubernetes @ 07/22/23 13:18:40.562
  Jul 22 13:18:40.570: INFO: Pod update OK
  Jul 22 13:18:40.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1085" for this suite. @ 07/22/23 13:18:40.575
• [2.628 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/22/23 13:18:40.588
  Jul 22 13:18:40.588: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:18:40.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:40.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:40.621
  STEP: creating a collection of services @ 07/22/23 13:18:40.627
  Jul 22 13:18:40.627: INFO: Creating e2e-svc-a-48n47
  Jul 22 13:18:40.646: INFO: Creating e2e-svc-b-p2cs2
  Jul 22 13:18:40.671: INFO: Creating e2e-svc-c-vl2sk
  STEP: deleting service collection @ 07/22/23 13:18:40.71
  Jul 22 13:18:40.759: INFO: Collection of services has been deleted
  Jul 22 13:18:40.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6895" for this suite. @ 07/22/23 13:18:40.767
• [0.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/22/23 13:18:40.78
  Jul 22 13:18:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:18:40.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:40.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:40.814
  STEP: creating service in namespace services-4136 @ 07/22/23 13:18:40.823
  STEP: creating service affinity-nodeport in namespace services-4136 @ 07/22/23 13:18:40.823
  STEP: creating replication controller affinity-nodeport in namespace services-4136 @ 07/22/23 13:18:40.848
  I0722 13:18:40.861642      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4136, replica count: 3
  E0722 13:18:40.977492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:41.977645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:42.977723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:18:43.913308      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 13:18:43.928: INFO: Creating new exec pod
  E0722 13:18:43.978542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:44.979604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:45.980167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:46.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4136 exec execpod-affinityhlx4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0722 13:18:46.980728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:47.156: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul 22 13:18:47.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:47.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4136 exec execpod-affinityhlx4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.142 80'
  Jul 22 13:18:47.368: INFO: stderr: "+ nc -v -t -w 2 10.152.183.142 80\n+ echo hostName\nConnection to 10.152.183.142 80 port [tcp/http] succeeded!\n"
  Jul 22 13:18:47.368: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:47.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4136 exec execpod-affinityhlx4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.81.237 30996'
  Jul 22 13:18:47.603: INFO: stderr: "+ nc -v -t -w 2 172.31.81.237 30996\n+ echo hostName\nConnection to 172.31.81.237 30996 port [tcp/*] succeeded!\n"
  Jul 22 13:18:47.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:47.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4136 exec execpod-affinityhlx4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.55 30996'
  Jul 22 13:18:47.827: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.55 30996\nConnection to 172.31.15.55 30996 port [tcp/*] succeeded!\n"
  Jul 22 13:18:47.827: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:18:47.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-4136 exec execpod-affinityhlx4j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.15.55:30996/ ; done'
  E0722 13:18:47.981647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:48.160: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.15.55:30996/\n"
  Jul 22 13:18:48.160: INFO: stdout: "\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs\naffinity-nodeport-slcgs"
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Received response from host: affinity-nodeport-slcgs
  Jul 22 13:18:48.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 13:18:48.166: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4136, will wait for the garbage collector to delete the pods @ 07/22/23 13:18:48.192
  Jul 22 13:18:48.268: INFO: Deleting ReplicationController affinity-nodeport took: 14.119058ms
  Jul 22 13:18:48.369: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.8553ms
  E0722 13:18:48.982327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:49.982692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4136" for this suite. @ 07/22/23 13:18:50.904
• [10.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/22/23 13:18:50.919
  Jul 22 13:18:50.919: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:18:50.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:50.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:50.954
  Jul 22 13:18:50.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 create -f -'
  E0722 13:18:50.982952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:51.983168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:52.015: INFO: stderr: ""
  Jul 22 13:18:52.015: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul 22 13:18:52.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 create -f -'
  Jul 22 13:18:52.333: INFO: stderr: ""
  Jul 22 13:18:52.333: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/22/23 13:18:52.334
  E0722 13:18:52.983261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:18:53.342: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:18:53.342: INFO: Found 1 / 1
  Jul 22 13:18:53.342: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 22 13:18:53.351: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:18:53.351: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 22 13:18:53.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 describe pod agnhost-primary-29tkh'
  Jul 22 13:18:53.457: INFO: stderr: ""
  Jul 22 13:18:53.457: INFO: stdout: "Name:             agnhost-primary-29tkh\nNamespace:        kubectl-7865\nPriority:         0\nService Account:  default\nNode:             ip-172-31-15-55/172.31.15.55\nStart Time:       Sat, 22 Jul 2023 13:18:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.196.152\nIPs:\n  IP:           192.168.196.152\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5becf82fcac14fdf22dfa68b04bc914811443782d0b7992872e201a08cba06cb\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 22 Jul 2023 13:18:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2cs8k (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2cs8k:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7865/agnhost-primary-29tkh to ip-172-31-15-55\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jul 22 13:18:53.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 describe rc agnhost-primary'
  Jul 22 13:18:53.565: INFO: stderr: ""
  Jul 22 13:18:53.565: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7865\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-29tkh\n"
  Jul 22 13:18:53.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 describe service agnhost-primary'
  Jul 22 13:18:53.679: INFO: stderr: ""
  Jul 22 13:18:53.679: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7865\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.37\nIPs:               10.152.183.37\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.196.152:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul 22 13:18:53.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 describe node ip-172-31-15-55'
  Jul 22 13:18:53.822: INFO: stderr: ""
  Jul 22 13:18:53.822: INFO: stdout: "Name:               ip-172-31-15-55\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    juju-charm=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-15-55\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 22 Jul 2023 11:54:25 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-15-55\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 22 Jul 2023 13:18:50 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 22 Jul 2023 13:16:21 +0000   Sat, 22 Jul 2023 11:54:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 22 Jul 2023 13:16:21 +0000   Sat, 22 Jul 2023 11:54:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 22 Jul 2023 13:16:21 +0000   Sat, 22 Jul 2023 11:54:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 22 Jul 2023 13:16:21 +0000   Sat, 22 Jul 2023 11:55:18 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.15.55\n  Hostname:    ip-172-31-15-55\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8046112Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7943712Ki\n  pods:               110\nSystem Info:\n  Machine ID:                      ec2289cd62f7478e071f8a68567a6291\n  System UUID:                     ec2289cd-62f7-478e-071f-8a68567a6291\n  Boot ID:                         c43d347c-760c-4266-b21f-f4a8d19b59cc\n  Kernel Version:                  5.19.0-1028-aws\n  OS Image:                        Ubuntu 22.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.6.8\n  Kubelet Version:                 v1.27.4\n  Kube-Proxy Version:              v1.27.4\nNon-terminated Pods:               (5 in total)\n  Namespace                        Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                        ----                                                       ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  default-http-backend-kubernetes-worker-65fc475d49-v87jq    10m (0%)      10m (0%)    20Mi (0%)        20Mi (0%)      119s\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-8wmnx           0 (0%)        0 (0%)      0 (0%)           0 (0%)         107s\n  kube-system                      calico-kube-controllers-79b76dbbcc-4dlzp                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         119s\n  kubectl-7865                     agnhost-primary-29tkh                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  sonobuoy                         sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                10m (0%)   10m (0%)\n  memory             20Mi (0%)  20Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
  Jul 22 13:18:53.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-7865 describe namespace kubectl-7865'
  Jul 22 13:18:53.920: INFO: stderr: ""
  Jul 22 13:18:53.920: INFO: stdout: "Name:         kubectl-7865\nLabels:       e2e-framework=kubectl\n              e2e-run=c04793bf-29a9-4416-8432-0365d1fcd818\n              kubernetes.io/metadata.name=kubectl-7865\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul 22 13:18:53.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7865" for this suite. @ 07/22/23 13:18:53.928
• [3.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/22/23 13:18:53.942
  Jul 22 13:18:53.942: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/22/23 13:18:53.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:18:53.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:18:53.972
  E0722 13:18:53.983942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the container to handle the HTTPGet hook request. @ 07/22/23 13:18:53.987
  E0722 13:18:54.984556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:55.984880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/22/23 13:18:56.024
  E0722 13:18:56.985006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:57.985183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 07/22/23 13:18:58.051
  E0722 13:18:58.985257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:18:59.985408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 07/22/23 13:19:00.079
  Jul 22 13:19:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9969" for this suite. @ 07/22/23 13:19:00.106
• [6.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/22/23 13:19:00.122
  Jul 22 13:19:00.122: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 13:19:00.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:19:00.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:19:00.176
  STEP: apply creating a deployment @ 07/22/23 13:19:00.186
  Jul 22 13:19:00.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1958" for this suite. @ 07/22/23 13:19:00.224
• [0.113 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/22/23 13:19:00.237
  Jul 22 13:19:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 13:19:00.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:19:00.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:19:00.277
  STEP: Creating service test in namespace statefulset-9671 @ 07/22/23 13:19:00.288
  STEP: Creating statefulset ss in namespace statefulset-9671 @ 07/22/23 13:19:00.299
  Jul 22 13:19:00.328: INFO: Found 0 stateful pods, waiting for 1
  E0722 13:19:00.986150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:01.986375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:02.986773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:03.986901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:04.987058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:05.987699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:06.988131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:07.988482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:08.988554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:09.988700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:19:10.339: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/22/23 13:19:10.351
  STEP: Getting /status @ 07/22/23 13:19:10.368
  Jul 22 13:19:10.375: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/22/23 13:19:10.375
  Jul 22 13:19:10.391: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/22/23 13:19:10.391
  Jul 22 13:19:10.397: INFO: Observed &StatefulSet event: ADDED
  Jul 22 13:19:10.397: INFO: Found Statefulset ss in namespace statefulset-9671 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 13:19:10.397: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/22/23 13:19:10.397
  Jul 22 13:19:10.397: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 22 13:19:10.409: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/22/23 13:19:10.409
  Jul 22 13:19:10.412: INFO: Observed &StatefulSet event: ADDED
  Jul 22 13:19:10.412: INFO: Deleting all statefulset in ns statefulset-9671
  Jul 22 13:19:10.418: INFO: Scaling statefulset ss to 0
  E0722 13:19:10.989383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:11.989348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:12.989697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:13.989810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:14.989978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:15.990098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:16.990241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:17.991443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:18.991698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:19.991798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:19:20.445: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:19:20.451: INFO: Deleting statefulset ss
  Jul 22 13:19:20.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9671" for this suite. @ 07/22/23 13:19:20.492
• [20.272 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/22/23 13:19:20.511
  Jul 22 13:19:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:19:20.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:19:20.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:19:20.556
  STEP: Creating the pod @ 07/22/23 13:19:20.572
  E0722 13:19:20.992535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:21.993012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:22.993883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:19:23.148: INFO: Successfully updated pod "annotationupdated0733940-bcaa-4ffb-9521-ecb6c6805fe6"
  E0722 13:19:23.993968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:24.994331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:19:25.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3911" for this suite. @ 07/22/23 13:19:25.182
• [4.682 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/22/23 13:19:25.194
  Jul 22 13:19:25.194: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename lease-test @ 07/22/23 13:19:25.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:19:25.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:19:25.224
  Jul 22 13:19:25.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-9767" for this suite. @ 07/22/23 13:19:25.336
• [0.155 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/22/23 13:19:25.349
  Jul 22 13:19:25.349: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption @ 07/22/23 13:19:25.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:19:25.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:19:25.384
  Jul 22 13:19:25.415: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:19:25.994798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:26.995779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:27.996341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:28.997188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:29.997307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:30.998250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:31.998354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:32.998816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:33.999911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:35.000331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:36.000481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:37.000648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:38.001768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:39.002876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:40.002941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:41.003633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:42.003727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:43.003972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:44.005002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:45.005083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:46.006140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:47.006275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:48.006446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:49.007146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:50.007388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:51.008227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:52.008301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:53.008490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:54.009064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:55.009268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:56.009279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:57.009594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:58.010211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:19:59.010379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:00.010696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:01.011546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:02.011656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:03.011721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:04.012159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:05.012350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:06.013336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:07.013470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:08.014152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:09.014537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:10.015140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:11.015322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:12.015597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:13.015933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:14.017015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:15.017158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:16.018060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:17.018483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:18.018590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:19.018694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:20.019520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:21.019956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:22.020595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:23.020647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:24.021322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:25.021428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:20:25.446: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/22/23 13:20:25.451
  Jul 22 13:20:25.451: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/22/23 13:20:25.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:20:25.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:20:25.51
  STEP: Finding an available node @ 07/22/23 13:20:25.533
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/22/23 13:20:25.533
  E0722 13:20:26.021757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:27.021823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/22/23 13:20:27.576
  Jul 22 13:20:27.612: INFO: found a healthy node: ip-172-31-15-55
  E0722 13:20:28.022559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:29.023154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:30.024067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:31.025236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:32.025970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:33.026424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:20:33.746: INFO: pods created so far: [1 1 1]
  Jul 22 13:20:33.746: INFO: length of pods created so far: 3
  E0722 13:20:34.026986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:35.027150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:20:35.764: INFO: pods created so far: [2 2 1]
  E0722 13:20:36.027437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:37.027519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:38.028323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:39.028401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:40.028656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:41.029114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:42.029219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:20:42.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 13:20:42.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3592" for this suite. @ 07/22/23 13:20:42.889
  STEP: Destroying namespace "sched-preemption-4788" for this suite. @ 07/22/23 13:20:42.903
• [77.564 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/22/23 13:20:42.917
  Jul 22 13:20:42.917: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context-test @ 07/22/23 13:20:42.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:20:42.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:20:42.96
  E0722 13:20:43.029871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:44.030189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:45.031044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:46.031351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:47.032013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:20:47.040: INFO: Got logs for pod "busybox-privileged-false-327905a3-bcf5-4891-aac9-8fc905677f8e": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul 22 13:20:47.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8619" for this suite. @ 07/22/23 13:20:47.057
• [4.152 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/22/23 13:20:47.07
  Jul 22 13:20:47.070: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:20:47.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:20:47.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:20:47.098
  E0722 13:20:48.032382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:49.032375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:50.032868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:51.033110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:52.033226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:53.034151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:54.034189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:55.034339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:56.034453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:57.034574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:58.034670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:20:59.034812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:00.034876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:01.034992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:02.035079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:03.035208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:04.035620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:05.036226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:06.036405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:07.036955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:08.036992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:09.037110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:09.208: INFO: Container started at 2023-07-22 13:20:47 +0000 UTC, pod became ready at 2023-07-22 13:21:07 +0000 UTC
  Jul 22 13:21:09.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-8605" for this suite. @ 07/22/23 13:21:09.213
• [22.162 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/22/23 13:21:09.234
  Jul 22 13:21:09.234: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:21:09.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:09.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:09.277
  STEP: Setting up server cert @ 07/22/23 13:21:09.339
  E0722 13:21:10.038126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:21:10.189
  STEP: Deploying the webhook pod @ 07/22/23 13:21:10.204
  STEP: Wait for the deployment to be ready @ 07/22/23 13:21:10.232
  Jul 22 13:21:10.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:21:11.038248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:12.038394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:21:12.271
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:21:12.288
  E0722 13:21:13.038538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:13.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/22/23 13:21:13.296
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/22/23 13:21:13.325
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/22/23 13:21:13.339
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/22/23 13:21:13.355
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/22/23 13:21:13.374
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/22/23 13:21:13.385
  Jul 22 13:21:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7045" for this suite. @ 07/22/23 13:21:13.477
  STEP: Destroying namespace "webhook-markers-77" for this suite. @ 07/22/23 13:21:13.492
• [4.272 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/22/23 13:21:13.506
  Jul 22 13:21:13.506: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename watch @ 07/22/23 13:21:13.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:13.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:13.544
  STEP: getting a starting resourceVersion @ 07/22/23 13:21:13.548
  STEP: starting a background goroutine to produce watch events @ 07/22/23 13:21:13.553
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/22/23 13:21:13.553
  E0722 13:21:14.038546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:15.039141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:16.040205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:16.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5572" for this suite. @ 07/22/23 13:21:16.367
• [2.912 seconds]
------------------------------
SS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/22/23 13:21:16.419
  Jul 22 13:21:16.419: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename prestop @ 07/22/23 13:21:16.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:16.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:16.467
  STEP: Creating server pod server in namespace prestop-127 @ 07/22/23 13:21:16.472
  STEP: Waiting for pods to come up. @ 07/22/23 13:21:16.488
  E0722 13:21:17.040176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:18.040315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-127 @ 07/22/23 13:21:18.509
  E0722 13:21:19.040430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:20.040872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 07/22/23 13:21:20.532
  E0722 13:21:21.040967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:22.041198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:23.041716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:24.042376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:25.042835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:25.556: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul 22 13:21:25.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/22/23 13:21:25.569
  STEP: Destroying namespace "prestop-127" for this suite. @ 07/22/23 13:21:25.589
• [9.201 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/22/23 13:21:25.623
  Jul 22 13:21:25.623: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 13:21:25.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:25.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:25.668
  Jul 22 13:21:25.680: INFO: Got root ca configmap in namespace "svcaccounts-1451"
  Jul 22 13:21:25.690: INFO: Deleted root ca configmap in namespace "svcaccounts-1451"
  E0722 13:21:26.043472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 07/22/23 13:21:26.191
  Jul 22 13:21:26.196: INFO: Recreated root ca configmap in namespace "svcaccounts-1451"
  Jul 22 13:21:26.204: INFO: Updated root ca configmap in namespace "svcaccounts-1451"
  STEP: waiting for the root ca configmap reconciled @ 07/22/23 13:21:26.705
  Jul 22 13:21:26.711: INFO: Reconciled root ca configmap in namespace "svcaccounts-1451"
  Jul 22 13:21:26.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1451" for this suite. @ 07/22/23 13:21:26.716
• [1.105 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/22/23 13:21:26.729
  Jul 22 13:21:26.729: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:21:26.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:26.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:26.764
  STEP: Creating secret with name secret-test-aef12483-4703-4c2e-b388-2dc48fa38cb5 @ 07/22/23 13:21:26.771
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:21:26.778
  E0722 13:21:27.043971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:28.044296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:29.044413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:30.044658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:21:30.815
  Jul 22 13:21:30.835: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-6df5e893-5647-45ba-a498-c6c1da1f06c7 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:21:30.86
  Jul 22 13:21:30.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6738" for this suite. @ 07/22/23 13:21:30.891
• [4.173 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/22/23 13:21:30.902
  Jul 22 13:21:30.902: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubelet-test @ 07/22/23 13:21:30.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:30.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:30.936
  E0722 13:21:31.044795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:32.045826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:32.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1928" for this suite. @ 07/22/23 13:21:32.986
• [2.099 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/22/23 13:21:33.002
  Jul 22 13:21:33.002: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 13:21:33.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:33.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:33.034
  E0722 13:21:33.048497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating Indexed job @ 07/22/23 13:21:33.049
  STEP: Ensuring job reaches completions @ 07/22/23 13:21:33.058
  E0722 13:21:34.048853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:35.049264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:36.049888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:37.050106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:38.050418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:39.050691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:40.051883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:41.052501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 07/22/23 13:21:41.065
  Jul 22 13:21:41.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1612" for this suite. @ 07/22/23 13:21:41.078
• [8.087 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/22/23 13:21:41.089
  Jul 22 13:21:41.089: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:21:41.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:41.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:41.126
  STEP: creating the pod @ 07/22/23 13:21:41.13
  STEP: submitting the pod to kubernetes @ 07/22/23 13:21:41.13
  STEP: verifying QOS class is set on the pod @ 07/22/23 13:21:41.143
  Jul 22 13:21:41.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9555" for this suite. @ 07/22/23 13:21:41.156
• [0.079 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/22/23 13:21:41.168
  Jul 22 13:21:41.169: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 13:21:41.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:41.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:41.317
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/22/23 13:21:41.321
  Jul 22 13:21:41.322: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:21:42.052529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:43.052847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:44.053850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:45.054966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:46.055656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:47.055935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:48.056213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/22/23 13:21:48.455
  Jul 22 13:21:48.455: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:21:49.056451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:50.056537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:50.067: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:21:51.057238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:52.057377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:53.058150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:54.059142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:55.059174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:56.060212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:56.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1597" for this suite. @ 07/22/23 13:21:56.876
• [15.718 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/22/23 13:21:56.889
  Jul 22 13:21:56.889: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:21:56.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:21:56.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:21:56.932
  STEP: creating service endpoint-test2 in namespace services-7138 @ 07/22/23 13:21:56.935
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7138 to expose endpoints map[] @ 07/22/23 13:21:56.95
  Jul 22 13:21:56.956: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0722 13:21:57.061131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:21:57.975: INFO: successfully validated that service endpoint-test2 in namespace services-7138 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7138 @ 07/22/23 13:21:57.976
  E0722 13:21:58.061593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:21:59.061711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7138 to expose endpoints map[pod1:[80]] @ 07/22/23 13:22:00.013
  Jul 22 13:22:00.032: INFO: successfully validated that service endpoint-test2 in namespace services-7138 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/22/23 13:22:00.033
  Jul 22 13:22:00.033: INFO: Creating new exec pod
  E0722 13:22:00.062616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:01.062801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:02.063503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:03.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0722 13:22:03.063713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:03.225: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:03.225: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:22:03.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.41 80'
  Jul 22 13:22:03.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.41 80\nConnection to 10.152.183.41 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:03.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7138 @ 07/22/23 13:22:03.409
  E0722 13:22:04.064114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:05.064508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7138 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/22/23 13:22:05.438
  Jul 22 13:22:05.462: INFO: successfully validated that service endpoint-test2 in namespace services-7138 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/22/23 13:22:05.463
  E0722 13:22:06.064483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:06.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 22 13:22:06.630: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:06.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:22:06.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.41 80'
  Jul 22 13:22:06.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.41 80\nConnection to 10.152.183.41 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:06.824: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7138 @ 07/22/23 13:22:06.824
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7138 to expose endpoints map[pod2:[80]] @ 07/22/23 13:22:06.841
  Jul 22 13:22:06.866: INFO: successfully validated that service endpoint-test2 in namespace services-7138 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/22/23 13:22:06.866
  E0722 13:22:07.065005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:07.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 22 13:22:08.049: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:08.049: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 22 13:22:08.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=services-7138 exec execpodvhhb8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.41 80'
  E0722 13:22:08.065560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:08.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.41 80\nConnection to 10.152.183.41 80 port [tcp/http] succeeded!\n"
  Jul 22 13:22:08.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7138 @ 07/22/23 13:22:08.284
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7138 to expose endpoints map[] @ 07/22/23 13:22:08.303
  E0722 13:22:09.065813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:09.320: INFO: successfully validated that service endpoint-test2 in namespace services-7138 exposes endpoints map[]
  Jul 22 13:22:09.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7138" for this suite. @ 07/22/23 13:22:09.356
• [12.476 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/22/23 13:22:09.367
  Jul 22 13:22:09.367: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:22:09.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:09.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:09.402
  STEP: Creating configMap with name projected-configmap-test-volume-f03975c3-c022-4731-a3dc-fd86e8be1c90 @ 07/22/23 13:22:09.406
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:22:09.414
  E0722 13:22:10.066174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:11.066343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:12.066494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:13.066568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:22:13.449
  Jul 22 13:22:13.454: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-9f86859c-299c-417e-93e8-6f178072d5ea container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:22:13.467
  Jul 22 13:22:13.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5095" for this suite. @ 07/22/23 13:22:13.503
• [4.150 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/22/23 13:22:13.517
  Jul 22 13:22:13.517: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context-test @ 07/22/23 13:22:13.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:13.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:13.56
  E0722 13:22:14.066712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:15.067283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:16.068100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:17.068531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:17.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5305" for this suite. @ 07/22/23 13:22:17.643
• [4.135 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/22/23 13:22:17.653
  Jul 22 13:22:17.653: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/22/23 13:22:17.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:17.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:17.682
  Jul 22 13:22:17.685: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:22:18.068611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:18.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-473" for this suite. @ 07/22/23 13:22:18.737
• [1.098 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/22/23 13:22:18.753
  Jul 22 13:22:18.753: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:22:18.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:18.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:18.796
  STEP: Setting up server cert @ 07/22/23 13:22:18.859
  E0722 13:22:19.069033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:22:19.429
  STEP: Deploying the webhook pod @ 07/22/23 13:22:19.439
  STEP: Wait for the deployment to be ready @ 07/22/23 13:22:19.456
  Jul 22 13:22:19.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:22:20.069449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:21.069571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:22:21.489
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:22:21.506
  E0722 13:22:22.069773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:22.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/22/23 13:22:22.512
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/22/23 13:22:22.534
  Jul 22 13:22:22.534: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:22:22.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-777" for this suite. @ 07/22/23 13:22:22.643
  STEP: Destroying namespace "webhook-markers-7620" for this suite. @ 07/22/23 13:22:22.654
• [3.911 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 07/22/23 13:22:22.669
  Jul 22 13:22:22.669: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 13:22:22.67
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:22.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:22.747
  STEP: Creating simple DaemonSet "daemon-set" @ 07/22/23 13:22:22.779
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 13:22:22.786
  Jul 22 13:22:22.792: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:22.792: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:22.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:22:22.798: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:22:23.069940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:23.806: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:23.806: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:23.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:22:23.811: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:22:24.070088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:24.806: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:24.806: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:24.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:22:24.813: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/22/23 13:22:24.818
  Jul 22 13:22:24.841: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:24.841: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:24.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:22:24.846: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:22:25.070848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:25.855: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:25.855: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:25.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:22:25.862: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:22:26.071004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:26.854: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:26.854: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:26.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:22:26.859: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:22:27.071435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:27.852: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:27.852: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:22:27.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:22:27.858: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 13:22:27.864
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5671, will wait for the garbage collector to delete the pods @ 07/22/23 13:22:27.864
  Jul 22 13:22:27.930: INFO: Deleting DaemonSet.extensions daemon-set took: 10.673201ms
  Jul 22 13:22:28.031: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.556999ms
  E0722 13:22:28.072362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:29.072708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:29.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:22:29.338: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 13:22:29.343: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32816"},"items":null}

  Jul 22 13:22:29.348: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32816"},"items":null}

  Jul 22 13:22:29.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5671" for this suite. @ 07/22/23 13:22:29.374
• [6.715 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/22/23 13:22:29.386
  Jul 22 13:22:29.386: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename discovery @ 07/22/23 13:22:29.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:29.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:29.411
  STEP: Setting up server cert @ 07/22/23 13:22:29.419
  Jul 22 13:22:29.988: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul 22 13:22:29.990: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul 22 13:22:29.990: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul 22 13:22:29.990: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul 22 13:22:29.990: INFO: Checking APIGroup: apps
  Jul 22 13:22:29.991: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul 22 13:22:29.991: INFO: Versions found [{apps/v1 v1}]
  Jul 22 13:22:29.991: INFO: apps/v1 matches apps/v1
  Jul 22 13:22:29.991: INFO: Checking APIGroup: events.k8s.io
  Jul 22 13:22:29.992: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul 22 13:22:29.992: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul 22 13:22:29.992: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul 22 13:22:29.992: INFO: Checking APIGroup: authentication.k8s.io
  Jul 22 13:22:29.993: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul 22 13:22:29.993: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul 22 13:22:29.993: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul 22 13:22:29.993: INFO: Checking APIGroup: authorization.k8s.io
  Jul 22 13:22:29.995: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul 22 13:22:29.995: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul 22 13:22:29.995: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul 22 13:22:29.995: INFO: Checking APIGroup: autoscaling
  Jul 22 13:22:29.997: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul 22 13:22:29.997: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul 22 13:22:29.997: INFO: autoscaling/v2 matches autoscaling/v2
  Jul 22 13:22:29.997: INFO: Checking APIGroup: batch
  Jul 22 13:22:29.998: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul 22 13:22:29.998: INFO: Versions found [{batch/v1 v1}]
  Jul 22 13:22:29.999: INFO: batch/v1 matches batch/v1
  Jul 22 13:22:29.999: INFO: Checking APIGroup: certificates.k8s.io
  Jul 22 13:22:30.001: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul 22 13:22:30.001: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul 22 13:22:30.001: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul 22 13:22:30.001: INFO: Checking APIGroup: networking.k8s.io
  Jul 22 13:22:30.003: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul 22 13:22:30.003: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul 22 13:22:30.003: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul 22 13:22:30.003: INFO: Checking APIGroup: policy
  Jul 22 13:22:30.004: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul 22 13:22:30.004: INFO: Versions found [{policy/v1 v1}]
  Jul 22 13:22:30.004: INFO: policy/v1 matches policy/v1
  Jul 22 13:22:30.004: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul 22 13:22:30.006: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul 22 13:22:30.006: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul 22 13:22:30.006: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul 22 13:22:30.006: INFO: Checking APIGroup: storage.k8s.io
  Jul 22 13:22:30.007: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul 22 13:22:30.007: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul 22 13:22:30.007: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul 22 13:22:30.007: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul 22 13:22:30.008: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul 22 13:22:30.008: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul 22 13:22:30.008: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul 22 13:22:30.008: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul 22 13:22:30.010: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul 22 13:22:30.010: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul 22 13:22:30.010: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul 22 13:22:30.010: INFO: Checking APIGroup: scheduling.k8s.io
  Jul 22 13:22:30.012: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul 22 13:22:30.012: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul 22 13:22:30.012: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul 22 13:22:30.012: INFO: Checking APIGroup: coordination.k8s.io
  Jul 22 13:22:30.014: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul 22 13:22:30.014: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul 22 13:22:30.014: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul 22 13:22:30.015: INFO: Checking APIGroup: node.k8s.io
  Jul 22 13:22:30.016: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul 22 13:22:30.016: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul 22 13:22:30.016: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul 22 13:22:30.016: INFO: Checking APIGroup: discovery.k8s.io
  Jul 22 13:22:30.018: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul 22 13:22:30.018: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul 22 13:22:30.018: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul 22 13:22:30.018: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul 22 13:22:30.020: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul 22 13:22:30.021: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul 22 13:22:30.021: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul 22 13:22:30.021: INFO: Checking APIGroup: metrics.k8s.io
  Jul 22 13:22:30.023: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jul 22 13:22:30.023: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jul 22 13:22:30.023: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jul 22 13:22:30.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-9345" for this suite. @ 07/22/23 13:22:30.029
• [0.655 seconds]
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/22/23 13:22:30.041
  Jul 22 13:22:30.041: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:22:30.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:30.072
  E0722 13:22:30.072702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:30.077
  STEP: Create a pod @ 07/22/23 13:22:30.08
  E0722 13:22:31.072881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:32.073101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/22/23 13:22:32.106
  Jul 22 13:22:32.117: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul 22 13:22:32.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5022" for this suite. @ 07/22/23 13:22:32.123
• [2.094 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/22/23 13:22:32.135
  Jul 22 13:22:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubelet-test @ 07/22/23 13:22:32.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:32.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:32.167
  E0722 13:22:33.073565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:34.074327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:35.074739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:36.074857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:22:36.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2666" for this suite. @ 07/22/23 13:22:36.204
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/22/23 13:22:36.217
  Jul 22 13:22:36.217: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename watch @ 07/22/23 13:22:36.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:36.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:36.293
  STEP: creating a watch on configmaps with a certain label @ 07/22/23 13:22:36.299
  STEP: creating a new configmap @ 07/22/23 13:22:36.311
  STEP: modifying the configmap once @ 07/22/23 13:22:36.318
  STEP: changing the label value of the configmap @ 07/22/23 13:22:36.333
  STEP: Expecting to observe a delete notification for the watched object @ 07/22/23 13:22:36.347
  Jul 22 13:22:36.347: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32921 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 13:22:36.348: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32922 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 13:22:36.348: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32923 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/22/23 13:22:36.349
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/22/23 13:22:36.363
  E0722 13:22:37.075036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:38.075592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:39.075713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:40.075943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:41.076853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:42.077954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:43.078034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:44.078225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:45.078416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:46.078463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 07/22/23 13:22:46.364
  STEP: modifying the configmap a third time @ 07/22/23 13:22:46.376
  STEP: deleting the configmap @ 07/22/23 13:22:46.388
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/22/23 13:22:46.398
  Jul 22 13:22:46.398: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32976 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 13:22:46.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32977 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 13:22:46.399: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2119  37a20852-4387-463e-861e-16e5be1de826 32978 0 2023-07-22 13:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-22 13:22:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 22 13:22:46.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2119" for this suite. @ 07/22/23 13:22:46.406
• [10.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/22/23 13:22:46.418
  Jul 22 13:22:46.418: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption @ 07/22/23 13:22:46.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:22:46.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:22:46.457
  Jul 22 13:22:46.486: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:22:47.079545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:48.079584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:49.080749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:50.080856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:51.080888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:52.081087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:53.082155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:54.082322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:55.082615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:56.082743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:57.083870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:58.083950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:22:59.084071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:00.084198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:01.084437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:02.084698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:03.085025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:04.085060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:05.085673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:06.085766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:07.085894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:08.086110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:09.086560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:10.086435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:11.087543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:12.087620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:13.087753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:14.087864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:15.088137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:16.088372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:17.089148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:18.090155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:19.090403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:20.090625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:21.091422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:22.091715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:23.091888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:24.091978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:25.092705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:26.093055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:27.093200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:28.093332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:29.094215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:30.094357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:31.094697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:32.094764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:33.095243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:34.095297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:35.096396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:36.096475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:37.096661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:38.097674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:39.098189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:40.098332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:41.098637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:42.098978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:43.099436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:44.099539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:45.099736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:46.099788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:23:46.518: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/22/23 13:23:46.529
  Jul 22 13:23:46.529: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/22/23 13:23:46.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:23:46.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:23:46.563
  Jul 22 13:23:46.596: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul 22 13:23:46.600: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul 22 13:23:46.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 22 13:23:46.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3341" for this suite. @ 07/22/23 13:23:46.732
  STEP: Destroying namespace "sched-preemption-1825" for this suite. @ 07/22/23 13:23:46.745
• [60.338 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/22/23 13:23:46.757
  Jul 22 13:23:46.757: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-pred @ 07/22/23 13:23:46.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:23:46.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:23:46.787
  Jul 22 13:23:46.790: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 22 13:23:46.801: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 13:23:46.807: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-15-55 before test
  Jul 22 13:23:46.816: INFO: default-http-backend-kubernetes-worker-65fc475d49-v87jq from ingress-nginx-kubernetes-worker started at 2023-07-22 13:16:54 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.816: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 22 13:23:46.817: INFO: nginx-ingress-controller-kubernetes-worker-8wmnx from ingress-nginx-kubernetes-worker started at 2023-07-22 13:17:06 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.817: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:23:46.817: INFO: calico-kube-controllers-79b76dbbcc-4dlzp from kube-system started at 2023-07-22 13:16:54 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.817: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 22 13:23:46.817: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-wb49j from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:23:46.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:23:46.818: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 13:23:46.818: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-26-93 before test
  Jul 22 13:23:46.832: INFO: nginx-ingress-controller-kubernetes-worker-9f8fn from ingress-nginx-kubernetes-worker started at 2023-07-22 12:03:49 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.832: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: sonobuoy from sonobuoy started at 2023-07-22 12:04:59 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.832: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: sonobuoy-e2e-job-09ff55b4a2944177 from sonobuoy started at 2023-07-22 12:05:02 +0000 UTC (2 container statuses recorded)
  Jul 22 13:23:46.832: INFO: 	Container e2e ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-l8p6k from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:23:46.832: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 22 13:23:46.832: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-81-237 before test
  Jul 22 13:23:46.847: INFO: nginx-ingress-controller-kubernetes-worker-r6kjp from ingress-nginx-kubernetes-worker started at 2023-07-22 11:54:28 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: coredns-5c7f76ccb8-cvjf4 from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container coredns ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: kube-state-metrics-5b95b4459c-pp6nn from kube-system started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: metrics-server-v0.5.2-6cf8c8b69c-lv5pw from kube-system started at 2023-07-22 11:54:21 +0000 UTC (2 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: dashboard-metrics-scraper-6b8586b5c9-29rq6 from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: kubernetes-dashboard-6869f4cd5f-5xvbw from kubernetes-dashboard started at 2023-07-22 11:54:21 +0000 UTC (1 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: sonobuoy-systemd-logs-daemon-set-3e53dc35c922460d-s457l from sonobuoy started at 2023-07-22 12:05:03 +0000 UTC (2 container statuses recorded)
  Jul 22 13:23:46.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 22 13:23:46.847: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/22/23 13:23:46.847
  E0722 13:23:47.100393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:48.100641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/22/23 13:23:48.88
  STEP: Trying to apply a random label on the found node. @ 07/22/23 13:23:48.921
  STEP: verifying the node has the label kubernetes.io/e2e-e3d388a7-bf0d-49b8-b6eb-59b7cbf06cb7 95 @ 07/22/23 13:23:48.938
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/22/23 13:23:48.945
  E0722 13:23:49.101656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:50.102164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.15.55 on the node which pod4 resides and expect not scheduled @ 07/22/23 13:23:50.979
  E0722 13:23:51.102545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:52.102722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:53.103453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:54.104646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:55.105422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:56.106158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:57.106906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:58.107042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:23:59.107574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:00.110417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:01.111061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:02.111186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:03.111604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:04.111790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:05.112795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:06.113080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:07.113584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:08.113769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:09.114735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:10.115108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:11.115805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:12.116180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:13.117112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:14.117266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:15.117718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:16.118420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:17.118752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:18.118947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:19.119484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:20.119889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:21.120366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:22.121061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:23.121987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:24.122128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:25.122361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:26.122504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:27.122720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:28.123016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:29.123134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:30.123473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:31.124494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:32.124955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:33.125107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:34.126180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:35.126244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:36.127312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:37.127466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:38.127600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:39.127729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:40.127845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:41.128445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:42.128464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:43.128648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:44.129178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:45.129518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:46.129598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:47.129753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:48.131570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:49.131999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:50.132071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:51.132201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:52.132381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:53.132706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:54.133149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:55.133181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:56.133327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:57.133448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:58.133579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:24:59.133706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:00.133808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:01.133943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:02.134260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:03.134507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:04.135089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:05.135208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:06.135456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:07.135606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:08.135741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:09.136133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:10.136206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:11.136313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:12.137091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:13.138154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:14.138201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:15.138624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:16.138715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:17.138872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:18.139168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:19.139301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:20.139527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:21.139651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:22.139991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:23.140386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:24.140442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:25.140638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:26.142057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:27.142380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:28.142897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:29.143268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:30.143702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:31.143815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:32.144110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:33.151221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:34.151313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:35.151406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:36.151674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:37.151936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:38.152192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:39.152668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:40.153053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:41.154120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:42.154319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:43.155334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:44.156192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:45.156525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:46.157275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:47.157916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:48.158542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:49.159571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:50.159710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:51.160207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:52.160432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:53.161457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:54.162325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:55.162373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:56.163290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:57.163377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:58.163499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:25:59.163627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:00.164728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:01.165081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:02.165174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:03.166158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:04.166470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:05.166748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:06.166847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:07.167259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:08.167835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:09.175423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:10.176485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:11.177169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:12.178162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:13.178312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:14.178917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:15.179135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:16.179347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:17.179546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:18.180243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:19.180754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:20.181445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:21.181638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:22.181761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:23.182182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:24.182455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:25.182591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:26.182701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:27.183362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:28.183367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:29.183804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:30.184283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:31.184805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:32.185064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:33.185114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:34.186244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:35.186614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:36.187248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:37.187501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:38.187636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:39.188084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:40.188639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:41.189303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:42.190139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:43.190533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:44.190697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:45.190814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:46.191661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:47.191829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:48.192468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:49.192849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:50.193072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:51.194181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:52.194295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:53.194414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:54.194566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:55.194673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:56.194880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:57.195205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:58.195960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:26:59.196102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:00.196320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:01.197040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:02.197101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:03.197203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:04.197362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:05.197464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:06.197644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:07.197726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:08.197810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:09.198124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:10.198206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:11.198309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:12.198997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:13.199099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:14.200189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:15.200534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:16.201424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:17.201530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:18.201720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:19.216193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:20.216308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:21.216982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:22.217605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:23.217706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:24.218244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:25.218246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:26.218285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:27.218404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:28.218574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:29.218699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:30.218714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:31.218835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:32.219879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:33.219995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:34.220133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:35.220229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:36.221051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:37.221163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:38.221782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:39.221899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:40.222723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:41.223615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:42.224279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:43.224403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:44.225291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:45.226323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:46.226916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:47.227058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:48.227187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:49.227335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:50.227965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:51.228132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:52.228497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:53.232512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:54.233132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:55.233500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:56.234242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:57.234477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:58.234881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:27:59.235525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:00.235715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:01.236526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:02.237542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:03.237682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:04.238414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:05.240133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:06.240329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:07.240784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:08.240833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:09.241079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:10.242077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:11.242176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:12.242452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:13.242472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:14.243427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:15.243587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:16.243735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:17.243885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:18.244823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:19.245006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:20.246071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:21.247214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:22.246955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:23.247178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:24.247541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:25.247693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:26.247737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:27.248031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:28.248164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:29.248617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:30.249212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:31.250243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:32.251154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:33.252384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:34.252502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:35.253483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:36.253613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:37.253852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:38.254604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:39.254934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:40.256048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:41.256131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:42.256995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:43.257113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:44.257405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:45.258201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:46.259054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:47.259940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:48.260080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:49.260936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:50.261252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-e3d388a7-bf0d-49b8-b6eb-59b7cbf06cb7 off the node ip-172-31-15-55 @ 07/22/23 13:28:50.993
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3d388a7-bf0d-49b8-b6eb-59b7cbf06cb7 @ 07/22/23 13:28:51.016
  Jul 22 13:28:51.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2730" for this suite. @ 07/22/23 13:28:51.032
• [304.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/22/23 13:28:51.044
  Jul 22 13:28:51.045: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:28:51.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:28:51.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:28:51.075
  STEP: Creating pod liveness-a2bfb7fb-e27e-4828-8de3-75f13e0a7347 in namespace container-probe-4031 @ 07/22/23 13:28:51.079
  E0722 13:28:51.262143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:52.262490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:28:53.106: INFO: Started pod liveness-a2bfb7fb-e27e-4828-8de3-75f13e0a7347 in namespace container-probe-4031
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 13:28:53.107
  Jul 22 13:28:53.112: INFO: Initial restart count of pod liveness-a2bfb7fb-e27e-4828-8de3-75f13e0a7347 is 0
  E0722 13:28:53.263190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:54.263356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:55.263949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:56.264057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:57.264170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:58.264288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:28:59.264470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:00.264853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:01.265366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:02.265446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:03.266025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:04.266121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:05.266622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:06.267162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:07.267998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:08.268144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:09.268784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:10.269192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:11.269704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:12.269820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:13.180: INFO: Restart count of pod container-probe-4031/liveness-a2bfb7fb-e27e-4828-8de3-75f13e0a7347 is now 1 (20.068715945s elapsed)
  Jul 22 13:29:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:29:13.187
  STEP: Destroying namespace "container-probe-4031" for this suite. @ 07/22/23 13:29:13.203
• [22.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/22/23 13:29:13.224
  Jul 22 13:29:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 13:29:13.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:13.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:13.257
  E0722 13:29:13.269911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:13.285: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0722 13:29:14.270954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:15.271302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:16.271488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:17.271701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:18.271852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:18.291: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 13:29:18.292
  STEP: Scaling up "test-rs" replicaset  @ 07/22/23 13:29:18.292
  Jul 22 13:29:18.307: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/22/23 13:29:18.307
  W0722 13:29:18.318370      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 22 13:29:18.320: INFO: observed ReplicaSet test-rs in namespace replicaset-3133 with ReadyReplicas 1, AvailableReplicas 1
  Jul 22 13:29:18.447: INFO: observed ReplicaSet test-rs in namespace replicaset-3133 with ReadyReplicas 1, AvailableReplicas 1
  Jul 22 13:29:18.490: INFO: observed ReplicaSet test-rs in namespace replicaset-3133 with ReadyReplicas 1, AvailableReplicas 1
  Jul 22 13:29:18.538: INFO: observed ReplicaSet test-rs in namespace replicaset-3133 with ReadyReplicas 1, AvailableReplicas 1
  E0722 13:29:19.272706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:20.104: INFO: observed ReplicaSet test-rs in namespace replicaset-3133 with ReadyReplicas 2, AvailableReplicas 2
  Jul 22 13:29:20.150: INFO: observed Replicaset test-rs in namespace replicaset-3133 with ReadyReplicas 3 found true
  Jul 22 13:29:20.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3133" for this suite. @ 07/22/23 13:29:20.157
• [6.943 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 07/22/23 13:29:20.168
  Jul 22 13:29:20.168: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename field-validation @ 07/22/23 13:29:20.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:20.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:20.2
  Jul 22 13:29:20.217: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  W0722 13:29:20.218313      19 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc006734450 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0722 13:29:20.272971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:21.273806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:22.273939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0722 13:29:22.815886      19 warnings.go:70] unknown field "alpha"
  W0722 13:29:22.815909      19 warnings.go:70] unknown field "beta"
  W0722 13:29:22.815914      19 warnings.go:70] unknown field "delta"
  W0722 13:29:22.815920      19 warnings.go:70] unknown field "epsilon"
  W0722 13:29:22.815925      19 warnings.go:70] unknown field "gamma"
  E0722 13:29:23.275825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:23.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-482" for this suite. @ 07/22/23 13:29:23.389
• [3.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/22/23 13:29:23.399
  Jul 22 13:29:23.399: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:29:23.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:23.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:23.435
  STEP: creating an Endpoint @ 07/22/23 13:29:23.445
  STEP: waiting for available Endpoint @ 07/22/23 13:29:23.453
  STEP: listing all Endpoints @ 07/22/23 13:29:23.455
  STEP: updating the Endpoint @ 07/22/23 13:29:23.46
  STEP: fetching the Endpoint @ 07/22/23 13:29:23.471
  STEP: patching the Endpoint @ 07/22/23 13:29:23.477
  STEP: fetching the Endpoint @ 07/22/23 13:29:23.49
  STEP: deleting the Endpoint by Collection @ 07/22/23 13:29:23.495
  STEP: waiting for Endpoint deletion @ 07/22/23 13:29:23.508
  STEP: fetching the Endpoint @ 07/22/23 13:29:23.511
  Jul 22 13:29:23.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1238" for this suite. @ 07/22/23 13:29:23.525
• [0.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/22/23 13:29:23.544
  Jul 22 13:29:23.544: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:29:23.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:23.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:23.579
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/22/23 13:29:23.583
  Jul 22 13:29:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9936 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 22 13:29:23.718: INFO: stderr: ""
  Jul 22 13:29:23.718: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/22/23 13:29:23.719
  E0722 13:29:24.279265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:25.279962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:26.280285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:27.280277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:28.280451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/22/23 13:29:28.77
  Jul 22 13:29:28.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9936 get pod e2e-test-httpd-pod -o json'
  Jul 22 13:29:28.862: INFO: stderr: ""
  Jul 22 13:29:28.862: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-22T13:29:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9936\",\n        \"resourceVersion\": \"33994\",\n        \"uid\": \"5dc1c031-8311-4716-a8e4-f2b2337131af\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-s56xl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-15-55\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-s56xl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-22T13:29:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-22T13:29:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-22T13:29:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-22T13:29:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c2694629dd3fdd3ce1d5b91fc1037e998086a95f22d2837da70faca4d8bb906d\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-22T13:29:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.15.55\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.196.185\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.196.185\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-22T13:29:23Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/22/23 13:29:28.863
  Jul 22 13:29:28.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9936 replace -f -'
  E0722 13:29:29.281610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:29.358: INFO: stderr: ""
  Jul 22 13:29:29.358: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/22/23 13:29:29.358
  Jul 22 13:29:29.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-9936 delete pods e2e-test-httpd-pod'
  E0722 13:29:30.282328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:29:30.883: INFO: stderr: ""
  Jul 22 13:29:30.883: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 22 13:29:30.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9936" for this suite. @ 07/22/23 13:29:30.89
• [7.355 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/22/23 13:29:30.9
  Jul 22 13:29:30.900: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename containers @ 07/22/23 13:29:30.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:30.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:30.931
  STEP: Creating a pod to test override all @ 07/22/23 13:29:30.937
  E0722 13:29:31.283243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:32.283611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:33.284588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:34.284695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:29:34.974
  Jul 22 13:29:34.980: INFO: Trying to get logs from node ip-172-31-15-55 pod client-containers-e85f9617-027f-47dc-989d-2a2fcd950d99 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:29:35.017
  Jul 22 13:29:35.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4112" for this suite. @ 07/22/23 13:29:35.048
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/22/23 13:29:35.06
  Jul 22 13:29:35.060: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename services @ 07/22/23 13:29:35.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:35.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:35.103
  STEP: fetching services @ 07/22/23 13:29:35.108
  Jul 22 13:29:35.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5244" for this suite. @ 07/22/23 13:29:35.123
• [0.073 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/22/23 13:29:35.133
  Jul 22 13:29:35.133: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:29:35.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:29:35.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:29:35.163
  STEP: Creating secret with name s-test-opt-del-67d4aa35-7279-4ec0-8570-b392d6ce4640 @ 07/22/23 13:29:35.176
  STEP: Creating secret with name s-test-opt-upd-be9d0b21-273e-420d-bcb9-e95f4584932e @ 07/22/23 13:29:35.183
  STEP: Creating the pod @ 07/22/23 13:29:35.19
  E0722 13:29:35.284862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:36.286499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:37.287106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-67d4aa35-7279-4ec0-8570-b392d6ce4640 @ 07/22/23 13:29:37.294
  STEP: Updating secret s-test-opt-upd-be9d0b21-273e-420d-bcb9-e95f4584932e @ 07/22/23 13:29:37.304
  STEP: Creating secret with name s-test-opt-create-1e33b6d8-6564-45ea-b9b5-2634fdf05a1a @ 07/22/23 13:29:37.312
  STEP: waiting to observe update in volume @ 07/22/23 13:29:37.32
  E0722 13:29:38.287193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:39.287559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:40.288428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:41.289164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:42.289529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:43.290159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:44.290590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:45.290481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:46.290865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:47.290953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:48.291059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:49.291199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:50.291817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:51.291856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:52.292076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:53.292198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:54.293004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:55.293117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:56.293261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:57.293360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:58.293629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:29:59.294158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:00.294344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:01.294418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:02.294560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:03.294714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:04.294813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:05.295127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:06.295580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:07.295879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:08.296859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:09.297089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:10.298137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:11.298279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:12.298453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:13.298540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:14.299539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:15.299667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:16.299785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:17.300125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:18.300262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:19.300528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:20.301303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:21.301409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:22.301945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:23.302008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:24.302136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:25.302249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:26.302344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:27.302463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:28.302668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:29.302787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:30.303616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:31.304228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:32.304340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:33.305097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:34.305175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:35.306149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:36.306267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:37.306389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:38.307087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:39.307205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:40.308147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:41.308540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:42.309618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:43.309731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:44.309882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:45.310002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:46.310225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:47.311147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:48.311721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:49.311827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:50.312686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:51.313098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:52.314142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:53.314302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:54.314423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:55.317520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:56.318189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:57.318317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:58.318977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:30:59.319132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:00.319957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:01.321414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:02.322414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:03.322525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:31:03.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4928" for this suite. @ 07/22/23 13:31:03.947
• [88.826 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/22/23 13:31:03.961
  Jul 22 13:31:03.961: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 13:31:03.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:03.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:03.998
  STEP: Creating a pod to test substitution in volume subpath @ 07/22/23 13:31:04.003
  E0722 13:31:04.322877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:05.323794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:06.324664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:07.325092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:31:08.037
  Jul 22 13:31:08.042: INFO: Trying to get logs from node ip-172-31-26-93 pod var-expansion-4db14d37-76f3-4b66-8758-b18f36efe5cb container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 13:31:08.078
  Jul 22 13:31:08.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4856" for this suite. @ 07/22/23 13:31:08.116
• [4.168 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/22/23 13:31:08.131
  Jul 22 13:31:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename certificates @ 07/22/23 13:31:08.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:08.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:08.172
  E0722 13:31:08.325756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:09.325902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 07/22/23 13:31:09.769
  STEP: getting /apis/certificates.k8s.io @ 07/22/23 13:31:09.776
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/22/23 13:31:09.779
  STEP: creating @ 07/22/23 13:31:09.784
  STEP: getting @ 07/22/23 13:31:09.826
  STEP: listing @ 07/22/23 13:31:09.832
  STEP: watching @ 07/22/23 13:31:09.838
  Jul 22 13:31:09.838: INFO: starting watch
  STEP: patching @ 07/22/23 13:31:09.84
  STEP: updating @ 07/22/23 13:31:09.851
  Jul 22 13:31:09.861: INFO: waiting for watch events with expected annotations
  Jul 22 13:31:09.861: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/22/23 13:31:09.861
  STEP: patching /approval @ 07/22/23 13:31:09.868
  STEP: updating /approval @ 07/22/23 13:31:09.878
  STEP: getting /status @ 07/22/23 13:31:09.89
  STEP: patching /status @ 07/22/23 13:31:09.898
  STEP: updating /status @ 07/22/23 13:31:09.913
  STEP: deleting @ 07/22/23 13:31:09.926
  STEP: deleting a collection @ 07/22/23 13:31:09.949
  Jul 22 13:31:09.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7074" for this suite. @ 07/22/23 13:31:09.984
• [1.867 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/22/23 13:31:09.998
  Jul 22 13:31:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:31:09.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:10.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:10.028
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/22/23 13:31:10.038
  E0722 13:31:10.326225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:11.326367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:12.326850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:13.327468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:14.328269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:15.328266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:31:16.223
  Jul 22 13:31:16.231: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-df110d2b-fa0e-44c4-8af4-dd61eefeb96e container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:31:16.241
  Jul 22 13:31:16.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8199" for this suite. @ 07/22/23 13:31:16.276
• [6.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/22/23 13:31:16.293
  Jul 22 13:31:16.294: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename runtimeclass @ 07/22/23 13:31:16.295
  E0722 13:31:16.328827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:16.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:16.366
  Jul 22 13:31:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3621" for this suite. @ 07/22/23 13:31:16.411
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/22/23 13:31:16.427
  Jul 22 13:31:16.427: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename endpointslice @ 07/22/23 13:31:16.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:16.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:16.472
  Jul 22 13:31:16.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9989" for this suite. @ 07/22/23 13:31:16.599
• [0.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/22/23 13:31:16.62
  Jul 22 13:31:16.620: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 13:31:16.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:16.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:16.66
  Jul 22 13:31:16.690: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0722 13:31:17.329027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:18.329143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:19.329285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:20.329999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:21.330712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:31:21.697: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 13:31:21.697
  Jul 22 13:31:21.697: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/22/23 13:31:21.715
  Jul 22 13:31:21.737: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6595  06b6bd06-e8d0-44f7-aa5e-b1c6161c422d 34531 1 2023-07-22 13:31:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-22 13:31:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001837108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 22 13:31:21.748: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jul 22 13:31:21.748: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul 22 13:31:21.749: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6595  6e641fea-9e6a-4eca-9a34-520501d08a2d 34535 1 2023-07-22 13:31:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 06b6bd06-e8d0-44f7-aa5e-b1c6161c422d 0xc001837447 0xc001837448}] [] [{e2e.test Update apps/v1 2023-07-22 13:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:31:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-22 13:31:21 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"06b6bd06-e8d0-44f7-aa5e-b1c6161c422d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001837508 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 13:31:21.757: INFO: Pod "test-cleanup-controller-hcd2g" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-hcd2g test-cleanup-controller- deployment-6595  094990ac-bed4-48cc-8be2-dfc5e3a94ecc 34516 0 2023-07-22 13:31:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 6e641fea-9e6a-4eca-9a34-520501d08a2d 0xc001837807 0xc001837808}] [] [{kube-controller-manager Update v1 2023-07-22 13:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e641fea-9e6a-4eca-9a34-520501d08a2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:31:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdqz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdqz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:31:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.191,StartTime:2023-07-22 13:31:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:31:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c98780debdb4c55d76bdb8f73bb101a0386b3374edf8185a65c283af4570360f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.191,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:31:21.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6595" for this suite. @ 07/22/23 13:31:21.763
• [5.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/22/23 13:31:21.789
  Jul 22 13:31:21.789: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 13:31:21.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:21.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:21.844
  STEP: Creating service test in namespace statefulset-5086 @ 07/22/23 13:31:21.86
  STEP: Creating statefulset ss in namespace statefulset-5086 @ 07/22/23 13:31:21.87
  Jul 22 13:31:21.892: INFO: Found 0 stateful pods, waiting for 1
  E0722 13:31:22.330814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:23.331342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:24.331397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:25.331607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:26.332284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:27.332789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:28.333043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:29.333129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:30.333438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:31.333641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:31:31.903: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/22/23 13:31:31.914
  STEP: updating a scale subresource @ 07/22/23 13:31:31.918
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/22/23 13:31:31.927
  STEP: Patch a scale subresource @ 07/22/23 13:31:31.938
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/22/23 13:31:31.947
  Jul 22 13:31:31.956: INFO: Deleting all statefulset in ns statefulset-5086
  Jul 22 13:31:31.964: INFO: Scaling statefulset ss to 0
  E0722 13:31:32.334060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:33.334231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:34.344235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:35.335342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:36.335472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:37.335593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:38.335776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:39.335971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:40.336774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:41.337259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:31:42.018: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:31:42.023: INFO: Deleting statefulset ss
  Jul 22 13:31:42.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5086" for this suite. @ 07/22/23 13:31:42.052
• [20.274 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/22/23 13:31:42.066
  Jul 22 13:31:42.066: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/22/23 13:31:42.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:31:42.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:31:42.115
  Jul 22 13:31:42.125: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:31:42.337910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:43.338116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:44.339021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:45.339169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:46.339813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:47.340115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:48.340528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:49.340620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:50.341306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:51.341425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:52.342239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:53.342378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:54.342749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:55.343195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:56.343343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:57.343444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:58.344129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:31:59.344241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:00.344593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:01.345086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:02.345909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:03.346176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:04.346669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:05.347491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:06.348090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:07.348510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:08.348716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:09.349166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:10.350103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:11.350451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:12.350678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:13.350791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:14.350754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:15.351295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:16.352061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:17.352154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:18.352523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:19.352955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:20.353487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:21.353655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:22.354778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:23.355454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:24.355671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:25.356350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:26.357084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:27.357219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:28.358252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:29.358361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:30.359085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:31.359462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:32.360293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:33.360424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:34.360867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:35.361103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:36.361694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:37.361795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:38.362177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:39.362567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:40.363258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:41.363665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:32:42.154: INFO: Waiting for terminating namespaces to be deleted...
  Jul 22 13:32:42.160: INFO: Starting informer...
  STEP: Starting pods... @ 07/22/23 13:32:42.16
  E0722 13:32:42.364280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:32:42.393: INFO: Pod1 is running on ip-172-31-15-55. Tainting Node
  E0722 13:32:43.364728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:44.365529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:32:44.629: INFO: Pod2 is running on ip-172-31-15-55. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/22/23 13:32:44.629
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/22/23 13:32:44.652
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/22/23 13:32:44.658
  E0722 13:32:45.366357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:46.366619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:47.367006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:48.367100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:49.367537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:32:50.340: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0722 13:32:50.368657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:51.368598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:52.369503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:53.370170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:54.370318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:55.370601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:56.370767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:57.370862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:58.371741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:32:59.372125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:00.372828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:01.373085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:02.373194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:03.373303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:04.373421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:05.374378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:06.374510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:07.374658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:08.374767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:09.374887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:10.375917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:33:10.399: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul 22 13:33:10.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/22/23 13:33:10.423
  STEP: Destroying namespace "taint-multiple-pods-7462" for this suite. @ 07/22/23 13:33:10.429
• [88.377 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/22/23 13:33:10.451
  Jul 22 13:33:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 13:33:10.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:10.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:10.49
  E0722 13:33:11.376003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:12.376129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:13.376258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:14.377084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:15.377274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:16.377348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:17.377467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:18.377601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:19.377705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:20.378769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:21.379751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:22.379840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:23.380091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:24.380743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:25.382960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:26.383698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:27.383857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/22/23 13:33:27.499
  E0722 13:33:28.383976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:29.384092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:30.384121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:31.384255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:32.385077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/22/23 13:33:32.505
  STEP: Ensuring resource quota status is calculated @ 07/22/23 13:33:32.514
  E0722 13:33:33.385191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:34.386144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 07/22/23 13:33:34.522
  STEP: Ensuring resource quota status captures configMap creation @ 07/22/23 13:33:34.545
  E0722 13:33:35.386307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:36.386397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 07/22/23 13:33:36.551
  STEP: Ensuring resource quota status released usage @ 07/22/23 13:33:36.561
  E0722 13:33:37.386633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:38.386722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:33:38.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9508" for this suite. @ 07/22/23 13:33:38.575
• [28.133 seconds]
------------------------------
SSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/22/23 13:33:38.585
  Jul 22 13:33:38.585: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 13:33:38.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:38.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:38.621
  STEP: Creating configMap that has name configmap-test-emptyKey-08ff8453-bcd6-4ea2-9e32-5b7328d400e5 @ 07/22/23 13:33:38.625
  Jul 22 13:33:38.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5817" for this suite. @ 07/22/23 13:33:38.639
• [0.065 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/22/23 13:33:38.651
  Jul 22 13:33:38.651: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:33:38.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:38.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:38.695
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/22/23 13:33:38.705
  E0722 13:33:39.387700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:40.387998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:41.388224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:42.388542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:33:42.738
  Jul 22 13:33:42.745: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-b60a380b-2343-4301-8e78-85f697c5191e container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:33:42.77
  Jul 22 13:33:42.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4912" for this suite. @ 07/22/23 13:33:42.797
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/22/23 13:33:42.806
  Jul 22 13:33:42.806: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename server-version @ 07/22/23 13:33:42.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:42.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:42.835
  STEP: Request ServerVersion @ 07/22/23 13:33:42.839
  STEP: Confirm major version @ 07/22/23 13:33:42.84
  Jul 22 13:33:42.840: INFO: Major version: 1
  STEP: Confirm minor version @ 07/22/23 13:33:42.841
  Jul 22 13:33:42.841: INFO: cleanMinorVersion: 27
  Jul 22 13:33:42.841: INFO: Minor version: 27
  Jul 22 13:33:42.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-4518" for this suite. @ 07/22/23 13:33:42.849
• [0.053 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/22/23 13:33:42.862
  Jul 22 13:33:42.862: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename containers @ 07/22/23 13:33:42.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:42.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:42.903
  STEP: Creating a pod to test override command @ 07/22/23 13:33:42.909
  E0722 13:33:43.388848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:44.389124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:45.389938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:46.390660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:33:46.949
  Jul 22 13:33:46.956: INFO: Trying to get logs from node ip-172-31-15-55 pod client-containers-50fc1dc3-0c31-4304-9ea8-8c114482fa23 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:33:46.966
  Jul 22 13:33:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-816" for this suite. @ 07/22/23 13:33:47.006
• [4.167 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/22/23 13:33:47.03
  Jul 22 13:33:47.030: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 13:33:47.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:33:47.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:33:47.063
  STEP: Creating service test in namespace statefulset-2901 @ 07/22/23 13:33:47.07
  STEP: Creating stateful set ss in namespace statefulset-2901 @ 07/22/23 13:33:47.08
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2901 @ 07/22/23 13:33:47.093
  Jul 22 13:33:47.101: INFO: Found 0 stateful pods, waiting for 1
  E0722 13:33:47.391075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:48.391202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:49.391311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:50.391992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:51.392229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:52.392485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:53.392465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:54.392576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:55.393629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:56.394181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:33:57.110: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/22/23 13:33:57.11
  Jul 22 13:33:57.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 13:33:57.295: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 13:33:57.295: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 13:33:57.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 13:33:57.303: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0722 13:33:57.394325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:58.394576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:33:59.394716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:00.395672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:01.395806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:02.395925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:03.396195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:04.396467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:05.397810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:06.397966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:07.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 13:34:07.309: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:34:07.337: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Jul 22 13:34:07.337: INFO: ss-0  ip-172-31-15-55  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:47 +0000 UTC  }]
  Jul 22 13:34:07.337: INFO: 
  Jul 22 13:34:07.337: INFO: StatefulSet ss has not reached scale 3, at 1
  E0722 13:34:07.398462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:08.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990752924s
  E0722 13:34:08.399283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:09.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982818814s
  E0722 13:34:09.399977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:10.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973120694s
  E0722 13:34:10.400593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:11.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.961331724s
  E0722 13:34:11.401529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:12.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.954428955s
  E0722 13:34:12.402207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:13.391: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946419742s
  E0722 13:34:13.402618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:14.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938922287s
  E0722 13:34:14.403625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:15.404014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:15.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932320082s
  E0722 13:34:16.404430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:16.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 924.052686ms
  E0722 13:34:17.405086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2901 @ 07/22/23 13:34:17.412
  Jul 22 13:34:17.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 13:34:17.630: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 22 13:34:17.630: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 13:34:17.630: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 13:34:17.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 13:34:17.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 22 13:34:17.838: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 13:34:17.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 13:34:17.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 22 13:34:18.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 22 13:34:18.051: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 22 13:34:18.051: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 22 13:34:18.057: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0722 13:34:18.406180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:19.406450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:20.407174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:21.407399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:22.407503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:23.407832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:24.408028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:25.408817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:26.409096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:27.409288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:28.066: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:34:28.066: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:34:28.066: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/22/23 13:34:28.066
  Jul 22 13:34:28.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 13:34:28.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 13:34:28.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 13:34:28.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 13:34:28.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0722 13:34:28.409985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:28.455: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 13:34:28.455: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 13:34:28.455: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 13:34:28.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=statefulset-2901 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 22 13:34:28.651: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 22 13:34:28.651: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 22 13:34:28.651: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 22 13:34:28.651: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:34:28.664: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0722 13:34:29.410438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:30.411056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:31.411947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:32.412067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:33.412221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:34.412424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:35.413130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:36.413146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:37.413276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:38.413436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:38.679: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 13:34:38.679: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 13:34:38.679: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 22 13:34:38.705: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jul 22 13:34:38.706: INFO: ss-0  ip-172-31-15-55   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:33:47 +0000 UTC  }]
  Jul 22 13:34:38.706: INFO: ss-1  ip-172-31-26-93   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  }]
  Jul 22 13:34:38.706: INFO: ss-2  ip-172-31-81-237  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  }]
  Jul 22 13:34:38.706: INFO: 
  Jul 22 13:34:38.706: INFO: StatefulSet ss has not reached scale 0, at 3
  E0722 13:34:39.413977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:39.713: INFO: POD   NODE              PHASE      GRACE  CONDITIONS
  Jul 22 13:34:39.713: INFO: ss-1  ip-172-31-26-93   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:28 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:28 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  }]
  Jul 22 13:34:39.713: INFO: ss-2  ip-172-31-81-237  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:29 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-22 13:34:07 +0000 UTC  }]
  Jul 22 13:34:39.714: INFO: 
  Jul 22 13:34:39.714: INFO: StatefulSet ss has not reached scale 0, at 2
  E0722 13:34:40.414711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:40.719: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98416207s
  E0722 13:34:41.415355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:41.731: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.977338705s
  E0722 13:34:42.415760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:42.742: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966172648s
  E0722 13:34:43.420733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:43.747: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.955065476s
  E0722 13:34:44.421099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:44.755: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.950630726s
  E0722 13:34:45.421400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:45.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.942570411s
  E0722 13:34:46.422368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:46.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.935330758s
  E0722 13:34:47.421966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:34:47.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 929.909904ms
  E0722 13:34:48.422136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2901 @ 07/22/23 13:34:48.776
  Jul 22 13:34:48.783: INFO: Scaling statefulset ss to 0
  Jul 22 13:34:48.813: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:34:48.819: INFO: Deleting all statefulset in ns statefulset-2901
  Jul 22 13:34:48.826: INFO: Scaling statefulset ss to 0
  Jul 22 13:34:48.848: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:34:48.855: INFO: Deleting statefulset ss
  Jul 22 13:34:48.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2901" for this suite. @ 07/22/23 13:34:48.891
• [61.875 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/22/23 13:34:48.909
  Jul 22 13:34:48.909: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename limitrange @ 07/22/23 13:34:48.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:34:48.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:34:48.943
  STEP: Creating LimitRange "e2e-limitrange-bwl5s" in namespace "limitrange-6675" @ 07/22/23 13:34:48.951
  STEP: Creating another limitRange in another namespace @ 07/22/23 13:34:48.959
  Jul 22 13:34:48.989: INFO: Namespace "e2e-limitrange-bwl5s-4320" created
  Jul 22 13:34:48.990: INFO: Creating LimitRange "e2e-limitrange-bwl5s" in namespace "e2e-limitrange-bwl5s-4320"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-bwl5s" @ 07/22/23 13:34:49.002
  Jul 22 13:34:49.008: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-bwl5s" in "limitrange-6675" namespace @ 07/22/23 13:34:49.008
  Jul 22 13:34:49.026: INFO: LimitRange "e2e-limitrange-bwl5s" has been patched
  STEP: Delete LimitRange "e2e-limitrange-bwl5s" by Collection with labelSelector: "e2e-limitrange-bwl5s=patched" @ 07/22/23 13:34:49.026
  STEP: Confirm that the limitRange "e2e-limitrange-bwl5s" has been deleted @ 07/22/23 13:34:49.041
  Jul 22 13:34:49.041: INFO: Requesting list of LimitRange to confirm quantity
  Jul 22 13:34:49.053: INFO: Found 0 LimitRange with label "e2e-limitrange-bwl5s=patched"
  Jul 22 13:34:49.053: INFO: LimitRange "e2e-limitrange-bwl5s" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-bwl5s" @ 07/22/23 13:34:49.053
  Jul 22 13:34:49.061: INFO: Found 1 limitRange
  Jul 22 13:34:49.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6675" for this suite. @ 07/22/23 13:34:49.067
  STEP: Destroying namespace "e2e-limitrange-bwl5s-4320" for this suite. @ 07/22/23 13:34:49.077
• [0.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/22/23 13:34:49.093
  Jul 22 13:34:49.094: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subpath @ 07/22/23 13:34:49.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:34:49.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:34:49.125
  STEP: Setting up data @ 07/22/23 13:34:49.134
  STEP: Creating pod pod-subpath-test-projected-5ps9 @ 07/22/23 13:34:49.151
  STEP: Creating a pod to test atomic-volume-subpath @ 07/22/23 13:34:49.151
  E0722 13:34:49.422238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:50.422835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:51.423013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:52.423103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:53.423195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:54.423389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:55.423497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:56.423599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:57.424196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:58.424310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:34:59.424869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:00.425283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:01.426151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:02.426270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:03.429065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:04.429325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:05.430065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:06.430178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:07.430621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:08.430748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:09.431767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:10.431802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:11.431894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:12.432123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:35:13.261
  Jul 22 13:35:13.267: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-subpath-test-projected-5ps9 container test-container-subpath-projected-5ps9: <nil>
  STEP: delete the pod @ 07/22/23 13:35:13.283
  STEP: Deleting pod pod-subpath-test-projected-5ps9 @ 07/22/23 13:35:13.306
  Jul 22 13:35:13.306: INFO: Deleting pod "pod-subpath-test-projected-5ps9" in namespace "subpath-5340"
  Jul 22 13:35:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5340" for this suite. @ 07/22/23 13:35:13.319
• [24.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/22/23 13:35:13.338
  Jul 22 13:35:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:35:13.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:35:13.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:35:13.387
  E0722 13:35:13.432659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 07/22/23 13:35:13.453
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:35:13.971
  STEP: Deploying the webhook pod @ 07/22/23 13:35:13.981
  STEP: Wait for the deployment to be ready @ 07/22/23 13:35:13.996
  Jul 22 13:35:14.006: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0722 13:35:14.433332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:15.434409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:35:16.023
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:35:16.043
  E0722 13:35:16.434955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:35:17.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/22/23 13:35:17.204
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/22/23 13:35:17.264
  STEP: Deleting the collection of validation webhooks @ 07/22/23 13:35:17.301
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/22/23 13:35:17.389
  Jul 22 13:35:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 13:35:17.435577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8904" for this suite. @ 07/22/23 13:35:17.479
  STEP: Destroying namespace "webhook-markers-1045" for this suite. @ 07/22/23 13:35:17.492
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/22/23 13:35:17.505
  Jul 22 13:35:17.505: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subjectreview @ 07/22/23 13:35:17.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:35:17.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:35:17.535
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-869" @ 07/22/23 13:35:17.541
  Jul 22 13:35:17.548: INFO: saUsername: "system:serviceaccount:subjectreview-869:e2e"
  Jul 22 13:35:17.548: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-869"}
  Jul 22 13:35:17.548: INFO: saUID: "e72f6c3c-3111-4bf0-8279-396bd4330bab"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-869:e2e" @ 07/22/23 13:35:17.549
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-869:e2e" @ 07/22/23 13:35:17.549
  Jul 22 13:35:17.553: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-869:e2e" api 'list' configmaps in "subjectreview-869" namespace @ 07/22/23 13:35:17.553
  Jul 22 13:35:17.557: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-869:e2e" @ 07/22/23 13:35:17.557
  Jul 22 13:35:17.561: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul 22 13:35:17.561: INFO: LocalSubjectAccessReview has been verified
  Jul 22 13:35:17.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-869" for this suite. @ 07/22/23 13:35:17.569
• [0.074 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/22/23 13:35:17.58
  Jul 22 13:35:17.580: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename init-container @ 07/22/23 13:35:17.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:35:17.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:35:17.614
  STEP: creating the pod @ 07/22/23 13:35:17.618
  Jul 22 13:35:17.619: INFO: PodSpec: initContainers in spec.initContainers
  E0722 13:35:18.435660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:19.435751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:20.436396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:21.436512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:35:22.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5831" for this suite. @ 07/22/23 13:35:22.054
• [4.485 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/22/23 13:35:22.066
  Jul 22 13:35:22.066: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption @ 07/22/23 13:35:22.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:35:22.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:35:22.099
  Jul 22 13:35:22.124: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:35:22.437563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:23.437700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:24.438593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:25.439465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:26.440071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:27.440104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:28.440756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:29.440862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:30.441202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:31.441314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:32.445803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:33.446113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:34.446236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:35.448038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:36.448685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:37.449513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:38.449579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:39.450210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:40.451260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:41.452155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:42.452754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:43.453084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:44.454003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:45.454542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:46.455170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:47.455266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:48.456276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:49.456710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:50.457547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:51.457988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:52.458771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:53.459022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:54.459101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:55.459302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:56.459435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:57.459486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:58.459800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:35:59.460866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:00.460995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:01.461351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:02.461466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:03.461598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:04.461687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:05.462316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:06.463101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:07.463233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:08.463723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:09.464187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:10.464389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:11.464463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:12.464965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:13.465142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:14.466253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:15.466706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:16.467995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:17.468324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:18.468584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:19.468740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:20.469159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:21.470825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:22.158: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/22/23 13:36:22.164
  Jul 22 13:36:22.200: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 22 13:36:22.211: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 22 13:36:22.242: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 22 13:36:22.254: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 22 13:36:22.279: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 22 13:36:22.290: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/22/23 13:36:22.29
  E0722 13:36:22.471310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:23.471197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/22/23 13:36:24.345
  E0722 13:36:24.471323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:25.471652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:26.471910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:27.472055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:28.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 13:36:28.472709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "sched-preemption-8917" for this suite. @ 07/22/23 13:36:28.494
• [66.441 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/22/23 13:36:28.507
  Jul 22 13:36:28.507: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 13:36:28.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:36:28.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:36:28.546
  Jul 22 13:36:28.554: INFO: Creating deployment "webserver-deployment"
  Jul 22 13:36:28.562: INFO: Waiting for observed generation 1
  E0722 13:36:29.474065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:30.474953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:30.579: INFO: Waiting for all required pods to come up
  Jul 22 13:36:30.587: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/22/23 13:36:30.587
  E0722 13:36:31.475161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:32.475299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:32.644: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul 22 13:36:32.656: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul 22 13:36:32.678: INFO: Updating deployment webserver-deployment
  Jul 22 13:36:32.678: INFO: Waiting for observed generation 2
  E0722 13:36:33.476316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:34.476599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:34.690: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul 22 13:36:34.697: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul 22 13:36:34.704: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 22 13:36:34.721: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul 22 13:36:34.721: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul 22 13:36:34.725: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 22 13:36:34.736: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul 22 13:36:34.736: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul 22 13:36:34.753: INFO: Updating deployment webserver-deployment
  Jul 22 13:36:34.753: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul 22 13:36:34.771: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul 22 13:36:34.780: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul 22 13:36:34.801: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2804  e7de2c67-f845-40f5-ba45-340d2dacaa09 36428 3 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b4058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-22 13:36:32 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-22 13:36:34 +0000 UTC,LastTransitionTime:2023-07-22 13:36:34 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul 22 13:36:34.810: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2804  ce89ab75-b526-4779-aa1c-cfd1373c1052 36424 3 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e7de2c67-f845-40f5-ba45-340d2dacaa09 0xc0042b4577 0xc0042b4578}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7de2c67-f845-40f5-ba45-340d2dacaa09\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b4618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 13:36:34.811: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul 22 13:36:34.811: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2804  9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 36421 3 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e7de2c67-f845-40f5-ba45-340d2dacaa09 0xc0042b4487 0xc0042b4488}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7de2c67-f845-40f5-ba45-340d2dacaa09\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b4518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 13:36:34.825: INFO: Pod "webserver-deployment-67bd4bf6dc-5f2w9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5f2w9 webserver-deployment-67bd4bf6dc- deployment-2804  0789bbd5-daa0-4f8b-908f-43f6a9184f27 36261 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b4b17 0xc0042b4b18}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbtrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbtrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.39,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aad467930c47cdde415c85dda4e87f8fc2ba9e2dfbbde079ceed32cbc465f964,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.39,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.826: INFO: Pod "webserver-deployment-67bd4bf6dc-hbhgk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hbhgk webserver-deployment-67bd4bf6dc- deployment-2804  fff8d3c4-efc5-466d-9006-f818353149a8 36431 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b4d07 0xc0042b4d08}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq6d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq6d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.827: INFO: Pod "webserver-deployment-67bd4bf6dc-hkfg2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hkfg2 webserver-deployment-67bd4bf6dc- deployment-2804  cfe97d7d-52c1-42b5-87a8-6c1b30bfa3a3 36227 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b4e70 0xc0042b4e71}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.121.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-44gfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-44gfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-81-237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.81.237,PodIP:192.168.121.253,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9cb6cf6655ab945ed15db08be8ff80a1f131f846982c2f9c317be18cafe6cef6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.121.253,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.827: INFO: Pod "webserver-deployment-67bd4bf6dc-kfm7t" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kfm7t webserver-deployment-67bd4bf6dc- deployment-2804  e4e6820a-5ffa-421e-be75-716ad68a6700 36221 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5057 0xc0042b5058}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.121.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4xkv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4xkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-81-237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.81.237,PodIP:192.168.121.243,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c6c3fef6541c033b328009ce40d1d830a9342575b8335b9a52c5029957dd5b15,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.121.243,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.828: INFO: Pod "webserver-deployment-67bd4bf6dc-mqlm7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mqlm7 webserver-deployment-67bd4bf6dc- deployment-2804  6771fad0-0f8c-46f3-a1b9-398791272e15 36432 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5247 0xc0042b5248}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2457,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2457,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.828: INFO: Pod "webserver-deployment-67bd4bf6dc-rmhrc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rmhrc webserver-deployment-67bd4bf6dc- deployment-2804  bfa17d8e-89cc-4dab-b974-b4647816f1cb 36264 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5387 0xc0042b5388}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4shr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4shr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.38,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a18e214814e0bf5d31c3cfbe7cc030169f12ec2981bc27ebd5b8954d38fea9b0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.38,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.829: INFO: Pod "webserver-deployment-67bd4bf6dc-sp4d9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sp4d9 webserver-deployment-67bd4bf6dc- deployment-2804  8fe2c185-6ac4-474f-b9db-b579882486dc 36258 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5577 0xc0042b5578}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.121.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vnfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vnfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-81-237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.81.237,PodIP:192.168.121.252,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e14e6112641fc71802c2e23a66477e56ee01ad86241db685ebf0fef965ba6c96,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.121.252,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.829: INFO: Pod "webserver-deployment-67bd4bf6dc-tctjr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tctjr webserver-deployment-67bd4bf6dc- deployment-2804  c1dd8083-8c5b-4ee1-9f7a-ac16b64fa2f3 36433 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5767 0xc0042b5768}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpzrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpzrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.831: INFO: Pod "webserver-deployment-67bd4bf6dc-vww4l" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vww4l webserver-deployment-67bd4bf6dc- deployment-2804  74428361-b875-4f6d-ad61-71a0515b388f 36235 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b58a7 0xc0042b58a8}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrhx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrhx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.35,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e0f84ce72ed35ad2d2f2a08f0c0f0c761cb2a48602b5e49040a572fbd9efd77,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.832: INFO: Pod "webserver-deployment-67bd4bf6dc-w9ld7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w9ld7 webserver-deployment-67bd4bf6dc- deployment-2804  635c0dcd-0d32-4706-bcb0-355d7f9a3ea1 36246 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5a97 0xc0042b5a98}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqfwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqfwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.139,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ef95255b065a268caf0e28586ceace7c7d1d4e3302774b85ffa4a668d22c7e37,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.139,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.832: INFO: Pod "webserver-deployment-67bd4bf6dc-wpwrf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wpwrf webserver-deployment-67bd4bf6dc- deployment-2804  35d52e81-8fb0-4f2f-a5f4-b452e8d7cfe5 36248 0 2023-07-22 13:36:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 9ffa3b20-040a-4aa5-85a2-e1c8b4e44284 0xc0042b5c87 0xc0042b5c88}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ffa3b20-040a-4aa5-85a2-e1c8b4e44284\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl7fz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl7fz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.144,StartTime:2023-07-22 13:36:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:36:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad512fcb80738eb38305364645462050a1abfa2f80bf27936ff7f07f2faf9c5f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.144,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.833: INFO: Pod "webserver-deployment-7b75d79cf5-5d44f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5d44f webserver-deployment-7b75d79cf5- deployment-2804  308f16fc-2fd4-4b7a-ba9b-cf54c7a5ff0f 36321 0 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0042b5e77 0xc0042b5e78}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsm7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsm7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:,StartTime:2023-07-22 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.837: INFO: Pod "webserver-deployment-7b75d79cf5-7d2jv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7d2jv webserver-deployment-7b75d79cf5- deployment-2804  4407ba21-b8d0-43d0-ade5-22bd7ff039dd 36292 0 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8067 0xc0043c8068}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4b4pp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4b4pp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:,StartTime:2023-07-22 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.838: INFO: Pod "webserver-deployment-7b75d79cf5-hdpzp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hdpzp webserver-deployment-7b75d79cf5- deployment-2804  56f6a86a-bca0-4350-b2d3-6a438274cc66 36436 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8257 0xc0043c8258}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk5nz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk5nz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-81-237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.838: INFO: Pod "webserver-deployment-7b75d79cf5-mrxjx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mrxjx webserver-deployment-7b75d79cf5- deployment-2804  78ca4c6f-8cc4-45ae-933e-e14dbba175b5 36437 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c83d0 0xc0043c83d1}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wwg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wwg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.840: INFO: Pod "webserver-deployment-7b75d79cf5-mvrc7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mvrc7 webserver-deployment-7b75d79cf5- deployment-2804  085c4201-fe91-4b5c-9962-e62e6428d04b 36397 0 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8517 0xc0043c8518}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.121.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cljsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cljsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-81-237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.81.237,PodIP:192.168.121.251,StartTime:2023-07-22 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.121.251,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.840: INFO: Pod "webserver-deployment-7b75d79cf5-nn558" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nn558 webserver-deployment-7b75d79cf5- deployment-2804  629414c8-5498-453a-b131-429980004aec 36407 0 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8737 0xc0043c8738}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mz5s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mz5s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.40,StartTime:2023-07-22 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.40,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.841: INFO: Pod "webserver-deployment-7b75d79cf5-p2b56" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-p2b56 webserver-deployment-7b75d79cf5- deployment-2804  9bee8995-e9d0-43ec-a79d-e46a51f4cb61 36440 0 2023-07-22 13:36:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8957 0xc0043c8958}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hp2tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hp2tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.841: INFO: Pod "webserver-deployment-7b75d79cf5-zmbz9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zmbz9 webserver-deployment-7b75d79cf5- deployment-2804  b49c7490-23d5-4854-95cc-f775b766640e 36404 0 2023-07-22 13:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 ce89ab75-b526-4779-aa1c-cfd1373c1052 0xc0043c8aa7 0xc0043c8aa8}] [] [{kube-controller-manager Update v1 2023-07-22 13:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce89ab75-b526-4779-aa1c-cfd1373c1052\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.120.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24x52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24x52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-93,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.26.93,PodIP:192.168.120.41,StartTime:2023-07-22 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.120.41,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:36:34.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2804" for this suite. @ 07/22/23 13:36:34.865
• [6.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 07/22/23 13:36:34.88
  Jul 22 13:36:34.880: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 13:36:34.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:36:34.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:36:34.991
  Jul 22 13:36:35.055: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/22/23 13:36:35.07
  Jul 22 13:36:35.079: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:35.079: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:35.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:36:35.086: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:36:35.477041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:36.093: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:36.093: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:36.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:36:36.098: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:36:36.477062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:37.095: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:37.095: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:37.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:36:37.100: INFO: Node ip-172-31-26-93 is running 0 daemon pod, expected 1
  E0722 13:36:37.477607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:38.095: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:38.095: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:38.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:36:38.101: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/22/23 13:36:38.122
  STEP: Check that daemon pods images are updated. @ 07/22/23 13:36:38.138
  Jul 22 13:36:38.154: INFO: Wrong image for pod: daemon-set-bt54z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:38.155: INFO: Wrong image for pod: daemon-set-qqnhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:38.167: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:38.167: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:38.478164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:39.174: INFO: Wrong image for pod: daemon-set-bt54z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:39.174: INFO: Pod daemon-set-mn8r4 is not available
  Jul 22 13:36:39.174: INFO: Wrong image for pod: daemon-set-qqnhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:39.187: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:39.187: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:39.478992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:40.179: INFO: Wrong image for pod: daemon-set-bt54z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:40.179: INFO: Pod daemon-set-mn8r4 is not available
  Jul 22 13:36:40.179: INFO: Wrong image for pod: daemon-set-qqnhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:40.189: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:40.189: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:40.479947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:41.172: INFO: Wrong image for pod: daemon-set-qqnhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:41.177: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:41.177: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:41.480746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:42.175: INFO: Pod daemon-set-4pcr7 is not available
  Jul 22 13:36:42.175: INFO: Wrong image for pod: daemon-set-qqnhf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 22 13:36:42.182: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:42.182: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:42.480835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:43.179: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:43.180: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0722 13:36:43.480877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:44.174: INFO: Pod daemon-set-mctwn is not available
  Jul 22 13:36:44.182: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:44.182: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/22/23 13:36:44.183
  Jul 22 13:36:44.189: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:44.190: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:44.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 22 13:36:44.196: INFO: Node ip-172-31-81-237 is running 0 daemon pod, expected 1
  E0722 13:36:44.481611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:45.204: INFO: DaemonSet pods can't tolerate node ip-172-31-24-255 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:45.204: INFO: DaemonSet pods can't tolerate node ip-172-31-8-181 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 22 13:36:45.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 22 13:36:45.211: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 13:36:45.253
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2911, will wait for the garbage collector to delete the pods @ 07/22/23 13:36:45.253
  Jul 22 13:36:45.320: INFO: Deleting DaemonSet.extensions daemon-set took: 10.376269ms
  Jul 22 13:36:45.421: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.934447ms
  E0722 13:36:45.482113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:46.482850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:36:47.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:36:47.329: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 13:36:47.335: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36985"},"items":null}

  Jul 22 13:36:47.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36985"},"items":null}

  Jul 22 13:36:47.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2911" for this suite. @ 07/22/23 13:36:47.37
• [12.499 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/22/23 13:36:47.382
  Jul 22 13:36:47.383: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename endpointslice @ 07/22/23 13:36:47.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:36:47.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:36:47.415
  E0722 13:36:47.484207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:48.484627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:49.484763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:50.484859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:51.485133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:52.485303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/22/23 13:36:52.513
  E0722 13:36:53.485997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:54.486437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:55.487441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:56.487537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:57.487665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/22/23 13:36:57.526
  E0722 13:36:58.488468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:36:59.488578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:00.488897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:01.489190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:02.489307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/22/23 13:37:02.537
  E0722 13:37:03.490163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:04.490474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:05.491508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:06.491816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:07.491969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/22/23 13:37:07.551
  Jul 22 13:37:07.586: INFO: EndpointSlice for Service endpointslice-7656/example-named-port not found
  E0722 13:37:08.492772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:09.493164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:10.493646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:11.493742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:12.494272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:13.495134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:14.495278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:15.496269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:16.496513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:17.496577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:37:17.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7656" for this suite. @ 07/22/23 13:37:17.61
• [30.239 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/22/23 13:37:17.622
  Jul 22 13:37:17.622: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 13:37:17.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:37:17.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:37:17.663
  STEP: creating a ServiceAccount @ 07/22/23 13:37:17.668
  STEP: watching for the ServiceAccount to be added @ 07/22/23 13:37:17.686
  STEP: patching the ServiceAccount @ 07/22/23 13:37:17.698
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/22/23 13:37:17.709
  STEP: deleting the ServiceAccount @ 07/22/23 13:37:17.715
  Jul 22 13:37:17.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6690" for this suite. @ 07/22/23 13:37:17.741
• [0.129 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/22/23 13:37:17.751
  Jul 22 13:37:17.752: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 13:37:17.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:37:17.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:37:17.782
  STEP: Creating a job @ 07/22/23 13:37:17.789
  STEP: Ensuring active pods == parallelism @ 07/22/23 13:37:17.8
  E0722 13:37:18.498051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:19.498523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 07/22/23 13:37:19.808
  STEP: deleting Job.batch foo in namespace job-9200, will wait for the garbage collector to delete the pods @ 07/22/23 13:37:19.808
  Jul 22 13:37:19.872: INFO: Deleting Job.batch foo took: 10.258858ms
  Jul 22 13:37:19.973: INFO: Terminating Job.batch foo pods took: 100.819026ms
  E0722 13:37:20.498609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:21.498621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:22.498953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:23.499802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:24.500354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:25.500508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:26.501343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:27.502362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:28.503430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:29.503705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:30.504489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:31.505235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:32.505916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:33.506408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:34.507404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:35.507937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:36.508447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:37.509110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:38.510075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:39.510921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:40.510975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:41.511613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:42.512661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:43.513610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:44.513893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:45.513987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:46.514141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:47.515155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:48.515901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:49.516664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:50.517235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 07/22/23 13:37:51.174
  Jul 22 13:37:51.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9200" for this suite. @ 07/22/23 13:37:51.187
• [33.446 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/22/23 13:37:51.2
  Jul 22 13:37:51.200: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename subpath @ 07/22/23 13:37:51.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:37:51.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:37:51.23
  STEP: Setting up data @ 07/22/23 13:37:51.24
  STEP: Creating pod pod-subpath-test-configmap-jz2r @ 07/22/23 13:37:51.263
  STEP: Creating a pod to test atomic-volume-subpath @ 07/22/23 13:37:51.263
  E0722 13:37:51.517873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:52.518100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:53.519110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:54.519300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:55.520343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:56.521251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:57.522085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:58.522506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:37:59.523206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:00.523932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:01.525971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:02.525246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:03.525887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:04.526040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:05.526234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:06.526380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:07.526413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:08.526527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:09.527255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:10.527612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:11.527702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:12.527996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:13.528513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:14.528678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:38:15.371
  Jul 22 13:38:15.381: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-subpath-test-configmap-jz2r container test-container-subpath-configmap-jz2r: <nil>
  STEP: delete the pod @ 07/22/23 13:38:15.41
  STEP: Deleting pod pod-subpath-test-configmap-jz2r @ 07/22/23 13:38:15.441
  Jul 22 13:38:15.441: INFO: Deleting pod "pod-subpath-test-configmap-jz2r" in namespace "subpath-4340"
  Jul 22 13:38:15.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4340" for this suite. @ 07/22/23 13:38:15.454
• [24.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/22/23 13:38:15.467
  Jul 22 13:38:15.467: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:38:15.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:15.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:15.505
  STEP: validating cluster-info @ 07/22/23 13:38:15.51
  Jul 22 13:38:15.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-2637 cluster-info'
  E0722 13:38:15.529373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:38:15.606: INFO: stderr: ""
  Jul 22 13:38:15.606: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul 22 13:38:15.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2637" for this suite. @ 07/22/23 13:38:15.615
• [0.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/22/23 13:38:15.629
  Jul 22 13:38:15.629: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 13:38:15.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:15.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:15.666
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/22/23 13:38:15.672
  Jul 22 13:38:15.673: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:38:16.529503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:38:17.351: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:38:17.530012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:18.534402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:19.535245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:20.535775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:21.535829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:22.536060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:23.536461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:38:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2328" for this suite. @ 07/22/23 13:38:23.891
• [8.273 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/22/23 13:38:23.903
  Jul 22 13:38:23.903: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:38:23.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:23.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:23.933
  STEP: Creating a pod to test downward API volume plugin @ 07/22/23 13:38:23.938
  E0722 13:38:24.537663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:25.538468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:26.538594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:27.538716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:38:27.967
  Jul 22 13:38:27.971: INFO: Trying to get logs from node ip-172-31-15-55 pod downwardapi-volume-f0ece2b2-bce4-4bf2-b136-9b9174e677d8 container client-container: <nil>
  STEP: delete the pod @ 07/22/23 13:38:27.983
  Jul 22 13:38:28.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3369" for this suite. @ 07/22/23 13:38:28.017
• [4.128 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/22/23 13:38:28.031
  Jul 22 13:38:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replicaset @ 07/22/23 13:38:28.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:28.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:28.066
  STEP: Create a ReplicaSet @ 07/22/23 13:38:28.073
  STEP: Verify that the required pods have come up @ 07/22/23 13:38:28.082
  Jul 22 13:38:28.088: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0722 13:38:28.538850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:29.539115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:30.539463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:31.539536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:32.540041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:38:33.096: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/22/23 13:38:33.096
  Jul 22 13:38:33.105: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/22/23 13:38:33.105
  STEP: DeleteCollection of the ReplicaSets @ 07/22/23 13:38:33.111
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/22/23 13:38:33.142
  Jul 22 13:38:33.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3280" for this suite. @ 07/22/23 13:38:33.169
• [5.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/22/23 13:38:33.211
  Jul 22 13:38:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-runtime @ 07/22/23 13:38:33.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:33.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:33.257
  STEP: create the container @ 07/22/23 13:38:33.268
  W0722 13:38:33.287272      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/22/23 13:38:33.287
  E0722 13:38:33.541818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:34.542808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:35.543840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:36.543956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/22/23 13:38:37.325
  STEP: the container should be terminated @ 07/22/23 13:38:37.33
  STEP: the termination message should be set @ 07/22/23 13:38:37.33
  Jul 22 13:38:37.330: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/22/23 13:38:37.33
  Jul 22 13:38:37.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9501" for this suite. @ 07/22/23 13:38:37.367
• [4.166 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/22/23 13:38:37.377
  Jul 22 13:38:37.377: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename configmap @ 07/22/23 13:38:37.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:37.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:37.406
  STEP: Creating configMap with name configmap-test-volume-1cbf7d0e-c7fa-41b8-8764-b4e8e2142338 @ 07/22/23 13:38:37.413
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:38:37.42
  E0722 13:38:37.544727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:38.545068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:39.545646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:40.546173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:38:41.452
  Jul 22 13:38:41.459: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-configmaps-100cc6e6-e1b3-43e2-9fb6-e85d8538028d container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:38:41.471
  Jul 22 13:38:41.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-657" for this suite. @ 07/22/23 13:38:41.504
• [4.137 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/22/23 13:38:41.514
  Jul 22 13:38:41.514: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 13:38:41.515
  E0722 13:38:41.546697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:38:41.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:38:41.553
  STEP: create the rc @ 07/22/23 13:38:41.564
  W0722 13:38:41.572230      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0722 13:38:42.547005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:43.548098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:44.549017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:45.549347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:46.549601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:47.549920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/22/23 13:38:47.578
  STEP: wait for the rc to be deleted @ 07/22/23 13:38:47.591
  E0722 13:38:48.550326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:49.550746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:50.551232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:51.551411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:52.556160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/22/23 13:38:52.604
  E0722 13:38:53.557778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:54.559210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:55.561397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:56.561543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:57.561679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:58.561739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:38:59.561821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:00.562521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:01.562934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:02.563328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:03.563379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:04.563522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:05.564288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:06.564375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:07.564493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:08.564611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:09.565711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:10.566677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:11.566973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:12.567128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:13.567238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:14.567366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:15.567474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:16.567605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:17.567858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:18.567838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:19.568157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:20.569157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:21.570200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:22.570281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/22/23 13:39:22.62
  W0722 13:39:22.627085      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 13:39:22.627: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 13:39:22.627: INFO: Deleting pod "simpletest.rc-277jt" in namespace "gc-4735"
  Jul 22 13:39:22.643: INFO: Deleting pod "simpletest.rc-2vjxm" in namespace "gc-4735"
  Jul 22 13:39:22.661: INFO: Deleting pod "simpletest.rc-4s8xv" in namespace "gc-4735"
  Jul 22 13:39:22.679: INFO: Deleting pod "simpletest.rc-4xcrv" in namespace "gc-4735"
  Jul 22 13:39:22.694: INFO: Deleting pod "simpletest.rc-5444m" in namespace "gc-4735"
  Jul 22 13:39:22.717: INFO: Deleting pod "simpletest.rc-56qcj" in namespace "gc-4735"
  Jul 22 13:39:22.738: INFO: Deleting pod "simpletest.rc-597fk" in namespace "gc-4735"
  Jul 22 13:39:22.755: INFO: Deleting pod "simpletest.rc-5k5x6" in namespace "gc-4735"
  Jul 22 13:39:22.776: INFO: Deleting pod "simpletest.rc-5lckh" in namespace "gc-4735"
  Jul 22 13:39:22.794: INFO: Deleting pod "simpletest.rc-6225p" in namespace "gc-4735"
  Jul 22 13:39:22.809: INFO: Deleting pod "simpletest.rc-6m5tf" in namespace "gc-4735"
  Jul 22 13:39:22.828: INFO: Deleting pod "simpletest.rc-6tk5z" in namespace "gc-4735"
  Jul 22 13:39:22.846: INFO: Deleting pod "simpletest.rc-6xxq6" in namespace "gc-4735"
  Jul 22 13:39:22.862: INFO: Deleting pod "simpletest.rc-7hk6v" in namespace "gc-4735"
  Jul 22 13:39:22.881: INFO: Deleting pod "simpletest.rc-7m6jw" in namespace "gc-4735"
  Jul 22 13:39:22.898: INFO: Deleting pod "simpletest.rc-7p45s" in namespace "gc-4735"
  Jul 22 13:39:22.921: INFO: Deleting pod "simpletest.rc-7thv7" in namespace "gc-4735"
  Jul 22 13:39:22.938: INFO: Deleting pod "simpletest.rc-7tq7z" in namespace "gc-4735"
  Jul 22 13:39:22.956: INFO: Deleting pod "simpletest.rc-88tk8" in namespace "gc-4735"
  Jul 22 13:39:22.978: INFO: Deleting pod "simpletest.rc-8c5pn" in namespace "gc-4735"
  Jul 22 13:39:22.995: INFO: Deleting pod "simpletest.rc-8g5x8" in namespace "gc-4735"
  Jul 22 13:39:23.011: INFO: Deleting pod "simpletest.rc-8qflf" in namespace "gc-4735"
  Jul 22 13:39:23.036: INFO: Deleting pod "simpletest.rc-8r7c9" in namespace "gc-4735"
  Jul 22 13:39:23.051: INFO: Deleting pod "simpletest.rc-95vdm" in namespace "gc-4735"
  Jul 22 13:39:23.068: INFO: Deleting pod "simpletest.rc-9rpqv" in namespace "gc-4735"
  Jul 22 13:39:23.090: INFO: Deleting pod "simpletest.rc-bg4tf" in namespace "gc-4735"
  Jul 22 13:39:23.108: INFO: Deleting pod "simpletest.rc-bsjkq" in namespace "gc-4735"
  Jul 22 13:39:23.122: INFO: Deleting pod "simpletest.rc-ckc5m" in namespace "gc-4735"
  Jul 22 13:39:23.140: INFO: Deleting pod "simpletest.rc-cn5z2" in namespace "gc-4735"
  Jul 22 13:39:23.172: INFO: Deleting pod "simpletest.rc-cnn9w" in namespace "gc-4735"
  Jul 22 13:39:23.190: INFO: Deleting pod "simpletest.rc-cqwhk" in namespace "gc-4735"
  Jul 22 13:39:23.207: INFO: Deleting pod "simpletest.rc-cx68h" in namespace "gc-4735"
  Jul 22 13:39:23.226: INFO: Deleting pod "simpletest.rc-dk8j7" in namespace "gc-4735"
  Jul 22 13:39:23.241: INFO: Deleting pod "simpletest.rc-dxv4z" in namespace "gc-4735"
  Jul 22 13:39:23.263: INFO: Deleting pod "simpletest.rc-fjh74" in namespace "gc-4735"
  Jul 22 13:39:23.290: INFO: Deleting pod "simpletest.rc-gnbgn" in namespace "gc-4735"
  Jul 22 13:39:23.313: INFO: Deleting pod "simpletest.rc-gnfm2" in namespace "gc-4735"
  Jul 22 13:39:23.333: INFO: Deleting pod "simpletest.rc-gvn4m" in namespace "gc-4735"
  Jul 22 13:39:23.362: INFO: Deleting pod "simpletest.rc-gxplw" in namespace "gc-4735"
  Jul 22 13:39:23.382: INFO: Deleting pod "simpletest.rc-h2x7r" in namespace "gc-4735"
  Jul 22 13:39:23.395: INFO: Deleting pod "simpletest.rc-hhpnq" in namespace "gc-4735"
  Jul 22 13:39:23.412: INFO: Deleting pod "simpletest.rc-hs46z" in namespace "gc-4735"
  Jul 22 13:39:23.428: INFO: Deleting pod "simpletest.rc-jf4cf" in namespace "gc-4735"
  Jul 22 13:39:23.449: INFO: Deleting pod "simpletest.rc-jj86z" in namespace "gc-4735"
  Jul 22 13:39:23.476: INFO: Deleting pod "simpletest.rc-jkxtf" in namespace "gc-4735"
  Jul 22 13:39:23.492: INFO: Deleting pod "simpletest.rc-jnkgn" in namespace "gc-4735"
  Jul 22 13:39:23.508: INFO: Deleting pod "simpletest.rc-jpdpr" in namespace "gc-4735"
  Jul 22 13:39:23.532: INFO: Deleting pod "simpletest.rc-jpnhh" in namespace "gc-4735"
  Jul 22 13:39:23.548: INFO: Deleting pod "simpletest.rc-l6lwm" in namespace "gc-4735"
  Jul 22 13:39:23.562: INFO: Deleting pod "simpletest.rc-l8vrg" in namespace "gc-4735"
  E0722 13:39:23.570664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:23.581: INFO: Deleting pod "simpletest.rc-lf66d" in namespace "gc-4735"
  Jul 22 13:39:23.602: INFO: Deleting pod "simpletest.rc-lhqwg" in namespace "gc-4735"
  Jul 22 13:39:23.623: INFO: Deleting pod "simpletest.rc-lrmlr" in namespace "gc-4735"
  Jul 22 13:39:23.637: INFO: Deleting pod "simpletest.rc-lzp4z" in namespace "gc-4735"
  Jul 22 13:39:23.663: INFO: Deleting pod "simpletest.rc-mb4ck" in namespace "gc-4735"
  Jul 22 13:39:23.679: INFO: Deleting pod "simpletest.rc-mjqrk" in namespace "gc-4735"
  Jul 22 13:39:23.699: INFO: Deleting pod "simpletest.rc-mnbbb" in namespace "gc-4735"
  Jul 22 13:39:23.716: INFO: Deleting pod "simpletest.rc-mp4f6" in namespace "gc-4735"
  Jul 22 13:39:23.731: INFO: Deleting pod "simpletest.rc-mv9f8" in namespace "gc-4735"
  Jul 22 13:39:23.747: INFO: Deleting pod "simpletest.rc-n4vhz" in namespace "gc-4735"
  Jul 22 13:39:23.762: INFO: Deleting pod "simpletest.rc-nnhpg" in namespace "gc-4735"
  Jul 22 13:39:23.776: INFO: Deleting pod "simpletest.rc-ppdpx" in namespace "gc-4735"
  Jul 22 13:39:23.791: INFO: Deleting pod "simpletest.rc-q59ht" in namespace "gc-4735"
  Jul 22 13:39:23.809: INFO: Deleting pod "simpletest.rc-q5zc5" in namespace "gc-4735"
  Jul 22 13:39:23.827: INFO: Deleting pod "simpletest.rc-qg8cr" in namespace "gc-4735"
  Jul 22 13:39:23.848: INFO: Deleting pod "simpletest.rc-qlwjk" in namespace "gc-4735"
  Jul 22 13:39:23.865: INFO: Deleting pod "simpletest.rc-qpf8m" in namespace "gc-4735"
  Jul 22 13:39:23.879: INFO: Deleting pod "simpletest.rc-qtq45" in namespace "gc-4735"
  Jul 22 13:39:23.894: INFO: Deleting pod "simpletest.rc-qvxc7" in namespace "gc-4735"
  Jul 22 13:39:23.910: INFO: Deleting pod "simpletest.rc-r6fqp" in namespace "gc-4735"
  Jul 22 13:39:23.928: INFO: Deleting pod "simpletest.rc-r7dbd" in namespace "gc-4735"
  Jul 22 13:39:23.944: INFO: Deleting pod "simpletest.rc-r7s64" in namespace "gc-4735"
  Jul 22 13:39:23.960: INFO: Deleting pod "simpletest.rc-rbk95" in namespace "gc-4735"
  Jul 22 13:39:23.976: INFO: Deleting pod "simpletest.rc-rkxs7" in namespace "gc-4735"
  Jul 22 13:39:23.991: INFO: Deleting pod "simpletest.rc-rtbxg" in namespace "gc-4735"
  Jul 22 13:39:24.006: INFO: Deleting pod "simpletest.rc-rvfgh" in namespace "gc-4735"
  Jul 22 13:39:24.019: INFO: Deleting pod "simpletest.rc-rwkjc" in namespace "gc-4735"
  Jul 22 13:39:24.069: INFO: Deleting pod "simpletest.rc-rxm7z" in namespace "gc-4735"
  Jul 22 13:39:24.122: INFO: Deleting pod "simpletest.rc-scgc8" in namespace "gc-4735"
  Jul 22 13:39:24.173: INFO: Deleting pod "simpletest.rc-sfhj7" in namespace "gc-4735"
  Jul 22 13:39:24.221: INFO: Deleting pod "simpletest.rc-shjkf" in namespace "gc-4735"
  Jul 22 13:39:24.275: INFO: Deleting pod "simpletest.rc-sjmhp" in namespace "gc-4735"
  Jul 22 13:39:24.325: INFO: Deleting pod "simpletest.rc-slwwx" in namespace "gc-4735"
  Jul 22 13:39:24.378: INFO: Deleting pod "simpletest.rc-ssttv" in namespace "gc-4735"
  Jul 22 13:39:24.435: INFO: Deleting pod "simpletest.rc-sx2bg" in namespace "gc-4735"
  Jul 22 13:39:24.476: INFO: Deleting pod "simpletest.rc-sxtjv" in namespace "gc-4735"
  Jul 22 13:39:24.524: INFO: Deleting pod "simpletest.rc-tbwn2" in namespace "gc-4735"
  E0722 13:39:24.571223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:24.571: INFO: Deleting pod "simpletest.rc-tc5kn" in namespace "gc-4735"
  Jul 22 13:39:24.620: INFO: Deleting pod "simpletest.rc-tskf6" in namespace "gc-4735"
  Jul 22 13:39:24.671: INFO: Deleting pod "simpletest.rc-v66sv" in namespace "gc-4735"
  Jul 22 13:39:24.726: INFO: Deleting pod "simpletest.rc-vj7bn" in namespace "gc-4735"
  Jul 22 13:39:24.771: INFO: Deleting pod "simpletest.rc-vpcmh" in namespace "gc-4735"
  Jul 22 13:39:24.825: INFO: Deleting pod "simpletest.rc-vzdbz" in namespace "gc-4735"
  Jul 22 13:39:24.874: INFO: Deleting pod "simpletest.rc-wkdk4" in namespace "gc-4735"
  Jul 22 13:39:24.920: INFO: Deleting pod "simpletest.rc-x6kn9" in namespace "gc-4735"
  Jul 22 13:39:24.973: INFO: Deleting pod "simpletest.rc-x7wkd" in namespace "gc-4735"
  Jul 22 13:39:25.025: INFO: Deleting pod "simpletest.rc-xpdpg" in namespace "gc-4735"
  Jul 22 13:39:25.075: INFO: Deleting pod "simpletest.rc-xtlpx" in namespace "gc-4735"
  Jul 22 13:39:25.125: INFO: Deleting pod "simpletest.rc-zb6q6" in namespace "gc-4735"
  Jul 22 13:39:25.171: INFO: Deleting pod "simpletest.rc-ztrrx" in namespace "gc-4735"
  Jul 22 13:39:25.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4735" for this suite. @ 07/22/23 13:39:25.262
• [43.802 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/22/23 13:39:25.318
  Jul 22 13:39:25.318: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:39:25.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:39:25.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:39:25.353
  STEP: Setting up server cert @ 07/22/23 13:39:25.389
  E0722 13:39:25.571416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:39:26.236
  STEP: Deploying the webhook pod @ 07/22/23 13:39:26.256
  STEP: Wait for the deployment to be ready @ 07/22/23 13:39:26.279
  Jul 22 13:39:26.291: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:39:26.571781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:27.572010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:28.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:39:28.572009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:29.572441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:30.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:39:30.572558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:31.572949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:32.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:39:32.573789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:33.575323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:34.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 39, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:39:34.575426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:35.576434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:39:36.326
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:39:36.346
  E0722 13:39:36.576773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:37.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/22/23 13:39:37.354
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/22/23 13:39:37.354
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/22/23 13:39:37.391
  E0722 13:39:37.577594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/22/23 13:39:38.414
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/22/23 13:39:38.414
  E0722 13:39:38.578227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 07/22/23 13:39:39.457
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/22/23 13:39:39.457
  E0722 13:39:39.579289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:40.579482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:41.580165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:42.580625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:43.580762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/22/23 13:39:44.526
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/22/23 13:39:44.526
  E0722 13:39:44.580845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:45.581938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:46.582109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:47.582146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:48.582285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:49.582522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:49.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5685" for this suite. @ 07/22/23 13:39:49.694
  STEP: Destroying namespace "webhook-markers-1252" for this suite. @ 07/22/23 13:39:49.708
• [24.399 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/22/23 13:39:49.719
  Jul 22 13:39:49.720: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:39:49.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:39:49.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:39:49.754
  STEP: Creating projection with secret that has name secret-emptykey-test-f92c7172-cc96-4cec-9130-22711cfaea3b @ 07/22/23 13:39:49.758
  Jul 22 13:39:49.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3476" for this suite. @ 07/22/23 13:39:49.769
• [0.060 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 07/22/23 13:39:49.781
  Jul 22 13:39:49.781: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename daemonsets @ 07/22/23 13:39:49.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:39:49.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:39:49.812
  Jul 22 13:39:49.850: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/22/23 13:39:49.858
  Jul 22 13:39:49.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:49.862: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/22/23 13:39:49.862
  Jul 22 13:39:49.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:49.890: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:39:50.582633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:50.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:50.896: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:39:51.583248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:51.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 22 13:39:51.895: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/22/23 13:39:51.901
  Jul 22 13:39:51.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 22 13:39:51.933: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0722 13:39:52.583840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:52.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:52.941: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/22/23 13:39:52.941
  Jul 22 13:39:52.961: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:52.961: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:39:53.584803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:53.968: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:53.968: INFO: Node ip-172-31-15-55 is running 0 daemon pod, expected 1
  E0722 13:39:54.585300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:54.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 22 13:39:54.968: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/22/23 13:39:54.978
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2159, will wait for the garbage collector to delete the pods @ 07/22/23 13:39:54.978
  Jul 22 13:39:55.043: INFO: Deleting DaemonSet.extensions daemon-set took: 8.693215ms
  Jul 22 13:39:55.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.556234ms
  E0722 13:39:55.586223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:39:56.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 22 13:39:56.151: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 22 13:39:56.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40417"},"items":null}

  Jul 22 13:39:56.162: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40417"},"items":null}

  Jul 22 13:39:56.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2159" for this suite. @ 07/22/23 13:39:56.21
• [6.445 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/22/23 13:39:56.228
  Jul 22 13:39:56.228: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename deployment @ 07/22/23 13:39:56.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:39:56.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:39:56.26
  Jul 22 13:39:56.267: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul 22 13:39:56.289: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0722 13:39:56.586972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:57.587106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:58.587532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:39:59.587818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:00.588563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:40:01.295: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/22/23 13:40:01.295
  Jul 22 13:40:01.296: INFO: Creating deployment "test-rolling-update-deployment"
  Jul 22 13:40:01.526: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul 22 13:40:01.538: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0722 13:40:01.588891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:02.589171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:40:03.552: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul 22 13:40:03.557: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul 22 13:40:03.576: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5684  0d165647-04a1-406b-bcfe-0919f704f43f 40504 1 2023-07-22 13:40:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-22 13:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:40:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004145558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-22 13:40:01 +0000 UTC,LastTransitionTime:2023-07-22 13:40:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-22 13:40:03 +0000 UTC,LastTransitionTime:2023-07-22 13:40:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 22 13:40:03.582: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-5684  ea679bb2-5085-42ec-84a6-452b15389d12 40494 1 2023-07-22 13:40:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 0d165647-04a1-406b-bcfe-0919f704f43f 0xc004145a67 0xc004145a68}] [] [{kube-controller-manager Update apps/v1 2023-07-22 13:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d165647-04a1-406b-bcfe-0919f704f43f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:40:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004145b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 22 13:40:03.582: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul 22 13:40:03.583: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5684  b84833b6-a410-4c69-a22a-cd1935c2f963 40503 2 2023-07-22 13:39:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 0d165647-04a1-406b-bcfe-0919f704f43f 0xc004145937 0xc004145938}] [] [{e2e.test Update apps/v1 2023-07-22 13:39:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:40:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d165647-04a1-406b-bcfe-0919f704f43f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-22 13:40:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041459f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  E0722 13:40:03.590236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:40:03.591: INFO: Pod "test-rolling-update-deployment-656d657cd8-bvntw" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-bvntw test-rolling-update-deployment-656d657cd8- deployment-5684  a7002b2b-af05-4218-a5e5-67f01cdec9c2 40493 0 2023-07-22 13:40:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 ea679bb2-5085-42ec-84a6-452b15389d12 0xc004145f87 0xc004145f88}] [] [{kube-controller-manager Update v1 2023-07-22 13:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea679bb2-5085-42ec-84a6-452b15389d12\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-22 13:40:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.196.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdt7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdt7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-55,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:40:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:40:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-22 13:40:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.55,PodIP:192.168.196.135,StartTime:2023-07-22 13:40:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-22 13:40:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cb76fcceacaa68a2b143b4ea2ae0ac06c4a1966ce6cb22b67c9464062b446248,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.196.135,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 22 13:40:03.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5684" for this suite. @ 07/22/23 13:40:03.601
• [7.383 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/22/23 13:40:03.613
  Jul 22 13:40:03.613: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename events @ 07/22/23 13:40:03.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:40:03.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:40:03.649
  STEP: Create set of events @ 07/22/23 13:40:03.656
  Jul 22 13:40:03.663: INFO: created test-event-1
  Jul 22 13:40:03.675: INFO: created test-event-2
  Jul 22 13:40:03.685: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/22/23 13:40:03.685
  STEP: delete collection of events @ 07/22/23 13:40:03.692
  Jul 22 13:40:03.692: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/22/23 13:40:03.74
  Jul 22 13:40:03.741: INFO: requesting list of events to confirm quantity
  Jul 22 13:40:03.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5634" for this suite. @ 07/22/23 13:40:03.754
• [0.152 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/22/23 13:40:03.766
  Jul 22 13:40:03.766: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:40:03.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:40:03.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:40:03.855
  STEP: Setting up server cert @ 07/22/23 13:40:03.894
  E0722 13:40:04.591273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:40:04.737
  STEP: Deploying the webhook pod @ 07/22/23 13:40:04.753
  STEP: Wait for the deployment to be ready @ 07/22/23 13:40:04.777
  Jul 22 13:40:04.802: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:40:05.591519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:06.591765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:40:06.822
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:40:06.84
  E0722 13:40:07.591865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:40:07.841: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 22 13:40:07.847: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7088-crds.webhook.example.com via the AdmissionRegistration API @ 07/22/23 13:40:08.363
  STEP: Creating a custom resource while v1 is storage version @ 07/22/23 13:40:08.393
  E0722 13:40:08.592754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:09.592862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/22/23 13:40:10.484
  STEP: Patching the custom resource while v2 is storage version @ 07/22/23 13:40:10.51
  E0722 13:40:10.593695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:40:10.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5898" for this suite. @ 07/22/23 13:40:11.241
  STEP: Destroying namespace "webhook-markers-5609" for this suite. @ 07/22/23 13:40:11.252
• [7.500 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/22/23 13:40:11.267
  Jul 22 13:40:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:40:11.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:40:11.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:40:11.307
  STEP: Creating secret with name secret-test-fdbcf0f8-9740-4aaa-b907-b509ef496fc8 @ 07/22/23 13:40:11.313
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:40:11.322
  E0722 13:40:11.593929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:12.594169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:13.594851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:14.594946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:40:15.36
  Jul 22 13:40:15.369: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-c4aa195b-c7eb-4037-9893-840e84824a99 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:40:15.412
  Jul 22 13:40:15.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-749" for this suite. @ 07/22/23 13:40:15.448
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/22/23 13:40:15.463
  Jul 22 13:40:15.463: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-watch @ 07/22/23 13:40:15.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:40:15.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:40:15.533
  Jul 22 13:40:15.545: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:40:15.595496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:16.596355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:17.596500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 07/22/23 13:40:18.134
  Jul 22 13:40:18.142: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:18Z]] name:name1 resourceVersion:40716 uid:500157c1-c672-4c62-ae4e-848ff2cf0319] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:40:18.597159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:19.597333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:20.597993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:21.598162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:22.598586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:23.599064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:24.599670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:25.600301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:26.600364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:27.602663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 07/22/23 13:40:28.143
  Jul 22 13:40:28.154: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:28Z]] name:name2 resourceVersion:40747 uid:b96c793e-099d-469a-b3c7-2e09c3d9aa15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:40:28.602823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:29.603520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:30.604585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:31.605081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:32.605247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:33.605269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:34.605424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:35.605612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:36.605983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:37.606125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 07/22/23 13:40:38.154
  Jul 22 13:40:38.171: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:38Z]] name:name1 resourceVersion:40767 uid:500157c1-c672-4c62-ae4e-848ff2cf0319] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:40:38.606266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:39.606432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:40.607333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:41.607417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:42.607586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:43.607722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:44.608085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:45.608631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:46.608846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:47.609130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 07/22/23 13:40:48.172
  Jul 22 13:40:48.182: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:48Z]] name:name2 resourceVersion:40786 uid:b96c793e-099d-469a-b3c7-2e09c3d9aa15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:40:48.610199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:49.610912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:50.610959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:51.611110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:52.611195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:53.611265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:54.611411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:55.611507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:56.611853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:57.611988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 07/22/23 13:40:58.183
  Jul 22 13:40:58.195: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:38Z]] name:name1 resourceVersion:40805 uid:500157c1-c672-4c62-ae4e-848ff2cf0319] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:40:58.612766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:40:59.613195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:00.613796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:01.614158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:02.614481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:03.614844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:04.615209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:05.615645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:06.616121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:07.616655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 07/22/23 13:41:08.196
  Jul 22 13:41:08.208: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-22T13:40:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-22T13:40:48Z]] name:name2 resourceVersion:40825 uid:b96c793e-099d-469a-b3c7-2e09c3d9aa15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0722 13:41:08.617541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:09.617607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:10.618052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:11.618495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:12.619995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:13.619051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:14.619169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:15.619686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:16.619808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:17.620182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:18.620718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:18.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-521" for this suite. @ 07/22/23 13:41:18.737
• [63.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/22/23 13:41:18.75
  Jul 22 13:41:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:41:18.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:18.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:18.806
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/22/23 13:41:18.814
  E0722 13:41:19.621556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:20.622204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:21.622365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:22.622377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:41:22.846
  Jul 22 13:41:22.851: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-71968934-826d-4aab-a92b-063a59a0e44f container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:41:22.86
  Jul 22 13:41:22.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9881" for this suite. @ 07/22/23 13:41:22.886
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/22/23 13:41:22.901
  Jul 22 13:41:22.901: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:41:22.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:22.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:22.932
  STEP: creating Agnhost RC @ 07/22/23 13:41:22.938
  Jul 22 13:41:22.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-3131 create -f -'
  Jul 22 13:41:23.560: INFO: stderr: ""
  Jul 22 13:41:23.560: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/22/23 13:41:23.56
  E0722 13:41:23.622456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:24.565: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:41:24.565: INFO: Found 0 / 1
  E0722 13:41:24.623185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:25.568: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:41:25.568: INFO: Found 1 / 1
  Jul 22 13:41:25.568: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/22/23 13:41:25.568
  Jul 22 13:41:25.575: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:41:25.575: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 22 13:41:25.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-3131 patch pod agnhost-primary-fs65m -p {"metadata":{"annotations":{"x":"y"}}}'
  E0722 13:41:25.623752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:25.691: INFO: stderr: ""
  Jul 22 13:41:25.691: INFO: stdout: "pod/agnhost-primary-fs65m patched\n"
  STEP: checking annotations @ 07/22/23 13:41:25.691
  Jul 22 13:41:25.697: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 22 13:41:25.697: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 22 13:41:25.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3131" for this suite. @ 07/22/23 13:41:25.704
• [2.814 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/22/23 13:41:25.717
  Jul 22 13:41:25.717: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:41:25.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:25.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:25.767
  STEP: creating pod @ 07/22/23 13:41:25.773
  E0722 13:41:26.624486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:27.625086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:27.814: INFO: Pod pod-hostip-1260131b-dcae-4470-9923-d0667a65d646 has hostIP: 172.31.15.55
  Jul 22 13:41:27.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6175" for this suite. @ 07/22/23 13:41:27.82
• [2.118 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/22/23 13:41:27.835
  Jul 22 13:41:27.836: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 13:41:27.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:27.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:27.869
  STEP: Creating a ResourceQuota with terminating scope @ 07/22/23 13:41:27.876
  STEP: Ensuring ResourceQuota status is calculated @ 07/22/23 13:41:27.883
  E0722 13:41:28.625915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:29.626305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 07/22/23 13:41:29.892
  STEP: Ensuring ResourceQuota status is calculated @ 07/22/23 13:41:29.911
  E0722 13:41:30.626458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:31.627224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 07/22/23 13:41:31.916
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/22/23 13:41:31.937
  E0722 13:41:32.628283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:33.631087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/22/23 13:41:33.943
  E0722 13:41:34.631333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:35.632420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/22/23 13:41:35.948
  STEP: Ensuring resource quota status released the pod usage @ 07/22/23 13:41:35.963
  E0722 13:41:36.633106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:37.633158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 07/22/23 13:41:37.97
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/22/23 13:41:37.986
  E0722 13:41:38.633163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:39.633409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/22/23 13:41:39.992
  E0722 13:41:40.634145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:41.634503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/22/23 13:41:41.998
  STEP: Ensuring resource quota status released the pod usage @ 07/22/23 13:41:42.014
  E0722 13:41:42.634569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:43.634884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5973" for this suite. @ 07/22/23 13:41:44.037
• [16.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/22/23 13:41:44.055
  Jul 22 13:41:44.055: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 13:41:44.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:44.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:44.09
  Jul 22 13:41:44.152: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e2d9ac10-555c-4f6e-bfee-d55a65ea56a7", Controller:(*bool)(0xc00487aeb6), BlockOwnerDeletion:(*bool)(0xc00487aeb7)}}
  Jul 22 13:41:44.177: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d3ea15d9-4591-4fb8-954e-77c8991ab0d1", Controller:(*bool)(0xc00693f2ce), BlockOwnerDeletion:(*bool)(0xc00693f2cf)}}
  Jul 22 13:41:44.187: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"17bb7021-a69a-4478-a4ee-bbb3a033f72a", Controller:(*bool)(0xc00487b146), BlockOwnerDeletion:(*bool)(0xc00487b147)}}
  E0722 13:41:44.635011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:45.635653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:46.636158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:47.636333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:48.636624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:41:49.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8708" for this suite. @ 07/22/23 13:41:49.217
• [5.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/22/23 13:41:49.232
  Jul 22 13:41:49.232: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename resourcequota @ 07/22/23 13:41:49.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:41:49.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:41:49.275
  STEP: Discovering how many secrets are in namespace by default @ 07/22/23 13:41:49.283
  E0722 13:41:49.636680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:50.637572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:51.638668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:52.638838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:53.639694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/22/23 13:41:54.289
  E0722 13:41:54.639911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:55.640996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:56.641645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:57.641711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:41:58.642719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/22/23 13:41:59.293
  STEP: Ensuring resource quota status is calculated @ 07/22/23 13:41:59.303
  E0722 13:41:59.643647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:00.644207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 07/22/23 13:42:01.311
  STEP: Ensuring resource quota status captures secret creation @ 07/22/23 13:42:01.326
  E0722 13:42:01.644450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:02.648107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 07/22/23 13:42:03.333
  STEP: Ensuring resource quota status released usage @ 07/22/23 13:42:03.348
  E0722 13:42:03.648648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:04.648751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:42:05.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8794" for this suite. @ 07/22/23 13:42:05.373
• [16.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/22/23 13:42:05.388
  Jul 22 13:42:05.388: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename runtimeclass @ 07/22/23 13:42:05.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:42:05.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:42:05.424
  E0722 13:42:05.649222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:06.649533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:42:07.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5009" for this suite. @ 07/22/23 13:42:07.486
• [2.109 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/22/23 13:42:07.498
  Jul 22 13:42:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/22/23 13:42:07.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:42:07.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:42:07.534
  STEP: creating a target pod @ 07/22/23 13:42:07.541
  E0722 13:42:07.650568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:08.650681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 07/22/23 13:42:09.574
  E0722 13:42:09.651112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:10.651213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 07/22/23 13:42:11.604
  Jul 22 13:42:11.604: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2527 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 22 13:42:11.604: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  Jul 22 13:42:11.605: INFO: ExecWithOptions: Clientset creation
  Jul 22 13:42:11.605: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-2527/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  E0722 13:42:11.652308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:42:11.694: INFO: Exec stderr: ""
  Jul 22 13:42:11.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-2527" for this suite. @ 07/22/23 13:42:11.715
• [4.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/22/23 13:42:11.737
  Jul 22 13:42:11.737: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:42:11.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:42:11.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:42:11.786
  STEP: Create set of pods @ 07/22/23 13:42:11.799
  Jul 22 13:42:11.816: INFO: created test-pod-1
  Jul 22 13:42:11.831: INFO: created test-pod-2
  Jul 22 13:42:11.839: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/22/23 13:42:11.839
  E0722 13:42:12.652417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:13.652534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/22/23 13:42:13.922
  Jul 22 13:42:13.928: INFO: Pod quantity 3 is different from expected quantity 0
  E0722 13:42:14.652671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:42:14.935: INFO: Pod quantity 3 is different from expected quantity 0
  E0722 13:42:15.653223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:42:15.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1365" for this suite. @ 07/22/23 13:42:15.944
• [4.221 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/22/23 13:42:15.96
  Jul 22 13:42:15.960: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sched-preemption @ 07/22/23 13:42:15.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:42:15.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:42:15.997
  Jul 22 13:42:16.026: INFO: Waiting up to 1m0s for all nodes to be ready
  E0722 13:42:16.653538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:17.653638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:18.654141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:19.654287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:20.654454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:21.654545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:22.655444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:23.656283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:24.657844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:25.658854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:26.659003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:27.659524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:28.660330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:29.660564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:30.661102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:31.661254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:32.662198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:33.662302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:34.662631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:35.662979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:36.663841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:37.664478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:38.665682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:39.665876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:40.666136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:41.666479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:42.667332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:43.667955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:44.668665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:45.669129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:46.669205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:47.669398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:48.669836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:49.669961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:50.670241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:51.670910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:52.671590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:53.671744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:54.672150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:55.672722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:56.672833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:57.673086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:58.674152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:42:59.674271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:00.674857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:01.674972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:02.675994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:03.676364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:04.677052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:05.677780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:06.678933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:07.679441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:08.679633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:09.680203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:10.680848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:11.681204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:12.682156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:13.682652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:14.683495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:15.683467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:16.054: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/22/23 13:43:16.061
  Jul 22 13:43:16.089: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 22 13:43:16.099: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 22 13:43:16.191: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 22 13:43:16.208: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 22 13:43:16.244: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 22 13:43:16.254: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/22/23 13:43:16.254
  E0722 13:43:16.684548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:17.685312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:18.685368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:19.685499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/22/23 13:43:20.307
  E0722 13:43:20.686177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:21.686458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:22.686797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:23.687002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:24.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9012" for this suite. @ 07/22/23 13:43:24.581
• [68.636 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/22/23 13:43:24.601
  Jul 22 13:43:24.601: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename replication-controller @ 07/22/23 13:43:24.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:43:24.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:43:24.678
  E0722 13:43:24.687734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Given a ReplicationController is created @ 07/22/23 13:43:24.694
  STEP: When the matched label of one of its pods change @ 07/22/23 13:43:24.713
  Jul 22 13:43:24.734: INFO: Pod name pod-release: Found 0 pods out of 1
  E0722 13:43:25.688729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:26.689159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:27.689359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:28.689472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:29.690170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:29.740: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/22/23 13:43:29.758
  E0722 13:43:30.690458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:30.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6269" for this suite. @ 07/22/23 13:43:30.777
• [6.190 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/22/23 13:43:30.792
  Jul 22 13:43:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename aggregator @ 07/22/23 13:43:30.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:43:30.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:43:30.831
  Jul 22 13:43:30.835: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Registering the sample API server. @ 07/22/23 13:43:30.837
  E0722 13:43:31.691329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:31.899: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul 22 13:43:31.941: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0722 13:43:32.692547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:33.692632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:34.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:34.693692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:35.694386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:36.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:36.694511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:37.695042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:38.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:38.696008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:39.696840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:40.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:40.697720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:41.697962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:42.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:42.698556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:43.698637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:44.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:44.698815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:45.699624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:46.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:46.700440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:47.700584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:48.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:48.701064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:49.701113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:50.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:50.702123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:51.702227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:52.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:52.702676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:53.702869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:54.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0722 13:43:54.703558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:55.703615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:43:56.145: INFO: Waited 123.236541ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/22/23 13:43:56.244
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/22/23 13:43:56.249
  STEP: List APIServices @ 07/22/23 13:43:56.261
  Jul 22 13:43:56.274: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/22/23 13:43:56.274
  Jul 22 13:43:56.313: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/22/23 13:43:56.313
  Jul 22 13:43:56.410: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 22, 13, 43, 56, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/22/23 13:43:56.41
  Jul 22 13:43:56.417: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-22 13:43:56 +0000 UTC Passed all checks passed}
  Jul 22 13:43:56.417: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 13:43:56.417: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/22/23 13:43:56.417
  Jul 22 13:43:56.435: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-104556464" @ 07/22/23 13:43:56.436
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/22/23 13:43:56.45
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/22/23 13:43:56.459
  STEP: Patch APIService Status @ 07/22/23 13:43:56.466
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/22/23 13:43:56.477
  Jul 22 13:43:56.483: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-22 13:43:56 +0000 UTC Passed all checks passed}
  Jul 22 13:43:56.483: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 22 13:43:56.483: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul 22 13:43:56.483: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/22/23 13:43:56.484
  STEP: Confirm that the generated APIService has been deleted @ 07/22/23 13:43:56.492
  Jul 22 13:43:56.493: INFO: Requesting list of APIServices to confirm quantity
  Jul 22 13:43:56.501: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul 22 13:43:56.501: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul 22 13:43:56.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5300" for this suite. @ 07/22/23 13:43:56.694
• [25.912 seconds]
------------------------------
  E0722 13:43:56.704560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/22/23 13:43:56.708
  Jul 22 13:43:56.708: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename sysctl @ 07/22/23 13:43:56.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:43:56.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:43:56.737
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/22/23 13:43:56.744
  STEP: Watching for error events or started pod @ 07/22/23 13:43:56.755
  E0722 13:43:57.704641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:43:58.704808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/22/23 13:43:58.763
  E0722 13:43:59.705209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:00.705362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 07/22/23 13:44:00.784
  STEP: Getting logs from the pod @ 07/22/23 13:44:00.784
  STEP: Checking that the sysctl is actually updated @ 07/22/23 13:44:00.806
  Jul 22 13:44:00.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5550" for this suite. @ 07/22/23 13:44:00.814
• [4.117 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/22/23 13:44:00.825
  Jul 22 13:44:00.825: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:44:00.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:00.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:00.863
  STEP: Creating secret with name secret-test-ede09acd-4ef6-4238-922c-ecfe516178e6 @ 07/22/23 13:44:00.872
  STEP: Creating a pod to test consume secrets @ 07/22/23 13:44:00.879
  E0722 13:44:01.705446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:02.705595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:03.705738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:04.706233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:44:04.915
  Jul 22 13:44:04.921: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-secrets-f4982bc1-925b-412a-a178-1aabe0b457e9 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/22/23 13:44:04.934
  Jul 22 13:44:04.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9032" for this suite. @ 07/22/23 13:44:04.964
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/22/23 13:44:04.983
  Jul 22 13:44:04.983: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename svcaccounts @ 07/22/23 13:44:04.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:05.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:05.036
  Jul 22 13:44:05.063: INFO: created pod
  E0722 13:44:05.707026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:06.707193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:07.707334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:08.707447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:09.708019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:10.708132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:44:11.097
  E0722 13:44:11.708261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:12.708304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:13.708491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:14.708598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:15.708640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:16.708830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:17.709180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:18.709510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:19.709637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:20.710264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:21.710578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:22.710808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:23.711250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:24.711481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:25.712165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:26.712284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:27.712763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:28.713083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:29.713284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:30.713945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:31.714063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:32.714454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:33.714924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:34.715080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:35.715610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:36.715720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:37.715944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:38.716065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:39.716183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:40.716945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:41.097: INFO: polling logs
  Jul 22 13:44:41.111: INFO: Pod logs: 
  I0722 13:44:07.409307       1 log.go:198] OK: Got token
  I0722 13:44:07.409367       1 log.go:198] validating with in-cluster discovery
  I0722 13:44:07.409792       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0722 13:44:07.409845       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4976:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690034045, NotBefore:1690033445, IssuedAt:1690033445, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4976", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cfa28ce2-6736-4e4f-9a12-b4f267b68778"}}}
  I0722 13:44:07.422410       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0722 13:44:07.430447       1 log.go:198] OK: Validated signature on JWT
  I0722 13:44:07.432721       1 log.go:198] OK: Got valid claims from token!
  I0722 13:44:07.432894       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4976:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690034045, NotBefore:1690033445, IssuedAt:1690033445, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4976", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cfa28ce2-6736-4e4f-9a12-b4f267b68778"}}}

  Jul 22 13:44:41.111: INFO: completed pod
  Jul 22 13:44:41.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4976" for this suite. @ 07/22/23 13:44:41.128
• [36.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/22/23 13:44:41.145
  Jul 22 13:44:41.145: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/22/23 13:44:41.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:41.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:41.193
  Jul 22 13:44:41.201: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:44:41.717772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:42.717814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/22/23 13:44:43.086
  Jul 22 13:44:43.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 create -f -'
  E0722 13:44:43.717915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:44.217: INFO: stderr: ""
  Jul 22 13:44:44.217: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 22 13:44:44.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 delete e2e-test-crd-publish-openapi-604-crds test-foo'
  Jul 22 13:44:44.346: INFO: stderr: ""
  Jul 22 13:44:44.346: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul 22 13:44:44.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 apply -f -'
  E0722 13:44:44.718989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:44.742: INFO: stderr: ""
  Jul 22 13:44:44.743: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 22 13:44:44.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 delete e2e-test-crd-publish-openapi-604-crds test-foo'
  Jul 22 13:44:44.841: INFO: stderr: ""
  Jul 22 13:44:44.841: INFO: stdout: "e2e-test-crd-publish-openapi-604-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/22/23 13:44:44.841
  Jul 22 13:44:44.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 create -f -'
  Jul 22 13:44:45.191: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/22/23 13:44:45.191
  Jul 22 13:44:45.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 create -f -'
  E0722 13:44:45.719122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:45.963: INFO: rc: 1
  Jul 22 13:44:45.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 apply -f -'
  Jul 22 13:44:46.220: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/22/23 13:44:46.22
  Jul 22 13:44:46.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 create -f -'
  Jul 22 13:44:46.471: INFO: rc: 1
  Jul 22 13:44:46.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 --namespace=crd-publish-openapi-1180 apply -f -'
  E0722 13:44:46.719258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:46.742: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/22/23 13:44:46.742
  Jul 22 13:44:46.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 explain e2e-test-crd-publish-openapi-604-crds'
  Jul 22 13:44:47.035: INFO: stderr: ""
  Jul 22 13:44:47.035: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-604-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/22/23 13:44:47.036
  Jul 22 13:44:47.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 explain e2e-test-crd-publish-openapi-604-crds.metadata'
  Jul 22 13:44:47.348: INFO: stderr: ""
  Jul 22 13:44:47.348: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-604-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul 22 13:44:47.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 explain e2e-test-crd-publish-openapi-604-crds.spec'
  Jul 22 13:44:47.598: INFO: stderr: ""
  Jul 22 13:44:47.598: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-604-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul 22 13:44:47.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 explain e2e-test-crd-publish-openapi-604-crds.spec.bars'
  E0722 13:44:47.720412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:47.918: INFO: stderr: ""
  Jul 22 13:44:47.918: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-604-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/22/23 13:44:47.918
  Jul 22 13:44:47.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=crd-publish-openapi-1180 explain e2e-test-crd-publish-openapi-604-crds.spec.bars2'
  Jul 22 13:44:48.215: INFO: rc: 1
  E0722 13:44:48.720431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:49.721499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:49.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1180" for this suite. @ 07/22/23 13:44:49.9
• [8.764 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/22/23 13:44:49.91
  Jul 22 13:44:49.910: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename security-context @ 07/22/23 13:44:49.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:49.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:49.944
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/22/23 13:44:49.949
  E0722 13:44:50.722295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:51.722466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:52.722569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:53.722717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:44:53.983
  Jul 22 13:44:53.987: INFO: Trying to get logs from node ip-172-31-15-55 pod security-context-2b1230bc-db42-4f6b-8a08-30a6b648b0f0 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:44:53.998
  Jul 22 13:44:54.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8566" for this suite. @ 07/22/23 13:44:54.025
• [4.125 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/22/23 13:44:54.036
  Jul 22 13:44:54.036: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename kubectl @ 07/22/23 13:44:54.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:54.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:54.071
  STEP: create deployment with httpd image @ 07/22/23 13:44:54.076
  Jul 22 13:44:54.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-1047 create -f -'
  E0722 13:44:54.723197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:54.845: INFO: stderr: ""
  Jul 22 13:44:54.845: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/22/23 13:44:54.845
  Jul 22 13:44:54.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-1047 diff -f -'
  Jul 22 13:44:55.267: INFO: rc: 1
  Jul 22 13:44:55.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-53067953 --namespace=kubectl-1047 delete -f -'
  Jul 22 13:44:55.391: INFO: stderr: ""
  Jul 22 13:44:55.391: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul 22 13:44:55.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1047" for this suite. @ 07/22/23 13:44:55.399
• [1.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/22/23 13:44:55.413
  Jul 22 13:44:55.414: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename pods @ 07/22/23 13:44:55.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:55.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:55.448
  Jul 22 13:44:55.452: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: creating the pod @ 07/22/23 13:44:55.452
  STEP: submitting the pod to kubernetes @ 07/22/23 13:44:55.453
  E0722 13:44:55.723628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:56.723801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:44:57.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3633" for this suite. @ 07/22/23 13:44:57.592
• [2.189 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/22/23 13:44:57.603
  Jul 22 13:44:57.603: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename webhook @ 07/22/23 13:44:57.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:44:57.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:44:57.639
  STEP: Setting up server cert @ 07/22/23 13:44:57.683
  E0722 13:44:57.723984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/22/23 13:44:58.417
  STEP: Deploying the webhook pod @ 07/22/23 13:44:58.428
  STEP: Wait for the deployment to be ready @ 07/22/23 13:44:58.446
  Jul 22 13:44:58.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0722 13:44:58.724149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:44:59.724225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/22/23 13:45:00.476
  STEP: Verifying the service has paired with the endpoint @ 07/22/23 13:45:00.49
  E0722 13:45:00.724772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:45:01.491: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 22 13:45:01.498: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  E0722 13:45:01.725021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/22/23 13:45:02.022
  STEP: Creating a custom resource that should be denied by the webhook @ 07/22/23 13:45:02.056
  E0722 13:45:02.725287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:03.725424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/22/23 13:45:04.103
  STEP: Updating the custom resource with disallowed data should be denied @ 07/22/23 13:45:04.113
  STEP: Deleting the custom resource should be denied @ 07/22/23 13:45:04.131
  STEP: Remove the offending key and value from the custom resource data @ 07/22/23 13:45:04.144
  STEP: Deleting the updated custom resource should be successful @ 07/22/23 13:45:04.16
  Jul 22 13:45:04.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0722 13:45:04.725606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7297" for this suite. @ 07/22/23 13:45:04.82
  STEP: Destroying namespace "webhook-markers-7560" for this suite. @ 07/22/23 13:45:04.831
• [7.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/22/23 13:45:04.844
  Jul 22 13:45:04.844: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:45:04.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:45:04.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:45:04.882
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/22/23 13:45:04.889
  E0722 13:45:05.725660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:06.725783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:07.725897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:08.726040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:45:08.927
  Jul 22 13:45:08.932: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-91afcb7c-67e3-4b56-a595-bd7b7d13eb1d container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:45:08.95
  Jul 22 13:45:08.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1401" for this suite. @ 07/22/23 13:45:08.991
• [4.162 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/22/23 13:45:09.008
  Jul 22 13:45:09.008: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename statefulset @ 07/22/23 13:45:09.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:45:09.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:45:09.041
  STEP: Creating service test in namespace statefulset-1220 @ 07/22/23 13:45:09.046
  STEP: Creating a new StatefulSet @ 07/22/23 13:45:09.057
  Jul 22 13:45:09.076: INFO: Found 0 stateful pods, waiting for 3
  E0722 13:45:09.726311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:10.726721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:11.727255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:12.727333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:13.727348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:14.731166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:15.731856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:16.732254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:17.732422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:18.732496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:45:19.084: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:45:19.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:45:19.084: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/22/23 13:45:19.099
  Jul 22 13:45:19.125: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/22/23 13:45:19.126
  E0722 13:45:19.732593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:20.733602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:21.733785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:22.733844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:23.734019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:24.734388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:25.734478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:26.734878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:27.735220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:28.735344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/22/23 13:45:29.152
  STEP: Performing a canary update @ 07/22/23 13:45:29.152
  Jul 22 13:45:29.186: INFO: Updating stateful set ss2
  Jul 22 13:45:29.205: INFO: Waiting for Pod statefulset-1220/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0722 13:45:29.736595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:30.737359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:31.737446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:32.737978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:33.738333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:34.738466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:35.739202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:36.739479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:37.739605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:38.740662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/22/23 13:45:39.218
  Jul 22 13:45:39.279: INFO: Found 1 stateful pods, waiting for 3
  E0722 13:45:39.741168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:40.741733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:41.742168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:42.742483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:43.742561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:44.742666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:45.743409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:46.743537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:47.743663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:48.743781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:45:49.288: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:45:49.288: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 22 13:45:49.288: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/22/23 13:45:49.331
  Jul 22 13:45:49.367: INFO: Updating stateful set ss2
  Jul 22 13:45:49.383: INFO: Waiting for Pod statefulset-1220/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0722 13:45:49.744805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:50.744769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:51.745131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:52.745202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:53.745294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:54.745487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:55.745656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:56.746690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:57.746772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:45:58.746893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:45:59.426: INFO: Updating stateful set ss2
  Jul 22 13:45:59.437: INFO: Waiting for StatefulSet statefulset-1220/ss2 to complete update
  Jul 22 13:45:59.438: INFO: Waiting for Pod statefulset-1220/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0722 13:45:59.747010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:00.747708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:01.748154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:02.748490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:03.748572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:04.749211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:05.752351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:06.752294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:07.752436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:08.753054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:09.451: INFO: Deleting all statefulset in ns statefulset-1220
  Jul 22 13:46:09.458: INFO: Scaling statefulset ss2 to 0
  E0722 13:46:09.753806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:10.754160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:11.754401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:12.754431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:13.754523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:14.755006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:15.755715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:16.756161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:17.756554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:18.756671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:19.493: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 22 13:46:19.503: INFO: Deleting statefulset ss2
  Jul 22 13:46:19.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1220" for this suite. @ 07/22/23 13:46:19.537
• [70.542 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/22/23 13:46:19.554
  Jul 22 13:46:19.554: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename downward-api @ 07/22/23 13:46:19.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:46:19.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:46:19.615
  STEP: Creating the pod @ 07/22/23 13:46:19.62
  E0722 13:46:19.757326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:20.758202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:21.758829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:22.198: INFO: Successfully updated pod "labelsupdatec53ec57a-15b3-485c-8fc2-69a92d84fae0"
  E0722 13:46:22.759693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:23.759871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:24.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3309" for this suite. @ 07/22/23 13:46:24.239
• [4.698 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/22/23 13:46:24.254
  Jul 22 13:46:24.254: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename var-expansion @ 07/22/23 13:46:24.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:46:24.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:46:24.299
  STEP: Creating a pod to test substitution in container's command @ 07/22/23 13:46:24.312
  E0722 13:46:24.760120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:25.760978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:26.762062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:27.762462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:46:28.364
  Jul 22 13:46:28.371: INFO: Trying to get logs from node ip-172-31-15-55 pod var-expansion-b55afaf0-c582-4b85-9652-1abd58db66e5 container dapi-container: <nil>
  STEP: delete the pod @ 07/22/23 13:46:28.382
  Jul 22 13:46:28.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5512" for this suite. @ 07/22/23 13:46:28.408
• [4.162 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/22/23 13:46:28.417
  Jul 22 13:46:28.417: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename proxy @ 07/22/23 13:46:28.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:46:28.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:46:28.449
  STEP: starting an echo server on multiple ports @ 07/22/23 13:46:28.47
  STEP: creating replication controller proxy-service-77ngq in namespace proxy-3213 @ 07/22/23 13:46:28.471
  I0722 13:46:28.501265      19 runners.go:194] Created replication controller with name: proxy-service-77ngq, namespace: proxy-3213, replica count: 1
  E0722 13:46:28.763047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:46:29.551963      19 runners.go:194] proxy-service-77ngq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0722 13:46:29.763297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0722 13:46:30.552805      19 runners.go:194] proxy-service-77ngq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 22 13:46:30.561: INFO: setup took 2.108270433s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/22/23 13:46:30.561
  Jul 22 13:46:30.573: INFO: (0) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 12.28875ms)
  Jul 22 13:46:30.576: INFO: (0) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 15.330148ms)
  Jul 22 13:46:30.577: INFO: (0) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 16.33606ms)
  Jul 22 13:46:30.584: INFO: (0) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 23.139314ms)
  Jul 22 13:46:30.585: INFO: (0) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 23.265795ms)
  Jul 22 13:46:30.585: INFO: (0) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 23.70816ms)
  Jul 22 13:46:30.585: INFO: (0) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 23.215014ms)
  Jul 22 13:46:30.585: INFO: (0) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 23.989374ms)
  Jul 22 13:46:30.585: INFO: (0) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 24.067384ms)
  Jul 22 13:46:30.588: INFO: (0) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 26.91362ms)
  Jul 22 13:46:30.590: INFO: (0) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 28.203955ms)
  Jul 22 13:46:30.590: INFO: (0) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 28.714941ms)
  Jul 22 13:46:30.591: INFO: (0) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 29.343559ms)
  Jul 22 13:46:30.591: INFO: (0) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 29.485881ms)
  Jul 22 13:46:30.592: INFO: (0) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 29.981856ms)
  Jul 22 13:46:30.592: INFO: (0) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 30.843557ms)
  Jul 22 13:46:30.600: INFO: (1) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 7.461161ms)
  Jul 22 13:46:30.601: INFO: (1) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 8.901108ms)
  Jul 22 13:46:30.602: INFO: (1) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 9.348994ms)
  Jul 22 13:46:30.603: INFO: (1) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 10.740871ms)
  Jul 22 13:46:30.604: INFO: (1) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 10.791712ms)
  Jul 22 13:46:30.605: INFO: (1) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 12.233779ms)
  Jul 22 13:46:30.605: INFO: (1) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 11.914966ms)
  Jul 22 13:46:30.607: INFO: (1) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 14.73543ms)
  Jul 22 13:46:30.608: INFO: (1) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 14.510468ms)
  Jul 22 13:46:30.610: INFO: (1) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 17.216281ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 17.494874ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 18.433495ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 17.417442ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 17.902969ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 19.007342ms)
  Jul 22 13:46:30.611: INFO: (1) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 19.034492ms)
  Jul 22 13:46:30.619: INFO: (2) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 7.102567ms)
  Jul 22 13:46:30.621: INFO: (2) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 8.294861ms)
  Jul 22 13:46:30.622: INFO: (2) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 9.449585ms)
  Jul 22 13:46:30.622: INFO: (2) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 9.443745ms)
  Jul 22 13:46:30.625: INFO: (2) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 12.368742ms)
  Jul 22 13:46:30.625: INFO: (2) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 12.326461ms)
  Jul 22 13:46:30.625: INFO: (2) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 12.745896ms)
  Jul 22 13:46:30.627: INFO: (2) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 14.230744ms)
  Jul 22 13:46:30.627: INFO: (2) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.313435ms)
  Jul 22 13:46:30.628: INFO: (2) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 15.335498ms)
  Jul 22 13:46:30.628: INFO: (2) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 15.764133ms)
  Jul 22 13:46:30.628: INFO: (2) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 15.076025ms)
  Jul 22 13:46:30.630: INFO: (2) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 16.259889ms)
  Jul 22 13:46:30.630: INFO: (2) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 17.493904ms)
  Jul 22 13:46:30.630: INFO: (2) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 17.093819ms)
  Jul 22 13:46:30.630: INFO: (2) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 18.273914ms)
  Jul 22 13:46:30.638: INFO: (3) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 6.988315ms)
  Jul 22 13:46:30.641: INFO: (3) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 10.122253ms)
  Jul 22 13:46:30.641: INFO: (3) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 9.802099ms)
  Jul 22 13:46:30.642: INFO: (3) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 10.931484ms)
  Jul 22 13:46:30.643: INFO: (3) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 12.404462ms)
  Jul 22 13:46:30.643: INFO: (3) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 11.756144ms)
  Jul 22 13:46:30.644: INFO: (3) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 12.382902ms)
  Jul 22 13:46:30.644: INFO: (3) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 12.210169ms)
  Jul 22 13:46:30.645: INFO: (3) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 13.533776ms)
  Jul 22 13:46:30.645: INFO: (3) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.594716ms)
  Jul 22 13:46:30.647: INFO: (3) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 15.326957ms)
  Jul 22 13:46:30.647: INFO: (3) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 15.420688ms)
  Jul 22 13:46:30.647: INFO: (3) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 15.946755ms)
  Jul 22 13:46:30.647: INFO: (3) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 16.521692ms)
  Jul 22 13:46:30.648: INFO: (3) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 15.874934ms)
  Jul 22 13:46:30.648: INFO: (3) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 16.783445ms)
  Jul 22 13:46:30.655: INFO: (4) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 6.653192ms)
  Jul 22 13:46:30.656: INFO: (4) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 7.152157ms)
  Jul 22 13:46:30.659: INFO: (4) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 10.365117ms)
  Jul 22 13:46:30.663: INFO: (4) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.257792ms)
  Jul 22 13:46:30.666: INFO: (4) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 16.034766ms)
  Jul 22 13:46:30.667: INFO: (4) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 16.732294ms)
  Jul 22 13:46:30.667: INFO: (4) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 17.635355ms)
  Jul 22 13:46:30.667: INFO: (4) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 17.403353ms)
  Jul 22 13:46:30.667: INFO: (4) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 17.644025ms)
  Jul 22 13:46:30.667: INFO: (4) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 17.550445ms)
  Jul 22 13:46:30.668: INFO: (4) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 18.559117ms)
  Jul 22 13:46:30.668: INFO: (4) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 18.720559ms)
  Jul 22 13:46:30.669: INFO: (4) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 19.383837ms)
  Jul 22 13:46:30.670: INFO: (4) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 20.526831ms)
  Jul 22 13:46:30.675: INFO: (4) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 25.090467ms)
  Jul 22 13:46:30.679: INFO: (4) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 29.779184ms)
  Jul 22 13:46:30.690: INFO: (5) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 11.367429ms)
  Jul 22 13:46:30.692: INFO: (5) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 12.365471ms)
  Jul 22 13:46:30.693: INFO: (5) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 13.547966ms)
  Jul 22 13:46:30.693: INFO: (5) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.04722ms)
  Jul 22 13:46:30.694: INFO: (5) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 14.65884ms)
  Jul 22 13:46:30.694: INFO: (5) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 14.944093ms)
  Jul 22 13:46:30.695: INFO: (5) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 15.924865ms)
  Jul 22 13:46:30.695: INFO: (5) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 15.640302ms)
  Jul 22 13:46:30.695: INFO: (5) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 15.921175ms)
  Jul 22 13:46:30.699: INFO: (5) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 20.023375ms)
  Jul 22 13:46:30.700: INFO: (5) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 20.049505ms)
  Jul 22 13:46:30.700: INFO: (5) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 20.431541ms)
  Jul 22 13:46:30.700: INFO: (5) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 20.37675ms)
  Jul 22 13:46:30.700: INFO: (5) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 20.037945ms)
  Jul 22 13:46:30.704: INFO: (5) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 24.385828ms)
  Jul 22 13:46:30.704: INFO: (5) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 24.753583ms)
  Jul 22 13:46:30.718: INFO: (6) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 12.786086ms)
  Jul 22 13:46:30.718: INFO: (6) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 13.316283ms)
  Jul 22 13:46:30.721: INFO: (6) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 15.968365ms)
  Jul 22 13:46:30.721: INFO: (6) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 15.747973ms)
  Jul 22 13:46:30.723: INFO: (6) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 17.93539ms)
  Jul 22 13:46:30.724: INFO: (6) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 18.492366ms)
  Jul 22 13:46:30.724: INFO: (6) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 17.96223ms)
  Jul 22 13:46:30.724: INFO: (6) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 17.800637ms)
  Jul 22 13:46:30.724: INFO: (6) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 18.666198ms)
  Jul 22 13:46:30.724: INFO: (6) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 19.208855ms)
  Jul 22 13:46:30.725: INFO: (6) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 19.819783ms)
  Jul 22 13:46:30.725: INFO: (6) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 20.751973ms)
  Jul 22 13:46:30.728: INFO: (6) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 22.175361ms)
  Jul 22 13:46:30.728: INFO: (6) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 22.240342ms)
  Jul 22 13:46:30.729: INFO: (6) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 23.309975ms)
  Jul 22 13:46:30.741: INFO: (6) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 35.690737ms)
  E0722 13:46:30.764020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:30.764: INFO: (7) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 21.584104ms)
  Jul 22 13:46:30.767: INFO: (7) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 24.738723ms)
  Jul 22 13:46:30.767: INFO: (7) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 25.332899ms)
  Jul 22 13:46:30.768: INFO: (7) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 25.232138ms)
  Jul 22 13:46:30.768: INFO: (7) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 25.840866ms)
  Jul 22 13:46:30.769: INFO: (7) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 26.578165ms)
  Jul 22 13:46:30.769: INFO: (7) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 26.99267ms)
  Jul 22 13:46:30.772: INFO: (7) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 30.384541ms)
  Jul 22 13:46:30.773: INFO: (7) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 30.460003ms)
  Jul 22 13:46:30.773: INFO: (7) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 31.254462ms)
  Jul 22 13:46:30.773: INFO: (7) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 31.285622ms)
  Jul 22 13:46:30.774: INFO: (7) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 31.556865ms)
  Jul 22 13:46:30.775: INFO: (7) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 33.592541ms)
  Jul 22 13:46:30.775: INFO: (7) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 33.52912ms)
  Jul 22 13:46:30.775: INFO: (7) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 32.950703ms)
  Jul 22 13:46:30.776: INFO: (7) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 33.408939ms)
  Jul 22 13:46:30.785: INFO: (8) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 8.498174ms)
  Jul 22 13:46:30.788: INFO: (8) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 11.581271ms)
  Jul 22 13:46:30.791: INFO: (8) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 14.299945ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 17.802918ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 17.750077ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 17.96692ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 18.970802ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 17.99591ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 18.490066ms)
  Jul 22 13:46:30.795: INFO: (8) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 18.587847ms)
  Jul 22 13:46:30.796: INFO: (8) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 18.350744ms)
  Jul 22 13:46:30.797: INFO: (8) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 20.033745ms)
  Jul 22 13:46:30.797: INFO: (8) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 20.382409ms)
  Jul 22 13:46:30.800: INFO: (8) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 22.903011ms)
  Jul 22 13:46:30.800: INFO: (8) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 22.218482ms)
  Jul 22 13:46:30.800: INFO: (8) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 22.148611ms)
  Jul 22 13:46:30.811: INFO: (9) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 9.544737ms)
  Jul 22 13:46:30.814: INFO: (9) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 12.682015ms)
  Jul 22 13:46:30.815: INFO: (9) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 12.969658ms)
  Jul 22 13:46:30.815: INFO: (9) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 13.406814ms)
  Jul 22 13:46:30.816: INFO: (9) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 14.224434ms)
  Jul 22 13:46:30.816: INFO: (9) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.045921ms)
  Jul 22 13:46:30.817: INFO: (9) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 15.622061ms)
  Jul 22 13:46:30.818: INFO: (9) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 16.157438ms)
  Jul 22 13:46:30.819: INFO: (9) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 17.520224ms)
  Jul 22 13:46:30.820: INFO: (9) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 18.00357ms)
  Jul 22 13:46:30.821: INFO: (9) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 19.582139ms)
  Jul 22 13:46:30.821: INFO: (9) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 19.303636ms)
  Jul 22 13:46:30.822: INFO: (9) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 19.940934ms)
  Jul 22 13:46:30.822: INFO: (9) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 20.535701ms)
  Jul 22 13:46:30.822: INFO: (9) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 20.804115ms)
  Jul 22 13:46:30.822: INFO: (9) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 20.765844ms)
  Jul 22 13:46:30.829: INFO: (10) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 6.56371ms)
  Jul 22 13:46:30.832: INFO: (10) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 9.355264ms)
  Jul 22 13:46:30.832: INFO: (10) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 8.474694ms)
  Jul 22 13:46:30.835: INFO: (10) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 11.932116ms)
  Jul 22 13:46:30.835: INFO: (10) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 11.728183ms)
  Jul 22 13:46:30.837: INFO: (10) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 12.480993ms)
  Jul 22 13:46:30.837: INFO: (10) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.070032ms)
  Jul 22 13:46:30.839: INFO: (10) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 15.55684ms)
  Jul 22 13:46:30.839: INFO: (10) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 15.163956ms)
  Jul 22 13:46:30.840: INFO: (10) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 15.171916ms)
  Jul 22 13:46:30.838: INFO: (10) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 13.656887ms)
  Jul 22 13:46:30.841: INFO: (10) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 18.253474ms)
  Jul 22 13:46:30.843: INFO: (10) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 19.353267ms)
  Jul 22 13:46:30.843: INFO: (10) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 18.311553ms)
  Jul 22 13:46:30.844: INFO: (10) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 19.881932ms)
  Jul 22 13:46:30.844: INFO: (10) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 19.935253ms)
  Jul 22 13:46:30.852: INFO: (11) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 7.399721ms)
  Jul 22 13:46:30.856: INFO: (11) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 10.848553ms)
  Jul 22 13:46:30.856: INFO: (11) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 10.60644ms)
  Jul 22 13:46:30.860: INFO: (11) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 15.001013ms)
  Jul 22 13:46:30.860: INFO: (11) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 14.841832ms)
  Jul 22 13:46:30.860: INFO: (11) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 15.238346ms)
  Jul 22 13:46:30.863: INFO: (11) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 17.588215ms)
  Jul 22 13:46:30.873: INFO: (11) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 27.251503ms)
  Jul 22 13:46:30.873: INFO: (11) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 27.79351ms)
  Jul 22 13:46:30.873: INFO: (11) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 28.204625ms)
  Jul 22 13:46:30.877: INFO: (11) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 31.06183ms)
  Jul 22 13:46:30.884: INFO: (11) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 39.363001ms)
  Jul 22 13:46:30.885: INFO: (11) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 38.925546ms)
  Jul 22 13:46:30.885: INFO: (11) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 39.043888ms)
  Jul 22 13:46:30.885: INFO: (11) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 39.25904ms)
  Jul 22 13:46:30.885: INFO: (11) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 39.324041ms)
  Jul 22 13:46:30.907: INFO: (12) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 21.1821ms)
  Jul 22 13:46:30.912: INFO: (12) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 25.676254ms)
  Jul 22 13:46:30.915: INFO: (12) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 30.28922ms)
  Jul 22 13:46:30.915: INFO: (12) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 29.334258ms)
  Jul 22 13:46:30.922: INFO: (12) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 35.267152ms)
  Jul 22 13:46:30.922: INFO: (12) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 35.911889ms)
  Jul 22 13:46:30.923: INFO: (12) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 38.000885ms)
  Jul 22 13:46:30.923: INFO: (12) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 37.946995ms)
  Jul 22 13:46:30.923: INFO: (12) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 38.182577ms)
  Jul 22 13:46:30.923: INFO: (12) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 37.891363ms)
  Jul 22 13:46:30.924: INFO: (12) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 37.726691ms)
  Jul 22 13:46:30.930: INFO: (12) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 44.357742ms)
  Jul 22 13:46:30.930: INFO: (12) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 44.042718ms)
  Jul 22 13:46:30.930: INFO: (12) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 43.907778ms)
  Jul 22 13:46:30.930: INFO: (12) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 43.789166ms)
  Jul 22 13:46:30.930: INFO: (12) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 44.19937ms)
  Jul 22 13:46:30.950: INFO: (13) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 19.523839ms)
  Jul 22 13:46:30.953: INFO: (13) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 22.432995ms)
  Jul 22 13:46:30.971: INFO: (13) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 40.225122ms)
  Jul 22 13:46:30.971: INFO: (13) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 40.653167ms)
  Jul 22 13:46:30.971: INFO: (13) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 40.615157ms)
  Jul 22 13:46:30.971: INFO: (13) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 39.953319ms)
  Jul 22 13:46:30.971: INFO: (13) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 40.366154ms)
  Jul 22 13:46:30.977: INFO: (13) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 46.214505ms)
  Jul 22 13:46:30.978: INFO: (13) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 46.772752ms)
  Jul 22 13:46:30.978: INFO: (13) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 46.334466ms)
  Jul 22 13:46:30.979: INFO: (13) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 48.278291ms)
  Jul 22 13:46:30.979: INFO: (13) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 47.994138ms)
  Jul 22 13:46:30.981: INFO: (13) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 50.244874ms)
  Jul 22 13:46:30.983: INFO: (13) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 52.496342ms)
  Jul 22 13:46:30.986: INFO: (13) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 54.947172ms)
  Jul 22 13:46:30.988: INFO: (13) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 56.630963ms)
  Jul 22 13:46:31.008: INFO: (14) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 19.796662ms)
  Jul 22 13:46:31.026: INFO: (14) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 37.110094ms)
  Jul 22 13:46:31.026: INFO: (14) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 37.802223ms)
  Jul 22 13:46:31.026: INFO: (14) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 37.270396ms)
  Jul 22 13:46:31.031: INFO: (14) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 42.267567ms)
  Jul 22 13:46:31.031: INFO: (14) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 42.331458ms)
  Jul 22 13:46:31.032: INFO: (14) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 43.32031ms)
  Jul 22 13:46:31.032: INFO: (14) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 43.967758ms)
  Jul 22 13:46:31.037: INFO: (14) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 48.057718ms)
  Jul 22 13:46:31.040: INFO: (14) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 51.695212ms)
  Jul 22 13:46:31.040: INFO: (14) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 51.864874ms)
  Jul 22 13:46:31.041: INFO: (14) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 52.30026ms)
  Jul 22 13:46:31.042: INFO: (14) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 54.186533ms)
  Jul 22 13:46:31.050: INFO: (14) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 60.965966ms)
  Jul 22 13:46:31.051: INFO: (14) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 61.704985ms)
  Jul 22 13:46:31.051: INFO: (14) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 62.946609ms)
  Jul 22 13:46:31.085: INFO: (15) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 33.078695ms)
  Jul 22 13:46:31.085: INFO: (15) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 33.620802ms)
  Jul 22 13:46:31.087: INFO: (15) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 35.690356ms)
  Jul 22 13:46:31.088: INFO: (15) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 36.157713ms)
  Jul 22 13:46:31.094: INFO: (15) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 42.574771ms)
  Jul 22 13:46:31.101: INFO: (15) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 49.251202ms)
  Jul 22 13:46:31.102: INFO: (15) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 49.738119ms)
  Jul 22 13:46:31.102: INFO: (15) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 49.947021ms)
  Jul 22 13:46:31.102: INFO: (15) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 49.8781ms)
  Jul 22 13:46:31.102: INFO: (15) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 50.334636ms)
  Jul 22 13:46:31.102: INFO: (15) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 50.968893ms)
  Jul 22 13:46:31.104: INFO: (15) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 51.973306ms)
  Jul 22 13:46:31.104: INFO: (15) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 52.418121ms)
  Jul 22 13:46:31.105: INFO: (15) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 53.450134ms)
  Jul 22 13:46:31.106: INFO: (15) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 54.331374ms)
  Jul 22 13:46:31.106: INFO: (15) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 54.650138ms)
  Jul 22 13:46:31.113: INFO: (16) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 6.732802ms)
  Jul 22 13:46:31.116: INFO: (16) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 9.441656ms)
  Jul 22 13:46:31.118: INFO: (16) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 11.316348ms)
  Jul 22 13:46:31.118: INFO: (16) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 11.705083ms)
  Jul 22 13:46:31.120: INFO: (16) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 12.737815ms)
  Jul 22 13:46:31.120: INFO: (16) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 13.647587ms)
  Jul 22 13:46:31.121: INFO: (16) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.419464ms)
  Jul 22 13:46:31.121: INFO: (16) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 14.012881ms)
  Jul 22 13:46:31.123: INFO: (16) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 15.406418ms)
  Jul 22 13:46:31.123: INFO: (16) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 15.242606ms)
  Jul 22 13:46:31.124: INFO: (16) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 17.028258ms)
  Jul 22 13:46:31.124: INFO: (16) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 16.626853ms)
  Jul 22 13:46:31.125: INFO: (16) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 16.649464ms)
  Jul 22 13:46:31.126: INFO: (16) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 18.401725ms)
  Jul 22 13:46:31.127: INFO: (16) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 19.58799ms)
  Jul 22 13:46:31.128: INFO: (16) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 20.167367ms)
  Jul 22 13:46:31.136: INFO: (17) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 7.664934ms)
  Jul 22 13:46:31.136: INFO: (17) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 7.873917ms)
  Jul 22 13:46:31.137: INFO: (17) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 9.313464ms)
  Jul 22 13:46:31.138: INFO: (17) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 9.073451ms)
  Jul 22 13:46:31.138: INFO: (17) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 10.038963ms)
  Jul 22 13:46:31.139: INFO: (17) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 10.225225ms)
  Jul 22 13:46:31.140: INFO: (17) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 11.767293ms)
  Jul 22 13:46:31.141: INFO: (17) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 12.597283ms)
  Jul 22 13:46:31.141: INFO: (17) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 12.056937ms)
  Jul 22 13:46:31.142: INFO: (17) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 13.86915ms)
  Jul 22 13:46:31.142: INFO: (17) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 13.532095ms)
  Jul 22 13:46:31.143: INFO: (17) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 14.949603ms)
  Jul 22 13:46:31.143: INFO: (17) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.694239ms)
  Jul 22 13:46:31.143: INFO: (17) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 14.612938ms)
  Jul 22 13:46:31.143: INFO: (17) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 14.868042ms)
  Jul 22 13:46:31.144: INFO: (17) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 15.411039ms)
  Jul 22 13:46:31.157: INFO: (18) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 12.509123ms)
  Jul 22 13:46:31.157: INFO: (18) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 12.081087ms)
  Jul 22 13:46:31.157: INFO: (18) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 12.355721ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 13.96895ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 13.443885ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 13.246622ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.389786ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 14.492438ms)
  Jul 22 13:46:31.159: INFO: (18) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 14.319425ms)
  Jul 22 13:46:31.161: INFO: (18) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 15.378518ms)
  Jul 22 13:46:31.161: INFO: (18) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 15.996706ms)
  Jul 22 13:46:31.161: INFO: (18) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 15.52442ms)
  Jul 22 13:46:31.161: INFO: (18) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 16.531192ms)
  Jul 22 13:46:31.162: INFO: (18) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 16.331209ms)
  Jul 22 13:46:31.163: INFO: (18) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 17.713357ms)
  Jul 22 13:46:31.164: INFO: (18) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 18.920851ms)
  Jul 22 13:46:31.173: INFO: (19) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:462/proxy/: tls qux (200; 8.96043ms)
  Jul 22 13:46:31.176: INFO: (19) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:1080/proxy/rewriteme">... (200; 11.306879ms)
  Jul 22 13:46:31.178: INFO: (19) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.190751ms)
  Jul 22 13:46:31.178: INFO: (19) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:160/proxy/: foo (200; 13.223302ms)
  Jul 22 13:46:31.180: INFO: (19) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:460/proxy/: tls baz (200; 15.043743ms)
  Jul 22 13:46:31.180: INFO: (19) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:162/proxy/: bar (200; 15.184306ms)
  Jul 22 13:46:31.180: INFO: (19) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg:1080/proxy/rewriteme">test<... (200; 15.276716ms)
  Jul 22 13:46:31.180: INFO: (19) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname1/proxy/: foo (200; 15.804964ms)
  Jul 22 13:46:31.181: INFO: (19) /api/v1/namespaces/proxy-3213/pods/http:proxy-service-77ngq-bssrg:162/proxy/: bar (200; 16.074647ms)
  Jul 22 13:46:31.182: INFO: (19) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname2/proxy/: tls qux (200; 17.027858ms)
  Jul 22 13:46:31.182: INFO: (19) /api/v1/namespaces/proxy-3213/services/proxy-service-77ngq:portname2/proxy/: bar (200; 17.142529ms)
  Jul 22 13:46:31.183: INFO: (19) /api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/proxy-service-77ngq-bssrg/proxy/rewriteme">test</a> (200; 17.714897ms)
  Jul 22 13:46:31.183: INFO: (19) /api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/: <a href="/api/v1/namespaces/proxy-3213/pods/https:proxy-service-77ngq-bssrg:443/proxy/tlsrewritem... (200; 17.511845ms)
  Jul 22 13:46:31.183: INFO: (19) /api/v1/namespaces/proxy-3213/services/https:proxy-service-77ngq:tlsportname1/proxy/: tls baz (200; 17.98718ms)
  Jul 22 13:46:31.184: INFO: (19) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname1/proxy/: foo (200; 18.717108ms)
  Jul 22 13:46:31.184: INFO: (19) /api/v1/namespaces/proxy-3213/services/http:proxy-service-77ngq:portname2/proxy/: bar (200; 19.209675ms)
  Jul 22 13:46:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-77ngq in namespace proxy-3213, will wait for the garbage collector to delete the pods @ 07/22/23 13:46:31.189
  Jul 22 13:46:31.255: INFO: Deleting ReplicationController proxy-service-77ngq took: 11.107396ms
  Jul 22 13:46:31.355: INFO: Terminating ReplicationController proxy-service-77ngq pods took: 100.724011ms
  E0722 13:46:31.764945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:32.766292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:33.767095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-3213" for this suite. @ 07/22/23 13:46:34.256
• [5.849 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/22/23 13:46:34.273
  Jul 22 13:46:34.273: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename job @ 07/22/23 13:46:34.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:46:34.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:46:34.315
  STEP: Creating a job @ 07/22/23 13:46:34.322
  STEP: Ensuring job reaches completions @ 07/22/23 13:46:34.331
  E0722 13:46:34.768109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:35.768412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:36.769088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:37.769197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:38.770358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:39.770613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:40.770735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:41.770827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:42.771244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:43.771340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:44.772173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:45.772296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:46.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1072" for this suite. @ 07/22/23 13:46:46.342
• [12.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/22/23 13:46:46.361
  Jul 22 13:46:46.361: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename container-probe @ 07/22/23 13:46:46.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:46:46.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:46:46.406
  STEP: Creating pod test-webserver-1f336ed5-ad58-4e67-ac1e-d464b0888cd2 in namespace container-probe-2342 @ 07/22/23 13:46:46.423
  E0722 13:46:46.773306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:47.773424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:46:48.454: INFO: Started pod test-webserver-1f336ed5-ad58-4e67-ac1e-d464b0888cd2 in namespace container-probe-2342
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/22/23 13:46:48.454
  Jul 22 13:46:48.467: INFO: Initial restart count of pod test-webserver-1f336ed5-ad58-4e67-ac1e-d464b0888cd2 is 0
  E0722 13:46:48.774193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:49.774395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:50.774534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:51.774641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:52.775194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:53.775315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:54.775947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:55.776551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:56.777212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:57.777291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:58.777650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:46:59.777914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:00.778079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:01.778564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:02.778705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:03.778827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:04.778973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:05.779077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:06.779201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:07.779312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:08.779588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:09.779733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:10.780844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:11.781087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:12.781992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:13.782576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:14.782738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:15.782832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:16.782936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:17.783116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:18.783351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:19.783673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:20.784176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:21.784236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:22.784795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:23.784906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:24.785003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:25.785131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:26.785799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:27.785927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:28.786957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:29.787171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:30.787282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:31.787446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:32.787864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:33.787966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:34.788828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:35.788852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:36.789764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:37.790107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:38.791071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:39.792179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:40.792369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:41.792475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:42.793502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:43.793613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:44.794046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:45.794551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:46.794835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:47.795238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:48.795542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:49.796360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:50.796838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:51.797002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:52.797788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:53.798232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:54.798876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:55.798911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:56.800003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:57.800352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:58.800981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:47:59.801075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:00.801453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:01.801554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:02.802595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:03.802727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:04.803744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:05.803856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:06.804563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:07.805080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:08.805395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:09.805482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:10.805827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:11.805983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:12.807168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:13.807407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:14.808329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:15.808440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:16.809059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:17.809115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:18.810198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:19.810789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:20.811299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:21.811388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:22.812346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:23.812649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:24.812788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:25.813142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:26.813265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:27.814211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:28.814541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:29.814671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:30.814785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:31.814942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:32.815054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:33.815201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:34.815344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:35.815483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:36.815619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:37.815745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:38.815872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:39.816010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:40.817024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:41.817080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:42.817314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:43.817536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:44.817810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:45.818529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:46.818676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:47.818903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:48.819231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:49.819354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:50.820025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:51.820334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:52.820540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:53.821643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:54.821776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:55.822245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:56.822362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:57.823268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:58.823535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:48:59.823692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:00.824584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:01.825461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:02.826132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:03.827220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:04.827644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:05.828708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:06.829092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:07.830924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:08.830857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:09.831129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:10.831879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:11.832135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:12.832443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:13.833106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:14.833251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:15.834267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:16.834682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:17.835034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:18.835178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:19.835337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:20.836223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:21.836635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:22.837070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:23.837117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:24.837216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:25.837339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:26.838146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:27.838418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:28.838497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:29.838699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:30.839374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:31.839515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:32.839648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:33.839767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:34.839967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:35.840124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:36.841095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:37.841204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:38.841297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:39.841430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:40.842009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:41.842170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:42.842221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:43.843224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:44.843810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:45.844046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:46.844199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:47.844366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:48.845020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:49.845546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:50.846176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:51.846295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:52.847235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:53.847418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:54.848477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:55.848849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:56.849104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:57.849193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:58.849328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:49:59.849491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:00.850196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:01.850538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:02.850651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:03.850760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:04.851808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:05.852804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:06.852949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:07.853085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:08.853196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:09.853291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:10.854211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:11.854358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:12.854511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:13.854566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:14.854850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:15.855878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:16.855975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:17.856502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:18.856870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:19.857108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:20.857571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:21.858022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:22.858724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:23.859550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:24.859874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:25.860083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:26.860168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:27.860721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:28.860790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:29.861045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:30.861263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:31.861405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:32.861550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:33.862200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:34.862481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:35.863464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:36.863755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:37.863880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:38.864225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:39.869681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:40.870450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:41.871422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:42.871529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:43.872524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:44.872670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:45.872708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:46.873041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:47.874039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:48.875514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:50:49.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/22/23 13:50:49.394
  STEP: Destroying namespace "container-probe-2342" for this suite. @ 07/22/23 13:50:49.42
• [243.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/22/23 13:50:49.445
  Jul 22 13:50:49.445: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:50:49.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:50:49.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:50:49.502
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/22/23 13:50:49.511
  E0722 13:50:49.875437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:50.876353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:51.877177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:52.877384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:50:53.556
  Jul 22 13:50:53.561: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-98b88594-ac1e-4dff-9225-fbce27ab4d21 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:50:53.585
  Jul 22 13:50:53.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4325" for this suite. @ 07/22/23 13:50:53.611
• [4.176 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/22/23 13:50:53.624
  Jul 22 13:50:53.624: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:50:53.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:50:53.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:50:53.661
  STEP: Creating the pod @ 07/22/23 13:50:53.665
  E0722 13:50:53.877585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:54.877917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:55.878708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:50:56.250: INFO: Successfully updated pod "labelsupdate2f87bc24-d5cf-4449-b72d-b50f790ea01b"
  E0722 13:50:56.879241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:57.879462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 22 13:50:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3950" for this suite. @ 07/22/23 13:50:58.293
• [4.688 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/22/23 13:50:58.316
  Jul 22 13:50:58.316: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename projected @ 07/22/23 13:50:58.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:50:58.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:50:58.35
  STEP: Creating configMap with name projected-configmap-test-volume-2cf51842-0fca-4a43-9660-1d65a5fbf45d @ 07/22/23 13:50:58.416
  STEP: Creating a pod to test consume configMaps @ 07/22/23 13:50:58.428
  E0722 13:50:58.879603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:50:59.880033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:51:00.881009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:51:01.881114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:51:02.467
  Jul 22 13:51:02.472: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-projected-configmaps-6a678590-d07a-43b8-8aed-7a89c090fff6 container agnhost-container: <nil>
  STEP: delete the pod @ 07/22/23 13:51:02.487
  Jul 22 13:51:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9987" for this suite. @ 07/22/23 13:51:02.534
• [4.238 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/22/23 13:51:02.555
  Jul 22 13:51:02.555: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename emptydir @ 07/22/23 13:51:02.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:51:02.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:51:02.605
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/22/23 13:51:02.612
  E0722 13:51:02.881249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:51:03.881335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:51:04.881560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0722 13:51:05.881704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/22/23 13:51:06.652
  Jul 22 13:51:06.658: INFO: Trying to get logs from node ip-172-31-15-55 pod pod-c4578691-6c89-4e2c-b04e-705da326e051 container test-container: <nil>
  STEP: delete the pod @ 07/22/23 13:51:06.668
  Jul 22 13:51:06.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2443" for this suite. @ 07/22/23 13:51:06.698
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/22/23 13:51:06.719
  Jul 22 13:51:06.719: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename secrets @ 07/22/23 13:51:06.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:51:06.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:51:06.766
  Jul 22 13:51:06.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5912" for this suite. @ 07/22/23 13:51:06.856
• [0.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/22/23 13:51:06.877
  Jul 22 13:51:06.877: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename podtemplate @ 07/22/23 13:51:06.878
  E0722 13:51:06.882055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:51:06.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:51:06.962
  STEP: Create a pod template @ 07/22/23 13:51:06.968
  STEP: Replace a pod template @ 07/22/23 13:51:06.978
  Jul 22 13:51:06.993: INFO: Found updated podtemplate annotation: "true"

  Jul 22 13:51:06.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5962" for this suite. @ 07/22/23 13:51:07.001
• [0.143 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/22/23 13:51:07.021
  Jul 22 13:51:07.021: INFO: >>> kubeConfig: /tmp/kubeconfig-53067953
  STEP: Building a namespace api object, basename gc @ 07/22/23 13:51:07.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/22/23 13:51:07.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/22/23 13:51:07.059
  STEP: create the deployment @ 07/22/23 13:51:07.067
  W0722 13:51:07.078013      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/22/23 13:51:07.078
  STEP: delete the deployment @ 07/22/23 13:51:07.595
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/22/23 13:51:07.604
  E0722 13:51:07.882809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/22/23 13:51:08.137
  W0722 13:51:08.142106      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 22 13:51:08.142: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 22 13:51:08.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4672" for this suite. @ 07/22/23 13:51:08.148
• [1.137 seconds]
------------------------------
SSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul 22 13:51:08.160: INFO: Running AfterSuite actions on node 1
  Jul 22 13:51:08.161: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.067 seconds]
------------------------------

Ran 378 of 7207 Specs in 6347.549 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h45m48.08353164s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

