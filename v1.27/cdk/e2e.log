  I0603 12:00:36.811850      18 e2e.go:117] Starting e2e run "890fff61-bdc4-4aa7-8c13-2e2931076cd1" on Ginkgo node 1
  Jun  3 12:00:36.857: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1685793636 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jun  3 12:00:37.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:00:37.220: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jun  3 12:00:37.260: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jun  3 12:00:37.265: INFO: e2e test version: v1.27.2
  Jun  3 12:00:37.267: INFO: kube-apiserver version: v1.27.2
  Jun  3 12:00:37.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:00:37.275: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.056 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 06/03/23 12:00:37.949
  Jun  3 12:00:37.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename server-version @ 06/03/23 12:00:37.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:00:37.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:00:37.979
  STEP: Request ServerVersion @ 06/03/23 12:00:37.984
  STEP: Confirm major version @ 06/03/23 12:00:37.986
  Jun  3 12:00:37.986: INFO: Major version: 1
  STEP: Confirm minor version @ 06/03/23 12:00:37.987
  Jun  3 12:00:37.987: INFO: cleanMinorVersion: 27
  Jun  3 12:00:37.987: INFO: Minor version: 27
  Jun  3 12:00:37.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2429" for this suite. @ 06/03/23 12:00:37.993
• [0.051 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 06/03/23 12:00:38.002
  Jun  3 12:00:38.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename events @ 06/03/23 12:00:38.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:00:38.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:00:38.026
  STEP: Create set of events @ 06/03/23 12:00:38.03
  Jun  3 12:00:38.035: INFO: created test-event-1
  Jun  3 12:00:38.040: INFO: created test-event-2
  Jun  3 12:00:38.045: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 06/03/23 12:00:38.046
  STEP: delete collection of events @ 06/03/23 12:00:38.05
  Jun  3 12:00:38.050: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 06/03/23 12:00:38.072
  Jun  3 12:00:38.072: INFO: requesting list of events to confirm quantity
  Jun  3 12:00:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5838" for this suite. @ 06/03/23 12:00:38.08
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 06/03/23 12:00:38.092
  Jun  3 12:00:38.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:00:38.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:00:38.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:00:38.118
  Jun  3 12:00:38.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 06/03/23 12:00:39.565
  Jun  3 12:00:39.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 create -f -'
  Jun  3 12:00:40.331: INFO: stderr: ""
  Jun  3 12:00:40.331: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun  3 12:00:40.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 delete e2e-test-crd-publish-openapi-1085-crds test-foo'
  Jun  3 12:00:40.450: INFO: stderr: ""
  Jun  3 12:00:40.450: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jun  3 12:00:40.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 apply -f -'
  Jun  3 12:00:40.676: INFO: stderr: ""
  Jun  3 12:00:40.676: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jun  3 12:00:40.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 delete e2e-test-crd-publish-openapi-1085-crds test-foo'
  Jun  3 12:00:40.764: INFO: stderr: ""
  Jun  3 12:00:40.764: INFO: stdout: "e2e-test-crd-publish-openapi-1085-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 06/03/23 12:00:40.764
  Jun  3 12:00:40.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 create -f -'
  Jun  3 12:00:41.464: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 06/03/23 12:00:41.464
  Jun  3 12:00:41.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 create -f -'
  Jun  3 12:00:41.699: INFO: rc: 1
  Jun  3 12:00:41.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 apply -f -'
  Jun  3 12:00:41.940: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 06/03/23 12:00:41.94
  Jun  3 12:00:41.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 create -f -'
  Jun  3 12:00:42.190: INFO: rc: 1
  Jun  3 12:00:42.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 --namespace=crd-publish-openapi-8900 apply -f -'
  Jun  3 12:00:42.449: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 06/03/23 12:00:42.449
  Jun  3 12:00:42.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 explain e2e-test-crd-publish-openapi-1085-crds'
  Jun  3 12:00:42.696: INFO: stderr: ""
  Jun  3 12:00:42.696: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-1085-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 06/03/23 12:00:42.697
  Jun  3 12:00:42.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 explain e2e-test-crd-publish-openapi-1085-crds.metadata'
  Jun  3 12:00:42.937: INFO: stderr: ""
  Jun  3 12:00:42.937: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-1085-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jun  3 12:00:42.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 explain e2e-test-crd-publish-openapi-1085-crds.spec'
  Jun  3 12:00:43.191: INFO: stderr: ""
  Jun  3 12:00:43.191: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-1085-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jun  3 12:00:43.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 explain e2e-test-crd-publish-openapi-1085-crds.spec.bars'
  Jun  3 12:00:43.444: INFO: stderr: ""
  Jun  3 12:00:43.444: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-1085-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 06/03/23 12:00:43.445
  Jun  3 12:00:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8900 explain e2e-test-crd-publish-openapi-1085-crds.spec.bars2'
  Jun  3 12:00:43.701: INFO: rc: 1
  Jun  3 12:00:45.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8900" for this suite. @ 06/03/23 12:00:45.153
• [7.067 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 06/03/23 12:00:45.161
  Jun  3 12:00:45.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:00:45.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:00:45.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:00:45.184
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:00:45.189
  STEP: Saw pod success @ 06/03/23 12:00:55.232
  Jun  3 12:00:55.235: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-d91d1e58-c702-4461-940e-90f4784e244a container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:00:55.274
  Jun  3 12:00:55.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6682" for this suite. @ 06/03/23 12:00:55.298
• [10.148 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 06/03/23 12:00:55.31
  Jun  3 12:00:55.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 12:00:55.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:00:55.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:00:55.336
  STEP: Given a ReplicationController is created @ 06/03/23 12:00:55.34
  STEP: When the matched label of one of its pods change @ 06/03/23 12:00:55.349
  Jun  3 12:00:55.353: INFO: Pod name pod-release: Found 0 pods out of 1
  Jun  3 12:01:00.358: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/03/23 12:01:00.37
  Jun  3 12:01:01.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9944" for this suite. @ 06/03/23 12:01:01.382
• [6.080 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 06/03/23 12:01:01.391
  Jun  3 12:01:01.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 12:01:01.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:01:01.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:01:01.416
  STEP: Creating Indexed job @ 06/03/23 12:01:01.42
  STEP: Ensuring job reaches completions @ 06/03/23 12:01:01.428
  STEP: Ensuring pods with index for job exist @ 06/03/23 12:01:13.433
  Jun  3 12:01:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-13" for this suite. @ 06/03/23 12:01:13.445
• [12.062 seconds]
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 06/03/23 12:01:13.454
  Jun  3 12:01:13.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename cronjob @ 06/03/23 12:01:13.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:01:13.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:01:13.485
  STEP: Creating a ForbidConcurrent cronjob @ 06/03/23 12:01:13.489
  STEP: Ensuring a job is scheduled @ 06/03/23 12:01:13.498
  STEP: Ensuring exactly one is scheduled @ 06/03/23 12:02:01.503
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/03/23 12:02:01.506
  STEP: Ensuring no more jobs are scheduled @ 06/03/23 12:02:01.51
  STEP: Removing cronjob @ 06/03/23 12:07:01.518
  Jun  3 12:07:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2226" for this suite. @ 06/03/23 12:07:01.529
• [348.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 06/03/23 12:07:01.537
  Jun  3 12:07:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 12:07:01.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:07:01.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:07:01.569
  STEP: Creating service test in namespace statefulset-2744 @ 06/03/23 12:07:01.573
  STEP: Creating stateful set ss in namespace statefulset-2744 @ 06/03/23 12:07:01.581
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2744 @ 06/03/23 12:07:01.59
  Jun  3 12:07:01.595: INFO: Found 0 stateful pods, waiting for 1
  Jun  3 12:07:11.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 06/03/23 12:07:11.604
  Jun  3 12:07:11.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:07:11.792: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:07:11.792: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:07:11.792: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 12:07:11.795: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jun  3 12:07:21.803: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 12:07:21.803: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:07:21.820: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun  3 12:07:21.820: INFO: ss-0  ip-172-31-27-193  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:01 +0000 UTC  }]
  Jun  3 12:07:21.820: INFO: 
  Jun  3 12:07:21.820: INFO: StatefulSet ss has not reached scale 3, at 1
  Jun  3 12:07:22.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994997751s
  Jun  3 12:07:23.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989230267s
  Jun  3 12:07:24.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984232369s
  Jun  3 12:07:25.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979804429s
  Jun  3 12:07:26.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974677496s
  Jun  3 12:07:27.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97069556s
  Jun  3 12:07:28.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966309281s
  Jun  3 12:07:29.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961252591s
  Jun  3 12:07:30.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.493233ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2744 @ 06/03/23 12:07:31.864
  Jun  3 12:07:31.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 12:07:32.034: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 12:07:32.034: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 12:07:32.034: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 12:07:32.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 12:07:32.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun  3 12:07:32.196: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 12:07:32.196: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 12:07:32.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 12:07:32.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jun  3 12:07:32.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 12:07:32.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 12:07:32.370: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Jun  3 12:07:42.376: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 12:07:42.377: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 12:07:42.377: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 06/03/23 12:07:42.377
  Jun  3 12:07:42.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:07:42.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:07:42.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:07:42.544: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 12:07:42.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:07:42.706: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:07:42.706: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:07:42.706: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 12:07:42.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-2744 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:07:42.875: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:07:42.876: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:07:42.876: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 12:07:42.876: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:07:42.879: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Jun  3 12:07:52.891: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 12:07:52.891: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 12:07:52.891: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 12:07:52.910: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
  Jun  3 12:07:52.910: INFO: ss-0  ip-172-31-27-193  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:01 +0000 UTC  }]
  Jun  3 12:07:52.910: INFO: ss-1  ip-172-31-85-85   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  }]
  Jun  3 12:07:52.910: INFO: ss-2  ip-172-31-7-203   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  }]
  Jun  3 12:07:52.910: INFO: 
  Jun  3 12:07:52.910: INFO: StatefulSet ss has not reached scale 0, at 3
  Jun  3 12:07:53.914: INFO: POD   NODE             PHASE      GRACE  CONDITIONS
  Jun  3 12:07:53.914: INFO: ss-1  ip-172-31-85-85  Running    30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  }]
  Jun  3 12:07:53.914: INFO: ss-2  ip-172-31-7-203  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:43 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:07:21 +0000 UTC  }]
  Jun  3 12:07:53.914: INFO: 
  Jun  3 12:07:53.914: INFO: StatefulSet ss has not reached scale 0, at 2
  Jun  3 12:07:54.919: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989761868s
  Jun  3 12:07:55.924: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984567084s
  Jun  3 12:07:56.929: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980261255s
  Jun  3 12:07:57.933: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975822336s
  Jun  3 12:07:58.937: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971539732s
  Jun  3 12:07:59.941: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967192726s
  Jun  3 12:08:00.945: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.962744167s
  Jun  3 12:08:01.951: INFO: Verifying statefulset ss doesn't scale past 0 for another 958.160603ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2744 @ 06/03/23 12:08:02.951
  Jun  3 12:08:02.955: INFO: Scaling statefulset ss to 0
  Jun  3 12:08:02.968: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:08:02.971: INFO: Deleting all statefulset in ns statefulset-2744
  Jun  3 12:08:02.975: INFO: Scaling statefulset ss to 0
  Jun  3 12:08:02.989: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:08:02.992: INFO: Deleting statefulset ss
  Jun  3 12:08:03.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2744" for this suite. @ 06/03/23 12:08:03.01
• [61.481 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 06/03/23 12:08:03.02
  Jun  3 12:08:03.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption @ 06/03/23 12:08:03.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:08:03.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:08:03.046
  Jun  3 12:08:03.066: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 12:09:03.090: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/03/23 12:09:03.095
  Jun  3 12:09:03.119: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun  3 12:09:03.129: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun  3 12:09:03.151: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun  3 12:09:03.160: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun  3 12:09:03.199: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun  3 12:09:03.208: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/03/23 12:09:03.208
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 06/03/23 12:09:05.245
  Jun  3 12:09:09.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7047" for this suite. @ 06/03/23 12:09:09.378
• [66.367 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 06/03/23 12:09:09.395
  Jun  3 12:09:09.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 12:09:09.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:09:09.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:09:09.42
  Jun  3 12:09:09.453: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 06/03/23 12:09:09.462
  Jun  3 12:09:09.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:09.466: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 06/03/23 12:09:09.466
  Jun  3 12:09:09.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:09.501: INFO: Node ip-172-31-85-85 is running 0 daemon pod, expected 1
  Jun  3 12:09:10.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:10.506: INFO: Node ip-172-31-85-85 is running 0 daemon pod, expected 1
  Jun  3 12:09:11.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun  3 12:09:11.506: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 06/03/23 12:09:11.509
  Jun  3 12:09:11.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun  3 12:09:11.527: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jun  3 12:09:12.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:12.531: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 06/03/23 12:09:12.531
  Jun  3 12:09:12.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:12.545: INFO: Node ip-172-31-85-85 is running 0 daemon pod, expected 1
  Jun  3 12:09:13.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:13.550: INFO: Node ip-172-31-85-85 is running 0 daemon pod, expected 1
  Jun  3 12:09:14.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun  3 12:09:14.551: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 12:09:14.562
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9694, will wait for the garbage collector to delete the pods @ 06/03/23 12:09:14.562
  Jun  3 12:09:14.624: INFO: Deleting DaemonSet.extensions daemon-set took: 7.699311ms
  Jun  3 12:09:14.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.672756ms
  Jun  3 12:09:16.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:09:16.230: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 12:09:16.236: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3769"},"items":null}

  Jun  3 12:09:16.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3770"},"items":null}

  Jun  3 12:09:16.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9694" for this suite. @ 06/03/23 12:09:16.285
• [6.898 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 06/03/23 12:09:16.296
  Jun  3 12:09:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename cronjob @ 06/03/23 12:09:16.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:09:16.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:09:16.322
  STEP: Creating a cronjob @ 06/03/23 12:09:16.336
  STEP: Ensuring more than one job is running at a time @ 06/03/23 12:09:16.344
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 06/03/23 12:11:00.35
  STEP: Removing cronjob @ 06/03/23 12:11:00.353
  Jun  3 12:11:00.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3117" for this suite. @ 06/03/23 12:11:00.364
• [104.076 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 06/03/23 12:11:00.373
  Jun  3 12:11:00.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:11:00.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:00.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:00.409
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:11:00.413
  STEP: Saw pod success @ 06/03/23 12:11:08.445
  Jun  3 12:11:08.449: INFO: Trying to get logs from node ip-172-31-85-85 pod downwardapi-volume-f3ff23f3-8471-4e60-a217-c61a6ba66ba4 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:11:08.475
  Jun  3 12:11:08.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1183" for this suite. @ 06/03/23 12:11:08.498
• [8.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 06/03/23 12:11:08.508
  Jun  3 12:11:08.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename containers @ 06/03/23 12:11:08.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:08.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:08.534
  STEP: Creating a pod to test override arguments @ 06/03/23 12:11:08.539
  STEP: Saw pod success @ 06/03/23 12:11:12.563
  Jun  3 12:11:12.566: INFO: Trying to get logs from node ip-172-31-85-85 pod client-containers-9b4cbd66-07f7-4fd9-bad3-43aac27dff43 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:11:12.575
  Jun  3 12:11:12.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2990" for this suite. @ 06/03/23 12:11:12.595
• [4.093 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 06/03/23 12:11:12.602
  Jun  3 12:11:12.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 12:11:12.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:12.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:12.625
  STEP: Creating a test namespace @ 06/03/23 12:11:12.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:12.647
  STEP: Creating a service in the namespace @ 06/03/23 12:11:12.65
  STEP: Deleting the namespace @ 06/03/23 12:11:12.661
  STEP: Waiting for the namespace to be removed. @ 06/03/23 12:11:12.669
  STEP: Recreating the namespace @ 06/03/23 12:11:18.674
  STEP: Verifying there is no service in the namespace @ 06/03/23 12:11:18.691
  Jun  3 12:11:18.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4509" for this suite. @ 06/03/23 12:11:18.7
  STEP: Destroying namespace "nsdeletetest-5213" for this suite. @ 06/03/23 12:11:18.712
  Jun  3 12:11:18.716: INFO: Namespace nsdeletetest-5213 was already deleted
  STEP: Destroying namespace "nsdeletetest-7675" for this suite. @ 06/03/23 12:11:18.716
• [6.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 06/03/23 12:11:18.726
  Jun  3 12:11:18.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:11:18.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:18.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:18.751
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7723 @ 06/03/23 12:11:18.756
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/03/23 12:11:18.77
  STEP: creating service externalsvc in namespace services-7723 @ 06/03/23 12:11:18.77
  STEP: creating replication controller externalsvc in namespace services-7723 @ 06/03/23 12:11:18.783
  I0603 12:11:18.790971      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7723, replica count: 2
  I0603 12:11:21.842088      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 06/03/23 12:11:21.846
  Jun  3 12:11:21.859: INFO: Creating new exec pod
  Jun  3 12:11:23.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7723 exec execpodgbrpd -- /bin/sh -x -c nslookup clusterip-service.services-7723.svc.cluster.local'
  Jun  3 12:11:24.075: INFO: stderr: "+ nslookup clusterip-service.services-7723.svc.cluster.local\n"
  Jun  3 12:11:24.075: INFO: stdout: "Server:\t\t10.152.183.236\nAddress:\t10.152.183.236#53\n\nclusterip-service.services-7723.svc.cluster.local\tcanonical name = externalsvc.services-7723.svc.cluster.local.\nName:\texternalsvc.services-7723.svc.cluster.local\nAddress: 10.152.183.53\n\n"
  Jun  3 12:11:24.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7723, will wait for the garbage collector to delete the pods @ 06/03/23 12:11:24.081
  Jun  3 12:11:24.142: INFO: Deleting ReplicationController externalsvc took: 6.854044ms
  Jun  3 12:11:24.242: INFO: Terminating ReplicationController externalsvc pods took: 100.647677ms
  Jun  3 12:11:26.361: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7723" for this suite. @ 06/03/23 12:11:26.373
• [7.654 seconds]
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 06/03/23 12:11:26.381
  Jun  3 12:11:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename watch @ 06/03/23 12:11:26.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:26.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:26.406
  STEP: creating a new configmap @ 06/03/23 12:11:26.41
  STEP: modifying the configmap once @ 06/03/23 12:11:26.416
  STEP: modifying the configmap a second time @ 06/03/23 12:11:26.425
  STEP: deleting the configmap @ 06/03/23 12:11:26.434
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 06/03/23 12:11:26.441
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 06/03/23 12:11:26.443
  Jun  3 12:11:26.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-618  2dc04bc2-4f99-429d-9b27-b11e819481b4 4314 0 2023-06-03 12:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-03 12:11:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:11:26.444: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-618  2dc04bc2-4f99-429d-9b27-b11e819481b4 4315 0 2023-06-03 12:11:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-03 12:11:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:11:26.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-618" for this suite. @ 06/03/23 12:11:26.449
• [0.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 06/03/23 12:11:26.457
  Jun  3 12:11:26.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubelet-test @ 06/03/23 12:11:26.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:26.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:26.479
  STEP: Waiting for pod completion @ 06/03/23 12:11:26.502
  Jun  3 12:11:30.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3282" for this suite. @ 06/03/23 12:11:30.549
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 06/03/23 12:11:30.56
  Jun  3 12:11:30.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 12:11:30.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:11:30.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:11:30.583
  STEP: create the rc @ 06/03/23 12:11:30.593
  W0603 12:11:30.600521      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/03/23 12:11:36.606
  STEP: wait for the rc to be deleted @ 06/03/23 12:11:36.613
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 06/03/23 12:11:41.62
  STEP: Gathering metrics @ 06/03/23 12:12:11.633
  W0603 12:12:11.637499      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 12:12:11.637: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 12:12:11.638: INFO: Deleting pod "simpletest.rc-2cdw5" in namespace "gc-7993"
  Jun  3 12:12:11.649: INFO: Deleting pod "simpletest.rc-4hvv9" in namespace "gc-7993"
  Jun  3 12:12:11.663: INFO: Deleting pod "simpletest.rc-4ms2q" in namespace "gc-7993"
  Jun  3 12:12:11.678: INFO: Deleting pod "simpletest.rc-4rzhg" in namespace "gc-7993"
  Jun  3 12:12:11.695: INFO: Deleting pod "simpletest.rc-5f5l4" in namespace "gc-7993"
  Jun  3 12:12:11.709: INFO: Deleting pod "simpletest.rc-5glhb" in namespace "gc-7993"
  Jun  3 12:12:11.725: INFO: Deleting pod "simpletest.rc-68ffh" in namespace "gc-7993"
  Jun  3 12:12:11.736: INFO: Deleting pod "simpletest.rc-6gnjr" in namespace "gc-7993"
  Jun  3 12:12:11.750: INFO: Deleting pod "simpletest.rc-6qsqt" in namespace "gc-7993"
  Jun  3 12:12:11.765: INFO: Deleting pod "simpletest.rc-6rnqx" in namespace "gc-7993"
  Jun  3 12:12:11.781: INFO: Deleting pod "simpletest.rc-6w9qz" in namespace "gc-7993"
  Jun  3 12:12:11.797: INFO: Deleting pod "simpletest.rc-76x6s" in namespace "gc-7993"
  Jun  3 12:12:11.810: INFO: Deleting pod "simpletest.rc-7wv45" in namespace "gc-7993"
  Jun  3 12:12:11.826: INFO: Deleting pod "simpletest.rc-8crxs" in namespace "gc-7993"
  Jun  3 12:12:11.840: INFO: Deleting pod "simpletest.rc-8d5wj" in namespace "gc-7993"
  Jun  3 12:12:11.853: INFO: Deleting pod "simpletest.rc-8sq6g" in namespace "gc-7993"
  Jun  3 12:12:11.868: INFO: Deleting pod "simpletest.rc-96lp4" in namespace "gc-7993"
  Jun  3 12:12:11.883: INFO: Deleting pod "simpletest.rc-9ckhf" in namespace "gc-7993"
  Jun  3 12:12:11.899: INFO: Deleting pod "simpletest.rc-9wcj5" in namespace "gc-7993"
  Jun  3 12:12:11.916: INFO: Deleting pod "simpletest.rc-9x67p" in namespace "gc-7993"
  Jun  3 12:12:11.930: INFO: Deleting pod "simpletest.rc-9znvx" in namespace "gc-7993"
  Jun  3 12:12:11.942: INFO: Deleting pod "simpletest.rc-bc688" in namespace "gc-7993"
  Jun  3 12:12:11.958: INFO: Deleting pod "simpletest.rc-bgmbd" in namespace "gc-7993"
  Jun  3 12:12:11.974: INFO: Deleting pod "simpletest.rc-bmsvf" in namespace "gc-7993"
  Jun  3 12:12:11.987: INFO: Deleting pod "simpletest.rc-c5fmp" in namespace "gc-7993"
  Jun  3 12:12:12.001: INFO: Deleting pod "simpletest.rc-c6xhl" in namespace "gc-7993"
  Jun  3 12:12:12.014: INFO: Deleting pod "simpletest.rc-cgmdm" in namespace "gc-7993"
  Jun  3 12:12:12.031: INFO: Deleting pod "simpletest.rc-clfvx" in namespace "gc-7993"
  Jun  3 12:12:12.045: INFO: Deleting pod "simpletest.rc-d68hq" in namespace "gc-7993"
  Jun  3 12:12:12.066: INFO: Deleting pod "simpletest.rc-d9dwz" in namespace "gc-7993"
  Jun  3 12:12:12.082: INFO: Deleting pod "simpletest.rc-d9px5" in namespace "gc-7993"
  Jun  3 12:12:12.098: INFO: Deleting pod "simpletest.rc-dg4mc" in namespace "gc-7993"
  Jun  3 12:12:12.112: INFO: Deleting pod "simpletest.rc-dg8cm" in namespace "gc-7993"
  Jun  3 12:12:12.128: INFO: Deleting pod "simpletest.rc-f46ph" in namespace "gc-7993"
  Jun  3 12:12:12.144: INFO: Deleting pod "simpletest.rc-f5jt9" in namespace "gc-7993"
  Jun  3 12:12:12.165: INFO: Deleting pod "simpletest.rc-fs6bj" in namespace "gc-7993"
  Jun  3 12:12:12.179: INFO: Deleting pod "simpletest.rc-fx22s" in namespace "gc-7993"
  Jun  3 12:12:12.192: INFO: Deleting pod "simpletest.rc-ggjz7" in namespace "gc-7993"
  Jun  3 12:12:12.207: INFO: Deleting pod "simpletest.rc-glhsh" in namespace "gc-7993"
  Jun  3 12:12:12.223: INFO: Deleting pod "simpletest.rc-gs6k9" in namespace "gc-7993"
  Jun  3 12:12:12.237: INFO: Deleting pod "simpletest.rc-h5ndx" in namespace "gc-7993"
  Jun  3 12:12:12.249: INFO: Deleting pod "simpletest.rc-hp625" in namespace "gc-7993"
  Jun  3 12:12:12.265: INFO: Deleting pod "simpletest.rc-hq7x6" in namespace "gc-7993"
  Jun  3 12:12:12.280: INFO: Deleting pod "simpletest.rc-hrtlz" in namespace "gc-7993"
  Jun  3 12:12:12.294: INFO: Deleting pod "simpletest.rc-jmb7z" in namespace "gc-7993"
  Jun  3 12:12:12.309: INFO: Deleting pod "simpletest.rc-jmhkj" in namespace "gc-7993"
  Jun  3 12:12:12.324: INFO: Deleting pod "simpletest.rc-kg6vb" in namespace "gc-7993"
  Jun  3 12:12:12.338: INFO: Deleting pod "simpletest.rc-km5t7" in namespace "gc-7993"
  Jun  3 12:12:12.355: INFO: Deleting pod "simpletest.rc-kpv4g" in namespace "gc-7993"
  Jun  3 12:12:12.379: INFO: Deleting pod "simpletest.rc-kxc6r" in namespace "gc-7993"
  Jun  3 12:12:12.408: INFO: Deleting pod "simpletest.rc-kzcz8" in namespace "gc-7993"
  Jun  3 12:12:12.426: INFO: Deleting pod "simpletest.rc-mhx4s" in namespace "gc-7993"
  Jun  3 12:12:12.446: INFO: Deleting pod "simpletest.rc-mwgfs" in namespace "gc-7993"
  Jun  3 12:12:12.462: INFO: Deleting pod "simpletest.rc-mww5x" in namespace "gc-7993"
  Jun  3 12:12:12.475: INFO: Deleting pod "simpletest.rc-n7g82" in namespace "gc-7993"
  Jun  3 12:12:12.489: INFO: Deleting pod "simpletest.rc-ncmxl" in namespace "gc-7993"
  Jun  3 12:12:12.507: INFO: Deleting pod "simpletest.rc-nnzs4" in namespace "gc-7993"
  Jun  3 12:12:12.528: INFO: Deleting pod "simpletest.rc-np27n" in namespace "gc-7993"
  Jun  3 12:12:12.543: INFO: Deleting pod "simpletest.rc-p7thv" in namespace "gc-7993"
  Jun  3 12:12:12.558: INFO: Deleting pod "simpletest.rc-pnmwz" in namespace "gc-7993"
  Jun  3 12:12:12.571: INFO: Deleting pod "simpletest.rc-pqkwq" in namespace "gc-7993"
  Jun  3 12:12:12.587: INFO: Deleting pod "simpletest.rc-q8sj8" in namespace "gc-7993"
  Jun  3 12:12:12.604: INFO: Deleting pod "simpletest.rc-qdkts" in namespace "gc-7993"
  Jun  3 12:12:12.619: INFO: Deleting pod "simpletest.rc-qjmm8" in namespace "gc-7993"
  Jun  3 12:12:12.631: INFO: Deleting pod "simpletest.rc-qw8hb" in namespace "gc-7993"
  Jun  3 12:12:12.648: INFO: Deleting pod "simpletest.rc-qwlvw" in namespace "gc-7993"
  Jun  3 12:12:12.661: INFO: Deleting pod "simpletest.rc-r4fgf" in namespace "gc-7993"
  Jun  3 12:12:12.675: INFO: Deleting pod "simpletest.rc-r5q67" in namespace "gc-7993"
  Jun  3 12:12:12.688: INFO: Deleting pod "simpletest.rc-rgsrw" in namespace "gc-7993"
  Jun  3 12:12:12.703: INFO: Deleting pod "simpletest.rc-rhl8j" in namespace "gc-7993"
  Jun  3 12:12:12.734: INFO: Deleting pod "simpletest.rc-sdnhf" in namespace "gc-7993"
  Jun  3 12:12:12.786: INFO: Deleting pod "simpletest.rc-sdvw4" in namespace "gc-7993"
  Jun  3 12:12:12.838: INFO: Deleting pod "simpletest.rc-smmbx" in namespace "gc-7993"
  Jun  3 12:12:12.886: INFO: Deleting pod "simpletest.rc-ssvl7" in namespace "gc-7993"
  Jun  3 12:12:12.939: INFO: Deleting pod "simpletest.rc-stnxm" in namespace "gc-7993"
  Jun  3 12:12:12.984: INFO: Deleting pod "simpletest.rc-szc86" in namespace "gc-7993"
  Jun  3 12:12:13.035: INFO: Deleting pod "simpletest.rc-t8rxb" in namespace "gc-7993"
  Jun  3 12:12:13.084: INFO: Deleting pod "simpletest.rc-tlcqz" in namespace "gc-7993"
  Jun  3 12:12:13.136: INFO: Deleting pod "simpletest.rc-tp7vk" in namespace "gc-7993"
  Jun  3 12:12:13.196: INFO: Deleting pod "simpletest.rc-trrpk" in namespace "gc-7993"
  Jun  3 12:12:13.237: INFO: Deleting pod "simpletest.rc-txs4n" in namespace "gc-7993"
  Jun  3 12:12:13.285: INFO: Deleting pod "simpletest.rc-v5hf8" in namespace "gc-7993"
  Jun  3 12:12:13.338: INFO: Deleting pod "simpletest.rc-vkqp9" in namespace "gc-7993"
  Jun  3 12:12:13.387: INFO: Deleting pod "simpletest.rc-vl8m8" in namespace "gc-7993"
  Jun  3 12:12:13.437: INFO: Deleting pod "simpletest.rc-vqk7x" in namespace "gc-7993"
  Jun  3 12:12:13.488: INFO: Deleting pod "simpletest.rc-vtch7" in namespace "gc-7993"
  Jun  3 12:12:13.537: INFO: Deleting pod "simpletest.rc-vxp4p" in namespace "gc-7993"
  Jun  3 12:12:13.585: INFO: Deleting pod "simpletest.rc-whsnq" in namespace "gc-7993"
  Jun  3 12:12:13.639: INFO: Deleting pod "simpletest.rc-wm784" in namespace "gc-7993"
  Jun  3 12:12:13.686: INFO: Deleting pod "simpletest.rc-wpvll" in namespace "gc-7993"
  Jun  3 12:12:13.735: INFO: Deleting pod "simpletest.rc-wqvcq" in namespace "gc-7993"
  Jun  3 12:12:13.791: INFO: Deleting pod "simpletest.rc-wzl76" in namespace "gc-7993"
  Jun  3 12:12:13.833: INFO: Deleting pod "simpletest.rc-x4x8t" in namespace "gc-7993"
  Jun  3 12:12:13.882: INFO: Deleting pod "simpletest.rc-xchlz" in namespace "gc-7993"
  Jun  3 12:12:13.938: INFO: Deleting pod "simpletest.rc-xsdzc" in namespace "gc-7993"
  Jun  3 12:12:13.988: INFO: Deleting pod "simpletest.rc-z2r7l" in namespace "gc-7993"
  Jun  3 12:12:14.036: INFO: Deleting pod "simpletest.rc-z54fd" in namespace "gc-7993"
  Jun  3 12:12:14.086: INFO: Deleting pod "simpletest.rc-z7448" in namespace "gc-7993"
  Jun  3 12:12:14.136: INFO: Deleting pod "simpletest.rc-zkhxp" in namespace "gc-7993"
  Jun  3 12:12:14.187: INFO: Deleting pod "simpletest.rc-zs8sx" in namespace "gc-7993"
  Jun  3 12:12:14.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7993" for this suite. @ 06/03/23 12:12:14.278
• [43.770 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 06/03/23 12:12:14.335
  Jun  3 12:12:14.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pod-network-test @ 06/03/23 12:12:14.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:12:14.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:12:14.361
  STEP: Performing setup for networking test in namespace pod-network-test-7239 @ 06/03/23 12:12:14.365
  STEP: creating a selector @ 06/03/23 12:12:14.365
  STEP: Creating the service pods in kubernetes @ 06/03/23 12:12:14.365
  Jun  3 12:12:14.366: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/03/23 12:12:46.505
  Jun  3 12:12:48.544: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun  3 12:12:48.544: INFO: Going to poll 192.168.118.243 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 12:12:48.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.118.243:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7239 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:12:48.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:12:48.549: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:12:48.549: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7239/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.118.243%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun  3 12:12:48.647: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun  3 12:12:48.647: INFO: Going to poll 192.168.192.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 12:12:48.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.192.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7239 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:12:48.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:12:48.651: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:12:48.651: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7239/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.192.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun  3 12:12:48.728: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun  3 12:12:48.728: INFO: Going to poll 192.168.20.112 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 12:12:48.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.20.112:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7239 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:12:48.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:12:48.732: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:12:48.733: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-7239/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.20.112%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun  3 12:12:48.816: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun  3 12:12:48.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7239" for this suite. @ 06/03/23 12:12:48.821
• [34.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 06/03/23 12:12:48.83
  Jun  3 12:12:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:12:48.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:12:48.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:12:48.853
  STEP: Creating projection with secret that has name secret-emptykey-test-7e016e5c-efd1-4a10-9fd6-8c1f57ac9263 @ 06/03/23 12:12:48.857
  Jun  3 12:12:48.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-824" for this suite. @ 06/03/23 12:12:48.865
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 06/03/23 12:12:48.873
  Jun  3 12:12:48.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption @ 06/03/23 12:12:48.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:12:48.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:12:48.895
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:12:48.911
  STEP: Waiting for all pods to be running @ 06/03/23 12:12:50.951
  Jun  3 12:12:50.956: INFO: running pods: 0 < 3
  Jun  3 12:12:52.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-446" for this suite. @ 06/03/23 12:12:52.97
• [4.105 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 06/03/23 12:12:52.979
  Jun  3 12:12:52.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:12:52.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:12:53.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:12:53.013
  STEP: Creating the pod @ 06/03/23 12:12:53.02
  Jun  3 12:12:55.573: INFO: Successfully updated pod "labelsupdatea795693b-9951-44a6-b994-1f3432d323fd"
  Jun  3 12:12:57.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6148" for this suite. @ 06/03/23 12:12:57.595
• [4.626 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 06/03/23 12:12:57.611
  Jun  3 12:12:57.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:12:57.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:12:57.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:12:57.638
  STEP: Creating configMap with name projected-configmap-test-volume-8e888a7a-4ca1-4e27-9a6d-eea31000f289 @ 06/03/23 12:12:57.643
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:12:57.649
  STEP: Saw pod success @ 06/03/23 12:13:01.678
  Jun  3 12:13:01.683: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-dec272ba-07f3-45bc-b53d-5f232df79e9a container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:13:01.691
  Jun  3 12:13:01.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8881" for this suite. @ 06/03/23 12:13:01.716
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 06/03/23 12:13:01.725
  Jun  3 12:13:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 12:13:01.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:13:01.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:13:01.761
  STEP: Creating service test in namespace statefulset-1863 @ 06/03/23 12:13:01.766
  STEP: Creating statefulset ss in namespace statefulset-1863 @ 06/03/23 12:13:01.774
  Jun  3 12:13:01.784: INFO: Found 0 stateful pods, waiting for 1
  Jun  3 12:13:11.791: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 06/03/23 12:13:11.799
  STEP: updating a scale subresource @ 06/03/23 12:13:11.803
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/03/23 12:13:11.81
  STEP: Patch a scale subresource @ 06/03/23 12:13:11.814
  STEP: verifying the statefulset Spec.Replicas was modified @ 06/03/23 12:13:11.822
  Jun  3 12:13:11.828: INFO: Deleting all statefulset in ns statefulset-1863
  Jun  3 12:13:11.833: INFO: Scaling statefulset ss to 0
  Jun  3 12:13:21.866: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:13:21.870: INFO: Deleting statefulset ss
  Jun  3 12:13:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1863" for this suite. @ 06/03/23 12:13:21.889
• [20.172 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 06/03/23 12:13:21.898
  Jun  3 12:13:21.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:13:21.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:13:21.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:13:21.922
  STEP: Creating pod test-grpc-37b5067b-95ce-4566-ab2a-be5136b284a5 in namespace container-probe-753 @ 06/03/23 12:13:21.931
  Jun  3 12:13:23.947: INFO: Started pod test-grpc-37b5067b-95ce-4566-ab2a-be5136b284a5 in namespace container-probe-753
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:13:23.947
  Jun  3 12:13:23.951: INFO: Initial restart count of pod test-grpc-37b5067b-95ce-4566-ab2a-be5136b284a5 is 0
  Jun  3 12:17:24.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:17:24.544
  STEP: Destroying namespace "container-probe-753" for this suite. @ 06/03/23 12:17:24.558
• [242.669 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 06/03/23 12:17:24.568
  Jun  3 12:17:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:17:24.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:17:24.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:17:24.593
  STEP: Creating secret with name secret-test-5e6806f1-aa94-425c-824d-8628884687ea @ 06/03/23 12:17:24.622
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:17:24.628
  STEP: Saw pod success @ 06/03/23 12:17:28.65
  Jun  3 12:17:28.653: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-c47fd4b5-0166-478d-b1d6-4df97170d744 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:17:28.679
  Jun  3 12:17:28.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7134" for this suite. @ 06/03/23 12:17:28.702
  STEP: Destroying namespace "secret-namespace-8709" for this suite. @ 06/03/23 12:17:28.709
• [4.149 seconds]
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 06/03/23 12:17:28.718
  Jun  3 12:17:28.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename taint-single-pod @ 06/03/23 12:17:28.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:17:28.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:17:28.743
  Jun  3 12:17:28.747: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 12:18:28.766: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 12:18:28.770: INFO: Starting informer...
  STEP: Starting pod... @ 06/03/23 12:18:28.77
  Jun  3 12:18:28.987: INFO: Pod is running on ip-172-31-27-193. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/03/23 12:18:28.988
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/03/23 12:18:29.002
  STEP: Waiting short time to make sure Pod is queued for deletion @ 06/03/23 12:18:29.006
  Jun  3 12:18:29.006: INFO: Pod wasn't evicted. Proceeding
  Jun  3 12:18:29.006: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/03/23 12:18:29.02
  STEP: Waiting some time to make sure that toleration time passed. @ 06/03/23 12:18:29.027
  Jun  3 12:19:44.028: INFO: Pod wasn't evicted. Test successful
  Jun  3 12:19:44.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-6006" for this suite. @ 06/03/23 12:19:44.035
• [135.324 seconds]
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 06/03/23 12:19:44.042
  Jun  3 12:19:44.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:19:44.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:19:44.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:19:44.07
  STEP: Creating a pod to test downward api env vars @ 06/03/23 12:19:44.075
  STEP: Saw pod success @ 06/03/23 12:19:48.105
  Jun  3 12:19:48.110: INFO: Trying to get logs from node ip-172-31-27-193 pod downward-api-d718372e-b86d-42e2-8892-2623ae9f960e container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:19:48.135
  Jun  3 12:19:48.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5267" for this suite. @ 06/03/23 12:19:48.159
• [4.126 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 06/03/23 12:19:48.169
  Jun  3 12:19:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:19:48.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:19:48.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:19:48.197
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:19:48.203
  STEP: Saw pod success @ 06/03/23 12:19:52.234
  Jun  3 12:19:52.239: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-bad7d7a8-443e-4a09-952c-019613d24f03 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:19:52.247
  Jun  3 12:19:52.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4686" for this suite. @ 06/03/23 12:19:52.271
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 06/03/23 12:19:52.282
  Jun  3 12:19:52.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename endpointslice @ 06/03/23 12:19:52.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:19:52.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:19:52.318
  STEP: referencing a single matching pod @ 06/03/23 12:19:57.419
  STEP: referencing matching pods with named port @ 06/03/23 12:20:02.429
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 06/03/23 12:20:07.438
  STEP: recreating EndpointSlices after they've been deleted @ 06/03/23 12:20:12.446
  Jun  3 12:20:12.468: INFO: EndpointSlice for Service endpointslice-7407/example-named-port not found
  Jun  3 12:20:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7407" for this suite. @ 06/03/23 12:20:22.483
• [30.209 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 06/03/23 12:20:22.493
  Jun  3 12:20:22.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption @ 06/03/23 12:20:22.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:20:22.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:20:22.517
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:20:22.526
  STEP: Updating PodDisruptionBudget status @ 06/03/23 12:20:24.534
  STEP: Waiting for all pods to be running @ 06/03/23 12:20:24.544
  Jun  3 12:20:24.548: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 06/03/23 12:20:26.553
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:20:26.564
  STEP: Patching PodDisruptionBudget status @ 06/03/23 12:20:26.575
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:20:26.587
  Jun  3 12:20:26.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3390" for this suite. @ 06/03/23 12:20:26.598
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 06/03/23 12:20:26.607
  Jun  3 12:20:26.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:20:26.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:20:26.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:20:26.629
  STEP: Creating pod busybox-dfb92f99-0719-41ce-883c-f2887bca3e1e in namespace container-probe-6918 @ 06/03/23 12:20:26.633
  Jun  3 12:20:28.653: INFO: Started pod busybox-dfb92f99-0719-41ce-883c-f2887bca3e1e in namespace container-probe-6918
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:20:28.654
  Jun  3 12:20:28.658: INFO: Initial restart count of pod busybox-dfb92f99-0719-41ce-883c-f2887bca3e1e is 0
  Jun  3 12:24:29.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:24:29.254
  STEP: Destroying namespace "container-probe-6918" for this suite. @ 06/03/23 12:24:29.268
• [242.670 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 06/03/23 12:24:29.282
  Jun  3 12:24:29.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:24:29.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:29.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:29.307
  STEP: Creating a pod to test downward api env vars @ 06/03/23 12:24:29.316
  STEP: Saw pod success @ 06/03/23 12:24:33.344
  Jun  3 12:24:33.347: INFO: Trying to get logs from node ip-172-31-27-193 pod downward-api-cc5d95ba-e400-4aec-9688-c79cd9066319 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:24:33.373
  Jun  3 12:24:33.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5324" for this suite. @ 06/03/23 12:24:33.395
• [4.119 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 06/03/23 12:24:33.401
  Jun  3 12:24:33.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:24:33.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:33.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:33.428
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4425 @ 06/03/23 12:24:33.433
  STEP: changing the ExternalName service to type=NodePort @ 06/03/23 12:24:33.441
  STEP: creating replication controller externalname-service in namespace services-4425 @ 06/03/23 12:24:33.46
  I0603 12:24:33.468617      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4425, replica count: 2
  I0603 12:24:36.520684      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 12:24:36.520: INFO: Creating new exec pod
  Jun  3 12:24:39.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:39.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:39.712: INFO: stdout: ""
  Jun  3 12:24:40.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:40.875: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:40.875: INFO: stdout: ""
  Jun  3 12:24:41.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:41.868: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:41.868: INFO: stdout: ""
  Jun  3 12:24:42.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:42.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:42.887: INFO: stdout: ""
  Jun  3 12:24:43.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:43.870: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:43.870: INFO: stdout: ""
  Jun  3 12:24:44.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:24:44.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:44.871: INFO: stdout: "externalname-service-64qhv"
  Jun  3 12:24:44.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jun  3 12:24:45.021: INFO: stderr: "+ nc -v -t -w 2 10.152.183.206 80\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun  3 12:24:45.021: INFO: stdout: ""
  Jun  3 12:24:46.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jun  3 12:24:46.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.206 80\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:46.171: INFO: stdout: ""
  Jun  3 12:24:47.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jun  3 12:24:47.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.206 80\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:47.178: INFO: stdout: ""
  Jun  3 12:24:48.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.206 80'
  Jun  3 12:24:48.178: INFO: stderr: "+ nc -v -t -w 2 10.152.183.206 80\n+ echo hostName\nConnection to 10.152.183.206 80 port [tcp/http] succeeded!\n"
  Jun  3 12:24:48.178: INFO: stdout: "externalname-service-64qhv"
  Jun  3 12:24:48.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.85.85 30102'
  Jun  3 12:24:48.347: INFO: stderr: "+ nc -v -t -w 2 172.31.85.85 30102\n+ echo hostName\nConnection to 172.31.85.85 30102 port [tcp/*] succeeded!\n"
  Jun  3 12:24:48.347: INFO: stdout: "externalname-service-64qhv"
  Jun  3 12:24:48.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4425 exec execpodc2k2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.203 30102'
  Jun  3 12:24:48.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.203 30102\nConnection to 172.31.7.203 30102 port [tcp/*] succeeded!\n"
  Jun  3 12:24:48.511: INFO: stdout: "externalname-service-64qhv"
  Jun  3 12:24:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:24:48.516: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-4425" for this suite. @ 06/03/23 12:24:48.537
• [15.143 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 06/03/23 12:24:48.545
  Jun  3 12:24:48.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context-test @ 06/03/23 12:24:48.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:48.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:48.568
  Jun  3 12:24:52.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-800" for this suite. @ 06/03/23 12:24:52.611
• [4.073 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 06/03/23 12:24:52.619
  Jun  3 12:24:52.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:24:52.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:52.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:52.643
  STEP: Creating secret with name secret-test-map-fc58e847-a6a2-47f0-926e-5bd0e70edd48 @ 06/03/23 12:24:52.647
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:24:52.652
  STEP: Saw pod success @ 06/03/23 12:24:56.677
  Jun  3 12:24:56.681: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-2e7396c7-8ed6-4474-936c-51a01a1458f5 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:24:56.689
  Jun  3 12:24:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7483" for this suite. @ 06/03/23 12:24:56.711
• [4.101 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 06/03/23 12:24:56.721
  Jun  3 12:24:56.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 12:24:56.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:56.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:56.745
  STEP: Creating namespace "e2e-ns-w5wsr" @ 06/03/23 12:24:56.75
  Jun  3 12:24:56.769: INFO: Namespace "e2e-ns-w5wsr-3625" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-w5wsr-3625" @ 06/03/23 12:24:56.769
  Jun  3 12:24:56.780: INFO: Namespace "e2e-ns-w5wsr-3625" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-w5wsr-3625" @ 06/03/23 12:24:56.78
  Jun  3 12:24:56.789: INFO: Namespace "e2e-ns-w5wsr-3625" has []v1.FinalizerName{"kubernetes"}
  Jun  3 12:24:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8014" for this suite. @ 06/03/23 12:24:56.794
  STEP: Destroying namespace "e2e-ns-w5wsr-3625" for this suite. @ 06/03/23 12:24:56.801
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 06/03/23 12:24:56.812
  Jun  3 12:24:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:24:56.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:24:56.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:24:56.836
  STEP: Creating projection with secret that has name projected-secret-test-c246138b-fed0-46f9-af56-08bc920f1d01 @ 06/03/23 12:24:56.842
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:24:56.847
  STEP: Saw pod success @ 06/03/23 12:25:00.871
  Jun  3 12:25:00.875: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-secrets-bc7f2549-2b1d-4d37-99d5-5314adeeac76 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:25:00.882
  Jun  3 12:25:00.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6666" for this suite. @ 06/03/23 12:25:00.904
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 06/03/23 12:25:00.913
  Jun  3 12:25:00.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/03/23 12:25:00.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:00.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:00.945
  STEP: create the container to handle the HTTPGet hook request. @ 06/03/23 12:25:00.962
  STEP: create the pod with lifecycle hook @ 06/03/23 12:25:02.994
  STEP: check poststart hook @ 06/03/23 12:25:05.02
  STEP: delete the pod with lifecycle hook @ 06/03/23 12:25:05.045
  Jun  3 12:25:07.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2267" for this suite. @ 06/03/23 12:25:07.071
• [6.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 06/03/23 12:25:07.085
  Jun  3 12:25:07.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename csistoragecapacity @ 06/03/23 12:25:07.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:07.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:07.115
  STEP: getting /apis @ 06/03/23 12:25:07.121
  STEP: getting /apis/storage.k8s.io @ 06/03/23 12:25:07.133
  STEP: getting /apis/storage.k8s.io/v1 @ 06/03/23 12:25:07.136
  STEP: creating @ 06/03/23 12:25:07.138
  STEP: watching @ 06/03/23 12:25:07.162
  Jun  3 12:25:07.162: INFO: starting watch
  STEP: getting @ 06/03/23 12:25:07.175
  STEP: listing in namespace @ 06/03/23 12:25:07.18
  STEP: listing across namespaces @ 06/03/23 12:25:07.186
  STEP: patching @ 06/03/23 12:25:07.191
  STEP: updating @ 06/03/23 12:25:07.199
  Jun  3 12:25:07.207: INFO: waiting for watch events with expected annotations in namespace
  Jun  3 12:25:07.207: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 06/03/23 12:25:07.207
  STEP: deleting a collection @ 06/03/23 12:25:07.227
  Jun  3 12:25:07.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-8221" for this suite. @ 06/03/23 12:25:07.259
• [0.183 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 06/03/23 12:25:07.269
  Jun  3 12:25:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename proxy @ 06/03/23 12:25:07.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:07.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:07.298
  STEP: starting an echo server on multiple ports @ 06/03/23 12:25:07.318
  STEP: creating replication controller proxy-service-hwl4w in namespace proxy-2629 @ 06/03/23 12:25:07.318
  I0603 12:25:07.335329      18 runners.go:194] Created replication controller with name: proxy-service-hwl4w, namespace: proxy-2629, replica count: 1
  I0603 12:25:08.386897      18 runners.go:194] proxy-service-hwl4w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0603 12:25:09.387983      18 runners.go:194] proxy-service-hwl4w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 12:25:09.392: INFO: setup took 2.089177834s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 06/03/23 12:25:09.392
  Jun  3 12:25:09.414: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 21.544703ms)
  Jun  3 12:25:09.416: INFO: (0) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 23.63941ms)
  Jun  3 12:25:09.418: INFO: (0) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 25.47197ms)
  Jun  3 12:25:09.418: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 25.823351ms)
  Jun  3 12:25:09.420: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 27.456653ms)
  Jun  3 12:25:09.421: INFO: (0) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 28.157385ms)
  Jun  3 12:25:09.421: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 28.656862ms)
  Jun  3 12:25:09.421: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 28.768696ms)
  Jun  3 12:25:09.421: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 29.106876ms)
  Jun  3 12:25:09.422: INFO: (0) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 29.431957ms)
  Jun  3 12:25:09.423: INFO: (0) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 30.130679ms)
  Jun  3 12:25:09.423: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 30.256943ms)
  Jun  3 12:25:09.423: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 30.706288ms)
  Jun  3 12:25:09.423: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 30.640746ms)
  Jun  3 12:25:09.424: INFO: (0) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 31.09979ms)
  Jun  3 12:25:09.424: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 32.32609ms)
  Jun  3 12:25:09.440: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 14.842787ms)
  Jun  3 12:25:09.440: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 15.680084ms)
  Jun  3 12:25:09.441: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 15.890731ms)
  Jun  3 12:25:09.441: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 16.012905ms)
  Jun  3 12:25:09.441: INFO: (1) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 16.367247ms)
  Jun  3 12:25:09.443: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.176374ms)
  Jun  3 12:25:09.444: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.222149ms)
  Jun  3 12:25:09.445: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 19.58165ms)
  Jun  3 12:25:09.445: INFO: (1) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 20.321664ms)
  Jun  3 12:25:09.446: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 20.927743ms)
  Jun  3 12:25:09.446: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 20.83935ms)
  Jun  3 12:25:09.448: INFO: (1) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 23.180155ms)
  Jun  3 12:25:09.448: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 23.344861ms)
  Jun  3 12:25:09.448: INFO: (1) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 23.196406ms)
  Jun  3 12:25:09.448: INFO: (1) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 23.388392ms)
  Jun  3 12:25:09.449: INFO: (1) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 23.908249ms)
  Jun  3 12:25:09.465: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 15.349084ms)
  Jun  3 12:25:09.465: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 15.429766ms)
  Jun  3 12:25:09.465: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 15.714305ms)
  Jun  3 12:25:09.465: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 15.523859ms)
  Jun  3 12:25:09.468: INFO: (2) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 17.868965ms)
  Jun  3 12:25:09.468: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 18.237847ms)
  Jun  3 12:25:09.468: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 18.289248ms)
  Jun  3 12:25:09.468: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 18.279448ms)
  Jun  3 12:25:09.470: INFO: (2) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 20.383965ms)
  Jun  3 12:25:09.470: INFO: (2) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 21.348867ms)
  Jun  3 12:25:09.473: INFO: (2) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 23.800266ms)
  Jun  3 12:25:09.473: INFO: (2) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 24.21734ms)
  Jun  3 12:25:09.474: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 24.004732ms)
  Jun  3 12:25:09.474: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 24.53509ms)
  Jun  3 12:25:09.474: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 24.771857ms)
  Jun  3 12:25:09.475: INFO: (2) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 25.059896ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 16.14313ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 16.968876ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 17.189673ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 16.726308ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 16.847462ms)
  Jun  3 12:25:09.492: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 17.292036ms)
  Jun  3 12:25:09.495: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 19.758795ms)
  Jun  3 12:25:09.495: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 19.666842ms)
  Jun  3 12:25:09.496: INFO: (3) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 20.674175ms)
  Jun  3 12:25:09.496: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 20.084576ms)
  Jun  3 12:25:09.498: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 22.793113ms)
  Jun  3 12:25:09.498: INFO: (3) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 22.970439ms)
  Jun  3 12:25:09.499: INFO: (3) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 23.652851ms)
  Jun  3 12:25:09.500: INFO: (3) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 24.041643ms)
  Jun  3 12:25:09.500: INFO: (3) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 24.276151ms)
  Jun  3 12:25:09.500: INFO: (3) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 24.333833ms)
  Jun  3 12:25:09.514: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 14.165796ms)
  Jun  3 12:25:09.515: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 14.317311ms)
  Jun  3 12:25:09.519: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 19.037473ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 19.323922ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.446756ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 20.163688ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 19.623472ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.860229ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 20.203501ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 20.139458ms)
  Jun  3 12:25:09.520: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 19.740326ms)
  Jun  3 12:25:09.521: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 20.199401ms)
  Jun  3 12:25:09.522: INFO: (4) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 21.634767ms)
  Jun  3 12:25:09.524: INFO: (4) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 23.818837ms)
  Jun  3 12:25:09.524: INFO: (4) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 23.968781ms)
  Jun  3 12:25:09.524: INFO: (4) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 24.278591ms)
  Jun  3 12:25:09.543: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 18.548066ms)
  Jun  3 12:25:09.544: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 18.6372ms)
  Jun  3 12:25:09.551: INFO: (5) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 26.322017ms)
  Jun  3 12:25:09.552: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 27.607709ms)
  Jun  3 12:25:09.556: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 30.48099ms)
  Jun  3 12:25:09.556: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 30.955746ms)
  Jun  3 12:25:09.556: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 31.489493ms)
  Jun  3 12:25:09.556: INFO: (5) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 31.556205ms)
  Jun  3 12:25:09.557: INFO: (5) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 32.132023ms)
  Jun  3 12:25:09.557: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 32.089762ms)
  Jun  3 12:25:09.557: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 31.990429ms)
  Jun  3 12:25:09.557: INFO: (5) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 32.199725ms)
  Jun  3 12:25:09.557: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 32.426683ms)
  Jun  3 12:25:09.558: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 33.549329ms)
  Jun  3 12:25:09.558: INFO: (5) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 33.24403ms)
  Jun  3 12:25:09.559: INFO: (5) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 33.606471ms)
  Jun  3 12:25:09.571: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 12.560364ms)
  Jun  3 12:25:09.579: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.562359ms)
  Jun  3 12:25:09.580: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 20.551091ms)
  Jun  3 12:25:09.581: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 21.319266ms)
  Jun  3 12:25:09.581: INFO: (6) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 21.476481ms)
  Jun  3 12:25:09.581: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 21.74991ms)
  Jun  3 12:25:09.582: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 21.795171ms)
  Jun  3 12:25:09.582: INFO: (6) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 22.602927ms)
  Jun  3 12:25:09.582: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 22.441092ms)
  Jun  3 12:25:09.584: INFO: (6) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 24.471938ms)
  Jun  3 12:25:09.584: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 25.033446ms)
  Jun  3 12:25:09.584: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 24.877121ms)
  Jun  3 12:25:09.585: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 25.836561ms)
  Jun  3 12:25:09.585: INFO: (6) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 25.640925ms)
  Jun  3 12:25:09.586: INFO: (6) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 26.541614ms)
  Jun  3 12:25:09.586: INFO: (6) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 26.605366ms)
  Jun  3 12:25:09.597: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 10.494418ms)
  Jun  3 12:25:09.598: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 11.577152ms)
  Jun  3 12:25:09.599: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 12.077259ms)
  Jun  3 12:25:09.599: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 12.003796ms)
  Jun  3 12:25:09.603: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 16.095288ms)
  Jun  3 12:25:09.603: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 16.244472ms)
  Jun  3 12:25:09.603: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 16.082667ms)
  Jun  3 12:25:09.603: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 16.545302ms)
  Jun  3 12:25:09.603: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 16.439459ms)
  Jun  3 12:25:09.604: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 17.287706ms)
  Jun  3 12:25:09.604: INFO: (7) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 16.967526ms)
  Jun  3 12:25:09.606: INFO: (7) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 19.216878ms)
  Jun  3 12:25:09.606: INFO: (7) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 18.95867ms)
  Jun  3 12:25:09.606: INFO: (7) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 19.434455ms)
  Jun  3 12:25:09.607: INFO: (7) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 19.705754ms)
  Jun  3 12:25:09.607: INFO: (7) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 20.152768ms)
  Jun  3 12:25:09.621: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 14.141605ms)
  Jun  3 12:25:09.621: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 13.788063ms)
  Jun  3 12:25:09.626: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 17.933677ms)
  Jun  3 12:25:09.626: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 17.866034ms)
  Jun  3 12:25:09.627: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.549967ms)
  Jun  3 12:25:09.644: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 35.610086ms)
  Jun  3 12:25:09.646: INFO: (8) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 38.665814ms)
  Jun  3 12:25:09.648: INFO: (8) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 40.265966ms)
  Jun  3 12:25:09.651: INFO: (8) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 43.632834ms)
  Jun  3 12:25:09.652: INFO: (8) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 44.377987ms)
  Jun  3 12:25:09.654: INFO: (8) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 46.918269ms)
  Jun  3 12:25:09.654: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 46.157624ms)
  Jun  3 12:25:09.655: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 46.94076ms)
  Jun  3 12:25:09.655: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 46.489055ms)
  Jun  3 12:25:09.655: INFO: (8) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 47.393675ms)
  Jun  3 12:25:09.656: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 49.025287ms)
  Jun  3 12:25:09.705: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 48.286804ms)
  Jun  3 12:25:09.731: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 73.933329ms)
  Jun  3 12:25:09.734: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 77.71214ms)
  Jun  3 12:25:09.741: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 84.743127ms)
  Jun  3 12:25:09.741: INFO: (9) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 83.775665ms)
  Jun  3 12:25:09.765: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 107.921963ms)
  Jun  3 12:25:09.767: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 109.885076ms)
  Jun  3 12:25:09.768: INFO: (9) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 110.728903ms)
  Jun  3 12:25:09.768: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 110.917329ms)
  Jun  3 12:25:09.768: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 111.629481ms)
  Jun  3 12:25:09.768: INFO: (9) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 111.852079ms)
  Jun  3 12:25:09.768: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 111.58857ms)
  Jun  3 12:25:09.769: INFO: (9) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 111.814788ms)
  Jun  3 12:25:09.769: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 111.928552ms)
  Jun  3 12:25:09.769: INFO: (9) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 112.144318ms)
  Jun  3 12:25:09.769: INFO: (9) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 112.373796ms)
  Jun  3 12:25:09.829: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 59.483983ms)
  Jun  3 12:25:09.829: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 59.36525ms)
  Jun  3 12:25:09.853: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 82.952389ms)
  Jun  3 12:25:09.853: INFO: (10) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 83.346092ms)
  Jun  3 12:25:09.853: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 83.546897ms)
  Jun  3 12:25:09.853: INFO: (10) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 83.235408ms)
  Jun  3 12:25:09.854: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 83.828657ms)
  Jun  3 12:25:09.863: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 92.663451ms)
  Jun  3 12:25:09.868: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 98.018483ms)
  Jun  3 12:25:09.868: INFO: (10) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 98.109376ms)
  Jun  3 12:25:09.868: INFO: (10) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 98.20868ms)
  Jun  3 12:25:09.869: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 99.062957ms)
  Jun  3 12:25:09.870: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 99.452189ms)
  Jun  3 12:25:09.870: INFO: (10) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 100.045029ms)
  Jun  3 12:25:09.871: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 100.682889ms)
  Jun  3 12:25:09.884: INFO: (10) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 113.071278ms)
  Jun  3 12:25:09.896: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 12.349777ms)
  Jun  3 12:25:09.896: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 12.13449ms)
  Jun  3 12:25:09.902: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.130283ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 21.44929ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 21.77024ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 22.303218ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 22.083001ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 22.39566ms)
  Jun  3 12:25:09.906: INFO: (11) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 22.179883ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 22.335528ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 22.428461ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 22.254746ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 22.173313ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 22.37997ms)
  Jun  3 12:25:09.907: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 22.516145ms)
  Jun  3 12:25:09.908: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 23.30731ms)
  Jun  3 12:25:09.923: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 14.753275ms)
  Jun  3 12:25:09.923: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 15.612462ms)
  Jun  3 12:25:09.924: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 15.510268ms)
  Jun  3 12:25:09.924: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 15.908302ms)
  Jun  3 12:25:09.925: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 16.210321ms)
  Jun  3 12:25:09.925: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 16.041156ms)
  Jun  3 12:25:09.927: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 18.255368ms)
  Jun  3 12:25:09.927: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 19.167537ms)
  Jun  3 12:25:09.927: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 19.315042ms)
  Jun  3 12:25:09.927: INFO: (12) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 18.781774ms)
  Jun  3 12:25:09.929: INFO: (12) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 20.294683ms)
  Jun  3 12:25:09.931: INFO: (12) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 22.334018ms)
  Jun  3 12:25:09.931: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 22.742672ms)
  Jun  3 12:25:09.931: INFO: (12) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 22.808544ms)
  Jun  3 12:25:09.933: INFO: (12) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 24.689704ms)
  Jun  3 12:25:09.933: INFO: (12) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 24.86215ms)
  Jun  3 12:25:09.942: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 8.642109ms)
  Jun  3 12:25:09.957: INFO: (13) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 22.839985ms)
  Jun  3 12:25:09.957: INFO: (13) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 23.01125ms)
  Jun  3 12:25:09.957: INFO: (13) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 23.198037ms)
  Jun  3 12:25:09.957: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 23.282159ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 23.354351ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 23.403403ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 23.548168ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 23.62744ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 23.688292ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 23.755484ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 23.94093ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 24.104296ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 23.882958ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 23.983341ms)
  Jun  3 12:25:09.958: INFO: (13) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 23.460005ms)
  Jun  3 12:25:09.968: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 8.528324ms)
  Jun  3 12:25:09.969: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 9.93918ms)
  Jun  3 12:25:09.969: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 9.65342ms)
  Jun  3 12:25:09.969: INFO: (14) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 9.922939ms)
  Jun  3 12:25:09.973: INFO: (14) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 13.240706ms)
  Jun  3 12:25:09.974: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 14.403093ms)
  Jun  3 12:25:09.976: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 16.369307ms)
  Jun  3 12:25:09.976: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 16.722518ms)
  Jun  3 12:25:09.977: INFO: (14) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 17.818943ms)
  Jun  3 12:25:09.978: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 18.179025ms)
  Jun  3 12:25:09.978: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 18.461344ms)
  Jun  3 12:25:09.978: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.608169ms)
  Jun  3 12:25:09.979: INFO: (14) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 19.394874ms)
  Jun  3 12:25:09.979: INFO: (14) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 19.339252ms)
  Jun  3 12:25:09.980: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 20.398946ms)
  Jun  3 12:25:09.981: INFO: (14) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 20.894453ms)
  Jun  3 12:25:09.996: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 15.311572ms)
  Jun  3 12:25:09.996: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 15.53523ms)
  Jun  3 12:25:09.999: INFO: (15) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 17.120981ms)
  Jun  3 12:25:09.999: INFO: (15) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 17.561205ms)
  Jun  3 12:25:09.999: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 17.41454ms)
  Jun  3 12:25:10.001: INFO: (15) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 19.408584ms)
  Jun  3 12:25:10.002: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 21.14109ms)
  Jun  3 12:25:10.002: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 21.685647ms)
  Jun  3 12:25:10.002: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 21.212233ms)
  Jun  3 12:25:10.003: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 21.651886ms)
  Jun  3 12:25:10.003: INFO: (15) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 22.099741ms)
  Jun  3 12:25:10.003: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 22.043809ms)
  Jun  3 12:25:10.004: INFO: (15) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 22.272547ms)
  Jun  3 12:25:10.004: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 22.327518ms)
  Jun  3 12:25:10.004: INFO: (15) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 22.854065ms)
  Jun  3 12:25:10.004: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 22.946128ms)
  Jun  3 12:25:10.017: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 12.555294ms)
  Jun  3 12:25:10.023: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.368421ms)
  Jun  3 12:25:10.025: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 20.320944ms)
  Jun  3 12:25:10.026: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 20.888372ms)
  Jun  3 12:25:10.026: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 21.020797ms)
  Jun  3 12:25:10.026: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 21.46246ms)
  Jun  3 12:25:10.026: INFO: (16) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 22.250046ms)
  Jun  3 12:25:10.027: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 22.191164ms)
  Jun  3 12:25:10.027: INFO: (16) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 22.253696ms)
  Jun  3 12:25:10.027: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 22.858965ms)
  Jun  3 12:25:10.028: INFO: (16) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 22.938148ms)
  Jun  3 12:25:10.027: INFO: (16) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 22.611637ms)
  Jun  3 12:25:10.027: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 22.871205ms)
  Jun  3 12:25:10.028: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 22.67913ms)
  Jun  3 12:25:10.029: INFO: (16) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 24.050513ms)
  Jun  3 12:25:10.029: INFO: (16) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 23.926209ms)
  Jun  3 12:25:10.036: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 6.955734ms)
  Jun  3 12:25:10.042: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 12.998889ms)
  Jun  3 12:25:10.042: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 13.207535ms)
  Jun  3 12:25:10.043: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 13.413991ms)
  Jun  3 12:25:10.044: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 14.724914ms)
  Jun  3 12:25:10.045: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 15.047694ms)
  Jun  3 12:25:10.047: INFO: (17) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 18.02244ms)
  Jun  3 12:25:10.048: INFO: (17) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 18.173265ms)
  Jun  3 12:25:10.048: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 17.908107ms)
  Jun  3 12:25:10.052: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 22.66346ms)
  Jun  3 12:25:10.053: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 22.766603ms)
  Jun  3 12:25:10.053: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 22.741512ms)
  Jun  3 12:25:10.053: INFO: (17) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 23.168705ms)
  Jun  3 12:25:10.053: INFO: (17) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 23.324991ms)
  Jun  3 12:25:10.053: INFO: (17) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 23.172816ms)
  Jun  3 12:25:10.054: INFO: (17) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 23.93319ms)
  Jun  3 12:25:10.063: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 9.143295ms)
  Jun  3 12:25:10.067: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 11.48165ms)
  Jun  3 12:25:10.074: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.856887ms)
  Jun  3 12:25:10.074: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 19.075533ms)
  Jun  3 12:25:10.074: INFO: (18) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 19.324661ms)
  Jun  3 12:25:10.074: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.702223ms)
  Jun  3 12:25:10.074: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 19.952632ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 20.100806ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 20.417257ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 20.385216ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 20.50662ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 20.414017ms)
  Jun  3 12:25:10.075: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 21.085038ms)
  Jun  3 12:25:10.076: INFO: (18) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 21.058947ms)
  Jun  3 12:25:10.078: INFO: (18) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 23.676931ms)
  Jun  3 12:25:10.078: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 23.160975ms)
  Jun  3 12:25:10.091: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq/proxy/rewriteme">test</a> (200; 12.876354ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">... (200; 16.532652ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:460/proxy/: tls baz (200; 16.814781ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 16.426838ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:443/proxy/tlsrewritem... (200; 15.345023ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname2/proxy/: bar (200; 15.379305ms)
  Jun  3 12:25:10.095: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-hwl4w-58jtq:462/proxy/: tls qux (200; 15.694464ms)
  Jun  3 12:25:10.097: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:1080/proxy/rewriteme">test<... (200; 18.65751ms)
  Jun  3 12:25:10.097: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 18.536357ms)
  Jun  3 12:25:10.097: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:160/proxy/: foo (200; 18.070022ms)
  Jun  3 12:25:10.098: INFO: (19) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname1/proxy/: tls baz (200; 18.865777ms)
  Jun  3 12:25:10.099: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-hwl4w-58jtq:162/proxy/: bar (200; 19.912321ms)
  Jun  3 12:25:10.100: INFO: (19) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname2/proxy/: bar (200; 21.639997ms)
  Jun  3 12:25:10.100: INFO: (19) /api/v1/namespaces/proxy-2629/services/proxy-service-hwl4w:portname1/proxy/: foo (200; 21.806672ms)
  Jun  3 12:25:10.102: INFO: (19) /api/v1/namespaces/proxy-2629/services/http:proxy-service-hwl4w:portname1/proxy/: foo (200; 23.228477ms)
  Jun  3 12:25:10.103: INFO: (19) /api/v1/namespaces/proxy-2629/services/https:proxy-service-hwl4w:tlsportname2/proxy/: tls qux (200; 23.62238ms)
  Jun  3 12:25:10.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-hwl4w in namespace proxy-2629, will wait for the garbage collector to delete the pods @ 06/03/23 12:25:10.109
  Jun  3 12:25:10.175: INFO: Deleting ReplicationController proxy-service-hwl4w took: 9.733933ms
  Jun  3 12:25:10.275: INFO: Terminating ReplicationController proxy-service-hwl4w pods took: 100.170773ms
  STEP: Destroying namespace "proxy-2629" for this suite. @ 06/03/23 12:25:13.177
• [5.915 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 06/03/23 12:25:13.184
  Jun  3 12:25:13.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subpath @ 06/03/23 12:25:13.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:13.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:13.207
  STEP: Setting up data @ 06/03/23 12:25:13.215
  STEP: Creating pod pod-subpath-test-downwardapi-cmmk @ 06/03/23 12:25:13.228
  STEP: Creating a pod to test atomic-volume-subpath @ 06/03/23 12:25:13.228
  STEP: Saw pod success @ 06/03/23 12:25:37.307
  Jun  3 12:25:37.311: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-subpath-test-downwardapi-cmmk container test-container-subpath-downwardapi-cmmk: <nil>
  STEP: delete the pod @ 06/03/23 12:25:37.319
  STEP: Deleting pod pod-subpath-test-downwardapi-cmmk @ 06/03/23 12:25:37.337
  Jun  3 12:25:37.337: INFO: Deleting pod "pod-subpath-test-downwardapi-cmmk" in namespace "subpath-5107"
  Jun  3 12:25:37.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5107" for this suite. @ 06/03/23 12:25:37.346
• [24.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 06/03/23 12:25:37.357
  Jun  3 12:25:37.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/03/23 12:25:37.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:37.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:37.381
  STEP: fetching the /apis discovery document @ 06/03/23 12:25:37.386
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 06/03/23 12:25:37.388
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 06/03/23 12:25:37.388
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 06/03/23 12:25:37.388
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 06/03/23 12:25:37.39
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 06/03/23 12:25:37.39
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 06/03/23 12:25:37.392
  Jun  3 12:25:37.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-646" for this suite. @ 06/03/23 12:25:37.397
• [0.048 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 06/03/23 12:25:37.405
  Jun  3 12:25:37.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:25:37.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:37.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:37.43
  STEP: creating the pod @ 06/03/23 12:25:37.434
  STEP: submitting the pod to kubernetes @ 06/03/23 12:25:37.435
  STEP: verifying the pod is in kubernetes @ 06/03/23 12:25:39.46
  STEP: updating the pod @ 06/03/23 12:25:39.464
  Jun  3 12:25:39.978: INFO: Successfully updated pod "pod-update-eb0bcd88-1a3c-4440-b881-c6a0319c301f"
  STEP: verifying the updated pod is in kubernetes @ 06/03/23 12:25:39.982
  Jun  3 12:25:39.990: INFO: Pod update OK
  Jun  3 12:25:39.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-193" for this suite. @ 06/03/23 12:25:39.996
• [2.599 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 06/03/23 12:25:40.006
  Jun  3 12:25:40.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:25:40.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:40.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:40.031
  STEP: Setting up server cert @ 06/03/23 12:25:40.062
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:25:41.076
  STEP: Deploying the webhook pod @ 06/03/23 12:25:41.084
  STEP: Wait for the deployment to be ready @ 06/03/23 12:25:41.096
  Jun  3 12:25:41.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:25:43.117
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:25:43.128
  Jun  3 12:25:44.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/03/23 12:25:44.197
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/03/23 12:25:44.247
  STEP: Deleting the collection of validation webhooks @ 06/03/23 12:25:44.293
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/03/23 12:25:44.341
  Jun  3 12:25:44.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9107" for this suite. @ 06/03/23 12:25:44.397
  STEP: Destroying namespace "webhook-markers-5170" for this suite. @ 06/03/23 12:25:44.404
• [4.410 seconds]
------------------------------
SS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 06/03/23 12:25:44.416
  Jun  3 12:25:44.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:25:44.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:44.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:44.441
  STEP: creating a secret @ 06/03/23 12:25:44.446
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 06/03/23 12:25:44.452
  STEP: patching the secret @ 06/03/23 12:25:44.456
  STEP: deleting the secret using a LabelSelector @ 06/03/23 12:25:44.465
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 06/03/23 12:25:44.474
  Jun  3 12:25:44.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4989" for this suite. @ 06/03/23 12:25:44.484
• [0.075 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 06/03/23 12:25:44.492
  Jun  3 12:25:44.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename proxy @ 06/03/23 12:25:44.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:44.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:44.518
  Jun  3 12:25:44.522: INFO: Creating pod...
  Jun  3 12:25:46.541: INFO: Creating service...
  Jun  3 12:25:46.552: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=DELETE
  Jun  3 12:25:46.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun  3 12:25:46.559: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=OPTIONS
  Jun  3 12:25:46.565: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun  3 12:25:46.565: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=PATCH
  Jun  3 12:25:46.569: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun  3 12:25:46.569: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=POST
  Jun  3 12:25:46.573: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun  3 12:25:46.573: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=PUT
  Jun  3 12:25:46.577: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun  3 12:25:46.577: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=DELETE
  Jun  3 12:25:46.583: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun  3 12:25:46.583: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jun  3 12:25:46.589: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun  3 12:25:46.589: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=PATCH
  Jun  3 12:25:46.595: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun  3 12:25:46.595: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=POST
  Jun  3 12:25:46.601: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun  3 12:25:46.601: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=PUT
  Jun  3 12:25:46.607: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun  3 12:25:46.608: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=GET
  Jun  3 12:25:46.611: INFO: http.Client request:GET StatusCode:301
  Jun  3 12:25:46.611: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=GET
  Jun  3 12:25:46.617: INFO: http.Client request:GET StatusCode:301
  Jun  3 12:25:46.617: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/pods/agnhost/proxy?method=HEAD
  Jun  3 12:25:46.621: INFO: http.Client request:HEAD StatusCode:301
  Jun  3 12:25:46.621: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-3354/services/e2e-proxy-test-service/proxy?method=HEAD
  Jun  3 12:25:46.626: INFO: http.Client request:HEAD StatusCode:301
  Jun  3 12:25:46.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-3354" for this suite. @ 06/03/23 12:25:46.631
• [2.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 06/03/23 12:25:46.642
  Jun  3 12:25:46.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:25:46.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:25:46.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:25:46.672
  STEP: Creating pod liveness-331917d4-7d00-436e-9ad5-09c66c802f1f in namespace container-probe-3101 @ 06/03/23 12:25:46.676
  Jun  3 12:25:48.696: INFO: Started pod liveness-331917d4-7d00-436e-9ad5-09c66c802f1f in namespace container-probe-3101
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:25:48.696
  Jun  3 12:25:48.700: INFO: Initial restart count of pod liveness-331917d4-7d00-436e-9ad5-09c66c802f1f is 0
  Jun  3 12:26:08.756: INFO: Restart count of pod container-probe-3101/liveness-331917d4-7d00-436e-9ad5-09c66c802f1f is now 1 (20.055860619s elapsed)
  Jun  3 12:26:08.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:26:08.762
  STEP: Destroying namespace "container-probe-3101" for this suite. @ 06/03/23 12:26:08.775
• [22.141 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 06/03/23 12:26:08.783
  Jun  3 12:26:08.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:26:08.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:26:08.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:26:08.807
  STEP: Counting existing ResourceQuota @ 06/03/23 12:26:08.811
  STEP: Creating a ResourceQuota @ 06/03/23 12:26:13.815
  STEP: Ensuring resource quota status is calculated @ 06/03/23 12:26:13.821
  STEP: Creating a ReplicaSet @ 06/03/23 12:26:15.827
  STEP: Ensuring resource quota status captures replicaset creation @ 06/03/23 12:26:15.84
  STEP: Deleting a ReplicaSet @ 06/03/23 12:26:17.846
  STEP: Ensuring resource quota status released usage @ 06/03/23 12:26:17.853
  Jun  3 12:26:19.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5568" for this suite. @ 06/03/23 12:26:19.864
• [11.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 06/03/23 12:26:19.882
  Jun  3 12:26:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename events @ 06/03/23 12:26:19.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:26:19.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:26:19.927
  STEP: creating a test event @ 06/03/23 12:26:19.931
  STEP: listing all events in all namespaces @ 06/03/23 12:26:19.938
  STEP: patching the test event @ 06/03/23 12:26:19.948
  STEP: fetching the test event @ 06/03/23 12:26:19.956
  STEP: updating the test event @ 06/03/23 12:26:19.96
  STEP: getting the test event @ 06/03/23 12:26:19.972
  STEP: deleting the test event @ 06/03/23 12:26:19.977
  STEP: listing all events in all namespaces @ 06/03/23 12:26:19.986
  Jun  3 12:26:19.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6467" for this suite. @ 06/03/23 12:26:20.001
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 06/03/23 12:26:20.011
  Jun  3 12:26:20.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:26:20.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:26:20.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:26:20.036
  STEP: Creating configMap with name cm-test-opt-del-74f77040-b7a0-4b56-9ac7-9cee0ea23703 @ 06/03/23 12:26:20.045
  STEP: Creating configMap with name cm-test-opt-upd-329d9c64-e6a1-4b3a-b69b-f9ebcca4ec6c @ 06/03/23 12:26:20.05
  STEP: Creating the pod @ 06/03/23 12:26:20.056
  STEP: Deleting configmap cm-test-opt-del-74f77040-b7a0-4b56-9ac7-9cee0ea23703 @ 06/03/23 12:26:22.108
  STEP: Updating configmap cm-test-opt-upd-329d9c64-e6a1-4b3a-b69b-f9ebcca4ec6c @ 06/03/23 12:26:22.115
  STEP: Creating configMap with name cm-test-opt-create-79b7abd7-d0f7-46e8-bff2-1886b27d37d3 @ 06/03/23 12:26:22.12
  STEP: waiting to observe update in volume @ 06/03/23 12:26:22.126
  Jun  3 12:27:38.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1483" for this suite. @ 06/03/23 12:27:38.534
• [78.529 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 06/03/23 12:27:38.544
  Jun  3 12:27:38.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:27:38.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:38.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:38.573
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/03/23 12:27:38.577
  Jun  3 12:27:38.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4159 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun  3 12:27:38.666: INFO: stderr: ""
  Jun  3 12:27:38.666: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 06/03/23 12:27:38.666
  Jun  3 12:27:38.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4159 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jun  3 12:27:38.765: INFO: stderr: ""
  Jun  3 12:27:38.765: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/03/23 12:27:38.765
  Jun  3 12:27:38.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4159 delete pods e2e-test-httpd-pod'
  Jun  3 12:27:40.870: INFO: stderr: ""
  Jun  3 12:27:40.870: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun  3 12:27:40.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4159" for this suite. @ 06/03/23 12:27:40.875
• [2.340 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 06/03/23 12:27:40.884
  Jun  3 12:27:40.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:27:40.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:40.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:40.906
  STEP: creating pod @ 06/03/23 12:27:40.911
  Jun  3 12:27:42.939: INFO: Pod pod-hostip-02cefbf8-1986-4950-9c06-3fbc3c4a39c2 has hostIP: 172.31.85.85
  Jun  3 12:27:42.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4228" for this suite. @ 06/03/23 12:27:42.949
• [2.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 06/03/23 12:27:42.959
  Jun  3 12:27:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:27:42.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:42.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:42.983
  Jun  3 12:27:43.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2010" for this suite. @ 06/03/23 12:27:43.04
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 06/03/23 12:27:43.051
  Jun  3 12:27:43.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 12:27:43.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:43.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:43.078
  STEP: Creating simple DaemonSet "daemon-set" @ 06/03/23 12:27:43.109
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 12:27:43.117
  Jun  3 12:27:43.121: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:43.121: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:43.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:27:43.126: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:27:44.131: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:44.131: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:44.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:27:44.135: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:27:45.133: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:45.133: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:27:45.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 12:27:45.138: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 06/03/23 12:27:45.142
  STEP: DeleteCollection of the DaemonSets @ 06/03/23 12:27:45.146
  STEP: Verify that ReplicaSets have been deleted @ 06/03/23 12:27:45.155
  Jun  3 12:27:45.167: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10742"},"items":null}

  Jun  3 12:27:45.172: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10742"},"items":[{"metadata":{"name":"daemon-set-475tx","generateName":"daemon-set-","namespace":"daemonsets-1505","uid":"10b5c11f-e5b9-4192-96e9-ac84f5f4011d","resourceVersion":"10738","creationTimestamp":"2023-06-03T12:27:43Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1c79926-0c16-401c-bfe8-5d501cfcc472","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1c79926-0c16-401c-bfe8-5d501cfcc472\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rg6tv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rg6tv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-7-203","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-7-203"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"}],"hostIP":"172.31.7.203","podIP":"192.168.192.173","podIPs":[{"ip":"192.168.192.173"}],"startTime":"2023-06-03T12:27:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-03T12:27:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7c6185f0d7d12e23867eee00879c8b678ec485f2b013a175db1637e8d5c639be","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jjxgg","generateName":"daemon-set-","namespace":"daemonsets-1505","uid":"b9e64b41-29fa-491f-a36e-d7fee6be9ab5","resourceVersion":"10735","creationTimestamp":"2023-06-03T12:27:43Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1c79926-0c16-401c-bfe8-5d501cfcc472","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1c79926-0c16-401c-bfe8-5d501cfcc472\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-w7dfp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-w7dfp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-27-193","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-27-193"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"}],"hostIP":"172.31.27.193","podIP":"192.168.118.206","podIPs":[{"ip":"192.168.118.206"}],"startTime":"2023-06-03T12:27:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-03T12:27:44Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://531d6e58c6bc8491810ec3753f4e3b6c4e9c96f72fd779dd0f1e1f555f97926b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rdfd8","generateName":"daemon-set-","namespace":"daemonsets-1505","uid":"94adfe8a-9465-471b-a51f-fd6a02a040ce","resourceVersion":"10740","creationTimestamp":"2023-06-03T12:27:43Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e1c79926-0c16-401c-bfe8-5d501cfcc472","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1c79926-0c16-401c-bfe8-5d501cfcc472\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-03T12:27:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-95bg5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-95bg5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-85-85","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-85-85"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-03T12:27:43Z"}],"hostIP":"172.31.85.85","podIP":"192.168.20.120","podIPs":[{"ip":"192.168.20.120"}],"startTime":"2023-06-03T12:27:43Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-03T12:27:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0c512300894334dfeb25b07a9c06c18def36eb238ef0d334c74ecc4d0cef4f82","started":true}],"qosClass":"BestEffort"}}]}

  Jun  3 12:27:45.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1505" for this suite. @ 06/03/23 12:27:45.198
• [2.156 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 06/03/23 12:27:45.208
  Jun  3 12:27:45.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:27:45.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:45.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:45.237
  STEP: Creating configMap with name configmap-test-volume-map-2f3f0817-7124-4be4-a162-6c6d5b057eac @ 06/03/23 12:27:45.241
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:27:45.247
  STEP: Saw pod success @ 06/03/23 12:27:49.273
  Jun  3 12:27:49.279: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-2e89ad00-4a9e-44ec-86f4-9cccdcdf6ce4 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:27:49.288
  Jun  3 12:27:49.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9077" for this suite. @ 06/03/23 12:27:49.311
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 06/03/23 12:27:49.324
  Jun  3 12:27:49.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:27:49.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:49.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:49.351
  STEP: Setting up server cert @ 06/03/23 12:27:49.385
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:27:49.804
  STEP: Deploying the webhook pod @ 06/03/23 12:27:49.814
  STEP: Wait for the deployment to be ready @ 06/03/23 12:27:49.827
  Jun  3 12:27:49.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:27:51.851
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:27:51.868
  Jun  3 12:27:52.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 06/03/23 12:27:52.874
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 06/03/23 12:27:52.875
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 06/03/23 12:27:52.875
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 06/03/23 12:27:52.875
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 06/03/23 12:27:52.877
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/03/23 12:27:52.877
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 06/03/23 12:27:52.878
  Jun  3 12:27:52.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9019" for this suite. @ 06/03/23 12:27:52.926
  STEP: Destroying namespace "webhook-markers-6130" for this suite. @ 06/03/23 12:27:52.934
• [3.620 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 06/03/23 12:27:52.946
  Jun  3 12:27:52.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:27:52.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:52.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:52.976
  STEP: Creating projection with secret that has name projected-secret-test-map-5cbcf2e7-ce62-4c07-9faf-b357320c9046 @ 06/03/23 12:27:52.98
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:27:52.987
  STEP: Saw pod success @ 06/03/23 12:27:57.017
  Jun  3 12:27:57.021: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-secrets-d71ba04d-b481-45b0-8a7b-c93a77cc083d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:27:57.03
  Jun  3 12:27:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5364" for this suite. @ 06/03/23 12:27:57.054
• [4.118 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 06/03/23 12:27:57.064
  Jun  3 12:27:57.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:27:57.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:27:57.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:27:57.086
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:27:57.091
  STEP: Saw pod success @ 06/03/23 12:28:01.118
  Jun  3 12:28:01.122: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-1a05ad30-1735-43b2-b9f8-91d208b7dccb container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:28:01.13
  Jun  3 12:28:01.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4854" for this suite. @ 06/03/23 12:28:01.152
• [4.097 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 06/03/23 12:28:01.162
  Jun  3 12:28:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:28:01.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:28:01.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:28:01.187
  STEP: Creating configMap with name projected-configmap-test-volume-48f07436-fcf8-45bb-9c9a-494cb936ee55 @ 06/03/23 12:28:01.192
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:28:01.198
  STEP: Saw pod success @ 06/03/23 12:28:05.221
  Jun  3 12:28:05.225: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-b8ad0856-8431-48c2-a711-c8aba02cc207 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:28:05.233
  Jun  3 12:28:05.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9290" for this suite. @ 06/03/23 12:28:05.257
• [4.102 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 06/03/23 12:28:05.265
  Jun  3 12:28:05.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename cronjob @ 06/03/23 12:28:05.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:28:05.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:28:05.291
  STEP: Creating a ReplaceConcurrent cronjob @ 06/03/23 12:28:05.296
  STEP: Ensuring a job is scheduled @ 06/03/23 12:28:05.303
  STEP: Ensuring exactly one is scheduled @ 06/03/23 12:29:01.309
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 06/03/23 12:29:01.313
  STEP: Ensuring the job is replaced with a new one @ 06/03/23 12:29:01.317
  STEP: Removing cronjob @ 06/03/23 12:30:01.321
  Jun  3 12:30:01.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5277" for this suite. @ 06/03/23 12:30:01.336
• [116.079 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 06/03/23 12:30:01.345
  Jun  3 12:30:01.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:30:01.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:01.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:01.373
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/03/23 12:30:01.382
  Jun  3 12:30:01.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6432 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jun  3 12:30:01.470: INFO: stderr: ""
  Jun  3 12:30:01.470: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 06/03/23 12:30:01.47
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/03/23 12:30:06.522
  Jun  3 12:30:06.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6432 get pod e2e-test-httpd-pod -o json'
  Jun  3 12:30:06.614: INFO: stderr: ""
  Jun  3 12:30:06.614: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-03T12:30:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6432\",\n        \"resourceVersion\": \"11371\",\n        \"uid\": \"b3edb132-319e-477c-9fc1-113e0576ae3d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fh2hd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-85-85\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fh2hd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-03T12:30:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-03T12:30:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-03T12:30:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-03T12:30:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1be39a98f39e92ec7fd7d9c29fd8226e9553925ea470f6acf7d4b105e260ce0d\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-03T12:30:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.85.85\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.20.121\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.20.121\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-03T12:30:01Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 06/03/23 12:30:06.615
  Jun  3 12:30:06.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6432 replace -f -'
  Jun  3 12:30:07.048: INFO: stderr: ""
  Jun  3 12:30:07.048: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 06/03/23 12:30:07.048
  Jun  3 12:30:07.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6432 delete pods e2e-test-httpd-pod'
  Jun  3 12:30:09.201: INFO: stderr: ""
  Jun  3 12:30:09.201: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun  3 12:30:09.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6432" for this suite. @ 06/03/23 12:30:09.207
• [7.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 06/03/23 12:30:09.22
  Jun  3 12:30:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:30:09.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:09.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:09.246
  STEP: creating all guestbook components @ 06/03/23 12:30:09.25
  Jun  3 12:30:09.250: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jun  3 12:30:09.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:09.601: INFO: stderr: ""
  Jun  3 12:30:09.601: INFO: stdout: "service/agnhost-replica created\n"
  Jun  3 12:30:09.601: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jun  3 12:30:09.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:09.941: INFO: stderr: ""
  Jun  3 12:30:09.941: INFO: stdout: "service/agnhost-primary created\n"
  Jun  3 12:30:09.941: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jun  3 12:30:09.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:10.260: INFO: stderr: ""
  Jun  3 12:30:10.261: INFO: stdout: "service/frontend created\n"
  Jun  3 12:30:10.261: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jun  3 12:30:10.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:11.054: INFO: stderr: ""
  Jun  3 12:30:11.054: INFO: stdout: "deployment.apps/frontend created\n"
  Jun  3 12:30:11.054: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun  3 12:30:11.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:11.364: INFO: stderr: ""
  Jun  3 12:30:11.364: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jun  3 12:30:11.364: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jun  3 12:30:11.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 create -f -'
  Jun  3 12:30:11.808: INFO: stderr: ""
  Jun  3 12:30:11.808: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 06/03/23 12:30:11.808
  Jun  3 12:30:11.808: INFO: Waiting for all frontend pods to be Running.
  Jun  3 12:30:16.858: INFO: Waiting for frontend to serve content.
  Jun  3 12:30:16.872: INFO: Trying to add a new entry to the guestbook.
  Jun  3 12:30:16.885: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 06/03/23 12:30:16.895
  Jun  3 12:30:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:16.997: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:16.997: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 06/03/23 12:30:16.997
  Jun  3 12:30:16.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:17.109: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:17.109: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/03/23 12:30:17.109
  Jun  3 12:30:17.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:17.210: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:17.210: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/03/23 12:30:17.21
  Jun  3 12:30:17.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:17.302: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:17.302: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 06/03/23 12:30:17.302
  Jun  3 12:30:17.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:17.424: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:17.424: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 06/03/23 12:30:17.424
  Jun  3 12:30:17.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2826 delete --grace-period=0 --force -f -'
  Jun  3 12:30:17.584: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 12:30:17.584: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jun  3 12:30:17.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2826" for this suite. @ 06/03/23 12:30:17.588
• [8.376 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 06/03/23 12:30:17.598
  Jun  3 12:30:17.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:30:17.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:17.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:17.632
  STEP: Creating the pod @ 06/03/23 12:30:17.637
  Jun  3 12:30:20.202: INFO: Successfully updated pod "annotationupdate3a5f7b0a-4272-4531-8dda-469608a47bf7"
  Jun  3 12:30:22.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6927" for this suite. @ 06/03/23 12:30:22.224
• [4.634 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 06/03/23 12:30:22.238
  Jun  3 12:30:22.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename ingressclass @ 06/03/23 12:30:22.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:22.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:22.264
  STEP: getting /apis @ 06/03/23 12:30:22.269
  STEP: getting /apis/networking.k8s.io @ 06/03/23 12:30:22.275
  STEP: getting /apis/networking.k8s.iov1 @ 06/03/23 12:30:22.277
  STEP: creating @ 06/03/23 12:30:22.279
  STEP: getting @ 06/03/23 12:30:22.296
  STEP: listing @ 06/03/23 12:30:22.3
  STEP: watching @ 06/03/23 12:30:22.305
  Jun  3 12:30:22.305: INFO: starting watch
  STEP: patching @ 06/03/23 12:30:22.307
  STEP: updating @ 06/03/23 12:30:22.313
  Jun  3 12:30:22.319: INFO: waiting for watch events with expected annotations
  Jun  3 12:30:22.319: INFO: saw patched and updated annotations
  STEP: deleting @ 06/03/23 12:30:22.319
  STEP: deleting a collection @ 06/03/23 12:30:22.335
  Jun  3 12:30:22.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-5096" for this suite. @ 06/03/23 12:30:22.357
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 06/03/23 12:30:22.375
  Jun  3 12:30:22.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 12:30:22.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:22.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:22.402
  STEP: Creating a pod to test substitution in container's args @ 06/03/23 12:30:22.406
  STEP: Saw pod success @ 06/03/23 12:30:26.435
  Jun  3 12:30:26.440: INFO: Trying to get logs from node ip-172-31-85-85 pod var-expansion-974985ad-6a30-4b50-a786-946eeb40cbf4 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:30:26.468
  Jun  3 12:30:26.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5743" for this suite. @ 06/03/23 12:30:26.491
• [4.123 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 06/03/23 12:30:26.498
  Jun  3 12:30:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:30:26.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:26.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:26.531
  Jun  3 12:30:26.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/03/23 12:30:28.121
  Jun  3 12:30:28.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8498 --namespace=crd-publish-openapi-8498 create -f -'
  Jun  3 12:30:28.833: INFO: stderr: ""
  Jun  3 12:30:28.834: INFO: stdout: "e2e-test-crd-publish-openapi-5977-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun  3 12:30:28.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8498 --namespace=crd-publish-openapi-8498 delete e2e-test-crd-publish-openapi-5977-crds test-cr'
  Jun  3 12:30:28.948: INFO: stderr: ""
  Jun  3 12:30:28.948: INFO: stdout: "e2e-test-crd-publish-openapi-5977-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jun  3 12:30:28.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8498 --namespace=crd-publish-openapi-8498 apply -f -'
  Jun  3 12:30:29.201: INFO: stderr: ""
  Jun  3 12:30:29.201: INFO: stdout: "e2e-test-crd-publish-openapi-5977-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jun  3 12:30:29.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8498 --namespace=crd-publish-openapi-8498 delete e2e-test-crd-publish-openapi-5977-crds test-cr'
  Jun  3 12:30:29.292: INFO: stderr: ""
  Jun  3 12:30:29.292: INFO: stdout: "e2e-test-crd-publish-openapi-5977-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/03/23 12:30:29.292
  Jun  3 12:30:29.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8498 explain e2e-test-crd-publish-openapi-5977-crds'
  Jun  3 12:30:29.516: INFO: stderr: ""
  Jun  3 12:30:29.516: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-5977-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jun  3 12:30:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8498" for this suite. @ 06/03/23 12:30:31.074
• [4.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 06/03/23 12:30:31.085
  Jun  3 12:30:31.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 12:30:31.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:31.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:31.112
  STEP: create the deployment @ 06/03/23 12:30:31.116
  W0603 12:30:31.122122      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/03/23 12:30:31.122
  STEP: delete the deployment @ 06/03/23 12:30:31.633
  STEP: wait for all rs to be garbage collected @ 06/03/23 12:30:31.641
  STEP: expected 0 rs, got 1 rs @ 06/03/23 12:30:31.645
  STEP: expected 0 pods, got 2 pods @ 06/03/23 12:30:31.651
  STEP: Gathering metrics @ 06/03/23 12:30:32.169
  W0603 12:30:32.174274      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 12:30:32.174: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 12:30:32.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5677" for this suite. @ 06/03/23 12:30:32.179
• [1.101 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 06/03/23 12:30:32.189
  Jun  3 12:30:32.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:30:32.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:32.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:32.215
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:30:32.221
  STEP: Saw pod success @ 06/03/23 12:30:36.245
  Jun  3 12:30:36.250: INFO: Trying to get logs from node ip-172-31-85-85 pod downwardapi-volume-601b2716-acee-4967-943a-faf4455bd9fa container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:30:36.259
  Jun  3 12:30:36.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7939" for this suite. @ 06/03/23 12:30:36.289
• [4.107 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 06/03/23 12:30:36.298
  Jun  3 12:30:36.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:30:36.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:36.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:36.325
  STEP: creating service in namespace services-9817 @ 06/03/23 12:30:36.333
  STEP: creating service affinity-clusterip-transition in namespace services-9817 @ 06/03/23 12:30:36.334
  STEP: creating replication controller affinity-clusterip-transition in namespace services-9817 @ 06/03/23 12:30:36.353
  I0603 12:30:36.363197      18 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-9817, replica count: 3
  I0603 12:30:39.415859      18 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 12:30:39.423: INFO: Creating new exec pod
  Jun  3 12:30:42.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9817 exec execpod-affinityj7qth -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jun  3 12:30:42.620: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jun  3 12:30:42.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:30:42.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9817 exec execpod-affinityj7qth -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.199 80'
  Jun  3 12:30:42.775: INFO: stderr: "+ nc -v -t -w 2 10.152.183.199 80\n+ echo hostName\nConnection to 10.152.183.199 80 port [tcp/http] succeeded!\n"
  Jun  3 12:30:42.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:30:42.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9817 exec execpod-affinityj7qth -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.199:80/ ; done'
  Jun  3 12:30:43.038: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n"
  Jun  3 12:30:43.038: INFO: stdout: "\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-94vgn\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-5pczf\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-94vgn\naffinity-clusterip-transition-94vgn"
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.038: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-94vgn
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-5pczf
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-94vgn
  Jun  3 12:30:43.039: INFO: Received response from host: affinity-clusterip-transition-94vgn
  Jun  3 12:30:43.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9817 exec execpod-affinityj7qth -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.199:80/ ; done'
  Jun  3 12:30:43.303: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.199:80/\n"
  Jun  3 12:30:43.303: INFO: stdout: "\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx\naffinity-clusterip-transition-mkwzx"
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Received response from host: affinity-clusterip-transition-mkwzx
  Jun  3 12:30:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:30:43.309: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9817, will wait for the garbage collector to delete the pods @ 06/03/23 12:30:43.325
  Jun  3 12:30:43.388: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.922667ms
  Jun  3 12:30:43.488: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.516498ms
  STEP: Destroying namespace "services-9817" for this suite. @ 06/03/23 12:30:45.908
• [9.619 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 06/03/23 12:30:45.917
  Jun  3 12:30:45.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/03/23 12:30:45.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:30:45.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:30:45.944
  STEP: Creating 50 configmaps @ 06/03/23 12:30:45.95
  STEP: Creating RC which spawns configmap-volume pods @ 06/03/23 12:30:46.241
  Jun  3 12:30:46.275: INFO: Pod name wrapped-volume-race-cff8cb1f-eb81-4ea7-ac92-3fa19f8725ee: Found 1 pods out of 5
  Jun  3 12:30:51.282: INFO: Pod name wrapped-volume-race-cff8cb1f-eb81-4ea7-ac92-3fa19f8725ee: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/03/23 12:30:51.282
  STEP: Creating RC which spawns configmap-volume pods @ 06/03/23 12:30:51.311
  Jun  3 12:30:51.333: INFO: Pod name wrapped-volume-race-ed2205ef-27f2-4034-84ac-07a99a292441: Found 0 pods out of 5
  Jun  3 12:30:56.343: INFO: Pod name wrapped-volume-race-ed2205ef-27f2-4034-84ac-07a99a292441: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/03/23 12:30:56.343
  STEP: Creating RC which spawns configmap-volume pods @ 06/03/23 12:30:56.369
  Jun  3 12:30:56.391: INFO: Pod name wrapped-volume-race-5e1a147c-6c2a-4b4c-b976-94d370659142: Found 0 pods out of 5
  Jun  3 12:31:01.400: INFO: Pod name wrapped-volume-race-5e1a147c-6c2a-4b4c-b976-94d370659142: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 06/03/23 12:31:01.4
  Jun  3 12:31:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-5e1a147c-6c2a-4b4c-b976-94d370659142 in namespace emptydir-wrapper-3817, will wait for the garbage collector to delete the pods @ 06/03/23 12:31:01.427
  Jun  3 12:31:01.491: INFO: Deleting ReplicationController wrapped-volume-race-5e1a147c-6c2a-4b4c-b976-94d370659142 took: 9.395497ms
  Jun  3 12:31:01.592: INFO: Terminating ReplicationController wrapped-volume-race-5e1a147c-6c2a-4b4c-b976-94d370659142 pods took: 101.356222ms
  STEP: deleting ReplicationController wrapped-volume-race-ed2205ef-27f2-4034-84ac-07a99a292441 in namespace emptydir-wrapper-3817, will wait for the garbage collector to delete the pods @ 06/03/23 12:31:04.194
  Jun  3 12:31:04.259: INFO: Deleting ReplicationController wrapped-volume-race-ed2205ef-27f2-4034-84ac-07a99a292441 took: 8.934354ms
  Jun  3 12:31:04.360: INFO: Terminating ReplicationController wrapped-volume-race-ed2205ef-27f2-4034-84ac-07a99a292441 pods took: 101.000701ms
  STEP: deleting ReplicationController wrapped-volume-race-cff8cb1f-eb81-4ea7-ac92-3fa19f8725ee in namespace emptydir-wrapper-3817, will wait for the garbage collector to delete the pods @ 06/03/23 12:31:06.761
  Jun  3 12:31:06.826: INFO: Deleting ReplicationController wrapped-volume-race-cff8cb1f-eb81-4ea7-ac92-3fa19f8725ee took: 9.653125ms
  Jun  3 12:31:06.927: INFO: Terminating ReplicationController wrapped-volume-race-cff8cb1f-eb81-4ea7-ac92-3fa19f8725ee pods took: 100.957962ms
  STEP: Cleaning up the configMaps @ 06/03/23 12:31:09.528
  STEP: Destroying namespace "emptydir-wrapper-3817" for this suite. @ 06/03/23 12:31:09.891
• [23.981 seconds]
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 06/03/23 12:31:09.899
  Jun  3 12:31:09.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename endpointslice @ 06/03/23 12:31:09.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:09.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:09.924
  STEP: getting /apis @ 06/03/23 12:31:09.93
  STEP: getting /apis/discovery.k8s.io @ 06/03/23 12:31:09.936
  STEP: getting /apis/discovery.k8s.iov1 @ 06/03/23 12:31:09.938
  STEP: creating @ 06/03/23 12:31:09.94
  STEP: getting @ 06/03/23 12:31:09.959
  STEP: listing @ 06/03/23 12:31:09.963
  STEP: watching @ 06/03/23 12:31:09.969
  Jun  3 12:31:09.969: INFO: starting watch
  STEP: cluster-wide listing @ 06/03/23 12:31:09.971
  STEP: cluster-wide watching @ 06/03/23 12:31:09.976
  Jun  3 12:31:09.977: INFO: starting watch
  STEP: patching @ 06/03/23 12:31:09.979
  STEP: updating @ 06/03/23 12:31:09.985
  Jun  3 12:31:09.996: INFO: waiting for watch events with expected annotations
  Jun  3 12:31:09.996: INFO: saw patched and updated annotations
  STEP: deleting @ 06/03/23 12:31:09.996
  STEP: deleting a collection @ 06/03/23 12:31:10.011
  Jun  3 12:31:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5611" for this suite. @ 06/03/23 12:31:10.038
• [0.147 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 06/03/23 12:31:10.047
  Jun  3 12:31:10.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename containers @ 06/03/23 12:31:10.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:10.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:10.074
  STEP: Creating a pod to test override all @ 06/03/23 12:31:10.08
  STEP: Saw pod success @ 06/03/23 12:31:14.103
  Jun  3 12:31:14.107: INFO: Trying to get logs from node ip-172-31-27-193 pod client-containers-0fb27c86-38c3-43f5-a7c6-37a1a168a482 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:31:14.115
  Jun  3 12:31:14.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-758" for this suite. @ 06/03/23 12:31:14.139
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 06/03/23 12:31:14.149
  Jun  3 12:31:14.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename controllerrevisions @ 06/03/23 12:31:14.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:14.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:14.177
  STEP: Creating DaemonSet "e2e-5dntw-daemon-set" @ 06/03/23 12:31:14.209
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 12:31:14.217
  Jun  3 12:31:14.221: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:14.222: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:14.226: INFO: Number of nodes with available pods controlled by daemonset e2e-5dntw-daemon-set: 0
  Jun  3 12:31:14.226: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:31:15.233: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:15.233: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:15.238: INFO: Number of nodes with available pods controlled by daemonset e2e-5dntw-daemon-set: 1
  Jun  3 12:31:15.238: INFO: Node ip-172-31-7-203 is running 0 daemon pod, expected 1
  Jun  3 12:31:16.231: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:16.231: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:31:16.236: INFO: Number of nodes with available pods controlled by daemonset e2e-5dntw-daemon-set: 3
  Jun  3 12:31:16.236: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-5dntw-daemon-set
  STEP: Confirm DaemonSet "e2e-5dntw-daemon-set" successfully created with "daemonset-name=e2e-5dntw-daemon-set" label @ 06/03/23 12:31:16.24
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-5dntw-daemon-set" @ 06/03/23 12:31:16.249
  Jun  3 12:31:16.253: INFO: Located ControllerRevision: "e2e-5dntw-daemon-set-845f95f677"
  STEP: Patching ControllerRevision "e2e-5dntw-daemon-set-845f95f677" @ 06/03/23 12:31:16.257
  Jun  3 12:31:16.265: INFO: e2e-5dntw-daemon-set-845f95f677 has been patched
  STEP: Create a new ControllerRevision @ 06/03/23 12:31:16.265
  Jun  3 12:31:16.273: INFO: Created ControllerRevision: e2e-5dntw-daemon-set-5c9c88b7d9
  STEP: Confirm that there are two ControllerRevisions @ 06/03/23 12:31:16.273
  Jun  3 12:31:16.273: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun  3 12:31:16.277: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-5dntw-daemon-set-845f95f677" @ 06/03/23 12:31:16.278
  STEP: Confirm that there is only one ControllerRevision @ 06/03/23 12:31:16.285
  Jun  3 12:31:16.285: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun  3 12:31:16.289: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-5dntw-daemon-set-5c9c88b7d9" @ 06/03/23 12:31:16.294
  Jun  3 12:31:16.305: INFO: e2e-5dntw-daemon-set-5c9c88b7d9 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 06/03/23 12:31:16.305
  W0603 12:31:16.315740      18 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 06/03/23 12:31:16.316
  Jun  3 12:31:16.316: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun  3 12:31:17.321: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun  3 12:31:17.330: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-5dntw-daemon-set-5c9c88b7d9=updated" @ 06/03/23 12:31:17.33
  STEP: Confirm that there is only one ControllerRevision @ 06/03/23 12:31:17.339
  Jun  3 12:31:17.339: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jun  3 12:31:17.343: INFO: Found 1 ControllerRevisions
  Jun  3 12:31:17.348: INFO: ControllerRevision "e2e-5dntw-daemon-set-866559744d" has revision 3
  STEP: Deleting DaemonSet "e2e-5dntw-daemon-set" @ 06/03/23 12:31:17.351
  STEP: deleting DaemonSet.extensions e2e-5dntw-daemon-set in namespace controllerrevisions-2795, will wait for the garbage collector to delete the pods @ 06/03/23 12:31:17.352
  Jun  3 12:31:17.415: INFO: Deleting DaemonSet.extensions e2e-5dntw-daemon-set took: 8.416397ms
  Jun  3 12:31:17.516: INFO: Terminating DaemonSet.extensions e2e-5dntw-daemon-set pods took: 101.226021ms
  Jun  3 12:31:19.322: INFO: Number of nodes with available pods controlled by daemonset e2e-5dntw-daemon-set: 0
  Jun  3 12:31:19.322: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-5dntw-daemon-set
  Jun  3 12:31:19.327: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12895"},"items":null}

  Jun  3 12:31:19.331: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12895"},"items":null}

  Jun  3 12:31:19.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2795" for this suite. @ 06/03/23 12:31:19.352
• [5.211 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 06/03/23 12:31:19.36
  Jun  3 12:31:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption @ 06/03/23 12:31:19.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:19.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:19.385
  STEP: Creating a kubernetes client @ 06/03/23 12:31:19.39
  Jun  3 12:31:19.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption-2 @ 06/03/23 12:31:19.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:19.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:19.42
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:19.43
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:21.444
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:23.461
  STEP: listing a collection of PDBs across all namespaces @ 06/03/23 12:31:25.474
  STEP: listing a collection of PDBs in namespace disruption-3187 @ 06/03/23 12:31:25.478
  STEP: deleting a collection of PDBs @ 06/03/23 12:31:25.483
  STEP: Waiting for the PDB collection to be deleted @ 06/03/23 12:31:25.497
  Jun  3 12:31:25.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:31:25.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-6662" for this suite. @ 06/03/23 12:31:25.511
  STEP: Destroying namespace "disruption-3187" for this suite. @ 06/03/23 12:31:25.518
• [6.165 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 06/03/23 12:31:25.527
  Jun  3 12:31:25.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption @ 06/03/23 12:31:25.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:25.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:25.564
  STEP: creating the pdb @ 06/03/23 12:31:25.568
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:25.573
  STEP: updating the pdb @ 06/03/23 12:31:27.582
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:27.592
  STEP: patching the pdb @ 06/03/23 12:31:29.6
  STEP: Waiting for the pdb to be processed @ 06/03/23 12:31:29.612
  STEP: Waiting for the pdb to be deleted @ 06/03/23 12:31:31.628
  Jun  3 12:31:31.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6611" for this suite. @ 06/03/23 12:31:31.637
• [6.117 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 06/03/23 12:31:31.645
  Jun  3 12:31:31.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename containers @ 06/03/23 12:31:31.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:31.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:31.671
  Jun  3 12:31:33.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-430" for this suite. @ 06/03/23 12:31:33.71
• [2.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 06/03/23 12:31:33.722
  Jun  3 12:31:33.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:31:33.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:33.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:33.75
  STEP: Creating a ResourceQuota @ 06/03/23 12:31:33.756
  STEP: Getting a ResourceQuota @ 06/03/23 12:31:33.761
  STEP: Listing all ResourceQuotas with LabelSelector @ 06/03/23 12:31:33.765
  STEP: Patching the ResourceQuota @ 06/03/23 12:31:33.77
  STEP: Deleting a Collection of ResourceQuotas @ 06/03/23 12:31:33.777
  STEP: Verifying the deleted ResourceQuota @ 06/03/23 12:31:33.787
  Jun  3 12:31:33.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5775" for this suite. @ 06/03/23 12:31:33.797
• [0.083 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 06/03/23 12:31:33.806
  Jun  3 12:31:33.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:31:33.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:33.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:33.832
  STEP: Creating configMap that has name configmap-test-emptyKey-ff0cb934-8e45-40bf-a79c-7cb6c8390bee @ 06/03/23 12:31:33.84
  Jun  3 12:31:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6720" for this suite. @ 06/03/23 12:31:33.849
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 06/03/23 12:31:33.859
  Jun  3 12:31:33.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename runtimeclass @ 06/03/23 12:31:33.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:33.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:33.885
  Jun  3 12:31:33.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7793" for this suite. @ 06/03/23 12:31:33.931
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 06/03/23 12:31:33.945
  Jun  3 12:31:33.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:31:33.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:33.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:33.97
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/03/23 12:31:33.976
  STEP: Saw pod success @ 06/03/23 12:31:38.003
  Jun  3 12:31:38.008: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-d21a88f2-6b2c-43b1-9c40-ace5f209240a container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:31:38.02
  Jun  3 12:31:38.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-765" for this suite. @ 06/03/23 12:31:38.041
• [4.104 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 06/03/23 12:31:38.05
  Jun  3 12:31:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename watch @ 06/03/23 12:31:38.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:38.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:38.075
  STEP: creating a watch on configmaps with label A @ 06/03/23 12:31:38.079
  STEP: creating a watch on configmaps with label B @ 06/03/23 12:31:38.081
  STEP: creating a watch on configmaps with label A or B @ 06/03/23 12:31:38.083
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 06/03/23 12:31:38.085
  Jun  3 12:31:38.094: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13112 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:38.095: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13112 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 06/03/23 12:31:38.095
  Jun  3 12:31:38.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13113 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:38.106: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13113 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 06/03/23 12:31:38.106
  Jun  3 12:31:38.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13114 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:38.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13114 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 06/03/23 12:31:38.115
  Jun  3 12:31:38.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13115 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:38.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7804  cb4b71e8-3b32-4221-b11d-0b8fe5e282c5 13115 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 06/03/23 12:31:38.123
  Jun  3 12:31:38.128: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7804  cde6fe0a-11d7-4143-bcbc-798c248c376d 13116 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:38.128: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7804  cde6fe0a-11d7-4143-bcbc-798c248c376d 13116 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 06/03/23 12:31:48.129
  Jun  3 12:31:48.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7804  cde6fe0a-11d7-4143-bcbc-798c248c376d 13189 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:48.139: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7804  cde6fe0a-11d7-4143-bcbc-798c248c376d 13189 0 2023-06-03 12:31:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-03 12:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:31:58.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7804" for this suite. @ 06/03/23 12:31:58.146
• [20.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 06/03/23 12:31:58.159
  Jun  3 12:31:58.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename runtimeclass @ 06/03/23 12:31:58.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:58.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:58.185
  STEP: getting /apis @ 06/03/23 12:31:58.189
  STEP: getting /apis/node.k8s.io @ 06/03/23 12:31:58.194
  STEP: getting /apis/node.k8s.io/v1 @ 06/03/23 12:31:58.196
  STEP: creating @ 06/03/23 12:31:58.198
  STEP: watching @ 06/03/23 12:31:58.216
  Jun  3 12:31:58.216: INFO: starting watch
  STEP: getting @ 06/03/23 12:31:58.223
  STEP: listing @ 06/03/23 12:31:58.227
  STEP: patching @ 06/03/23 12:31:58.231
  STEP: updating @ 06/03/23 12:31:58.237
  Jun  3 12:31:58.243: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 06/03/23 12:31:58.244
  STEP: deleting a collection @ 06/03/23 12:31:58.259
  Jun  3 12:31:58.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7948" for this suite. @ 06/03/23 12:31:58.283
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 06/03/23 12:31:58.293
  Jun  3 12:31:58.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption @ 06/03/23 12:31:58.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:31:58.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:31:58.316
  Jun  3 12:31:58.335: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 12:32:58.357: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/03/23 12:32:58.361
  Jun  3 12:32:58.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/03/23 12:32:58.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:32:58.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:32:58.391
  STEP: Finding an available node @ 06/03/23 12:32:58.396
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/03/23 12:32:58.396
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/03/23 12:33:00.424
  Jun  3 12:33:00.438: INFO: found a healthy node: ip-172-31-27-193
  Jun  3 12:33:06.530: INFO: pods created so far: [1 1 1]
  Jun  3 12:33:06.531: INFO: length of pods created so far: 3
  Jun  3 12:33:08.543: INFO: pods created so far: [2 2 1]
  Jun  3 12:33:15.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:33:15.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9193" for this suite. @ 06/03/23 12:33:15.648
  STEP: Destroying namespace "sched-preemption-3113" for this suite. @ 06/03/23 12:33:15.656
• [77.372 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 06/03/23 12:33:15.668
  Jun  3 12:33:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 12:33:15.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:15.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:15.694
  Jun  3 12:33:15.705: INFO: Got root ca configmap in namespace "svcaccounts-6849"
  Jun  3 12:33:15.712: INFO: Deleted root ca configmap in namespace "svcaccounts-6849"
  STEP: waiting for a new root ca configmap created @ 06/03/23 12:33:16.213
  Jun  3 12:33:16.218: INFO: Recreated root ca configmap in namespace "svcaccounts-6849"
  Jun  3 12:33:16.225: INFO: Updated root ca configmap in namespace "svcaccounts-6849"
  STEP: waiting for the root ca configmap reconciled @ 06/03/23 12:33:16.725
  Jun  3 12:33:16.730: INFO: Reconciled root ca configmap in namespace "svcaccounts-6849"
  Jun  3 12:33:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6849" for this suite. @ 06/03/23 12:33:16.735
• [1.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 06/03/23 12:33:16.748
  Jun  3 12:33:16.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubelet-test @ 06/03/23 12:33:16.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:16.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:16.773
  Jun  3 12:33:20.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7956" for this suite. @ 06/03/23 12:33:20.805
• [4.065 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 06/03/23 12:33:20.814
  Jun  3 12:33:20.814: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 12:33:20.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:20.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:20.839
  Jun  3 12:33:20.846: INFO: Creating deployment "test-recreate-deployment"
  Jun  3 12:33:20.854: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jun  3 12:33:20.863: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jun  3 12:33:22.873: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jun  3 12:33:22.877: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jun  3 12:33:22.889: INFO: Updating deployment test-recreate-deployment
  Jun  3 12:33:22.890: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jun  3 12:33:22.974: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4745  ab0e93ee-c153-4240-9b96-8b145de3b6ca 13705 2 2023-06-03 12:33:20 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004114768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-03 12:33:22 +0000 UTC,LastTransitionTime:2023-06-03 12:33:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-06-03 12:33:22 +0000 UTC,LastTransitionTime:2023-06-03 12:33:20 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jun  3 12:33:22.979: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-4745  041b9a88-3f0f-4a34-aa19-ddeb14f8c9c5 13702 1 2023-06-03 12:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ab0e93ee-c153-4240-9b96-8b145de3b6ca 0xc004114b27 0xc004114b28}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab0e93ee-c153-4240-9b96-8b145de3b6ca\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004114bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:33:22.979: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jun  3 12:33:22.979: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-4745  d2b57b6a-11db-4aef-b1d3-12b37026ab9e 13693 2 2023-06-03 12:33:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ab0e93ee-c153-4240-9b96-8b145de3b6ca 0xc004114c37 0xc004114c38}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab0e93ee-c153-4240-9b96-8b145de3b6ca\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004114ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:33:22.985: INFO: Pod "test-recreate-deployment-54757ffd6c-9s88s" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-9s88s test-recreate-deployment-54757ffd6c- deployment-4745  e9bcb9c7-a21b-4bf3-803b-e258ef13b465 13704 0 2023-06-03 12:33:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 041b9a88-3f0f-4a34-aa19-ddeb14f8c9c5 0xc003ff5957 0xc003ff5958}] [] [{kube-controller-manager Update v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"041b9a88-3f0f-4a34-aa19-ddeb14f8c9c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:33:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktxmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktxmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:33:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:33:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:33:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:33:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:33:22.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4745" for this suite. @ 06/03/23 12:33:22.99
• [2.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 06/03/23 12:33:23.004
  Jun  3 12:33:23.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sysctl @ 06/03/23 12:33:23.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:23.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:23.033
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 06/03/23 12:33:23.038
  STEP: Watching for error events or started pod @ 06/03/23 12:33:23.049
  STEP: Waiting for pod completion @ 06/03/23 12:33:25.056
  STEP: Checking that the pod succeeded @ 06/03/23 12:33:27.071
  STEP: Getting logs from the pod @ 06/03/23 12:33:27.071
  STEP: Checking that the sysctl is actually updated @ 06/03/23 12:33:27.094
  Jun  3 12:33:27.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9823" for this suite. @ 06/03/23 12:33:27.1
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 06/03/23 12:33:27.108
  Jun  3 12:33:27.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context @ 06/03/23 12:33:27.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:27.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:27.137
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/03/23 12:33:27.143
  STEP: Saw pod success @ 06/03/23 12:33:31.17
  Jun  3 12:33:31.176: INFO: Trying to get logs from node ip-172-31-27-193 pod security-context-75b07e62-9273-4c0d-af6a-61396d433774 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:33:31.186
  Jun  3 12:33:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9367" for this suite. @ 06/03/23 12:33:31.211
• [4.112 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 06/03/23 12:33:31.221
  Jun  3 12:33:31.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-webhook @ 06/03/23 12:33:31.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:31.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:31.252
  STEP: Setting up server cert @ 06/03/23 12:33:31.257
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/03/23 12:33:31.663
  STEP: Deploying the custom resource conversion webhook pod @ 06/03/23 12:33:31.673
  STEP: Wait for the deployment to be ready @ 06/03/23 12:33:31.691
  Jun  3 12:33:31.701: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:33:33.714
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:33:33.726
  Jun  3 12:33:34.727: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun  3 12:33:34.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Creating a v1 custom resource @ 06/03/23 12:33:37.333
  STEP: v2 custom resource should be converted @ 06/03/23 12:33:37.34
  Jun  3 12:33:37.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9342" for this suite. @ 06/03/23 12:33:37.934
• [6.723 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 06/03/23 12:33:37.945
  Jun  3 12:33:37.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:33:37.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:37.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:37.981
  STEP: Setting up server cert @ 06/03/23 12:33:38.016
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:33:38.685
  STEP: Deploying the webhook pod @ 06/03/23 12:33:38.692
  STEP: Wait for the deployment to be ready @ 06/03/23 12:33:38.707
  Jun  3 12:33:38.717: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:33:40.731
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:33:40.744
  Jun  3 12:33:41.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun  3 12:33:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3608-crds.webhook.example.com via the AdmissionRegistration API @ 06/03/23 12:33:42.268
  STEP: Creating a custom resource while v1 is storage version @ 06/03/23 12:33:42.289
  STEP: Patching Custom Resource Definition to set v2 as storage @ 06/03/23 12:33:44.357
  STEP: Patching the custom resource while v2 is storage version @ 06/03/23 12:33:44.383
  Jun  3 12:33:44.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2456" for this suite. @ 06/03/23 12:33:45.035
  STEP: Destroying namespace "webhook-markers-7106" for this suite. @ 06/03/23 12:33:45.044
• [7.106 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 06/03/23 12:33:45.052
  Jun  3 12:33:45.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:33:45.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:45.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:45.078
  STEP: Creating projection with secret that has name projected-secret-test-map-7e2311c5-d9fe-4780-a1b4-bcc15b89ba1c @ 06/03/23 12:33:45.084
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:33:45.091
  STEP: Saw pod success @ 06/03/23 12:33:49.119
  Jun  3 12:33:49.123: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-secrets-f51fd7d5-0e75-4521-9282-bbcdcbce8e62 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:33:49.132
  Jun  3 12:33:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4228" for this suite. @ 06/03/23 12:33:49.157
• [4.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 06/03/23 12:33:49.173
  Jun  3 12:33:49.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context-test @ 06/03/23 12:33:49.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:49.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:49.2
  Jun  3 12:33:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9503" for this suite. @ 06/03/23 12:33:53.235
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 06/03/23 12:33:53.244
  Jun  3 12:33:53.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/03/23 12:33:53.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:53.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:53.27
  STEP: creating @ 06/03/23 12:33:53.275
  STEP: getting @ 06/03/23 12:33:53.295
  STEP: listing in namespace @ 06/03/23 12:33:53.3
  STEP: patching @ 06/03/23 12:33:53.304
  STEP: deleting @ 06/03/23 12:33:53.322
  Jun  3 12:33:53.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5949" for this suite. @ 06/03/23 12:33:53.341
• [0.106 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 06/03/23 12:33:53.351
  Jun  3 12:33:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:33:53.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:53.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:53.375
  STEP: Creating a pod to test emptydir volume type on node default medium @ 06/03/23 12:33:53.378
  STEP: Saw pod success @ 06/03/23 12:33:57.412
  Jun  3 12:33:57.425: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-ab2111e3-7f49-4495-ac97-85382e6f524a container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:33:57.434
  Jun  3 12:33:57.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6237" for this suite. @ 06/03/23 12:33:57.456
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 06/03/23 12:33:57.468
  Jun  3 12:33:57.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:33:57.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:33:57.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:33:57.504
  STEP: Setting up server cert @ 06/03/23 12:33:57.564
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:33:58.879
  STEP: Deploying the webhook pod @ 06/03/23 12:33:58.886
  STEP: Wait for the deployment to be ready @ 06/03/23 12:33:58.9
  Jun  3 12:33:58.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:34:00.923
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:34:00.935
  Jun  3 12:34:01.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/03/23 12:34:01.94
  STEP: create a pod that should be denied by the webhook @ 06/03/23 12:34:01.961
  STEP: create a pod that causes the webhook to hang @ 06/03/23 12:34:01.98
  STEP: create a configmap that should be denied by the webhook @ 06/03/23 12:34:11.99
  STEP: create a configmap that should be admitted by the webhook @ 06/03/23 12:34:12.051
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/03/23 12:34:12.062
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 06/03/23 12:34:12.072
  STEP: create a namespace that bypass the webhook @ 06/03/23 12:34:12.08
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 06/03/23 12:34:12.098
  Jun  3 12:34:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4587" for this suite. @ 06/03/23 12:34:12.17
  STEP: Destroying namespace "webhook-markers-8012" for this suite. @ 06/03/23 12:34:12.179
  STEP: Destroying namespace "exempted-namespace-4000" for this suite. @ 06/03/23 12:34:12.187
• [14.729 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 06/03/23 12:34:12.198
  Jun  3 12:34:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename aggregator @ 06/03/23 12:34:12.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:34:12.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:34:12.223
  Jun  3 12:34:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Registering the sample API server. @ 06/03/23 12:34:12.228
  Jun  3 12:34:12.536: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jun  3 12:34:12.575: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Jun  3 12:34:14.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:16.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:18.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:20.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:22.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:24.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:26.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:28.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:30.643: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:32.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:34.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jun  3 12:34:36.773: INFO: Waited 118.841033ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 06/03/23 12:34:36.832
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 06/03/23 12:34:36.84
  STEP: List APIServices @ 06/03/23 12:34:36.851
  Jun  3 12:34:36.859: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 06/03/23 12:34:36.859
  Jun  3 12:34:36.883: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 06/03/23 12:34:36.883
  Jun  3 12:34:36.901: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.June, 3, 12, 34, 36, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 06/03/23 12:34:36.901
  Jun  3 12:34:36.907: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-06-03 12:34:36 +0000 UTC Passed all checks passed}
  Jun  3 12:34:36.907: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 12:34:36.907: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 06/03/23 12:34:36.908
  Jun  3 12:34:36.927: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1585987289" @ 06/03/23 12:34:36.927
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 06/03/23 12:34:36.942
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 06/03/23 12:34:36.952
  STEP: Patch APIService Status @ 06/03/23 12:34:36.958
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 06/03/23 12:34:36.97
  Jun  3 12:34:36.979: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-06-03 12:34:36 +0000 UTC Passed all checks passed}
  Jun  3 12:34:36.979: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 12:34:36.979: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jun  3 12:34:36.979: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 06/03/23 12:34:36.979
  STEP: Confirm that the generated APIService has been deleted @ 06/03/23 12:34:36.986
  Jun  3 12:34:36.986: INFO: Requesting list of APIServices to confirm quantity
  Jun  3 12:34:36.993: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jun  3 12:34:36.993: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jun  3 12:34:37.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-1315" for this suite. @ 06/03/23 12:34:37.169
• [24.981 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 06/03/23 12:34:37.179
  Jun  3 12:34:37.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename tables @ 06/03/23 12:34:37.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:34:37.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:34:37.209
  Jun  3 12:34:37.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-7255" for this suite. @ 06/03/23 12:34:37.231
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 06/03/23 12:34:37.244
  Jun  3 12:34:37.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename init-container @ 06/03/23 12:34:37.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:34:37.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:34:37.277
  STEP: creating the pod @ 06/03/23 12:34:37.291
  Jun  3 12:34:37.292: INFO: PodSpec: initContainers in spec.initContainers
  Jun  3 12:34:41.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7122" for this suite. @ 06/03/23 12:34:41.72
• [4.484 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 06/03/23 12:34:41.734
  Jun  3 12:34:41.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 12:34:41.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:34:41.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:34:41.768
  Jun  3 12:34:41.773: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 06/03/23 12:34:42.791
  STEP: Checking rc "condition-test" has the desired failure condition set @ 06/03/23 12:34:42.799
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 06/03/23 12:34:43.808
  Jun  3 12:34:43.821: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 06/03/23 12:34:43.821
  Jun  3 12:34:44.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1584" for this suite. @ 06/03/23 12:34:44.839
• [3.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 06/03/23 12:34:44.849
  Jun  3 12:34:44.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:34:44.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:34:44.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:34:44.879
  STEP: Creating pod test-webserver-8da4d4cf-f9f9-4d0d-8101-4ce8c068645c in namespace container-probe-2255 @ 06/03/23 12:34:44.884
  Jun  3 12:34:46.902: INFO: Started pod test-webserver-8da4d4cf-f9f9-4d0d-8101-4ce8c068645c in namespace container-probe-2255
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:34:46.902
  Jun  3 12:34:46.908: INFO: Initial restart count of pod test-webserver-8da4d4cf-f9f9-4d0d-8101-4ce8c068645c is 0
  Jun  3 12:38:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:38:47.551
  STEP: Destroying namespace "container-probe-2255" for this suite. @ 06/03/23 12:38:47.567
• [242.728 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 06/03/23 12:38:47.578
  Jun  3 12:38:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:38:47.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:38:47.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:38:47.605
  STEP: Creating a ResourceQuota with terminating scope @ 06/03/23 12:38:47.612
  STEP: Ensuring ResourceQuota status is calculated @ 06/03/23 12:38:47.62
  STEP: Creating a ResourceQuota with not terminating scope @ 06/03/23 12:38:49.626
  STEP: Ensuring ResourceQuota status is calculated @ 06/03/23 12:38:49.631
  STEP: Creating a long running pod @ 06/03/23 12:38:51.637
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 06/03/23 12:38:51.654
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 06/03/23 12:38:53.659
  STEP: Deleting the pod @ 06/03/23 12:38:55.665
  STEP: Ensuring resource quota status released the pod usage @ 06/03/23 12:38:55.682
  STEP: Creating a terminating pod @ 06/03/23 12:38:57.689
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 06/03/23 12:38:57.707
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 06/03/23 12:38:59.712
  STEP: Deleting the pod @ 06/03/23 12:39:01.718
  STEP: Ensuring resource quota status released the pod usage @ 06/03/23 12:39:01.733
  Jun  3 12:39:03.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4569" for this suite. @ 06/03/23 12:39:03.746
• [16.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 06/03/23 12:39:03.757
  Jun  3 12:39:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:39:03.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:39:03.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:39:03.787
  STEP: Creating pod liveness-f6f87be1-46c2-4127-beef-07d559ecf053 in namespace container-probe-431 @ 06/03/23 12:39:03.795
  Jun  3 12:39:05.822: INFO: Started pod liveness-f6f87be1-46c2-4127-beef-07d559ecf053 in namespace container-probe-431
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:39:05.823
  Jun  3 12:39:05.827: INFO: Initial restart count of pod liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is 0
  Jun  3 12:39:25.889: INFO: Restart count of pod container-probe-431/liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is now 1 (20.062631765s elapsed)
  Jun  3 12:39:45.945: INFO: Restart count of pod container-probe-431/liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is now 2 (40.11817754s elapsed)
  Jun  3 12:40:05.999: INFO: Restart count of pod container-probe-431/liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is now 3 (1m0.172645056s elapsed)
  Jun  3 12:40:26.057: INFO: Restart count of pod container-probe-431/liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is now 4 (1m20.230660964s elapsed)
  Jun  3 12:41:28.217: INFO: Restart count of pod container-probe-431/liveness-f6f87be1-46c2-4127-beef-07d559ecf053 is now 5 (2m22.390062131s elapsed)
  Jun  3 12:41:28.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:41:28.222
  STEP: Destroying namespace "container-probe-431" for this suite. @ 06/03/23 12:41:28.236
• [144.488 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 06/03/23 12:41:28.247
  Jun  3 12:41:28.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:41:28.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:28.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:28.287
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/03/23 12:41:28.292
  STEP: Saw pod success @ 06/03/23 12:41:32.32
  Jun  3 12:41:32.324: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-96ceaf90-c5b5-4861-a9e2-36aadd7172b1 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:41:32.349
  Jun  3 12:41:32.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8793" for this suite. @ 06/03/23 12:41:32.374
• [4.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 06/03/23 12:41:32.383
  Jun  3 12:41:32.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:41:32.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:32.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:32.414
  STEP: creating a collection of services @ 06/03/23 12:41:32.42
  Jun  3 12:41:32.420: INFO: Creating e2e-svc-a-blsdb
  Jun  3 12:41:32.432: INFO: Creating e2e-svc-b-9prnx
  Jun  3 12:41:32.448: INFO: Creating e2e-svc-c-s5bw5
  STEP: deleting service collection @ 06/03/23 12:41:32.467
  Jun  3 12:41:32.502: INFO: Collection of services has been deleted
  Jun  3 12:41:32.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5375" for this suite. @ 06/03/23 12:41:32.508
• [0.135 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 06/03/23 12:41:32.519
  Jun  3 12:41:32.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:41:32.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:32.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:32.55
  STEP: Setting up server cert @ 06/03/23 12:41:32.584
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:41:33.458
  STEP: Deploying the webhook pod @ 06/03/23 12:41:33.469
  STEP: Wait for the deployment to be ready @ 06/03/23 12:41:33.483
  Jun  3 12:41:33.493: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 06/03/23 12:41:35.507
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:41:35.53
  Jun  3 12:41:36.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun  3 12:41:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 06/03/23 12:41:37.047
  STEP: Creating a custom resource that should be denied by the webhook @ 06/03/23 12:41:37.07
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 06/03/23 12:41:39.107
  STEP: Updating the custom resource with disallowed data should be denied @ 06/03/23 12:41:39.116
  STEP: Deleting the custom resource should be denied @ 06/03/23 12:41:39.13
  STEP: Remove the offending key and value from the custom resource data @ 06/03/23 12:41:39.139
  STEP: Deleting the updated custom resource should be successful @ 06/03/23 12:41:39.153
  Jun  3 12:41:39.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7365" for this suite. @ 06/03/23 12:41:39.736
  STEP: Destroying namespace "webhook-markers-2958" for this suite. @ 06/03/23 12:41:39.744
• [7.233 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 06/03/23 12:41:39.754
  Jun  3 12:41:39.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-pred @ 06/03/23 12:41:39.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:39.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:39.78
  Jun  3 12:41:39.785: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun  3 12:41:39.796: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 12:41:39.801: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-193 before test
  Jun  3 12:41:39.807: INFO: nginx-ingress-controller-kubernetes-worker-hdrw7 from ingress-nginx-kubernetes-worker started at 2023-06-03 12:18:41 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.807: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 12:41:39.807: INFO: sonobuoy from sonobuoy started at 2023-06-03 12:00:20 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.807: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun  3 12:41:39.807: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-9mjpr from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 12:41:39.807: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 12:41:39.807: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 12:41:39.807: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-7-203 before test
  Jun  3 12:41:39.814: INFO: nginx-ingress-controller-kubernetes-worker-js8tg from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: calico-kube-controllers-74bc8c9977-fzgz2 from kube-system started at 2023-06-03 11:55:43 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: coredns-5c7f76ccb8-p6pwx from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container coredns ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: kube-state-metrics-5b95b4459c-wk958 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: metrics-server-v0.5.2-6cf8c8b69c-88j59 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (2 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container metrics-server ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: dashboard-metrics-scraper-6b8586b5c9-28hlk from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: kubernetes-dashboard-6869f4cd5f-dv2kl from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-7fcft from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 12:41:39.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 12:41:39.814: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-85-85 before test
  Jun  3 12:41:39.822: INFO: default-http-backend-kubernetes-worker-65fc475d49-ddthh from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:24 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.822: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun  3 12:41:39.822: INFO: nginx-ingress-controller-kubernetes-worker-t855p from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 12:41:39.822: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 12:41:39.822: INFO: sonobuoy-e2e-job-56763c10077449b8 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 12:41:39.822: INFO: 	Container e2e ready: true, restart count 0
  Jun  3 12:41:39.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 12:41:39.822: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cwkk4 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 12:41:39.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 12:41:39.822: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/03/23 12:41:39.823
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/03/23 12:41:41.849
  STEP: Trying to apply a random label on the found node. @ 06/03/23 12:41:41.865
  STEP: verifying the node has the label kubernetes.io/e2e-6e5d06fb-f189-4ff5-b04e-41ed2bbdfcb9 42 @ 06/03/23 12:41:41.875
  STEP: Trying to relaunch the pod, now with labels. @ 06/03/23 12:41:41.881
  STEP: removing the label kubernetes.io/e2e-6e5d06fb-f189-4ff5-b04e-41ed2bbdfcb9 off the node ip-172-31-27-193 @ 06/03/23 12:41:43.902
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6e5d06fb-f189-4ff5-b04e-41ed2bbdfcb9 @ 06/03/23 12:41:43.917
  Jun  3 12:41:43.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5101" for this suite. @ 06/03/23 12:41:43.93
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 06/03/23 12:41:43.943
  Jun  3 12:41:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:41:43.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:43.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:43.967
  STEP: Creating configMap configmap-5496/configmap-test-128c7900-e2c3-4e8b-895f-4246973b028c @ 06/03/23 12:41:43.972
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:41:43.979
  STEP: Saw pod success @ 06/03/23 12:41:48.008
  Jun  3 12:41:48.011: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-cdf38e35-9984-42f4-93b9-373c3582bee8 container env-test: <nil>
  STEP: delete the pod @ 06/03/23 12:41:48.021
  Jun  3 12:41:48.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5496" for this suite. @ 06/03/23 12:41:48.047
• [4.112 seconds]
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 06/03/23 12:41:48.055
  Jun  3 12:41:48.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:41:48.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:48.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:48.08
  STEP: Creating Pod @ 06/03/23 12:41:48.085
  STEP: Reading file content from the nginx-container @ 06/03/23 12:41:50.11
  Jun  3 12:41:50.110: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5899 PodName:pod-sharedvolume-2b2eac26-5396-4d10-adb0-d2b481d2228d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:41:50.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:41:50.111: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:41:50.111: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-5899/pods/pod-sharedvolume-2b2eac26-5396-4d10-adb0-d2b481d2228d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jun  3 12:41:50.203: INFO: Exec stderr: ""
  Jun  3 12:41:50.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5899" for this suite. @ 06/03/23 12:41:50.208
• [2.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 06/03/23 12:41:50.22
  Jun  3 12:41:50.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 12:41:50.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:50.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:50.246
  Jun  3 12:41:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:41:52.282: INFO: Deleting pod "var-expansion-d6415565-6d5b-48e9-96f8-0e312d12c5d7" in namespace "var-expansion-2219"
  Jun  3 12:41:52.291: INFO: Wait up to 5m0s for pod "var-expansion-d6415565-6d5b-48e9-96f8-0e312d12c5d7" to be fully deleted
  STEP: Destroying namespace "var-expansion-2219" for this suite. @ 06/03/23 12:41:54.301
• [4.088 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 06/03/23 12:41:54.309
  Jun  3 12:41:54.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:41:54.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:54.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:54.336
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:41:54.341
  STEP: Saw pod success @ 06/03/23 12:41:58.37
  Jun  3 12:41:58.376: INFO: Trying to get logs from node ip-172-31-85-85 pod downwardapi-volume-db73d4d8-a3ee-4e17-8986-3d2829a8a454 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:41:58.404
  Jun  3 12:41:58.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8856" for this suite. @ 06/03/23 12:41:58.43
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 06/03/23 12:41:58.443
  Jun  3 12:41:58.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:41:58.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:41:58.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:41:58.473
  STEP: Creating secret with name secret-test-328042f7-29f2-44ff-9488-6f5aab5ab897 @ 06/03/23 12:41:58.478
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:41:58.485
  STEP: Saw pod success @ 06/03/23 12:42:02.514
  Jun  3 12:42:02.518: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-0c053321-3121-4b04-ba94-6215690ad43d container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:42:02.533
  Jun  3 12:42:02.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4350" for this suite. @ 06/03/23 12:42:02.567
• [4.137 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 06/03/23 12:42:02.581
  Jun  3 12:42:02.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:42:02.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:02.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:02.614
  STEP: Creating configMap configmap-1714/configmap-test-1fe20ce0-9c32-4d88-b64d-5e8a714afae3 @ 06/03/23 12:42:02.621
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:42:02.637
  STEP: Saw pod success @ 06/03/23 12:42:06.666
  Jun  3 12:42:06.671: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-ab687a75-f770-451f-80fd-487bdeb639eb container env-test: <nil>
  STEP: delete the pod @ 06/03/23 12:42:06.68
  Jun  3 12:42:06.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1714" for this suite. @ 06/03/23 12:42:06.705
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 06/03/23 12:42:06.714
  Jun  3 12:42:06.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 12:42:06.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:06.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:06.74
  Jun  3 12:42:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  W0603 12:42:09.321363      18 warnings.go:70] unknown field "alpha"
  W0603 12:42:09.321401      18 warnings.go:70] unknown field "beta"
  W0603 12:42:09.321410      18 warnings.go:70] unknown field "delta"
  W0603 12:42:09.321417      18 warnings.go:70] unknown field "epsilon"
  W0603 12:42:09.321423      18 warnings.go:70] unknown field "gamma"
  Jun  3 12:42:09.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2161" for this suite. @ 06/03/23 12:42:09.363
• [2.657 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 06/03/23 12:42:09.372
  Jun  3 12:42:09.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 12:42:09.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:09.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:09.399
  STEP: create the deployment @ 06/03/23 12:42:09.404
  W0603 12:42:09.412267      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 06/03/23 12:42:09.412
  STEP: delete the deployment @ 06/03/23 12:42:09.926
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 06/03/23 12:42:09.936
  STEP: Gathering metrics @ 06/03/23 12:42:10.469
  W0603 12:42:10.475407      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 12:42:10.475: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 12:42:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9438" for this suite. @ 06/03/23 12:42:10.482
• [1.118 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 06/03/23 12:42:10.491
  Jun  3 12:42:10.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 12:42:10.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:10.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:10.519
  STEP: reading a file in the container @ 06/03/23 12:42:12.558
  Jun  3 12:42:12.558: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1733 pod-service-account-41f63bec-9d8a-47a2-9fba-c4fefdc6485c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 06/03/23 12:42:12.722
  Jun  3 12:42:12.722: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1733 pod-service-account-41f63bec-9d8a-47a2-9fba-c4fefdc6485c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 06/03/23 12:42:12.878
  Jun  3 12:42:12.878: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1733 pod-service-account-41f63bec-9d8a-47a2-9fba-c4fefdc6485c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jun  3 12:42:13.040: INFO: Got root ca configmap in namespace "svcaccounts-1733"
  Jun  3 12:42:13.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1733" for this suite. @ 06/03/23 12:42:13.049
• [2.566 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 06/03/23 12:42:13.057
  Jun  3 12:42:13.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 12:42:13.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:13.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:13.082
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 06/03/23 12:42:13.087
  STEP: When a replicaset with a matching selector is created @ 06/03/23 12:42:15.114
  STEP: Then the orphan pod is adopted @ 06/03/23 12:42:15.121
  STEP: When the matched label of one of its pods change @ 06/03/23 12:42:16.132
  Jun  3 12:42:16.136: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 06/03/23 12:42:16.153
  Jun  3 12:42:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5811" for this suite. @ 06/03/23 12:42:17.168
• [4.119 seconds]
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 06/03/23 12:42:17.177
  Jun  3 12:42:17.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:42:17.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:42:17.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:42:17.201
  STEP: Creating pod busybox-f31b3793-b08e-474d-b565-37db6e6dc4cd in namespace container-probe-1873 @ 06/03/23 12:42:17.206
  Jun  3 12:42:19.229: INFO: Started pod busybox-f31b3793-b08e-474d-b565-37db6e6dc4cd in namespace container-probe-1873
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 12:42:19.23
  Jun  3 12:42:19.234: INFO: Initial restart count of pod busybox-f31b3793-b08e-474d-b565-37db6e6dc4cd is 0
  Jun  3 12:43:09.371: INFO: Restart count of pod container-probe-1873/busybox-f31b3793-b08e-474d-b565-37db6e6dc4cd is now 1 (50.136611949s elapsed)
  Jun  3 12:43:09.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:43:09.377
  STEP: Destroying namespace "container-probe-1873" for this suite. @ 06/03/23 12:43:09.392
• [52.223 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 06/03/23 12:43:09.401
  Jun  3 12:43:09.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:43:09.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:09.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:09.435
  STEP: Creating secret with name secret-test-6b48070f-3271-4722-bfde-9a759c8a06f6 @ 06/03/23 12:43:09.44
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:43:09.446
  STEP: Saw pod success @ 06/03/23 12:43:13.473
  Jun  3 12:43:13.478: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-b4fe9731-1ad8-4abb-b73a-e0a8b9dce870 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:43:13.487
  Jun  3 12:43:13.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2705" for this suite. @ 06/03/23 12:43:13.516
• [4.125 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 06/03/23 12:43:13.527
  Jun  3 12:43:13.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:43:13.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:13.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:13.556
  STEP: Counting existing ResourceQuota @ 06/03/23 12:43:13.562
  STEP: Creating a ResourceQuota @ 06/03/23 12:43:18.568
  STEP: Ensuring resource quota status is calculated @ 06/03/23 12:43:18.576
  STEP: Creating a Service @ 06/03/23 12:43:20.581
  STEP: Creating a NodePort Service @ 06/03/23 12:43:20.601
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 06/03/23 12:43:20.627
  STEP: Ensuring resource quota status captures service creation @ 06/03/23 12:43:20.651
  STEP: Deleting Services @ 06/03/23 12:43:22.656
  STEP: Ensuring resource quota status released usage @ 06/03/23 12:43:22.692
  Jun  3 12:43:24.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4843" for this suite. @ 06/03/23 12:43:24.706
• [11.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 06/03/23 12:43:24.729
  Jun  3 12:43:24.729: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:43:24.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:24.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:24.758
  STEP: create deployment with httpd image @ 06/03/23 12:43:24.763
  Jun  3 12:43:24.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-3908 create -f -'
  Jun  3 12:43:26.030: INFO: stderr: ""
  Jun  3 12:43:26.030: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 06/03/23 12:43:26.03
  Jun  3 12:43:26.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-3908 diff -f -'
  Jun  3 12:43:26.270: INFO: rc: 1
  Jun  3 12:43:26.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-3908 delete -f -'
  Jun  3 12:43:26.358: INFO: stderr: ""
  Jun  3 12:43:26.358: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jun  3 12:43:26.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3908" for this suite. @ 06/03/23 12:43:26.362
• [1.642 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 06/03/23 12:43:26.371
  Jun  3 12:43:26.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename certificates @ 06/03/23 12:43:26.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:26.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:26.399
  STEP: getting /apis @ 06/03/23 12:43:27.305
  STEP: getting /apis/certificates.k8s.io @ 06/03/23 12:43:27.311
  STEP: getting /apis/certificates.k8s.io/v1 @ 06/03/23 12:43:27.313
  STEP: creating @ 06/03/23 12:43:27.315
  STEP: getting @ 06/03/23 12:43:27.339
  STEP: listing @ 06/03/23 12:43:27.344
  STEP: watching @ 06/03/23 12:43:27.348
  Jun  3 12:43:27.348: INFO: starting watch
  STEP: patching @ 06/03/23 12:43:27.35
  STEP: updating @ 06/03/23 12:43:27.359
  Jun  3 12:43:27.367: INFO: waiting for watch events with expected annotations
  Jun  3 12:43:27.367: INFO: saw patched and updated annotations
  STEP: getting /approval @ 06/03/23 12:43:27.368
  STEP: patching /approval @ 06/03/23 12:43:27.372
  STEP: updating /approval @ 06/03/23 12:43:27.38
  STEP: getting /status @ 06/03/23 12:43:27.39
  STEP: patching /status @ 06/03/23 12:43:27.394
  STEP: updating /status @ 06/03/23 12:43:27.405
  STEP: deleting @ 06/03/23 12:43:27.414
  STEP: deleting a collection @ 06/03/23 12:43:27.43
  Jun  3 12:43:27.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-1908" for this suite. @ 06/03/23 12:43:27.455
• [1.092 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 06/03/23 12:43:27.465
  Jun  3 12:43:27.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:43:27.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:27.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:27.491
  STEP: Creating a pod to test downward api env vars @ 06/03/23 12:43:27.496
  STEP: Saw pod success @ 06/03/23 12:43:31.528
  Jun  3 12:43:31.532: INFO: Trying to get logs from node ip-172-31-85-85 pod downward-api-b2b66dcc-81e5-4c9c-b402-1e2433444f43 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:43:31.553
  Jun  3 12:43:31.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5143" for this suite. @ 06/03/23 12:43:31.576
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 06/03/23 12:43:31.589
  Jun  3 12:43:31.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:43:31.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:31.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:31.618
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/03/23 12:43:31.623
  STEP: Saw pod success @ 06/03/23 12:43:35.648
  Jun  3 12:43:35.653: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-d8ccc431-ca1e-44f1-bfef-dc206341515e container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:43:35.663
  Jun  3 12:43:35.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9193" for this suite. @ 06/03/23 12:43:35.684
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 06/03/23 12:43:35.696
  Jun  3 12:43:35.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename init-container @ 06/03/23 12:43:35.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:35.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:35.729
  STEP: creating the pod @ 06/03/23 12:43:35.734
  Jun  3 12:43:35.734: INFO: PodSpec: initContainers in spec.initContainers
  Jun  3 12:43:40.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1164" for this suite. @ 06/03/23 12:43:40.26
• [4.575 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 06/03/23 12:43:40.271
  Jun  3 12:43:40.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:43:40.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:40.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:40.302
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:43:40.309
  STEP: Saw pod success @ 06/03/23 12:43:44.342
  Jun  3 12:43:44.349: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-7b7e0819-c349-4d4d-8851-1a0e19153708 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:43:44.359
  Jun  3 12:43:44.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9074" for this suite. @ 06/03/23 12:43:44.389
• [4.129 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 06/03/23 12:43:44.402
  Jun  3 12:43:44.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 12:43:44.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:44.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:44.432
  STEP: Creating a job @ 06/03/23 12:43:44.44
  STEP: Ensure pods equal to parallelism count is attached to the job @ 06/03/23 12:43:44.449
  STEP: patching /status @ 06/03/23 12:43:46.456
  STEP: updating /status @ 06/03/23 12:43:46.467
  STEP: get /status @ 06/03/23 12:43:46.509
  Jun  3 12:43:46.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8796" for this suite. @ 06/03/23 12:43:46.522
• [2.130 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 06/03/23 12:43:46.533
  Jun  3 12:43:46.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:43:46.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:46.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:46.565
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/03/23 12:43:46.573
  STEP: Saw pod success @ 06/03/23 12:43:50.607
  Jun  3 12:43:50.614: INFO: Trying to get logs from node ip-172-31-85-85 pod pod-6ed77dd9-d888-4059-9bce-42b15cd9b646 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:43:50.623
  Jun  3 12:43:50.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8997" for this suite. @ 06/03/23 12:43:50.652
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 06/03/23 12:43:50.663
  Jun  3 12:43:50.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:43:50.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:50.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:50.708
  STEP: Setting up server cert @ 06/03/23 12:43:50.742
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:43:51.413
  STEP: Deploying the webhook pod @ 06/03/23 12:43:51.423
  STEP: Wait for the deployment to be ready @ 06/03/23 12:43:51.441
  Jun  3 12:43:51.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:43:53.47
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:43:53.485
  Jun  3 12:43:54.485: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 06/03/23 12:43:54.49
  STEP: Creating a custom resource definition that should be denied by the webhook @ 06/03/23 12:43:54.515
  Jun  3 12:43:54.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:43:54.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5546" for this suite. @ 06/03/23 12:43:54.609
  STEP: Destroying namespace "webhook-markers-443" for this suite. @ 06/03/23 12:43:54.622
• [3.970 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 06/03/23 12:43:54.637
  Jun  3 12:43:54.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:43:54.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:54.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:54.669
  STEP: Creating a pod to test downward api env vars @ 06/03/23 12:43:54.677
  STEP: Saw pod success @ 06/03/23 12:43:58.718
  Jun  3 12:43:58.722: INFO: Trying to get logs from node ip-172-31-85-85 pod downward-api-9ad5007c-caa7-4995-9ec2-803d74227738 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:43:58.735
  Jun  3 12:43:58.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3304" for this suite. @ 06/03/23 12:43:58.767
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 06/03/23 12:43:58.781
  Jun  3 12:43:58.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:43:58.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:43:58.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:43:58.813
  STEP: Setting up server cert @ 06/03/23 12:43:58.857
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:43:59.303
  STEP: Deploying the webhook pod @ 06/03/23 12:43:59.309
  STEP: Wait for the deployment to be ready @ 06/03/23 12:43:59.326
  Jun  3 12:43:59.337: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:44:01.351
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:44:01.368
  Jun  3 12:44:02.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 06/03/23 12:44:02.373
  STEP: create a pod that should be updated by the webhook @ 06/03/23 12:44:02.396
  Jun  3 12:44:02.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7778" for this suite. @ 06/03/23 12:44:02.506
  STEP: Destroying namespace "webhook-markers-1124" for this suite. @ 06/03/23 12:44:02.516
• [3.745 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 06/03/23 12:44:02.529
  Jun  3 12:44:02.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 12:44:02.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:02.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:02.564
  STEP: create the rc @ 06/03/23 12:44:02.571
  W0603 12:44:02.580831      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/03/23 12:44:07.585
  STEP: wait for all pods to be garbage collected @ 06/03/23 12:44:07.594
  STEP: Gathering metrics @ 06/03/23 12:44:12.603
  W0603 12:44:12.610831      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 12:44:12.611: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 12:44:12.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2877" for this suite. @ 06/03/23 12:44:12.621
• [10.102 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 06/03/23 12:44:12.637
  Jun  3 12:44:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:44:12.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:12.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:12.668
  STEP: creating the pod @ 06/03/23 12:44:12.674
  STEP: submitting the pod to kubernetes @ 06/03/23 12:44:12.674
  W0603 12:44:12.686707      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 06/03/23 12:44:14.704
  STEP: updating the pod @ 06/03/23 12:44:14.708
  Jun  3 12:44:15.222: INFO: Successfully updated pod "pod-update-activedeadlineseconds-71cb370e-3005-4dda-b8f2-bde0bd3963ae"
  Jun  3 12:44:19.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7923" for this suite. @ 06/03/23 12:44:19.244
• [6.616 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 06/03/23 12:44:19.257
  Jun  3 12:44:19.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 12:44:19.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:19.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:19.287
  STEP: Creating simple DaemonSet "daemon-set" @ 06/03/23 12:44:19.32
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 12:44:19.327
  Jun  3 12:44:19.332: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:19.332: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:19.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:44:19.337: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:44:20.343: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:20.343: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:20.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:44:20.348: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:44:21.346: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:21.346: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:44:21.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 12:44:21.353: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 06/03/23 12:44:21.359
  Jun  3 12:44:21.364: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 06/03/23 12:44:21.365
  Jun  3 12:44:21.377: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 06/03/23 12:44:21.377
  Jun  3 12:44:21.380: INFO: Observed &DaemonSet event: ADDED
  Jun  3 12:44:21.381: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.381: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.381: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.382: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.382: INFO: Found daemon set daemon-set in namespace daemonsets-9429 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun  3 12:44:21.382: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 06/03/23 12:44:21.382
  STEP: watching for the daemon set status to be patched @ 06/03/23 12:44:21.391
  Jun  3 12:44:21.394: INFO: Observed &DaemonSet event: ADDED
  Jun  3 12:44:21.394: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.394: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.395: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.395: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.395: INFO: Observed daemon set daemon-set in namespace daemonsets-9429 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun  3 12:44:21.395: INFO: Observed &DaemonSet event: MODIFIED
  Jun  3 12:44:21.395: INFO: Found daemon set daemon-set in namespace daemonsets-9429 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jun  3 12:44:21.395: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 12:44:21.4
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9429, will wait for the garbage collector to delete the pods @ 06/03/23 12:44:21.4
  Jun  3 12:44:21.464: INFO: Deleting DaemonSet.extensions daemon-set took: 7.425418ms
  Jun  3 12:44:21.564: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.13199ms
  Jun  3 12:44:23.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:44:23.170: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 12:44:23.174: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17257"},"items":null}

  Jun  3 12:44:23.179: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17258"},"items":null}

  Jun  3 12:44:23.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9429" for this suite. @ 06/03/23 12:44:23.207
• [3.958 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 06/03/23 12:44:23.218
  Jun  3 12:44:23.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename csiinlinevolumes @ 06/03/23 12:44:23.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:23.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:23.247
  STEP: creating @ 06/03/23 12:44:23.251
  STEP: getting @ 06/03/23 12:44:23.272
  STEP: listing @ 06/03/23 12:44:23.28
  STEP: deleting @ 06/03/23 12:44:23.285
  Jun  3 12:44:23.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8818" for this suite. @ 06/03/23 12:44:23.313
• [0.103 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 06/03/23 12:44:23.322
  Jun  3 12:44:23.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context-test @ 06/03/23 12:44:23.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:23.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:23.346
  Jun  3 12:44:27.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7351" for this suite. @ 06/03/23 12:44:27.386
• [4.074 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 06/03/23 12:44:27.396
  Jun  3 12:44:27.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename limitrange @ 06/03/23 12:44:27.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:27.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:27.427
  STEP: Creating LimitRange "e2e-limitrange-pvsbz" in namespace "limitrange-7210" @ 06/03/23 12:44:27.432
  STEP: Creating another limitRange in another namespace @ 06/03/23 12:44:27.44
  Jun  3 12:44:27.462: INFO: Namespace "e2e-limitrange-pvsbz-1467" created
  Jun  3 12:44:27.462: INFO: Creating LimitRange "e2e-limitrange-pvsbz" in namespace "e2e-limitrange-pvsbz-1467"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-pvsbz" @ 06/03/23 12:44:27.468
  Jun  3 12:44:27.473: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-pvsbz" in "limitrange-7210" namespace @ 06/03/23 12:44:27.474
  Jun  3 12:44:27.482: INFO: LimitRange "e2e-limitrange-pvsbz" has been patched
  STEP: Delete LimitRange "e2e-limitrange-pvsbz" by Collection with labelSelector: "e2e-limitrange-pvsbz=patched" @ 06/03/23 12:44:27.482
  STEP: Confirm that the limitRange "e2e-limitrange-pvsbz" has been deleted @ 06/03/23 12:44:27.493
  Jun  3 12:44:27.493: INFO: Requesting list of LimitRange to confirm quantity
  Jun  3 12:44:27.497: INFO: Found 0 LimitRange with label "e2e-limitrange-pvsbz=patched"
  Jun  3 12:44:27.497: INFO: LimitRange "e2e-limitrange-pvsbz" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-pvsbz" @ 06/03/23 12:44:27.497
  Jun  3 12:44:27.503: INFO: Found 1 limitRange
  Jun  3 12:44:27.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7210" for this suite. @ 06/03/23 12:44:27.509
  STEP: Destroying namespace "e2e-limitrange-pvsbz-1467" for this suite. @ 06/03/23 12:44:27.517
• [0.129 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 06/03/23 12:44:27.527
  Jun  3 12:44:27.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename init-container @ 06/03/23 12:44:27.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:27.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:27.554
  STEP: creating the pod @ 06/03/23 12:44:27.56
  Jun  3 12:44:27.560: INFO: PodSpec: initContainers in spec.initContainers
  Jun  3 12:44:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9918" for this suite. @ 06/03/23 12:44:31.393
• [3.874 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 06/03/23 12:44:31.402
  Jun  3 12:44:31.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:44:31.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:31.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:31.427
  STEP: starting the proxy server @ 06/03/23 12:44:31.433
  Jun  3 12:44:31.433: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-1117 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 06/03/23 12:44:31.495
  Jun  3 12:44:31.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1117" for this suite. @ 06/03/23 12:44:31.512
• [0.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 06/03/23 12:44:31.522
  Jun  3 12:44:31.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename watch @ 06/03/23 12:44:31.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:31.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:31.546
  STEP: getting a starting resourceVersion @ 06/03/23 12:44:31.552
  STEP: starting a background goroutine to produce watch events @ 06/03/23 12:44:31.557
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 06/03/23 12:44:31.558
  Jun  3 12:44:34.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9150" for this suite. @ 06/03/23 12:44:34.329
• [2.861 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 06/03/23 12:44:34.385
  Jun  3 12:44:34.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:44:34.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:34.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:34.413
  Jun  3 12:44:34.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: creating the pod @ 06/03/23 12:44:34.418
  STEP: submitting the pod to kubernetes @ 06/03/23 12:44:34.418
  Jun  3 12:44:36.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-202" for this suite. @ 06/03/23 12:44:36.541
• [2.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 06/03/23 12:44:36.554
  Jun  3 12:44:36.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 12:44:36.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:36.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:36.581
  Jun  3 12:44:58.666: INFO: Container started at 2023-06-03 12:44:37 +0000 UTC, pod became ready at 2023-06-03 12:44:56 +0000 UTC
  Jun  3 12:44:58.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9326" for this suite. @ 06/03/23 12:44:58.671
• [22.125 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 06/03/23 12:44:58.681
  Jun  3 12:44:58.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 12:44:58.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:44:58.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:44:58.708
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 06/03/23 12:44:58.715
  STEP: When a replication controller with a matching selector is created @ 06/03/23 12:45:00.743
  STEP: Then the orphan pod is adopted @ 06/03/23 12:45:00.751
  Jun  3 12:45:01.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9783" for this suite. @ 06/03/23 12:45:01.77
• [3.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 06/03/23 12:45:01.782
  Jun  3 12:45:01.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:45:01.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:01.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:01.806
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 06/03/23 12:45:01.811
  Jun  3 12:45:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:45:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:45:09.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-692" for this suite. @ 06/03/23 12:45:09.43
• [7.656 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 06/03/23 12:45:09.438
  Jun  3 12:45:09.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:45:09.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:09.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:09.466
  STEP: Counting existing ResourceQuota @ 06/03/23 12:45:09.47
  STEP: Creating a ResourceQuota @ 06/03/23 12:45:14.477
  STEP: Ensuring resource quota status is calculated @ 06/03/23 12:45:14.483
  STEP: Creating a Pod that fits quota @ 06/03/23 12:45:16.488
  STEP: Ensuring ResourceQuota status captures the pod usage @ 06/03/23 12:45:16.508
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 06/03/23 12:45:18.513
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 06/03/23 12:45:18.517
  STEP: Ensuring a pod cannot update its resource requirements @ 06/03/23 12:45:18.521
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 06/03/23 12:45:18.527
  STEP: Deleting the pod @ 06/03/23 12:45:20.532
  STEP: Ensuring resource quota status released the pod usage @ 06/03/23 12:45:20.549
  Jun  3 12:45:22.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2240" for this suite. @ 06/03/23 12:45:22.558
• [13.128 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 06/03/23 12:45:22.566
  Jun  3 12:45:22.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/03/23 12:45:22.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:22.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:22.599
  STEP: create the container to handle the HTTPGet hook request. @ 06/03/23 12:45:22.609
  STEP: create the pod with lifecycle hook @ 06/03/23 12:45:24.637
  STEP: check poststart hook @ 06/03/23 12:45:26.658
  STEP: delete the pod with lifecycle hook @ 06/03/23 12:45:26.667
  Jun  3 12:45:28.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9776" for this suite. @ 06/03/23 12:45:28.688
• [6.129 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 06/03/23 12:45:28.696
  Jun  3 12:45:28.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:45:28.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:28.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:28.722
  STEP: creating an Endpoint @ 06/03/23 12:45:28.735
  STEP: waiting for available Endpoint @ 06/03/23 12:45:28.744
  STEP: listing all Endpoints @ 06/03/23 12:45:28.747
  STEP: updating the Endpoint @ 06/03/23 12:45:28.753
  STEP: fetching the Endpoint @ 06/03/23 12:45:28.761
  STEP: patching the Endpoint @ 06/03/23 12:45:28.766
  STEP: fetching the Endpoint @ 06/03/23 12:45:28.778
  STEP: deleting the Endpoint by Collection @ 06/03/23 12:45:28.782
  STEP: waiting for Endpoint deletion @ 06/03/23 12:45:28.791
  STEP: fetching the Endpoint @ 06/03/23 12:45:28.794
  Jun  3 12:45:28.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2643" for this suite. @ 06/03/23 12:45:28.805
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 06/03/23 12:45:28.819
  Jun  3 12:45:28.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:45:28.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:28.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:28.843
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:45:28.848
  STEP: Saw pod success @ 06/03/23 12:45:32.876
  Jun  3 12:45:32.881: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-c80bc607-a6ac-417b-bc66-357ba8dd2821 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:45:32.902
  Jun  3 12:45:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3247" for this suite. @ 06/03/23 12:45:32.923
• [4.112 seconds]
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 06/03/23 12:45:32.931
  Jun  3 12:45:32.931: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption @ 06/03/23 12:45:32.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:45:32.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:45:32.959
  Jun  3 12:45:32.984: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 12:46:33.011: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 06/03/23 12:46:33.015
  Jun  3 12:46:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption-path @ 06/03/23 12:46:33.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:46:33.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:46:33.04
  Jun  3 12:46:33.064: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jun  3 12:46:33.069: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jun  3 12:46:33.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:46:33.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2646" for this suite. @ 06/03/23 12:46:33.166
  STEP: Destroying namespace "sched-preemption-5543" for this suite. @ 06/03/23 12:46:33.176
• [60.255 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 06/03/23 12:46:33.186
  Jun  3 12:46:33.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:46:33.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:46:33.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:46:33.212
  STEP: set up a multi version CRD @ 06/03/23 12:46:33.217
  Jun  3 12:46:33.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: rename a version @ 06/03/23 12:46:36.905
  STEP: check the new version name is served @ 06/03/23 12:46:36.924
  STEP: check the old version name is removed @ 06/03/23 12:46:39.052
  STEP: check the other version is not changed @ 06/03/23 12:46:39.833
  Jun  3 12:46:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-714" for this suite. @ 06/03/23 12:46:42.812
• [9.635 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 06/03/23 12:46:42.825
  Jun  3 12:46:42.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 12:46:42.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:46:42.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:46:42.849
  Jun  3 12:46:42.871: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun  3 12:46:47.877: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 12:46:47.878
  STEP: Scaling up "test-rs" replicaset  @ 06/03/23 12:46:47.878
  Jun  3 12:46:47.890: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 06/03/23 12:46:47.89
  W0603 12:46:47.902431      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun  3 12:46:47.905: INFO: observed ReplicaSet test-rs in namespace replicaset-7171 with ReadyReplicas 1, AvailableReplicas 1
  Jun  3 12:46:47.923: INFO: observed ReplicaSet test-rs in namespace replicaset-7171 with ReadyReplicas 1, AvailableReplicas 1
  Jun  3 12:46:47.940: INFO: observed ReplicaSet test-rs in namespace replicaset-7171 with ReadyReplicas 1, AvailableReplicas 1
  Jun  3 12:46:47.948: INFO: observed ReplicaSet test-rs in namespace replicaset-7171 with ReadyReplicas 1, AvailableReplicas 1
  Jun  3 12:46:49.167: INFO: observed ReplicaSet test-rs in namespace replicaset-7171 with ReadyReplicas 2, AvailableReplicas 2
  Jun  3 12:46:49.437: INFO: observed Replicaset test-rs in namespace replicaset-7171 with ReadyReplicas 3 found true
  Jun  3 12:46:49.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7171" for this suite. @ 06/03/23 12:46:49.443
• [6.627 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 06/03/23 12:46:49.452
  Jun  3 12:46:49.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-preemption @ 06/03/23 12:46:49.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:46:49.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:46:49.479
  Jun  3 12:46:49.501: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 12:47:49.529: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 06/03/23 12:47:49.533
  Jun  3 12:47:49.568: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jun  3 12:47:49.577: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jun  3 12:47:49.608: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jun  3 12:47:49.622: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jun  3 12:47:49.650: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jun  3 12:47:49.658: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 06/03/23 12:47:49.658
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 06/03/23 12:47:51.704
  Jun  3 12:47:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3804" for this suite. @ 06/03/23 12:47:55.828
• [66.387 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 06/03/23 12:47:55.84
  Jun  3 12:47:55.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:47:55.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:47:55.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:47:55.869
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 06/03/23 12:47:55.876
  STEP: Saw pod success @ 06/03/23 12:47:59.912
  Jun  3 12:47:59.918: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-77420f41-76a2-4756-851d-59aecad57a7b container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:47:59.948
  Jun  3 12:47:59.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4865" for this suite. @ 06/03/23 12:47:59.979
• [4.147 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 06/03/23 12:47:59.989
  Jun  3 12:47:59.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 12:47:59.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:00.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:00.017
  STEP: Creating a test headless service @ 06/03/23 12:48:00.026
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2252 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2252;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2252 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2252;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2252.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2252.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2252.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2252.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2252.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2252.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2252.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2252.svc;check="$$(dig +notcp +noall +answer +search 63.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.63_tcp@PTR;sleep 1; done
   @ 06/03/23 12:48:00.05
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2252 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2252;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2252 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2252;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2252.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2252.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2252.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2252.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2252.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2252.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2252.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2252.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2252.svc;check="$$(dig +notcp +noall +answer +search 63.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.63_udp@PTR;check="$$(dig +tcp +noall +answer +search 63.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.63_tcp@PTR;sleep 1; done
   @ 06/03/23 12:48:00.051
  STEP: creating a pod to probe DNS @ 06/03/23 12:48:00.051
  STEP: submitting the pod to kubernetes @ 06/03/23 12:48:00.051
  STEP: retrieving the pod @ 06/03/23 12:48:10.108
  STEP: looking for the results for each expected name from probers @ 06/03/23 12:48:10.113
  Jun  3 12:48:10.119: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.126: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.131: INFO: Unable to read wheezy_udp@dns-test-service.dns-2252 from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.137: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2252 from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.144: INFO: Unable to read wheezy_udp@dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.156: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.190: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.196: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.202: INFO: Unable to read jessie_udp@dns-test-service.dns-2252 from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-2252 from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.212: INFO: Unable to read jessie_udp@dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.219: INFO: Unable to read jessie_tcp@dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.231: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2252.svc from pod dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10: the server could not find the requested resource (get pods dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10)
  Jun  3 12:48:10.260: INFO: Lookups using dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2252 wheezy_tcp@dns-test-service.dns-2252 wheezy_udp@dns-test-service.dns-2252.svc wheezy_tcp@dns-test-service.dns-2252.svc wheezy_udp@_http._tcp.dns-test-service.dns-2252.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2252.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2252 jessie_tcp@dns-test-service.dns-2252 jessie_udp@dns-test-service.dns-2252.svc jessie_tcp@dns-test-service.dns-2252.svc jessie_udp@_http._tcp.dns-test-service.dns-2252.svc jessie_tcp@_http._tcp.dns-test-service.dns-2252.svc]

  Jun  3 12:48:15.384: INFO: DNS probes using dns-2252/dns-test-7ec0961c-a744-472c-ab7d-3a64950a0c10 succeeded

  Jun  3 12:48:15.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 12:48:15.388
  STEP: deleting the test service @ 06/03/23 12:48:15.404
  STEP: deleting the test headless service @ 06/03/23 12:48:15.429
  STEP: Destroying namespace "dns-2252" for this suite. @ 06/03/23 12:48:15.443
• [15.462 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 06/03/23 12:48:15.453
  Jun  3 12:48:15.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:48:15.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:15.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:15.488
  STEP: Creating configMap with name configmap-test-upd-c1c2fd3d-0c92-4b07-8fdf-9883a0e9eb71 @ 06/03/23 12:48:15.498
  STEP: Creating the pod @ 06/03/23 12:48:15.505
  STEP: Waiting for pod with text data @ 06/03/23 12:48:19.533
  STEP: Waiting for pod with binary data @ 06/03/23 12:48:19.543
  Jun  3 12:48:19.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8991" for this suite. @ 06/03/23 12:48:19.556
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 06/03/23 12:48:19.573
  Jun  3 12:48:19.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename runtimeclass @ 06/03/23 12:48:19.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:19.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:19.598
  STEP: Deleting RuntimeClass runtimeclass-176-delete-me @ 06/03/23 12:48:19.61
  STEP: Waiting for the RuntimeClass to disappear @ 06/03/23 12:48:19.617
  Jun  3 12:48:19.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-176" for this suite. @ 06/03/23 12:48:19.638
• [0.074 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 06/03/23 12:48:19.648
  Jun  3 12:48:19.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:48:19.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:19.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:19.676
  STEP: Creating configMap with name configmap-test-volume-0a1c1e93-4498-4057-ad92-ed9bcb52d541 @ 06/03/23 12:48:19.681
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:48:19.688
  STEP: Saw pod success @ 06/03/23 12:48:23.712
  Jun  3 12:48:23.717: INFO: Trying to get logs from node ip-172-31-85-85 pod pod-configmaps-887f898f-d8ce-4ebf-93a8-83cdc5dc2f52 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:48:23.742
  Jun  3 12:48:23.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8578" for this suite. @ 06/03/23 12:48:23.767
• [4.129 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 06/03/23 12:48:23.779
  Jun  3 12:48:23.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 12:48:23.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:23.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:23.809
  STEP: create the rc @ 06/03/23 12:48:23.819
  W0603 12:48:23.825993      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 06/03/23 12:48:29.832
  STEP: wait for the rc to be deleted @ 06/03/23 12:48:29.842
  Jun  3 12:48:30.871: INFO: 80 pods remaining
  Jun  3 12:48:30.872: INFO: 80 pods has nil DeletionTimestamp
  Jun  3 12:48:30.872: INFO: 
  Jun  3 12:48:31.857: INFO: 71 pods remaining
  Jun  3 12:48:31.857: INFO: 71 pods has nil DeletionTimestamp
  Jun  3 12:48:31.857: INFO: 
  Jun  3 12:48:32.863: INFO: 60 pods remaining
  Jun  3 12:48:32.863: INFO: 60 pods has nil DeletionTimestamp
  Jun  3 12:48:32.863: INFO: 
  Jun  3 12:48:33.874: INFO: 40 pods remaining
  Jun  3 12:48:33.874: INFO: 40 pods has nil DeletionTimestamp
  Jun  3 12:48:33.874: INFO: 
  Jun  3 12:48:34.856: INFO: 31 pods remaining
  Jun  3 12:48:34.856: INFO: 31 pods has nil DeletionTimestamp
  Jun  3 12:48:34.856: INFO: 
  Jun  3 12:48:35.861: INFO: 20 pods remaining
  Jun  3 12:48:35.861: INFO: 20 pods has nil DeletionTimestamp
  Jun  3 12:48:35.861: INFO: 
  STEP: Gathering metrics @ 06/03/23 12:48:36.853
  W0603 12:48:36.859288      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 12:48:36.859: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 12:48:36.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4920" for this suite. @ 06/03/23 12:48:36.864
• [13.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 06/03/23 12:48:36.877
  Jun  3 12:48:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:48:36.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:36.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:36.924
  STEP: set up a multi version CRD @ 06/03/23 12:48:36.948
  Jun  3 12:48:36.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: mark a version not serverd @ 06/03/23 12:48:41.815
  STEP: check the unserved version gets removed @ 06/03/23 12:48:41.839
  STEP: check the other version is not changed @ 06/03/23 12:48:43.949
  Jun  3 12:48:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8131" for this suite. @ 06/03/23 12:48:47.692
• [10.824 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 06/03/23 12:48:47.702
  Jun  3 12:48:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename discovery @ 06/03/23 12:48:47.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:47.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:47.727
  STEP: Setting up server cert @ 06/03/23 12:48:47.732
  Jun  3 12:48:48.536: INFO: Checking APIGroup: apiregistration.k8s.io
  Jun  3 12:48:48.537: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jun  3 12:48:48.537: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jun  3 12:48:48.537: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jun  3 12:48:48.537: INFO: Checking APIGroup: apps
  Jun  3 12:48:48.538: INFO: PreferredVersion.GroupVersion: apps/v1
  Jun  3 12:48:48.538: INFO: Versions found [{apps/v1 v1}]
  Jun  3 12:48:48.538: INFO: apps/v1 matches apps/v1
  Jun  3 12:48:48.538: INFO: Checking APIGroup: events.k8s.io
  Jun  3 12:48:48.539: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jun  3 12:48:48.539: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jun  3 12:48:48.539: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jun  3 12:48:48.539: INFO: Checking APIGroup: authentication.k8s.io
  Jun  3 12:48:48.541: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jun  3 12:48:48.541: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jun  3 12:48:48.541: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jun  3 12:48:48.541: INFO: Checking APIGroup: authorization.k8s.io
  Jun  3 12:48:48.542: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jun  3 12:48:48.542: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jun  3 12:48:48.542: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jun  3 12:48:48.542: INFO: Checking APIGroup: autoscaling
  Jun  3 12:48:48.543: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jun  3 12:48:48.543: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jun  3 12:48:48.543: INFO: autoscaling/v2 matches autoscaling/v2
  Jun  3 12:48:48.543: INFO: Checking APIGroup: batch
  Jun  3 12:48:48.545: INFO: PreferredVersion.GroupVersion: batch/v1
  Jun  3 12:48:48.545: INFO: Versions found [{batch/v1 v1}]
  Jun  3 12:48:48.545: INFO: batch/v1 matches batch/v1
  Jun  3 12:48:48.545: INFO: Checking APIGroup: certificates.k8s.io
  Jun  3 12:48:48.547: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jun  3 12:48:48.547: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jun  3 12:48:48.547: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jun  3 12:48:48.547: INFO: Checking APIGroup: networking.k8s.io
  Jun  3 12:48:48.549: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jun  3 12:48:48.549: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jun  3 12:48:48.549: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jun  3 12:48:48.549: INFO: Checking APIGroup: policy
  Jun  3 12:48:48.550: INFO: PreferredVersion.GroupVersion: policy/v1
  Jun  3 12:48:48.550: INFO: Versions found [{policy/v1 v1}]
  Jun  3 12:48:48.550: INFO: policy/v1 matches policy/v1
  Jun  3 12:48:48.550: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jun  3 12:48:48.557: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jun  3 12:48:48.557: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jun  3 12:48:48.557: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jun  3 12:48:48.557: INFO: Checking APIGroup: storage.k8s.io
  Jun  3 12:48:48.559: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jun  3 12:48:48.559: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jun  3 12:48:48.559: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jun  3 12:48:48.559: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jun  3 12:48:48.560: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jun  3 12:48:48.560: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jun  3 12:48:48.560: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jun  3 12:48:48.560: INFO: Checking APIGroup: apiextensions.k8s.io
  Jun  3 12:48:48.562: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jun  3 12:48:48.562: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jun  3 12:48:48.562: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jun  3 12:48:48.562: INFO: Checking APIGroup: scheduling.k8s.io
  Jun  3 12:48:48.563: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jun  3 12:48:48.563: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jun  3 12:48:48.563: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jun  3 12:48:48.563: INFO: Checking APIGroup: coordination.k8s.io
  Jun  3 12:48:48.564: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jun  3 12:48:48.564: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jun  3 12:48:48.564: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jun  3 12:48:48.564: INFO: Checking APIGroup: node.k8s.io
  Jun  3 12:48:48.566: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jun  3 12:48:48.566: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jun  3 12:48:48.566: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jun  3 12:48:48.566: INFO: Checking APIGroup: discovery.k8s.io
  Jun  3 12:48:48.567: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jun  3 12:48:48.567: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jun  3 12:48:48.567: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jun  3 12:48:48.567: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jun  3 12:48:48.569: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jun  3 12:48:48.569: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jun  3 12:48:48.569: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jun  3 12:48:48.569: INFO: Checking APIGroup: metrics.k8s.io
  Jun  3 12:48:48.571: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jun  3 12:48:48.571: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jun  3 12:48:48.571: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jun  3 12:48:48.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-6318" for this suite. @ 06/03/23 12:48:48.576
• [0.882 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 06/03/23 12:48:48.584
  Jun  3 12:48:48.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 12:48:48.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:48.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:48.607
  Jun  3 12:48:48.620: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jun  3 12:48:53.625: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 12:48:53.625
  Jun  3 12:48:53.625: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 06/03/23 12:48:53.639
  Jun  3 12:48:55.663: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-889  b87a16f2-f87c-47cc-950e-de5dff4df97b 20857 1 2023-06-03 12:48:53 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-03 12:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:48:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00557d168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-03 12:48:53 +0000 UTC,LastTransitionTime:2023-06-03 12:48:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-06-03 12:48:54 +0000 UTC,LastTransitionTime:2023-06-03 12:48:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun  3 12:48:55.667: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-889  79c88278-4a79-4324-9957-e7b6fdca5b22 20847 1 2023-06-03 12:48:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b87a16f2-f87c-47cc-950e-de5dff4df97b 0xc00557d537 0xc00557d538}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b87a16f2-f87c-47cc-950e-de5dff4df97b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:48:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00557d5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:48:55.671: INFO: Pod "test-cleanup-deployment-68b75d69f8-bxfv2" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-bxfv2 test-cleanup-deployment-68b75d69f8- deployment-889  a78c4e34-b14a-4201-8b1e-0ee818967589 20846 0 2023-06-03 12:48:53 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 79c88278-4a79-4324-9957-e7b6fdca5b22 0xc004967107 0xc004967108}] [] [{kube-controller-manager Update v1 2023-06-03 12:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79c88278-4a79-4324-9957-e7b6fdca5b22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:48:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj7nd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj7nd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:48:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:48:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.252,StartTime:2023-06-03 12:48:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:48:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7ebf61018f54d2695f7cd2fcb84b57834495250a51caa43914aecc28969e0d5b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.252,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:48:55.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-889" for this suite. @ 06/03/23 12:48:55.676
• [7.098 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 06/03/23 12:48:55.683
  Jun  3 12:48:55.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:48:55.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:55.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:55.704
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:48:55.712
  STEP: Saw pod success @ 06/03/23 12:48:59.737
  Jun  3 12:48:59.741: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-1602179e-7891-47b8-8487-9815d5b9c8ab container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:48:59.762
  Jun  3 12:48:59.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-845" for this suite. @ 06/03/23 12:48:59.782
• [4.106 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 06/03/23 12:48:59.79
  Jun  3 12:48:59.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 12:48:59.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:48:59.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:48:59.814
  Jun  3 12:48:59.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:49:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-603" for this suite. @ 06/03/23 12:49:02.429
• [2.647 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 06/03/23 12:49:02.438
  Jun  3 12:49:02.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:49:02.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:02.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:02.462
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5055 @ 06/03/23 12:49:02.467
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 06/03/23 12:49:02.483
  STEP: creating service externalsvc in namespace services-5055 @ 06/03/23 12:49:02.483
  STEP: creating replication controller externalsvc in namespace services-5055 @ 06/03/23 12:49:02.509
  I0603 12:49:02.523216      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5055, replica count: 2
  I0603 12:49:05.575925      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 06/03/23 12:49:05.58
  Jun  3 12:49:05.601: INFO: Creating new exec pod
  Jun  3 12:49:07.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-5055 exec execpodtrr9v -- /bin/sh -x -c nslookup nodeport-service.services-5055.svc.cluster.local'
  Jun  3 12:49:07.827: INFO: stderr: "+ nslookup nodeport-service.services-5055.svc.cluster.local\n"
  Jun  3 12:49:07.827: INFO: stdout: "Server:\t\t10.152.183.236\nAddress:\t10.152.183.236#53\n\nnodeport-service.services-5055.svc.cluster.local\tcanonical name = externalsvc.services-5055.svc.cluster.local.\nName:\texternalsvc.services-5055.svc.cluster.local\nAddress: 10.152.183.102\n\n"
  Jun  3 12:49:07.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5055, will wait for the garbage collector to delete the pods @ 06/03/23 12:49:07.833
  Jun  3 12:49:07.895: INFO: Deleting ReplicationController externalsvc took: 7.560426ms
  Jun  3 12:49:07.995: INFO: Terminating ReplicationController externalsvc pods took: 100.150741ms
  Jun  3 12:49:10.224: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5055" for this suite. @ 06/03/23 12:49:10.244
• [7.816 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 06/03/23 12:49:10.255
  Jun  3 12:49:10.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 12:49:10.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:10.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:10.289
  Jun  3 12:49:10.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  W0603 12:49:10.297312      18 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00159a710 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0603 12:49:12.859544      18 warnings.go:70] unknown field "alpha"
  W0603 12:49:12.859746      18 warnings.go:70] unknown field "beta"
  W0603 12:49:12.859841      18 warnings.go:70] unknown field "delta"
  W0603 12:49:12.859951      18 warnings.go:70] unknown field "epsilon"
  W0603 12:49:12.860063      18 warnings.go:70] unknown field "gamma"
  Jun  3 12:49:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9082" for this suite. @ 06/03/23 12:49:12.904
• [2.657 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 06/03/23 12:49:12.913
  Jun  3 12:49:12.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:49:12.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:12.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:12.936
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/03/23 12:49:12.941
  STEP: Saw pod success @ 06/03/23 12:49:16.966
  Jun  3 12:49:16.970: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-77f5db83-11ab-46fb-9a54-476314a483d1 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:49:16.978
  Jun  3 12:49:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8133" for this suite. @ 06/03/23 12:49:17
• [4.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 06/03/23 12:49:17.008
  Jun  3 12:49:17.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:49:17.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:17.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:17.029
  STEP: Creating configMap with name configmap-test-volume-map-55cb0195-6b4f-4fac-a845-d3ace884c1d7 @ 06/03/23 12:49:17.033
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:49:17.038
  STEP: Saw pod success @ 06/03/23 12:49:21.059
  Jun  3 12:49:21.063: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-d9a8afa6-9f23-41a3-8b7c-8d05e8a77fd6 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:49:21.071
  Jun  3 12:49:21.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3456" for this suite. @ 06/03/23 12:49:21.091
• [4.090 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 06/03/23 12:49:21.099
  Jun  3 12:49:21.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 12:49:21.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:21.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:21.123
  STEP: Setting up server cert @ 06/03/23 12:49:21.152
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 12:49:21.673
  STEP: Deploying the webhook pod @ 06/03/23 12:49:21.684
  STEP: Wait for the deployment to be ready @ 06/03/23 12:49:21.697
  Jun  3 12:49:21.705: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:49:23.717
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:49:23.745
  Jun  3 12:49:24.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 06/03/23 12:49:24.749
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 06/03/23 12:49:24.775
  STEP: Creating a configMap that should not be mutated @ 06/03/23 12:49:24.783
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 06/03/23 12:49:24.795
  STEP: Creating a configMap that should be mutated @ 06/03/23 12:49:24.804
  Jun  3 12:49:24.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-259" for this suite. @ 06/03/23 12:49:24.887
  STEP: Destroying namespace "webhook-markers-3741" for this suite. @ 06/03/23 12:49:24.897
• [3.806 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 06/03/23 12:49:24.921
  Jun  3 12:49:24.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 12:49:24.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:24.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:24.946
  STEP: Creating a job @ 06/03/23 12:49:24.949
  STEP: Ensuring job reaches completions @ 06/03/23 12:49:24.955
  Jun  3 12:49:36.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8664" for this suite. @ 06/03/23 12:49:36.967
• [12.054 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 06/03/23 12:49:36.976
  Jun  3 12:49:36.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 12:49:36.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:36.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:36.999
  STEP: Creating a pod to test env composition @ 06/03/23 12:49:37.003
  STEP: Saw pod success @ 06/03/23 12:49:41.026
  Jun  3 12:49:41.030: INFO: Trying to get logs from node ip-172-31-27-193 pod var-expansion-8c34d766-7890-44c3-8bfc-ee4af3ea22f7 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 12:49:41.037
  Jun  3 12:49:41.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9330" for this suite. @ 06/03/23 12:49:41.064
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 06/03/23 12:49:41.073
  Jun  3 12:49:41.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:49:41.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:41.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:41.099
  Jun  3 12:49:41.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/03/23 12:49:42.625
  Jun  3 12:49:42.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8231 --namespace=crd-publish-openapi-8231 create -f -'
  Jun  3 12:49:43.598: INFO: stderr: ""
  Jun  3 12:49:43.598: INFO: stdout: "e2e-test-crd-publish-openapi-548-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun  3 12:49:43.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8231 --namespace=crd-publish-openapi-8231 delete e2e-test-crd-publish-openapi-548-crds test-cr'
  Jun  3 12:49:43.765: INFO: stderr: ""
  Jun  3 12:49:43.765: INFO: stdout: "e2e-test-crd-publish-openapi-548-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jun  3 12:49:43.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8231 --namespace=crd-publish-openapi-8231 apply -f -'
  Jun  3 12:49:44.598: INFO: stderr: ""
  Jun  3 12:49:44.598: INFO: stdout: "e2e-test-crd-publish-openapi-548-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jun  3 12:49:44.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8231 --namespace=crd-publish-openapi-8231 delete e2e-test-crd-publish-openapi-548-crds test-cr'
  Jun  3 12:49:44.692: INFO: stderr: ""
  Jun  3 12:49:44.692: INFO: stdout: "e2e-test-crd-publish-openapi-548-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 06/03/23 12:49:44.692
  Jun  3 12:49:44.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-8231 explain e2e-test-crd-publish-openapi-548-crds'
  Jun  3 12:49:44.947: INFO: stderr: ""
  Jun  3 12:49:44.947: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-548-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Jun  3 12:49:46.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8231" for this suite. @ 06/03/23 12:49:46.404
• [5.340 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 06/03/23 12:49:46.416
  Jun  3 12:49:46.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 12:49:46.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:46.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:46.449
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 06/03/23 12:49:46.454
  STEP: Saw pod success @ 06/03/23 12:49:50.482
  Jun  3 12:49:50.486: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-ef31337d-3353-47e5-a94c-46485562cffd container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:49:50.506
  Jun  3 12:49:50.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4343" for this suite. @ 06/03/23 12:49:50.533
• [4.126 seconds]
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 06/03/23 12:49:50.542
  Jun  3 12:49:50.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 12:49:50.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:50.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:50.57
  Jun  3 12:49:50.575: INFO: Creating deployment "webserver-deployment"
  Jun  3 12:49:50.583: INFO: Waiting for observed generation 1
  Jun  3 12:49:52.596: INFO: Waiting for all required pods to come up
  Jun  3 12:49:52.602: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 06/03/23 12:49:52.602
  Jun  3 12:49:54.615: INFO: Waiting for deployment "webserver-deployment" to complete
  Jun  3 12:49:54.625: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jun  3 12:49:54.638: INFO: Updating deployment webserver-deployment
  Jun  3 12:49:54.638: INFO: Waiting for observed generation 2
  Jun  3 12:49:56.650: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jun  3 12:49:56.655: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jun  3 12:49:56.660: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun  3 12:49:56.674: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jun  3 12:49:56.674: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jun  3 12:49:56.679: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jun  3 12:49:56.690: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jun  3 12:49:56.690: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jun  3 12:49:56.705: INFO: Updating deployment webserver-deployment
  Jun  3 12:49:56.705: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jun  3 12:49:56.719: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jun  3 12:49:58.741: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jun  3 12:49:58.763: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-3251  fd450fbf-32e4-4f9a-99d9-7471f4c775eb 21930 3 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00557c3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-03 12:49:56 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-06-03 12:49:56 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jun  3 12:49:58.772: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-3251  89530a6f-d200-4dd5-9bf5-7e7382179418 21926 3 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fd450fbf-32e4-4f9a-99d9-7471f4c775eb 0xc00557c907 0xc00557c908}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd450fbf-32e4-4f9a-99d9-7471f4c775eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00557c9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:49:58.773: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jun  3 12:49:58.773: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-3251  fe447a93-6c4e-49ae-90fb-b85b9591ff26 21898 3 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fd450fbf-32e4-4f9a-99d9-7471f4c775eb 0xc00557c817 0xc00557c818}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd450fbf-32e4-4f9a-99d9-7471f4c775eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00557c8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:49:58.785: INFO: Pod "webserver-deployment-67bd4bf6dc-2cvfr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2cvfr webserver-deployment-67bd4bf6dc- deployment-3251  0e80978b-6939-4d80-88a3-51b6772c526f 21923 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557cf07 0xc00557cf08}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kr5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kr5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.790: INFO: Pod "webserver-deployment-67bd4bf6dc-2njmc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2njmc webserver-deployment-67bd4bf6dc- deployment-3251  93a80ebf-9250-4eed-bda1-3b98ea56d40b 21694 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557d117 0xc00557d118}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkwj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkwj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.64,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bc98847b32039763596a868b1030578877f1dcb8d58b51ff30d2c5b7051ffbc7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.64,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.793: INFO: Pod "webserver-deployment-67bd4bf6dc-9ws9p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9ws9p webserver-deployment-67bd4bf6dc- deployment-3251  404588fa-862c-47f1-8c64-c0dcfa23ae7d 21911 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557d307 0xc00557d308}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6vk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6vk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.794: INFO: Pod "webserver-deployment-67bd4bf6dc-blkfx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-blkfx webserver-deployment-67bd4bf6dc- deployment-3251  1d50eaf8-6fc0-47de-9750-189ecab47755 21918 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557d4d7 0xc00557d4d8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zghkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zghkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.799: INFO: Pod "webserver-deployment-67bd4bf6dc-cgdml" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cgdml webserver-deployment-67bd4bf6dc- deployment-3251  6de8b908-4b5d-42e4-ab2b-f59eddc861a5 21907 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557d6c7 0xc00557d6c8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9mm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9mm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.799: INFO: Pod "webserver-deployment-67bd4bf6dc-dxdh4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dxdh4 webserver-deployment-67bd4bf6dc- deployment-3251  64c9a633-4a3a-46a1-8460-d0c68f075311 21858 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557d8a7 0xc00557d8a8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m5gdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m5gdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.802: INFO: Pod "webserver-deployment-67bd4bf6dc-f5cmp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-f5cmp webserver-deployment-67bd4bf6dc- deployment-3251  e092977b-6a90-426b-90e8-c590985a556e 21908 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557da87 0xc00557da88}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgb7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgb7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.803: INFO: Pod "webserver-deployment-67bd4bf6dc-hw2f2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hw2f2 webserver-deployment-67bd4bf6dc- deployment-3251  a4423b9b-9de4-4d28-87fe-e8aafd22e0ff 21685 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557dc77 0xc00557dc78}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhjtz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhjtz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.203,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a386dd1397907e6a36012d92c736e3df7f86ffe0b356ddcec703d2abab3b080,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.203,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.803: INFO: Pod "webserver-deployment-67bd4bf6dc-jbbhg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jbbhg webserver-deployment-67bd4bf6dc- deployment-3251  71953363-b923-4833-a24c-f922ea5f1d52 21916 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc00557de67 0xc00557de68}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf7mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf7mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.804: INFO: Pod "webserver-deployment-67bd4bf6dc-mcblr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mcblr webserver-deployment-67bd4bf6dc- deployment-3251  6a5fcd4a-e7f4-4edb-9bf0-71d0b6a8bad4 21689 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4037 0xc0052f4038}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s4gn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s4gn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.72,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3fa067b434195a0379eb1ef97d025770f1433042a1bbf4c5cf935608344b533b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.72,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.808: INFO: Pod "webserver-deployment-67bd4bf6dc-mk757" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mk757 webserver-deployment-67bd4bf6dc- deployment-3251  6993decc-e4fc-4067-aacc-d22bc846d598 21709 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4227 0xc0052f4228}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lt5lc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lt5lc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:192.168.192.152,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a834c80e3069092996bc4cd80b35c2ee7749cbc7e8b38d5ff488b03c89c9f445,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.152,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.810: INFO: Pod "webserver-deployment-67bd4bf6dc-nmsv6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nmsv6 webserver-deployment-67bd4bf6dc- deployment-3251  f6322acf-c729-4b3f-ac7e-bafb87ddb08e 21697 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4417 0xc0052f4418}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ldvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ldvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.73,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ea6e13be8ff71461f09306bf43e0d7ee3bb931648ed7bc07f5ae49adce884b9b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.73,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.810: INFO: Pod "webserver-deployment-67bd4bf6dc-q5fg9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-q5fg9 webserver-deployment-67bd4bf6dc- deployment-3251  4e1f05af-b454-4ea7-a5a0-1c5c21687ed1 21902 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4607 0xc0052f4608}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4zwkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zwkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.815: INFO: Pod "webserver-deployment-67bd4bf6dc-qxqjz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qxqjz webserver-deployment-67bd4bf6dc- deployment-3251  fea99174-6a06-4ae1-b498-71b6fbeec0e5 21703 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4837 0xc0052f4838}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h57xt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h57xt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:192.168.192.146,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://214f561088d6071910b1d45ea3e8c4211701b2c860600159add054b66814438d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.146,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.816: INFO: Pod "webserver-deployment-67bd4bf6dc-scjdf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-scjdf webserver-deployment-67bd4bf6dc- deployment-3251  ea3ce02c-43eb-4625-9dfc-4f5778eb3aa9 21720 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4a27 0xc0052f4a28}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79r7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79r7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.207,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3c363cca6b9ab8852a9b5fadac7ee0655a8f050605e768f012bbd168939419e5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.207,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.817: INFO: Pod "webserver-deployment-67bd4bf6dc-sv2wb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sv2wb webserver-deployment-67bd4bf6dc- deployment-3251  45bce403-a8ef-4243-aee3-4afb76d17e5d 21881 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4c17 0xc0052f4c18}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f672m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f672m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.822: INFO: Pod "webserver-deployment-67bd4bf6dc-v25wt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-v25wt webserver-deployment-67bd4bf6dc- deployment-3251  eb2be2ac-a4ca-45b2-a325-1f27f85ec7af 21920 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4de7 0xc0052f4de8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xj5kq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xj5kq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.829: INFO: Pod "webserver-deployment-67bd4bf6dc-xc7c6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xc7c6 webserver-deployment-67bd4bf6dc- deployment-3251  4acd9953-0052-4475-92d9-81b9d0d24363 21913 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f4fb7 0xc0052f4fb8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqggh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqggh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.830: INFO: Pod "webserver-deployment-67bd4bf6dc-zrqpr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zrqpr webserver-deployment-67bd4bf6dc- deployment-3251  3e5ca07b-126e-4640-bafe-8f7aab2e12a6 21867 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f5187 0xc0052f5188}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64ghr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64ghr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.834: INFO: Pod "webserver-deployment-67bd4bf6dc-zz9mr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zz9mr webserver-deployment-67bd4bf6dc- deployment-3251  3712ded0-051f-4151-9160-987a5abd12bd 21706 0 2023-06-03 12:49:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fe447a93-6c4e-49ae-90fb-b85b9591ff26 0xc0052f5357 0xc0052f5358}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe447a93-6c4e-49ae-90fb-b85b9591ff26\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5pzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5pzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:192.168.192.160,StartTime:2023-06-03 12:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:49:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1aaf4356d6a0f3b62fd43b19c30228e03f58b4762343261fee5fbc1c6cf27408,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.160,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.834: INFO: Pod "webserver-deployment-7b75d79cf5-55kzb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-55kzb webserver-deployment-7b75d79cf5- deployment-3251  8805c575-017f-430b-bf63-38d42da79929 21834 0 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5547 0xc0052f5548}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbv27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbv27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.69,StartTime:2023-06-03 12:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.69,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.835: INFO: Pod "webserver-deployment-7b75d79cf5-66v56" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-66v56 webserver-deployment-7b75d79cf5- deployment-3251  ac43ec0d-4936-408d-b4ee-2c158732b352 21887 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5777 0xc0052f5778}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hs4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hs4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.835: INFO: Pod "webserver-deployment-7b75d79cf5-8scrp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8scrp webserver-deployment-7b75d79cf5- deployment-3251  f107c752-cfc5-4789-a7c6-2dad8d7809b3 21933 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5977 0xc0052f5978}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95595,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95595,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.836: INFO: Pod "webserver-deployment-7b75d79cf5-h46rk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-h46rk webserver-deployment-7b75d79cf5- deployment-3251  4aba5703-915b-4e41-899f-fac13b18b229 21825 0 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5b77 0xc0052f5b78}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrfpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrfpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.206,StartTime:2023-06-03 12:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.206,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.836: INFO: Pod "webserver-deployment-7b75d79cf5-hqgwd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hqgwd webserver-deployment-7b75d79cf5- deployment-3251  29cc5764-7dd2-4c23-9c1f-d9904a7c1813 21928 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5d97 0xc0052f5d98}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7k58z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7k58z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.837: INFO: Pod "webserver-deployment-7b75d79cf5-jj2jx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jj2jx webserver-deployment-7b75d79cf5- deployment-3251  14486f94-5037-4b02-9c56-99158a0aa8df 21829 0 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc0052f5f87 0xc0052f5f88}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j59f5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j59f5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.205,StartTime:2023-06-03 12:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.205,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.841: INFO: Pod "webserver-deployment-7b75d79cf5-jsfq7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jsfq7 webserver-deployment-7b75d79cf5- deployment-3251  14a8f0d7-12bd-480a-ae6f-bd252b039f39 21891 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5c1a7 0xc005d5c1a8}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x74tb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x74tb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.842: INFO: Pod "webserver-deployment-7b75d79cf5-lvx9r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lvx9r webserver-deployment-7b75d79cf5- deployment-3251  6d203164-89ab-4555-9333-1584b5bac73b 21927 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5c397 0xc005d5c398}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wldn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wldn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.842: INFO: Pod "webserver-deployment-7b75d79cf5-n9dtz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n9dtz webserver-deployment-7b75d79cf5- deployment-3251  d594e97c-891b-4467-bc1d-9724f6fea86b 21886 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5c587 0xc005d5c588}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdkz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdkz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.846: INFO: Pod "webserver-deployment-7b75d79cf5-ttkf2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ttkf2 webserver-deployment-7b75d79cf5- deployment-3251  c9686b6f-83de-4e4f-8fdd-265020810d04 21922 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5c787 0xc005d5c788}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4kp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4kp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.847: INFO: Pod "webserver-deployment-7b75d79cf5-vbsvc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vbsvc webserver-deployment-7b75d79cf5- deployment-3251  1175afa5-9e24-4922-b431-cfa0effc14a1 21838 0 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5c977 0xc005d5c978}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.192.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9wcs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9wcs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.203,PodIP:192.168.192.143,StartTime:2023-06-03 12:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.143,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.848: INFO: Pod "webserver-deployment-7b75d79cf5-wfrhc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wfrhc webserver-deployment-7b75d79cf5- deployment-3251  a113a067-1cf1-43e2-a560-e05196611624 21931 0 2023-06-03 12:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5cb97 0xc005d5cb98}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cn2dn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cn2dn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 12:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.848: INFO: Pod "webserver-deployment-7b75d79cf5-wt7d6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wt7d6 webserver-deployment-7b75d79cf5- deployment-3251  0b933ff3-0f8c-4101-b70e-46133a0aed68 21831 0 2023-06-03 12:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 89530a6f-d200-4dd5-9bf5-7e7382179418 0xc005d5cd87 0xc005d5cd88}] [] [{kube-controller-manager Update v1 2023-06-03 12:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89530a6f-d200-4dd5-9bf5-7e7382179418\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqmlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqmlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.68,StartTime:2023-06-03 12:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.68,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:49:58.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3251" for this suite. @ 06/03/23 12:49:58.855
• [8.321 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 06/03/23 12:49:58.864
  Jun  3 12:49:58.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:49:58.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:49:58.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:49:58.896
  STEP: Creating configMap with name projected-configmap-test-volume-map-a716756d-6089-4888-af9a-44ee862efc12 @ 06/03/23 12:49:58.902
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:49:58.911
  STEP: Saw pod success @ 06/03/23 12:50:02.941
  Jun  3 12:50:02.945: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-9ccc249a-a175-4a1e-80df-46997714f75a container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:50:02.954
  Jun  3 12:50:02.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3383" for this suite. @ 06/03/23 12:50:02.998
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 06/03/23 12:50:03.008
  Jun  3 12:50:03.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 12:50:03.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:50:03.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:50:03.079
  STEP: Creating service test in namespace statefulset-148 @ 06/03/23 12:50:03.089
  STEP: Creating a new StatefulSet @ 06/03/23 12:50:03.098
  Jun  3 12:50:03.112: INFO: Found 0 stateful pods, waiting for 3
  Jun  3 12:50:13.118: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 12:50:13.118: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 12:50:13.118: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 12:50:13.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-148 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:50:13.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:50:13.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:50:13.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/03/23 12:50:23.369
  Jun  3 12:50:23.391: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/03/23 12:50:23.391
  STEP: Updating Pods in reverse ordinal order @ 06/03/23 12:50:33.41
  Jun  3 12:50:33.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-148 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 12:50:33.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 12:50:33.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 12:50:33.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 06/03/23 12:50:53.606
  Jun  3 12:50:53.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-148 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 12:50:53.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 12:50:53.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 12:50:53.775: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 12:51:03.820: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 06/03/23 12:51:13.841
  Jun  3 12:51:13.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-148 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 12:51:14.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 12:51:14.007: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 12:51:14.007: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 12:51:24.034: INFO: Deleting all statefulset in ns statefulset-148
  Jun  3 12:51:24.038: INFO: Scaling statefulset ss2 to 0
  Jun  3 12:51:34.060: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 12:51:34.064: INFO: Deleting statefulset ss2
  Jun  3 12:51:34.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-148" for this suite. @ 06/03/23 12:51:34.085
• [91.086 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 06/03/23 12:51:34.094
  Jun  3 12:51:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:51:34.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:51:34.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:51:34.125
  STEP: Creating configMap with name cm-test-opt-del-8c9997cf-5585-4287-ba70-06578b26a971 @ 06/03/23 12:51:34.136
  STEP: Creating configMap with name cm-test-opt-upd-9f13bc7b-a670-44d9-947d-42f6ef823fb7 @ 06/03/23 12:51:34.146
  STEP: Creating the pod @ 06/03/23 12:51:34.156
  STEP: Deleting configmap cm-test-opt-del-8c9997cf-5585-4287-ba70-06578b26a971 @ 06/03/23 12:51:36.221
  STEP: Updating configmap cm-test-opt-upd-9f13bc7b-a670-44d9-947d-42f6ef823fb7 @ 06/03/23 12:51:36.228
  STEP: Creating configMap with name cm-test-opt-create-db4020c1-fca3-4605-beb5-933414b0995b @ 06/03/23 12:51:36.234
  STEP: waiting to observe update in volume @ 06/03/23 12:51:36.241
  Jun  3 12:52:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9641" for this suite. @ 06/03/23 12:52:52.674
• [78.587 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 06/03/23 12:52:52.682
  Jun  3 12:52:52.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:52:52.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:52:52.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:52:52.709
  STEP: Creating configMap with name projected-configmap-test-volume-1d939196-9f57-4bf6-b56f-4be11c716db7 @ 06/03/23 12:52:52.715
  STEP: Creating a pod to test consume configMaps @ 06/03/23 12:52:52.721
  STEP: Saw pod success @ 06/03/23 12:52:56.749
  Jun  3 12:52:56.755: INFO: Trying to get logs from node ip-172-31-85-85 pod pod-projected-configmaps-e442162b-f367-429b-8a45-9197168ee32a container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:52:56.777
  Jun  3 12:52:56.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5193" for this suite. @ 06/03/23 12:52:56.803
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 06/03/23 12:52:56.815
  Jun  3 12:52:56.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:52:56.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:52:56.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:52:56.842
  Jun  3 12:52:56.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7020" for this suite. @ 06/03/23 12:52:56.858
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 06/03/23 12:52:56.87
  Jun  3 12:52:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename podtemplate @ 06/03/23 12:52:56.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:52:56.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:52:56.898
  STEP: Create a pod template @ 06/03/23 12:52:56.904
  STEP: Replace a pod template @ 06/03/23 12:52:56.911
  Jun  3 12:52:56.922: INFO: Found updated podtemplate annotation: "true"

  Jun  3 12:52:56.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-600" for this suite. @ 06/03/23 12:52:56.929
• [0.066 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 06/03/23 12:52:56.937
  Jun  3 12:52:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context @ 06/03/23 12:52:56.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:52:56.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:52:56.966
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 06/03/23 12:52:56.971
  STEP: Saw pod success @ 06/03/23 12:53:01
  Jun  3 12:53:01.005: INFO: Trying to get logs from node ip-172-31-85-85 pod security-context-37a54add-2438-45eb-ab2e-ff27c608c304 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 12:53:01.013
  Jun  3 12:53:01.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3470" for this suite. @ 06/03/23 12:53:01.037
• [4.107 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 06/03/23 12:53:01.046
  Jun  3 12:53:01.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename prestop @ 06/03/23 12:53:01.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:01.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:01.074
  STEP: Creating server pod server in namespace prestop-3210 @ 06/03/23 12:53:01.079
  STEP: Waiting for pods to come up. @ 06/03/23 12:53:01.089
  STEP: Creating tester pod tester in namespace prestop-3210 @ 06/03/23 12:53:03.103
  STEP: Deleting pre-stop pod @ 06/03/23 12:53:05.124
  Jun  3 12:53:10.140: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jun  3 12:53:10.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 06/03/23 12:53:10.147
  STEP: Destroying namespace "prestop-3210" for this suite. @ 06/03/23 12:53:10.159
• [9.122 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 06/03/23 12:53:10.168
  Jun  3 12:53:10.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 12:53:10.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:10.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:10.195
  Jun  3 12:53:10.201: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jun  3 12:53:10.213: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jun  3 12:53:15.219: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 12:53:15.219
  Jun  3 12:53:15.219: INFO: Creating deployment "test-rolling-update-deployment"
  Jun  3 12:53:15.226: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jun  3 12:53:15.239: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jun  3 12:53:17.249: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jun  3 12:53:17.253: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jun  3 12:53:17.267: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-715  5e0951eb-1e1d-45a9-879e-0fbbd5f866c0 23528 1 2023-06-03 12:53:15 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-03 12:53:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:53:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dcbc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-03 12:53:15 +0000 UTC,LastTransitionTime:2023-06-03 12:53:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-06-03 12:53:16 +0000 UTC,LastTransitionTime:2023-06-03 12:53:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun  3 12:53:17.277: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-715  4a66756d-2a22-46a5-9073-f0b7a44be34a 23518 1 2023-06-03 12:53:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5e0951eb-1e1d-45a9-879e-0fbbd5f866c0 0xc0054501b7 0xc0054501b8}] [] [{kube-controller-manager Update apps/v1 2023-06-03 12:53:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0951eb-1e1d-45a9-879e-0fbbd5f866c0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:53:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005450268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:53:17.277: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jun  3 12:53:17.277: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-715  ce1d8024-e932-4e38-837a-023dc5f6364e 23527 2 2023-06-03 12:53:10 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5e0951eb-1e1d-45a9-879e-0fbbd5f866c0 0xc005450087 0xc005450088}] [] [{e2e.test Update apps/v1 2023-06-03 12:53:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:53:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5e0951eb-1e1d-45a9-879e-0fbbd5f866c0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-03 12:53:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005450148 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 12:53:17.282: INFO: Pod "test-rolling-update-deployment-656d657cd8-xdw7k" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-xdw7k test-rolling-update-deployment-656d657cd8- deployment-715  8507d0d0-c1e4-4843-853c-2c755179d7bd 23517 0 2023-06-03 12:53:15 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 4a66756d-2a22-46a5-9073-f0b7a44be34a 0xc0054506d7 0xc0054506d8}] [] [{kube-controller-manager Update v1 2023-06-03 12:53:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a66756d-2a22-46a5-9073-f0b7a44be34a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 12:53:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgqm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgqm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:53:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 12:53:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.80,StartTime:2023-06-03 12:53:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 12:53:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://25dcf4f725d74d2b9d42247e8dbaf9b2c4c961e66c604ae4688fe998a4504994,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 12:53:17.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-715" for this suite. @ 06/03/23 12:53:17.287
• [7.128 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 06/03/23 12:53:17.297
  Jun  3 12:53:17.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:53:17.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:17.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:17.327
  Jun  3 12:53:17.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: creating the pod @ 06/03/23 12:53:17.332
  STEP: submitting the pod to kubernetes @ 06/03/23 12:53:17.333
  Jun  3 12:53:19.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8018" for this suite. @ 06/03/23 12:53:19.4
• [2.111 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 06/03/23 12:53:19.408
  Jun  3 12:53:19.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 12:53:19.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:19.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:19.432
  Jun  3 12:53:19.463: INFO: created pod pod-service-account-defaultsa
  Jun  3 12:53:19.463: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jun  3 12:53:19.477: INFO: created pod pod-service-account-mountsa
  Jun  3 12:53:19.477: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jun  3 12:53:19.486: INFO: created pod pod-service-account-nomountsa
  Jun  3 12:53:19.487: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jun  3 12:53:19.496: INFO: created pod pod-service-account-defaultsa-mountspec
  Jun  3 12:53:19.496: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jun  3 12:53:19.507: INFO: created pod pod-service-account-mountsa-mountspec
  Jun  3 12:53:19.507: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jun  3 12:53:19.516: INFO: created pod pod-service-account-nomountsa-mountspec
  Jun  3 12:53:19.516: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jun  3 12:53:19.529: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jun  3 12:53:19.529: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jun  3 12:53:19.538: INFO: created pod pod-service-account-mountsa-nomountspec
  Jun  3 12:53:19.538: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jun  3 12:53:19.545: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jun  3 12:53:19.546: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jun  3 12:53:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5490" for this suite. @ 06/03/23 12:53:19.564
• [0.168 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 06/03/23 12:53:19.576
  Jun  3 12:53:19.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:53:19.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:19.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:19.61
  STEP: Creating secret with name secret-test-eb3e96e8-436b-47ce-8921-bf2cd40b5179 @ 06/03/23 12:53:19.615
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:53:19.623
  STEP: Saw pod success @ 06/03/23 12:53:23.655
  Jun  3 12:53:23.659: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-0174f885-8bfa-4c9c-a969-2ba679325a9d container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:53:23.667
  Jun  3 12:53:23.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8705" for this suite. @ 06/03/23 12:53:23.692
• [4.123 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 06/03/23 12:53:23.701
  Jun  3 12:53:23.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/03/23 12:53:23.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:23.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:23.729
  Jun  3 12:53:23.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:53:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5426" for this suite. @ 06/03/23 12:53:27.051
• [3.357 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 06/03/23 12:53:27.06
  Jun  3 12:53:27.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:53:27.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:27.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:27.091
  STEP: Creating projection with secret that has name projected-secret-test-7cbf7952-48d4-40b4-9c6b-cf163a4c08a0 @ 06/03/23 12:53:27.097
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:53:27.103
  STEP: Saw pod success @ 06/03/23 12:53:31.132
  Jun  3 12:53:31.137: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-secrets-9fa5a92a-1d23-4549-b936-fd9e9a0bcffe container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 12:53:31.144
  Jun  3 12:53:31.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8826" for this suite. @ 06/03/23 12:53:31.175
• [4.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 06/03/23 12:53:31.185
  Jun  3 12:53:31.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:53:31.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:31.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:31.209
  STEP: validating api versions @ 06/03/23 12:53:31.215
  Jun  3 12:53:31.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6449 api-versions'
  Jun  3 12:53:31.296: INFO: stderr: ""
  Jun  3 12:53:31.296: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jun  3 12:53:31.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6449" for this suite. @ 06/03/23 12:53:31.302
• [0.127 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 06/03/23 12:53:31.312
  Jun  3 12:53:31.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:53:31.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:31.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:31.337
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:53:31.342
  STEP: Saw pod success @ 06/03/23 12:53:35.371
  Jun  3 12:53:35.377: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-c8dc93c3-1b12-4769-a4c7-15c2453d46ca container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:53:35.386
  Jun  3 12:53:35.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3224" for this suite. @ 06/03/23 12:53:35.408
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 06/03/23 12:53:35.42
  Jun  3 12:53:35.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename watch @ 06/03/23 12:53:35.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:35.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:35.457
  STEP: creating a watch on configmaps with a certain label @ 06/03/23 12:53:35.467
  STEP: creating a new configmap @ 06/03/23 12:53:35.47
  STEP: modifying the configmap once @ 06/03/23 12:53:35.476
  STEP: changing the label value of the configmap @ 06/03/23 12:53:35.488
  STEP: Expecting to observe a delete notification for the watched object @ 06/03/23 12:53:35.499
  Jun  3 12:53:35.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23860 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:53:35.499: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23861 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:53:35.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23862 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 06/03/23 12:53:35.5
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 06/03/23 12:53:35.511
  STEP: changing the label value of the configmap back @ 06/03/23 12:53:45.514
  STEP: modifying the configmap a third time @ 06/03/23 12:53:45.53
  STEP: deleting the configmap @ 06/03/23 12:53:45.54
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 06/03/23 12:53:45.548
  Jun  3 12:53:45.548: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23916 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:53:45.548: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23917 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:53:45.549: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1965  ddde14e7-b133-4fdf-bd17-971e97c3af56 23919 0 2023-06-03 12:53:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-03 12:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 12:53:45.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1965" for this suite. @ 06/03/23 12:53:45.555
• [10.147 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 06/03/23 12:53:45.569
  Jun  3 12:53:45.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 12:53:45.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:45.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:45.604
  STEP: Creating a pod to test service account token:  @ 06/03/23 12:53:45.609
  STEP: Saw pod success @ 06/03/23 12:53:49.639
  Jun  3 12:53:49.644: INFO: Trying to get logs from node ip-172-31-27-193 pod test-pod-41e2f652-5961-44f0-830f-dd41827e4177 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:53:49.653
  Jun  3 12:53:49.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2983" for this suite. @ 06/03/23 12:53:49.677
• [4.117 seconds]
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 06/03/23 12:53:49.687
  Jun  3 12:53:49.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-webhook @ 06/03/23 12:53:49.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:49.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:49.714
  STEP: Setting up server cert @ 06/03/23 12:53:49.72
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 06/03/23 12:53:50.245
  STEP: Deploying the custom resource conversion webhook pod @ 06/03/23 12:53:50.255
  STEP: Wait for the deployment to be ready @ 06/03/23 12:53:50.27
  Jun  3 12:53:50.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 12:53:52.295
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 12:53:52.307
  Jun  3 12:53:53.307: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jun  3 12:53:53.312: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Creating a v1 custom resource @ 06/03/23 12:53:55.898
  STEP: Create a v2 custom resource @ 06/03/23 12:53:55.922
  STEP: List CRs in v1 @ 06/03/23 12:53:55.988
  STEP: List CRs in v2 @ 06/03/23 12:53:55.996
  Jun  3 12:53:56.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-8588" for this suite. @ 06/03/23 12:53:56.651
• [6.975 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 06/03/23 12:53:56.663
  Jun  3 12:53:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename podtemplate @ 06/03/23 12:53:56.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:56.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:56.703
  Jun  3 12:53:56.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6017" for this suite. @ 06/03/23 12:53:56.755
• [0.101 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 06/03/23 12:53:56.765
  Jun  3 12:53:56.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename ingress @ 06/03/23 12:53:56.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:56.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:56.793
  STEP: getting /apis @ 06/03/23 12:53:56.799
  STEP: getting /apis/networking.k8s.io @ 06/03/23 12:53:56.806
  STEP: getting /apis/networking.k8s.iov1 @ 06/03/23 12:53:56.809
  STEP: creating @ 06/03/23 12:53:56.811
  STEP: getting @ 06/03/23 12:53:56.836
  STEP: listing @ 06/03/23 12:53:56.841
  STEP: watching @ 06/03/23 12:53:56.848
  Jun  3 12:53:56.848: INFO: starting watch
  STEP: cluster-wide listing @ 06/03/23 12:53:56.851
  STEP: cluster-wide watching @ 06/03/23 12:53:56.857
  Jun  3 12:53:56.857: INFO: starting watch
  STEP: patching @ 06/03/23 12:53:56.859
  STEP: updating @ 06/03/23 12:53:56.866
  Jun  3 12:53:56.879: INFO: waiting for watch events with expected annotations
  Jun  3 12:53:56.879: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/03/23 12:53:56.88
  STEP: updating /status @ 06/03/23 12:53:56.899
  STEP: get /status @ 06/03/23 12:53:56.911
  STEP: deleting @ 06/03/23 12:53:56.918
  STEP: deleting a collection @ 06/03/23 12:53:56.936
  Jun  3 12:53:56.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-6894" for this suite. @ 06/03/23 12:53:56.966
• [0.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 06/03/23 12:53:56.976
  Jun  3 12:53:56.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 12:53:56.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:57.007
  STEP: creating a Namespace @ 06/03/23 12:53:57.012
  STEP: patching the Namespace @ 06/03/23 12:53:57.035
  STEP: get the Namespace and ensuring it has the label @ 06/03/23 12:53:57.044
  Jun  3 12:53:57.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4418" for this suite. @ 06/03/23 12:53:57.054
  STEP: Destroying namespace "nspatchtest-2fc14eb3-57b2-4b16-9338-361e008833ce-2937" for this suite. @ 06/03/23 12:53:57.063
• [0.096 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 06/03/23 12:53:57.073
  Jun  3 12:53:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 12:53:57.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:53:57.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:53:57.113
  STEP: creating secret secrets-3909/secret-test-3a36f5cb-d730-4976-b3a5-4208592e81d2 @ 06/03/23 12:53:57.12
  STEP: Creating a pod to test consume secrets @ 06/03/23 12:53:57.126
  STEP: Saw pod success @ 06/03/23 12:54:01.156
  Jun  3 12:54:01.161: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-53fb03ba-f5b1-4d19-9353-fc245f1803ae container env-test: <nil>
  STEP: delete the pod @ 06/03/23 12:54:01.17
  Jun  3 12:54:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3909" for this suite. @ 06/03/23 12:54:01.194
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 06/03/23 12:54:01.206
  Jun  3 12:54:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename runtimeclass @ 06/03/23 12:54:01.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:01.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:01.235
  Jun  3 12:54:01.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5933" for this suite. @ 06/03/23 12:54:01.257
• [0.059 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 06/03/23 12:54:01.266
  Jun  3 12:54:01.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename containers @ 06/03/23 12:54:01.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:01.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:01.294
  STEP: Creating a pod to test override command @ 06/03/23 12:54:01.301
  STEP: Saw pod success @ 06/03/23 12:54:05.332
  Jun  3 12:54:05.341: INFO: Trying to get logs from node ip-172-31-27-193 pod client-containers-d6595736-a69c-4da7-b6cd-03c11bd1c649 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 12:54:05.35
  Jun  3 12:54:05.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6422" for this suite. @ 06/03/23 12:54:05.376
• [4.118 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 06/03/23 12:54:05.385
  Jun  3 12:54:05.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubelet-test @ 06/03/23 12:54:05.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:05.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:05.413
  Jun  3 12:54:07.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7358" for this suite. @ 06/03/23 12:54:07.458
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 06/03/23 12:54:07.469
  Jun  3 12:54:07.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:54:07.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:07.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:07.496
  STEP: Counting existing ResourceQuota @ 06/03/23 12:54:07.502
  STEP: Creating a ResourceQuota @ 06/03/23 12:54:12.51
  STEP: Ensuring resource quota status is calculated @ 06/03/23 12:54:12.517
  Jun  3 12:54:14.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6417" for this suite. @ 06/03/23 12:54:14.527
• [7.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 06/03/23 12:54:14.538
  Jun  3 12:54:14.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-runtime @ 06/03/23 12:54:14.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:14.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:14.568
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 06/03/23 12:54:14.588
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 06/03/23 12:54:29.676
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 06/03/23 12:54:29.681
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 06/03/23 12:54:29.69
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 06/03/23 12:54:29.69
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 06/03/23 12:54:29.719
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 06/03/23 12:54:32.741
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 06/03/23 12:54:33.751
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 06/03/23 12:54:33.76
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 06/03/23 12:54:33.761
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 06/03/23 12:54:33.788
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 06/03/23 12:54:34.8
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 06/03/23 12:54:36.816
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 06/03/23 12:54:36.826
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 06/03/23 12:54:36.826
  Jun  3 12:54:36.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6344" for this suite. @ 06/03/23 12:54:36.863
• [22.334 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 06/03/23 12:54:36.872
  Jun  3 12:54:36.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename init-container @ 06/03/23 12:54:36.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:54:36.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:54:36.901
  STEP: creating the pod @ 06/03/23 12:54:36.907
  Jun  3 12:54:36.907: INFO: PodSpec: initContainers in spec.initContainers
  Jun  3 12:55:19.246: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0f0a7645-2c49-403e-b8d8-7cc9b5049aa7", GenerateName:"", Namespace:"init-container-5673", SelfLink:"", UID:"282e2102-d16a-49f6-8afe-cb2b97fd6f31", ResourceVersion:"24584", Generation:0, CreationTimestamp:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"907439572"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006e879f8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 3, 12, 55, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006e87a58), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zspqt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004965f20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zspqt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zspqt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zspqt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027e97c0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-27-193", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00436efc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027e9850)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027e9870)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027e9878), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027e987c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc007323d80), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.27.193", PodIP:"192.168.118.218", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.118.218"}}, StartTime:time.Date(2023, time.June, 3, 12, 54, 36, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00436f0a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00436f110)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://6e38b74d442951721560312896585497d752e08990dee09c360c57629aff0a7d", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004965fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004965f80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027e98f4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jun  3 12:55:19.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5673" for this suite. @ 06/03/23 12:55:19.252
• [42.388 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 06/03/23 12:55:19.265
  Jun  3 12:55:19.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 12:55:19.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:19.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:19.292
  Jun  3 12:55:19.296: INFO: Creating ReplicaSet my-hostname-basic-060d74cc-1e3a-40cb-8c6e-006e6532eba6
  Jun  3 12:55:19.310: INFO: Pod name my-hostname-basic-060d74cc-1e3a-40cb-8c6e-006e6532eba6: Found 0 pods out of 1
  Jun  3 12:55:24.317: INFO: Pod name my-hostname-basic-060d74cc-1e3a-40cb-8c6e-006e6532eba6: Found 1 pods out of 1
  Jun  3 12:55:24.317: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-060d74cc-1e3a-40cb-8c6e-006e6532eba6" is running
  Jun  3 12:55:24.323: INFO: Pod "my-hostname-basic-060d74cc-1e3a-40cb-8c6e-006e6532eba6-gwb9j" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:19 +0000 UTC Reason: Message:}])
  Jun  3 12:55:24.323: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/03/23 12:55:24.323
  Jun  3 12:55:24.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3802" for this suite. @ 06/03/23 12:55:24.347
• [5.089 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 06/03/23 12:55:24.355
  Jun  3 12:55:24.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:55:24.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:24.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:24.383
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4096 @ 06/03/23 12:55:24.389
  STEP: changing the ExternalName service to type=ClusterIP @ 06/03/23 12:55:24.397
  STEP: creating replication controller externalname-service in namespace services-4096 @ 06/03/23 12:55:24.419
  I0603 12:55:24.432552      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4096, replica count: 2
  I0603 12:55:27.484676      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 12:55:27.484: INFO: Creating new exec pod
  Jun  3 12:55:30.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4096 exec execpodswcz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jun  3 12:55:30.694: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun  3 12:55:30.694: INFO: stdout: "externalname-service-gh762"
  Jun  3 12:55:30.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4096 exec execpodswcz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.176 80'
  Jun  3 12:55:30.882: INFO: stderr: "+ nc -v -t -w 2 10.152.183.176 80\nConnection to 10.152.183.176 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun  3 12:55:30.882: INFO: stdout: ""
  Jun  3 12:55:31.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4096 exec execpodswcz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.176 80'
  Jun  3 12:55:32.040: INFO: stderr: "+ nc -v -t -w 2 10.152.183.176 80\nConnection to 10.152.183.176 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jun  3 12:55:32.040: INFO: stdout: ""
  Jun  3 12:55:32.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-4096 exec execpodswcz8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.176 80'
  Jun  3 12:55:33.043: INFO: stderr: "+ nc -v -t -w 2 10.152.183.176 80\n+ echo hostName\nConnection to 10.152.183.176 80 port [tcp/http] succeeded!\n"
  Jun  3 12:55:33.043: INFO: stdout: "externalname-service-gh762"
  Jun  3 12:55:33.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:55:33.048: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-4096" for this suite. @ 06/03/23 12:55:33.066
• [8.719 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 06/03/23 12:55:33.075
  Jun  3 12:55:33.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:55:33.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:33.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:33.102
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:55:33.109
  STEP: Saw pod success @ 06/03/23 12:55:37.137
  Jun  3 12:55:37.141: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-b45d6a16-1e83-43cc-b150-bc99679882d8 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:55:37.15
  Jun  3 12:55:37.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5417" for this suite. @ 06/03/23 12:55:37.175
• [4.108 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 06/03/23 12:55:37.184
  Jun  3 12:55:37.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 12:55:37.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:37.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:37.209
  STEP: Creating replication controller my-hostname-basic-b2e67264-537b-4b28-b440-06f1e29d5623 @ 06/03/23 12:55:37.214
  Jun  3 12:55:37.226: INFO: Pod name my-hostname-basic-b2e67264-537b-4b28-b440-06f1e29d5623: Found 0 pods out of 1
  Jun  3 12:55:42.232: INFO: Pod name my-hostname-basic-b2e67264-537b-4b28-b440-06f1e29d5623: Found 1 pods out of 1
  Jun  3 12:55:42.232: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b2e67264-537b-4b28-b440-06f1e29d5623" are running
  Jun  3 12:55:42.237: INFO: Pod "my-hostname-basic-b2e67264-537b-4b28-b440-06f1e29d5623-zkj2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-03 12:55:37 +0000 UTC Reason: Message:}])
  Jun  3 12:55:42.237: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 06/03/23 12:55:42.237
  Jun  3 12:55:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9481" for this suite. @ 06/03/23 12:55:42.257
• [5.083 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 06/03/23 12:55:42.267
  Jun  3 12:55:42.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 12:55:42.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:42.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:42.293
  STEP: Starting the proxy @ 06/03/23 12:55:42.298
  Jun  3 12:55:42.299: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6414 proxy --unix-socket=/tmp/kubectl-proxy-unix3971372193/test'
  STEP: retrieving proxy /api/ output @ 06/03/23 12:55:42.361
  Jun  3 12:55:42.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6414" for this suite. @ 06/03/23 12:55:42.368
• [0.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 06/03/23 12:55:42.385
  Jun  3 12:55:42.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 12:55:42.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:42.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:42.417
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 12:55:42.423
  STEP: Saw pod success @ 06/03/23 12:55:46.453
  Jun  3 12:55:46.458: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-f9b70455-bf11-48e4-97ca-dac619475007 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 12:55:46.467
  Jun  3 12:55:46.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8226" for this suite. @ 06/03/23 12:55:46.491
• [4.115 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 06/03/23 12:55:46.5
  Jun  3 12:55:46.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:55:46.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:46.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:46.527
  STEP: creating a Pod with a static label @ 06/03/23 12:55:46.54
  STEP: watching for Pod to be ready @ 06/03/23 12:55:46.552
  Jun  3 12:55:46.555: INFO: observed Pod pod-test in namespace pods-7736 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jun  3 12:55:46.558: INFO: observed Pod pod-test in namespace pods-7736 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC  }]
  Jun  3 12:55:46.578: INFO: observed Pod pod-test in namespace pods-7736 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC  }]
  Jun  3 12:55:48.360: INFO: Found Pod pod-test in namespace pods-7736 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-03 12:55:46 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 06/03/23 12:55:48.366
  STEP: getting the Pod and ensuring that it's patched @ 06/03/23 12:55:48.379
  STEP: replacing the Pod's status Ready condition to False @ 06/03/23 12:55:48.385
  STEP: check the Pod again to ensure its Ready conditions are False @ 06/03/23 12:55:48.4
  STEP: deleting the Pod via a Collection with a LabelSelector @ 06/03/23 12:55:48.4
  STEP: watching for the Pod to be deleted @ 06/03/23 12:55:48.412
  Jun  3 12:55:48.415: INFO: observed event type MODIFIED
  Jun  3 12:55:50.368: INFO: observed event type MODIFIED
  Jun  3 12:55:50.796: INFO: observed event type MODIFIED
  Jun  3 12:55:51.365: INFO: observed event type MODIFIED
  Jun  3 12:55:51.383: INFO: observed event type MODIFIED
  Jun  3 12:55:51.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7736" for this suite. @ 06/03/23 12:55:51.396
• [4.905 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 06/03/23 12:55:51.411
  Jun  3 12:55:51.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 12:55:51.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:51.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:51.436
  STEP: creating a ServiceAccount @ 06/03/23 12:55:51.442
  STEP: watching for the ServiceAccount to be added @ 06/03/23 12:55:51.454
  STEP: patching the ServiceAccount @ 06/03/23 12:55:51.459
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 06/03/23 12:55:51.466
  STEP: deleting the ServiceAccount @ 06/03/23 12:55:51.471
  Jun  3 12:55:51.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1411" for this suite. @ 06/03/23 12:55:51.494
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 06/03/23 12:55:51.504
  Jun  3 12:55:51.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 12:55:51.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:51.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:51.529
  STEP: creating a ConfigMap @ 06/03/23 12:55:51.538
  STEP: fetching the ConfigMap @ 06/03/23 12:55:51.545
  STEP: patching the ConfigMap @ 06/03/23 12:55:51.549
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 06/03/23 12:55:51.555
  STEP: deleting the ConfigMap by collection with a label selector @ 06/03/23 12:55:51.561
  STEP: listing all ConfigMaps in test namespace @ 06/03/23 12:55:51.572
  Jun  3 12:55:51.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-873" for this suite. @ 06/03/23 12:55:51.581
• [0.085 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 06/03/23 12:55:51.591
  Jun  3 12:55:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename hostport @ 06/03/23 12:55:51.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:55:51.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:55:51.619
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 06/03/23 12:55:51.63
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.7.203 on the node which pod1 resides and expect scheduled @ 06/03/23 12:55:53.654
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.7.203 but use UDP protocol on the node which pod2 resides @ 06/03/23 12:55:55.672
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 06/03/23 12:55:59.712
  Jun  3 12:55:59.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.7.203 http://127.0.0.1:54323/hostname] Namespace:hostport-7289 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:55:59.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:55:59.713: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:55:59.713: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7289/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.7.203+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.7.203, port: 54323 @ 06/03/23 12:55:59.797
  Jun  3 12:55:59.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.7.203:54323/hostname] Namespace:hostport-7289 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:55:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:55:59.798: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:55:59.798: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7289/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.7.203%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.7.203, port: 54323 UDP @ 06/03/23 12:55:59.866
  Jun  3 12:55:59.867: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.7.203 54323] Namespace:hostport-7289 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:55:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:55:59.867: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:55:59.867: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-7289/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.7.203+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jun  3 12:56:04.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-7289" for this suite. @ 06/03/23 12:56:04.943
• [13.361 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 06/03/23 12:56:04.953
  Jun  3 12:56:04.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/03/23 12:56:04.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:56:04.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:56:04.979
  STEP: create the container to handle the HTTPGet hook request. @ 06/03/23 12:56:04.991
  STEP: create the pod with lifecycle hook @ 06/03/23 12:56:07.019
  STEP: delete the pod with lifecycle hook @ 06/03/23 12:56:09.04
  STEP: check prestop hook @ 06/03/23 12:56:11.058
  Jun  3 12:56:11.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9805" for this suite. @ 06/03/23 12:56:11.094
• [6.151 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 06/03/23 12:56:11.105
  Jun  3 12:56:11.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:56:11.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:56:11.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:56:11.138
  STEP: Saw pod success @ 06/03/23 12:56:17.214
  Jun  3 12:56:17.218: INFO: Trying to get logs from node ip-172-31-27-193 pod client-envvars-26db232a-860b-47b4-a709-93ede53f6dc0 container env3cont: <nil>
  STEP: delete the pod @ 06/03/23 12:56:17.227
  Jun  3 12:56:17.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9931" for this suite. @ 06/03/23 12:56:17.254
• [6.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 06/03/23 12:56:17.27
  Jun  3 12:56:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 12:56:17.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:56:17.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:56:17.296
  STEP: Creating resourceQuota "e2e-rq-status-gl9ql" @ 06/03/23 12:56:17.306
  Jun  3 12:56:17.316: INFO: Resource quota "e2e-rq-status-gl9ql" reports spec: hard cpu limit of 500m
  Jun  3 12:56:17.316: INFO: Resource quota "e2e-rq-status-gl9ql" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-gl9ql" /status @ 06/03/23 12:56:17.316
  STEP: Confirm /status for "e2e-rq-status-gl9ql" resourceQuota via watch @ 06/03/23 12:56:17.328
  Jun  3 12:56:17.331: INFO: observed resourceQuota "e2e-rq-status-gl9ql" in namespace "resourcequota-9605" with hard status: v1.ResourceList(nil)
  Jun  3 12:56:17.331: INFO: Found resourceQuota "e2e-rq-status-gl9ql" in namespace "resourcequota-9605" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun  3 12:56:17.331: INFO: ResourceQuota "e2e-rq-status-gl9ql" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 06/03/23 12:56:17.337
  Jun  3 12:56:17.346: INFO: Resource quota "e2e-rq-status-gl9ql" reports spec: hard cpu limit of 1
  Jun  3 12:56:17.346: INFO: Resource quota "e2e-rq-status-gl9ql" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-gl9ql" /status @ 06/03/23 12:56:17.346
  STEP: Confirm /status for "e2e-rq-status-gl9ql" resourceQuota via watch @ 06/03/23 12:56:17.354
  Jun  3 12:56:17.357: INFO: observed resourceQuota "e2e-rq-status-gl9ql" in namespace "resourcequota-9605" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jun  3 12:56:17.358: INFO: Found resourceQuota "e2e-rq-status-gl9ql" in namespace "resourcequota-9605" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jun  3 12:56:17.358: INFO: ResourceQuota "e2e-rq-status-gl9ql" /status was patched
  STEP: Get "e2e-rq-status-gl9ql" /status @ 06/03/23 12:56:17.358
  Jun  3 12:56:17.364: INFO: Resourcequota "e2e-rq-status-gl9ql" reports status: hard cpu of 1
  Jun  3 12:56:17.364: INFO: Resourcequota "e2e-rq-status-gl9ql" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-gl9ql" /status before checking Spec is unchanged @ 06/03/23 12:56:17.369
  Jun  3 12:56:17.375: INFO: Resourcequota "e2e-rq-status-gl9ql" reports status: hard cpu of 2
  Jun  3 12:56:17.375: INFO: Resourcequota "e2e-rq-status-gl9ql" reports status: hard memory of 2Gi
  Jun  3 12:56:17.378: INFO: Found resourceQuota "e2e-rq-status-gl9ql" in namespace "resourcequota-9605" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jun  3 12:58:07.390: INFO: ResourceQuota "e2e-rq-status-gl9ql" Spec was unchanged and /status reset
  Jun  3 12:58:07.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9605" for this suite. @ 06/03/23 12:58:07.398
• [110.139 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 06/03/23 12:58:07.411
  Jun  3 12:58:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 12:58:07.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:07.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:07.442
  Jun  3 12:58:07.481: INFO: Create a RollingUpdate DaemonSet
  Jun  3 12:58:07.488: INFO: Check that daemon pods launch on every node of the cluster
  Jun  3 12:58:07.493: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:07.494: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:07.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:58:07.498: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:58:08.503: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:08.503: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:08.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jun  3 12:58:08.510: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  Jun  3 12:58:09.503: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:09.504: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:09.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 12:58:09.509: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jun  3 12:58:09.509: INFO: Update the DaemonSet to trigger a rollout
  Jun  3 12:58:09.519: INFO: Updating DaemonSet daemon-set
  Jun  3 12:58:11.539: INFO: Roll back the DaemonSet before rollout is complete
  Jun  3 12:58:11.550: INFO: Updating DaemonSet daemon-set
  Jun  3 12:58:11.550: INFO: Make sure DaemonSet rollback is complete
  Jun  3 12:58:11.554: INFO: Wrong image for pod: daemon-set-7xfrf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jun  3 12:58:11.554: INFO: Pod daemon-set-7xfrf is not available
  Jun  3 12:58:11.559: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:11.559: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:12.570: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:12.570: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:13.565: INFO: Pod daemon-set-f7dbx is not available
  Jun  3 12:58:13.570: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 12:58:13.570: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 12:58:13.578
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-91, will wait for the garbage collector to delete the pods @ 06/03/23 12:58:13.579
  Jun  3 12:58:13.640: INFO: Deleting DaemonSet.extensions daemon-set took: 6.652612ms
  Jun  3 12:58:13.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.848694ms
  Jun  3 12:58:15.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 12:58:15.746: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 12:58:15.750: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25680"},"items":null}

  Jun  3 12:58:15.755: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25680"},"items":null}

  Jun  3 12:58:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-91" for this suite. @ 06/03/23 12:58:15.778
• [8.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 06/03/23 12:58:15.788
  Jun  3 12:58:15.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:58:15.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:15.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:15.814
  STEP: creating service endpoint-test2 in namespace services-7609 @ 06/03/23 12:58:15.823
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7609 to expose endpoints map[] @ 06/03/23 12:58:15.837
  Jun  3 12:58:15.841: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Jun  3 12:58:16.853: INFO: successfully validated that service endpoint-test2 in namespace services-7609 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7609 @ 06/03/23 12:58:16.853
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7609 to expose endpoints map[pod1:[80]] @ 06/03/23 12:58:18.878
  Jun  3 12:58:18.894: INFO: successfully validated that service endpoint-test2 in namespace services-7609 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 06/03/23 12:58:18.894
  Jun  3 12:58:18.894: INFO: Creating new exec pod
  Jun  3 12:58:21.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun  3 12:58:22.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:22.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:22.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.183 80'
  Jun  3 12:58:22.264: INFO: stderr: "+ nc -v -t -w 2 10.152.183.183 80\n+ echo hostName\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:22.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7609 @ 06/03/23 12:58:22.264
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7609 to expose endpoints map[pod1:[80] pod2:[80]] @ 06/03/23 12:58:24.287
  Jun  3 12:58:24.303: INFO: successfully validated that service endpoint-test2 in namespace services-7609 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 06/03/23 12:58:24.303
  Jun  3 12:58:25.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun  3 12:58:25.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:25.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:25.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.183 80'
  Jun  3 12:58:25.634: INFO: stderr: "+ nc -v -t -w 2 10.152.183.183 80\n+ echo hostName\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:25.634: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7609 @ 06/03/23 12:58:25.634
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7609 to expose endpoints map[pod2:[80]] @ 06/03/23 12:58:25.652
  Jun  3 12:58:25.667: INFO: successfully validated that service endpoint-test2 in namespace services-7609 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 06/03/23 12:58:25.667
  Jun  3 12:58:26.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jun  3 12:58:26.832: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:26.832: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:26.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7609 exec execpodqbshq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.183 80'
  Jun  3 12:58:26.996: INFO: stderr: "+ nc -v -t -w 2 10.152.183.183 80\n+ echo hostName\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:26.996: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7609 @ 06/03/23 12:58:26.996
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7609 to expose endpoints map[] @ 06/03/23 12:58:27.01
  Jun  3 12:58:27.034: INFO: successfully validated that service endpoint-test2 in namespace services-7609 exposes endpoints map[]
  Jun  3 12:58:27.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7609" for this suite. @ 06/03/23 12:58:27.058
• [11.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 06/03/23 12:58:27.067
  Jun  3 12:58:27.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 12:58:27.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:27.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:27.096
  STEP: creating the pod @ 06/03/23 12:58:27.101
  STEP: setting up watch @ 06/03/23 12:58:27.102
  STEP: submitting the pod to kubernetes @ 06/03/23 12:58:27.207
  STEP: verifying the pod is in kubernetes @ 06/03/23 12:58:27.221
  STEP: verifying pod creation was observed @ 06/03/23 12:58:27.231
  STEP: deleting the pod gracefully @ 06/03/23 12:58:29.246
  STEP: verifying pod deletion was observed @ 06/03/23 12:58:29.254
  Jun  3 12:58:30.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-81" for this suite. @ 06/03/23 12:58:30.745
• [3.688 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 06/03/23 12:58:30.757
  Jun  3 12:58:30.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 12:58:30.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:30.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:30.781
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 06/03/23 12:58:30.786
  Jun  3 12:58:30.797: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7354  2e40107a-2c2a-44ea-911a-bef05ac392c8 25849 0 2023-06-03 12:58:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-03 12:58:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lppdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lppdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 06/03/23 12:58:32.808
  Jun  3 12:58:32.808: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7354 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:58:32.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:58:32.808: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:58:32.809: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7354/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 06/03/23 12:58:32.898
  Jun  3 12:58:32.898: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7354 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 12:58:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:58:32.899: INFO: ExecWithOptions: Clientset creation
  Jun  3 12:58:32.899: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-7354/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jun  3 12:58:32.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 12:58:32.988: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7354" for this suite. @ 06/03/23 12:58:33.003
• [2.256 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 06/03/23 12:58:33.015
  Jun  3 12:58:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:58:33.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:33.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:33.04
  STEP: creating a Service @ 06/03/23 12:58:33.052
  STEP: watching for the Service to be added @ 06/03/23 12:58:33.065
  Jun  3 12:58:33.069: INFO: Found Service test-service-zkf94 in namespace services-3708 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jun  3 12:58:33.070: INFO: Service test-service-zkf94 created
  STEP: Getting /status @ 06/03/23 12:58:33.07
  Jun  3 12:58:33.077: INFO: Service test-service-zkf94 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 06/03/23 12:58:33.077
  STEP: watching for the Service to be patched @ 06/03/23 12:58:33.087
  Jun  3 12:58:33.090: INFO: observed Service test-service-zkf94 in namespace services-3708 with annotations: map[] & LoadBalancer: {[]}
  Jun  3 12:58:33.090: INFO: Found Service test-service-zkf94 in namespace services-3708 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jun  3 12:58:33.090: INFO: Service test-service-zkf94 has service status patched
  STEP: updating the ServiceStatus @ 06/03/23 12:58:33.09
  Jun  3 12:58:33.107: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 06/03/23 12:58:33.107
  Jun  3 12:58:33.110: INFO: Observed Service test-service-zkf94 in namespace services-3708 with annotations: map[] & Conditions: {[]}
  Jun  3 12:58:33.110: INFO: Observed event: &Service{ObjectMeta:{test-service-zkf94  services-3708  28c32ff0-0991-4bac-8599-d124c543e8fe 25901 0 2023-06-03 12:58:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-03 12:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-03 12:58:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.61,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.61],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jun  3 12:58:33.111: INFO: Found Service test-service-zkf94 in namespace services-3708 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun  3 12:58:33.111: INFO: Service test-service-zkf94 has service status updated
  STEP: patching the service @ 06/03/23 12:58:33.112
  STEP: watching for the Service to be patched @ 06/03/23 12:58:33.131
  Jun  3 12:58:33.134: INFO: observed Service test-service-zkf94 in namespace services-3708 with labels: map[test-service-static:true]
  Jun  3 12:58:33.134: INFO: observed Service test-service-zkf94 in namespace services-3708 with labels: map[test-service-static:true]
  Jun  3 12:58:33.134: INFO: observed Service test-service-zkf94 in namespace services-3708 with labels: map[test-service-static:true]
  Jun  3 12:58:33.134: INFO: Found Service test-service-zkf94 in namespace services-3708 with labels: map[test-service:patched test-service-static:true]
  Jun  3 12:58:33.134: INFO: Service test-service-zkf94 patched
  STEP: deleting the service @ 06/03/23 12:58:33.134
  STEP: watching for the Service to be deleted @ 06/03/23 12:58:33.151
  Jun  3 12:58:33.154: INFO: Observed event: ADDED
  Jun  3 12:58:33.154: INFO: Observed event: MODIFIED
  Jun  3 12:58:33.154: INFO: Observed event: MODIFIED
  Jun  3 12:58:33.154: INFO: Observed event: MODIFIED
  Jun  3 12:58:33.155: INFO: Found Service test-service-zkf94 in namespace services-3708 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jun  3 12:58:33.155: INFO: Service test-service-zkf94 deleted
  Jun  3 12:58:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3708" for this suite. @ 06/03/23 12:58:33.16
• [0.153 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 06/03/23 12:58:33.17
  Jun  3 12:58:33.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 12:58:33.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:33.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:33.197
  STEP: creating service multi-endpoint-test in namespace services-6432 @ 06/03/23 12:58:33.201
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6432 to expose endpoints map[] @ 06/03/23 12:58:33.213
  Jun  3 12:58:33.218: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Jun  3 12:58:34.230: INFO: successfully validated that service multi-endpoint-test in namespace services-6432 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6432 @ 06/03/23 12:58:34.23
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6432 to expose endpoints map[pod1:[100]] @ 06/03/23 12:58:36.255
  Jun  3 12:58:36.270: INFO: successfully validated that service multi-endpoint-test in namespace services-6432 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-6432 @ 06/03/23 12:58:36.27
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6432 to expose endpoints map[pod1:[100] pod2:[101]] @ 06/03/23 12:58:38.295
  Jun  3 12:58:38.316: INFO: successfully validated that service multi-endpoint-test in namespace services-6432 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 06/03/23 12:58:38.316
  Jun  3 12:58:38.316: INFO: Creating new exec pod
  Jun  3 12:58:41.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-6432 exec execpodw8m8h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jun  3 12:58:41.496: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:41.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:41.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-6432 exec execpodw8m8h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.140 80'
  Jun  3 12:58:41.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.140 80\nConnection to 10.152.183.140 80 port [tcp/http] succeeded!\n"
  Jun  3 12:58:41.662: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:41.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-6432 exec execpodw8m8h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jun  3 12:58:41.822: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jun  3 12:58:41.822: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 12:58:41.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-6432 exec execpodw8m8h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.140 81'
  Jun  3 12:58:41.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.140 81\nConnection to 10.152.183.140 81 port [tcp/*] succeeded!\n"
  Jun  3 12:58:41.983: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6432 @ 06/03/23 12:58:41.983
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6432 to expose endpoints map[pod2:[101]] @ 06/03/23 12:58:41.996
  Jun  3 12:58:42.018: INFO: successfully validated that service multi-endpoint-test in namespace services-6432 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-6432 @ 06/03/23 12:58:42.018
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6432 to expose endpoints map[] @ 06/03/23 12:58:42.038
  Jun  3 12:58:42.049: INFO: successfully validated that service multi-endpoint-test in namespace services-6432 exposes endpoints map[]
  Jun  3 12:58:42.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6432" for this suite. @ 06/03/23 12:58:42.073
• [8.912 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 06/03/23 12:58:42.083
  Jun  3 12:58:42.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sysctl @ 06/03/23 12:58:42.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:42.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:42.112
  STEP: Creating a pod with one valid and two invalid sysctls @ 06/03/23 12:58:42.117
  Jun  3 12:58:42.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3401" for this suite. @ 06/03/23 12:58:42.13
• [0.054 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 06/03/23 12:58:42.138
  Jun  3 12:58:42.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 12:58:42.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:42.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:42.166
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 06/03/23 12:58:42.172
  Jun  3 12:58:42.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 06/03/23 12:58:48.31
  Jun  3 12:58:48.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:58:50.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 12:58:56.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-566" for this suite. @ 06/03/23 12:58:56.402
• [14.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 06/03/23 12:58:56.413
  Jun  3 12:58:56.413: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 12:58:56.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 12:58:56.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 12:58:56.44
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-9a2b4768-e92f-48a0-8d2b-7e687c17ab03 @ 06/03/23 12:58:56.451
  STEP: Creating the pod @ 06/03/23 12:58:56.458
  STEP: Updating configmap projected-configmap-test-upd-9a2b4768-e92f-48a0-8d2b-7e687c17ab03 @ 06/03/23 12:58:58.509
  STEP: waiting to observe update in volume @ 06/03/23 12:58:58.516
  Jun  3 13:00:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5566" for this suite. @ 06/03/23 13:00:28.998
• [92.594 seconds]
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 06/03/23 13:00:29.007
  Jun  3 13:00:29.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename endpointslicemirroring @ 06/03/23 13:00:29.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:00:29.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:00:29.041
  STEP: mirroring a new custom Endpoint @ 06/03/23 13:00:29.067
  Jun  3 13:00:29.089: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 06/03/23 13:00:31.094
  STEP: mirroring deletion of a custom Endpoint @ 06/03/23 13:00:31.113
  Jun  3 13:00:31.125: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jun  3 13:00:33.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7584" for this suite. @ 06/03/23 13:00:33.14
• [4.144 seconds]
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 06/03/23 13:00:33.152
  Jun  3 13:00:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename taint-multiple-pods @ 06/03/23 13:00:33.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:00:33.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:00:33.183
  Jun  3 13:00:33.189: INFO: Waiting up to 1m0s for all nodes to be ready
  Jun  3 13:01:33.216: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 13:01:33.220: INFO: Starting informer...
  STEP: Starting pods... @ 06/03/23 13:01:33.22
  Jun  3 13:01:33.444: INFO: Pod1 is running on ip-172-31-27-193. Tainting Node
  Jun  3 13:01:35.670: INFO: Pod2 is running on ip-172-31-27-193. Tainting Node
  STEP: Trying to apply a taint on the Node @ 06/03/23 13:01:35.67
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/03/23 13:01:35.685
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 06/03/23 13:01:35.69
  Jun  3 13:01:42.115: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jun  3 13:02:02.164: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jun  3 13:02:02.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 06/03/23 13:02:02.185
  STEP: Destroying namespace "taint-multiple-pods-6033" for this suite. @ 06/03/23 13:02:02.189
• [89.046 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 06/03/23 13:02:02.201
  Jun  3 13:02:02.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pod-network-test @ 06/03/23 13:02:02.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:02.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:02.285
  STEP: Performing setup for networking test in namespace pod-network-test-2982 @ 06/03/23 13:02:02.291
  STEP: creating a selector @ 06/03/23 13:02:02.291
  STEP: Creating the service pods in kubernetes @ 06/03/23 13:02:02.291
  Jun  3 13:02:02.291: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 06/03/23 13:02:24.414
  Jun  3 13:02:26.436: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun  3 13:02:26.436: INFO: Breadth first check of 192.168.118.192 on host 172.31.27.193...
  Jun  3 13:02:26.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.198:9080/dial?request=hostname&protocol=udp&host=192.168.118.192&port=8081&tries=1'] Namespace:pod-network-test-2982 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:26.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:26.441: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:26.442: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2982/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.118.192%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:02:26.527: INFO: Waiting for responses: map[]
  Jun  3 13:02:26.527: INFO: reached 192.168.118.192 after 0/1 tries
  Jun  3 13:02:26.527: INFO: Breadth first check of 192.168.192.179 on host 172.31.7.203...
  Jun  3 13:02:26.532: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.198:9080/dial?request=hostname&protocol=udp&host=192.168.192.179&port=8081&tries=1'] Namespace:pod-network-test-2982 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:26.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:26.532: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:26.533: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2982/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.192.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:02:26.619: INFO: Waiting for responses: map[]
  Jun  3 13:02:26.619: INFO: reached 192.168.192.179 after 0/1 tries
  Jun  3 13:02:26.619: INFO: Breadth first check of 192.168.20.89 on host 172.31.85.85...
  Jun  3 13:02:26.624: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.198:9080/dial?request=hostname&protocol=udp&host=192.168.20.89&port=8081&tries=1'] Namespace:pod-network-test-2982 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:26.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:26.625: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:26.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-2982/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.198%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.20.89%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:02:26.699: INFO: Waiting for responses: map[]
  Jun  3 13:02:26.699: INFO: reached 192.168.20.89 after 0/1 tries
  Jun  3 13:02:26.699: INFO: Going to retry 0 out of 3 pods....
  Jun  3 13:02:26.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2982" for this suite. @ 06/03/23 13:02:26.705
• [24.513 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 06/03/23 13:02:26.717
  Jun  3 13:02:26.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:02:26.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:26.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:26.743
  STEP: creating Agnhost RC @ 06/03/23 13:02:26.749
  Jun  3 13:02:26.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-1096 create -f -'
  Jun  3 13:02:27.515: INFO: stderr: ""
  Jun  3 13:02:27.515: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/03/23 13:02:27.515
  Jun  3 13:02:28.520: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:02:28.520: INFO: Found 0 / 1
  Jun  3 13:02:29.521: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:02:29.521: INFO: Found 1 / 1
  Jun  3 13:02:29.521: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 06/03/23 13:02:29.521
  Jun  3 13:02:29.526: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:02:29.526: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun  3 13:02:29.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-1096 patch pod agnhost-primary-qfsjz -p {"metadata":{"annotations":{"x":"y"}}}'
  Jun  3 13:02:29.616: INFO: stderr: ""
  Jun  3 13:02:29.616: INFO: stdout: "pod/agnhost-primary-qfsjz patched\n"
  STEP: checking annotations @ 06/03/23 13:02:29.616
  Jun  3 13:02:29.621: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:02:29.621: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun  3 13:02:29.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1096" for this suite. @ 06/03/23 13:02:29.626
• [2.916 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 06/03/23 13:02:29.633
  Jun  3 13:02:29.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename events @ 06/03/23 13:02:29.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:29.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:29.662
  STEP: creating a test event @ 06/03/23 13:02:29.666
  STEP: listing events in all namespaces @ 06/03/23 13:02:29.678
  STEP: listing events in test namespace @ 06/03/23 13:02:29.683
  STEP: listing events with field selection filtering on source @ 06/03/23 13:02:29.688
  STEP: listing events with field selection filtering on reportingController @ 06/03/23 13:02:29.693
  STEP: getting the test event @ 06/03/23 13:02:29.698
  STEP: patching the test event @ 06/03/23 13:02:29.702
  STEP: getting the test event @ 06/03/23 13:02:29.714
  STEP: updating the test event @ 06/03/23 13:02:29.718
  STEP: getting the test event @ 06/03/23 13:02:29.727
  STEP: deleting the test event @ 06/03/23 13:02:29.731
  STEP: listing events in all namespaces @ 06/03/23 13:02:29.741
  STEP: listing events in test namespace @ 06/03/23 13:02:29.747
  Jun  3 13:02:29.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8546" for this suite. @ 06/03/23 13:02:29.758
• [0.132 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 06/03/23 13:02:29.767
  Jun  3 13:02:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 13:02:29.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:29.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:29.792
  STEP: Creating service test in namespace statefulset-4420 @ 06/03/23 13:02:29.797
  STEP: Creating statefulset ss in namespace statefulset-4420 @ 06/03/23 13:02:29.808
  Jun  3 13:02:29.819: INFO: Found 0 stateful pods, waiting for 1
  Jun  3 13:02:39.825: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 06/03/23 13:02:39.834
  STEP: Getting /status @ 06/03/23 13:02:39.847
  Jun  3 13:02:39.852: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 06/03/23 13:02:39.853
  Jun  3 13:02:39.864: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 06/03/23 13:02:39.864
  Jun  3 13:02:39.867: INFO: Observed &StatefulSet event: ADDED
  Jun  3 13:02:39.867: INFO: Found Statefulset ss in namespace statefulset-4420 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 13:02:39.867: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 06/03/23 13:02:39.867
  Jun  3 13:02:39.867: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun  3 13:02:39.877: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 06/03/23 13:02:39.878
  Jun  3 13:02:39.880: INFO: Observed &StatefulSet event: ADDED
  Jun  3 13:02:39.880: INFO: Deleting all statefulset in ns statefulset-4420
  Jun  3 13:02:39.884: INFO: Scaling statefulset ss to 0
  Jun  3 13:02:49.908: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:02:49.911: INFO: Deleting statefulset ss
  Jun  3 13:02:49.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4420" for this suite. @ 06/03/23 13:02:49.933
• [20.175 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 06/03/23 13:02:49.944
  Jun  3 13:02:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 06/03/23 13:02:49.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:49.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:49.971
  STEP: Setting up the test @ 06/03/23 13:02:49.976
  STEP: Creating hostNetwork=false pod @ 06/03/23 13:02:49.976
  STEP: Creating hostNetwork=true pod @ 06/03/23 13:02:52.002
  STEP: Running the test @ 06/03/23 13:02:54.022
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 06/03/23 13:02:54.022
  Jun  3 13:02:54.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.023: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.023: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun  3 13:02:54.131: INFO: Exec stderr: ""
  Jun  3 13:02:54.131: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.132: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.132: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun  3 13:02:54.221: INFO: Exec stderr: ""
  Jun  3 13:02:54.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.222: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.222: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun  3 13:02:54.307: INFO: Exec stderr: ""
  Jun  3 13:02:54.307: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.308: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.309: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun  3 13:02:54.400: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 06/03/23 13:02:54.4
  Jun  3 13:02:54.400: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.402: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.402: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun  3 13:02:54.499: INFO: Exec stderr: ""
  Jun  3 13:02:54.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.500: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.500: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jun  3 13:02:54.572: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 06/03/23 13:02:54.572
  Jun  3 13:02:54.572: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.573: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.574: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun  3 13:02:54.659: INFO: Exec stderr: ""
  Jun  3 13:02:54.659: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.660: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.660: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jun  3 13:02:54.742: INFO: Exec stderr: ""
  Jun  3 13:02:54.742: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.743: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.743: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun  3 13:02:54.817: INFO: Exec stderr: ""
  Jun  3 13:02:54.817: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7230 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:02:54.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:02:54.818: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:02:54.819: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7230/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jun  3 13:02:54.915: INFO: Exec stderr: ""
  Jun  3 13:02:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-7230" for this suite. @ 06/03/23 13:02:54.92
• [4.983 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 06/03/23 13:02:54.929
  Jun  3 13:02:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:02:54.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:54.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:54.956
  STEP: Setting up server cert @ 06/03/23 13:02:54.993
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:02:55.615
  STEP: Deploying the webhook pod @ 06/03/23 13:02:55.625
  STEP: Wait for the deployment to be ready @ 06/03/23 13:02:55.64
  Jun  3 13:02:55.649: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 06/03/23 13:02:57.663
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:02:57.673
  Jun  3 13:02:58.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 06/03/23 13:02:58.756
  STEP: Creating a configMap that should be mutated @ 06/03/23 13:02:58.774
  STEP: Deleting the collection of validation webhooks @ 06/03/23 13:02:58.811
  STEP: Creating a configMap that should not be mutated @ 06/03/23 13:02:58.868
  Jun  3 13:02:58.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-74" for this suite. @ 06/03/23 13:02:58.932
  STEP: Destroying namespace "webhook-markers-5435" for this suite. @ 06/03/23 13:02:58.941
• [4.021 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 06/03/23 13:02:58.953
  Jun  3 13:02:58.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 13:02:58.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:02:58.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:02:58.979
  STEP: creating a ReplicationController @ 06/03/23 13:02:58.99
  STEP: waiting for RC to be added @ 06/03/23 13:02:58.996
  STEP: waiting for available Replicas @ 06/03/23 13:02:58.997
  STEP: patching ReplicationController @ 06/03/23 13:03:00.328
  STEP: waiting for RC to be modified @ 06/03/23 13:03:00.341
  STEP: patching ReplicationController status @ 06/03/23 13:03:00.342
  STEP: waiting for RC to be modified @ 06/03/23 13:03:00.35
  STEP: waiting for available Replicas @ 06/03/23 13:03:00.35
  STEP: fetching ReplicationController status @ 06/03/23 13:03:00.374
  STEP: patching ReplicationController scale @ 06/03/23 13:03:00.379
  STEP: waiting for RC to be modified @ 06/03/23 13:03:00.39
  STEP: waiting for ReplicationController's scale to be the max amount @ 06/03/23 13:03:00.39
  STEP: fetching ReplicationController; ensuring that it's patched @ 06/03/23 13:03:02.118
  STEP: updating ReplicationController status @ 06/03/23 13:03:02.124
  STEP: waiting for RC to be modified @ 06/03/23 13:03:02.132
  STEP: listing all ReplicationControllers @ 06/03/23 13:03:02.132
  STEP: checking that ReplicationController has expected values @ 06/03/23 13:03:02.138
  STEP: deleting ReplicationControllers by collection @ 06/03/23 13:03:02.139
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 06/03/23 13:03:02.152
  Jun  3 13:03:02.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0603 13:03:02.196105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9508" for this suite. @ 06/03/23 13:03:02.202
• [3.259 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 06/03/23 13:03:02.213
  Jun  3 13:03:02.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:03:02.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:03:02.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:03:02.252
  STEP: Creating the pod @ 06/03/23 13:03:02.263
  E0603 13:03:03.196305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:04.196416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:03:04.839: INFO: Successfully updated pod "annotationupdate9bf84ff6-001f-46fe-ab6c-9d1913f57f5a"
  E0603 13:03:05.197151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:06.197221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:03:06.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2222" for this suite. @ 06/03/23 13:03:06.861
• [4.658 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 06/03/23 13:03:06.872
  Jun  3 13:03:06.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 06/03/23 13:03:06.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:03:06.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:03:06.901
  STEP: creating a target pod @ 06/03/23 13:03:06.906
  E0603 13:03:07.197657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:08.197809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 06/03/23 13:03:08.934
  E0603 13:03:09.198279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:10.198558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 06/03/23 13:03:10.96
  Jun  3 13:03:10.960: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-532 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:03:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:03:10.961: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:03:10.961: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-532/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jun  3 13:03:11.039: INFO: Exec stderr: ""
  Jun  3 13:03:11.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-532" for this suite. @ 06/03/23 13:03:11.054
• [4.192 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 06/03/23 13:03:11.068
  Jun  3 13:03:11.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:03:11.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:03:11.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:03:11.096
  STEP: Creating secret with name s-test-opt-del-da08d717-22d6-42d7-a890-fe4d2bd85921 @ 06/03/23 13:03:11.108
  STEP: Creating secret with name s-test-opt-upd-3674bed5-4b58-4677-8ff0-c8a1fe9dc362 @ 06/03/23 13:03:11.113
  STEP: Creating the pod @ 06/03/23 13:03:11.119
  E0603 13:03:11.199400      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:12.200594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-da08d717-22d6-42d7-a890-fe4d2bd85921 @ 06/03/23 13:03:13.194
  E0603 13:03:13.200859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating secret s-test-opt-upd-3674bed5-4b58-4677-8ff0-c8a1fe9dc362 @ 06/03/23 13:03:13.202
  STEP: Creating secret with name s-test-opt-create-5c39d35d-10a4-428f-9e31-fbd9746a3c0f @ 06/03/23 13:03:13.209
  STEP: waiting to observe update in volume @ 06/03/23 13:03:13.219
  E0603 13:03:14.201349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:15.201471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:16.202064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:17.202272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:18.202553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:19.203139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:20.203797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:21.203927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:22.204549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:23.205097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:24.205701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:25.206337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:26.206935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:27.207573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:28.207815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:29.207961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:30.208064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:31.209130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:32.210153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:33.210393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:34.210497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:35.211119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:36.211308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:37.211959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:38.212052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:39.212160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:40.212275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:41.212702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:42.213406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:43.214068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:44.214151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:45.214386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:46.214576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:47.215330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:48.215417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:49.215539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:50.215644      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:51.215764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:52.216469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:53.217484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:54.218550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:55.218609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:56.219626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:57.220353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:58.220723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:03:59.220862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:00.220995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:01.221145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:02.221394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:03.221884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:04.222515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:05.222607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:06.222760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:07.223622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:08.223780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:09.224255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:10.224351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:11.224475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:12.225237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:13.225371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:14.225453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:15.226070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:16.226194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:17.226794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:18.227636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:19.227686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:20.227797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:21.227879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:22.228490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:23.228637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:24.228759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:25.228838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:26.228988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:27.229993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:28.230521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:29.230659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:30.231600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:31.231685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:32.231774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:33.232214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:34.232315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:35.232407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:04:35.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2019" for this suite. @ 06/03/23 13:04:35.681
• [84.622 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 06/03/23 13:04:35.691
  Jun  3 13:04:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 13:04:35.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:04:35.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:04:35.722
  STEP: create the rc1 @ 06/03/23 13:04:35.732
  STEP: create the rc2 @ 06/03/23 13:04:35.741
  E0603 13:04:36.232566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:37.232659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:38.233054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:39.233190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:40.233285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:41.233380      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 06/03/23 13:04:41.755
  E0603 13:04:42.233937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 06/03/23 13:04:42.318
  STEP: wait for the rc to be deleted @ 06/03/23 13:04:42.33
  E0603 13:04:43.234599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:44.235611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:45.240044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:46.242656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:47.243060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:04:47.349: INFO: 67 pods remaining
  Jun  3 13:04:47.349: INFO: 67 pods has nil DeletionTimestamp
  Jun  3 13:04:47.349: INFO: 
  E0603 13:04:48.250341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:49.250580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:50.252491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:51.254629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:52.254285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 06/03/23 13:04:52.346
  W0603 13:04:52.358797      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jun  3 13:04:52.359: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jun  3 13:04:52.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-26zwt" in namespace "gc-7436"
  Jun  3 13:04:52.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mtd5" in namespace "gc-7436"
  Jun  3 13:04:52.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rf4f" in namespace "gc-7436"
  Jun  3 13:04:52.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hvzl" in namespace "gc-7436"
  Jun  3 13:04:52.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-58rmw" in namespace "gc-7436"
  Jun  3 13:04:52.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mlgl" in namespace "gc-7436"
  Jun  3 13:04:52.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-6794b" in namespace "gc-7436"
  Jun  3 13:04:52.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-69k6v" in namespace "gc-7436"
  Jun  3 13:04:52.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gd62" in namespace "gc-7436"
  Jun  3 13:04:52.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h7mb" in namespace "gc-7436"
  Jun  3 13:04:52.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lwr2" in namespace "gc-7436"
  Jun  3 13:04:52.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qg8g" in namespace "gc-7436"
  Jun  3 13:04:52.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tch7" in namespace "gc-7436"
  Jun  3 13:04:52.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-72wps" in namespace "gc-7436"
  Jun  3 13:04:52.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-76tx5" in namespace "gc-7436"
  Jun  3 13:04:52.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lhkl" in namespace "gc-7436"
  Jun  3 13:04:52.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ngsj" in namespace "gc-7436"
  Jun  3 13:04:52.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rlhc" in namespace "gc-7436"
  Jun  3 13:04:52.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-842ht" in namespace "gc-7436"
  Jun  3 13:04:52.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-98c8m" in namespace "gc-7436"
  Jun  3 13:04:52.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hvjv" in namespace "gc-7436"
  Jun  3 13:04:52.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mmnm" in namespace "gc-7436"
  Jun  3 13:04:52.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjhzl" in namespace "gc-7436"
  Jun  3 13:04:52.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2thv" in namespace "gc-7436"
  Jun  3 13:04:52.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc8w9" in namespace "gc-7436"
  Jun  3 13:04:53.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfn2l" in namespace "gc-7436"
  Jun  3 13:04:53.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2f2m" in namespace "gc-7436"
  Jun  3 13:04:53.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpgks" in namespace "gc-7436"
  Jun  3 13:04:53.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqqk5" in namespace "gc-7436"
  Jun  3 13:04:53.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-drwb2" in namespace "gc-7436"
  Jun  3 13:04:53.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-f68gx" in namespace "gc-7436"
  E0603 13:04:53.254552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:04:53.262: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6jnv" in namespace "gc-7436"
  Jun  3 13:04:53.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgzzp" in namespace "gc-7436"
  Jun  3 13:04:53.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqkrf" in namespace "gc-7436"
  Jun  3 13:04:53.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsvdd" in namespace "gc-7436"
  Jun  3 13:04:53.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv5dn" in namespace "gc-7436"
  Jun  3 13:04:53.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6x2s" in namespace "gc-7436"
  Jun  3 13:04:53.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-g79wq" in namespace "gc-7436"
  Jun  3 13:04:53.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggnmb" in namespace "gc-7436"
  Jun  3 13:04:53.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmw47" in namespace "gc-7436"
  Jun  3 13:04:53.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtm4h" in namespace "gc-7436"
  Jun  3 13:04:53.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtmz6" in namespace "gc-7436"
  Jun  3 13:04:53.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvtk7" in namespace "gc-7436"
  Jun  3 13:04:53.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvw4x" in namespace "gc-7436"
  Jun  3 13:04:53.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2qk5" in namespace "gc-7436"
  Jun  3 13:04:53.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4p7b" in namespace "gc-7436"
  Jun  3 13:04:53.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8qtp" in namespace "gc-7436"
  Jun  3 13:04:53.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn22l" in namespace "gc-7436"
  Jun  3 13:04:53.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-hntlc" in namespace "gc-7436"
  Jun  3 13:04:53.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6cr2" in namespace "gc-7436"
  Jun  3 13:04:53.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7436" for this suite. @ 06/03/23 13:04:53.684
• [18.006 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 06/03/23 13:04:53.707
  Jun  3 13:04:53.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:04:53.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:04:53.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:04:53.745
  STEP: Creating a test externalName service @ 06/03/23 13:04:53.756
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:04:53.765
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:04:53.765
  STEP: creating a pod to probe DNS @ 06/03/23 13:04:53.765
  STEP: submitting the pod to kubernetes @ 06/03/23 13:04:53.765
  E0603 13:04:54.254546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:55.254996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:56.255314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:57.255973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:58.256142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:04:59.256183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:00.256594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:01.256765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:05:01.815
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:05:01.821
  Jun  3 13:05:01.835: INFO: DNS probes using dns-test-821d666a-8775-454f-899f-d87aa6857998 succeeded

  STEP: changing the externalName to bar.example.com @ 06/03/23 13:05:01.835
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:05:01.856
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:05:01.856
  STEP: creating a second pod to probe DNS @ 06/03/23 13:05:01.856
  STEP: submitting the pod to kubernetes @ 06/03/23 13:05:01.856
  E0603 13:05:02.257076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:03.257420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:04.257628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:05.258128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:06.258293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:07.258837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:08.259680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:09.259741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:10.260741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:11.260858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:05:11.905
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:05:11.91
  Jun  3 13:05:11.922: INFO: DNS probes using dns-test-45e17488-3a1b-441e-8d6b-f269b003af24 succeeded

  STEP: changing the service to type=ClusterIP @ 06/03/23 13:05:11.922
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:05:11.939
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5487.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5487.svc.cluster.local; sleep 1; done
   @ 06/03/23 13:05:11.94
  STEP: creating a third pod to probe DNS @ 06/03/23 13:05:11.94
  STEP: submitting the pod to kubernetes @ 06/03/23 13:05:11.946
  E0603 13:05:12.260952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:13.261415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:05:13.965
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:05:13.97
  Jun  3 13:05:13.983: INFO: DNS probes using dns-test-6528bb4f-9c3f-49c8-b2a5-81a921fbce65 succeeded

  Jun  3 13:05:13.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:05:13.988
  STEP: deleting the pod @ 06/03/23 13:05:14.005
  STEP: deleting the pod @ 06/03/23 13:05:14.022
  STEP: deleting the test externalName service @ 06/03/23 13:05:14.053
  STEP: Destroying namespace "dns-5487" for this suite. @ 06/03/23 13:05:14.079
• [20.388 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 06/03/23 13:05:14.096
  Jun  3 13:05:14.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 13:05:14.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:14.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:14.133
  STEP: apply creating a deployment @ 06/03/23 13:05:14.14
  Jun  3 13:05:14.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9920" for this suite. @ 06/03/23 13:05:14.176
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 06/03/23 13:05:14.189
  Jun  3 13:05:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-runtime @ 06/03/23 13:05:14.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:14.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:14.23
  STEP: create the container @ 06/03/23 13:05:14.238
  W0603 13:05:14.253832      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/03/23 13:05:14.254
  E0603 13:05:14.261930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:15.262546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:16.262689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:17.262836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/03/23 13:05:17.276
  STEP: the container should be terminated @ 06/03/23 13:05:17.28
  STEP: the termination message should be set @ 06/03/23 13:05:17.281
  Jun  3 13:05:17.281: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/03/23 13:05:17.281
  Jun  3 13:05:17.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3871" for this suite. @ 06/03/23 13:05:17.304
• [3.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 06/03/23 13:05:17.315
  Jun  3 13:05:17.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename limitrange @ 06/03/23 13:05:17.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:17.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:17.341
  STEP: Creating a LimitRange @ 06/03/23 13:05:17.351
  STEP: Setting up watch @ 06/03/23 13:05:17.351
  STEP: Submitting a LimitRange @ 06/03/23 13:05:17.456
  STEP: Verifying LimitRange creation was observed @ 06/03/23 13:05:17.462
  STEP: Fetching the LimitRange to ensure it has proper values @ 06/03/23 13:05:17.462
  Jun  3 13:05:17.467: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun  3 13:05:17.467: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 06/03/23 13:05:17.467
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 06/03/23 13:05:17.476
  Jun  3 13:05:17.482: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jun  3 13:05:17.483: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 06/03/23 13:05:17.483
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 06/03/23 13:05:17.492
  Jun  3 13:05:17.496: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jun  3 13:05:17.496: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 06/03/23 13:05:17.496
  STEP: Failing to create a Pod with more than max resources @ 06/03/23 13:05:17.5
  STEP: Updating a LimitRange @ 06/03/23 13:05:17.504
  STEP: Verifying LimitRange updating is effective @ 06/03/23 13:05:17.512
  E0603 13:05:18.263603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:19.263719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 06/03/23 13:05:19.517
  STEP: Failing to create a Pod with more than max resources @ 06/03/23 13:05:19.524
  STEP: Deleting a LimitRange @ 06/03/23 13:05:19.527
  STEP: Verifying the LimitRange was deleted @ 06/03/23 13:05:19.549
  E0603 13:05:20.264051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:21.264446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:22.264564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:23.264685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:24.264832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:24.554: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 06/03/23 13:05:24.555
  Jun  3 13:05:24.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6536" for this suite. @ 06/03/23 13:05:24.571
• [7.265 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 06/03/23 13:05:24.581
  Jun  3 13:05:24.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:05:24.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:24.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:24.607
  STEP: creating a replication controller @ 06/03/23 13:05:24.613
  Jun  3 13:05:24.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 create -f -'
  Jun  3 13:05:24.948: INFO: stderr: ""
  Jun  3 13:05:24.948: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/03/23 13:05:24.948
  Jun  3 13:05:24.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun  3 13:05:25.042: INFO: stderr: ""
  Jun  3 13:05:25.042: INFO: stdout: "update-demo-nautilus-7dr64 update-demo-nautilus-s7sfm "
  Jun  3 13:05:25.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:05:25.125: INFO: stderr: ""
  Jun  3 13:05:25.125: INFO: stdout: ""
  Jun  3 13:05:25.125: INFO: update-demo-nautilus-7dr64 is created but not running
  E0603 13:05:25.265286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:26.265428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:27.265518      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:28.265674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:29.265763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:30.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0603 13:05:30.266100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:30.266: INFO: stderr: ""
  Jun  3 13:05:30.266: INFO: stdout: "update-demo-nautilus-7dr64 update-demo-nautilus-s7sfm "
  Jun  3 13:05:30.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:05:30.395: INFO: stderr: ""
  Jun  3 13:05:30.395: INFO: stdout: "true"
  Jun  3 13:05:30.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:05:30.508: INFO: stderr: ""
  Jun  3 13:05:30.508: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:05:30.508: INFO: validating pod update-demo-nautilus-7dr64
  Jun  3 13:05:30.515: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:05:30.515: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:05:30.515: INFO: update-demo-nautilus-7dr64 is verified up and running
  Jun  3 13:05:30.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-s7sfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:05:30.646: INFO: stderr: ""
  Jun  3 13:05:30.646: INFO: stdout: "true"
  Jun  3 13:05:30.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-s7sfm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:05:30.768: INFO: stderr: ""
  Jun  3 13:05:30.768: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:05:30.768: INFO: validating pod update-demo-nautilus-s7sfm
  Jun  3 13:05:30.775: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:05:30.775: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:05:30.775: INFO: update-demo-nautilus-s7sfm is verified up and running
  STEP: scaling down the replication controller @ 06/03/23 13:05:30.775
  Jun  3 13:05:30.777: INFO: scanned /root for discovery docs: <nil>
  Jun  3 13:05:30.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0603 13:05:31.266982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:31.931: INFO: stderr: ""
  Jun  3 13:05:31.931: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/03/23 13:05:31.931
  Jun  3 13:05:31.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun  3 13:05:32.058: INFO: stderr: ""
  Jun  3 13:05:32.058: INFO: stdout: "update-demo-nautilus-7dr64 update-demo-nautilus-s7sfm "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 06/03/23 13:05:32.058
  E0603 13:05:32.267524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:33.268225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:34.268418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:35.268694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:36.268890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:37.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun  3 13:05:37.190: INFO: stderr: ""
  Jun  3 13:05:37.190: INFO: stdout: "update-demo-nautilus-7dr64 "
  Jun  3 13:05:37.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0603 13:05:37.269218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:37.314: INFO: stderr: ""
  Jun  3 13:05:37.314: INFO: stdout: "true"
  Jun  3 13:05:37.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:05:37.449: INFO: stderr: ""
  Jun  3 13:05:37.449: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:05:37.449: INFO: validating pod update-demo-nautilus-7dr64
  Jun  3 13:05:37.454: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:05:37.454: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:05:37.454: INFO: update-demo-nautilus-7dr64 is verified up and running
  STEP: scaling up the replication controller @ 06/03/23 13:05:37.454
  Jun  3 13:05:37.456: INFO: scanned /root for discovery docs: <nil>
  Jun  3 13:05:37.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0603 13:05:38.269778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:38.604: INFO: stderr: ""
  Jun  3 13:05:38.604: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/03/23 13:05:38.604
  Jun  3 13:05:38.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun  3 13:05:38.742: INFO: stderr: ""
  Jun  3 13:05:38.742: INFO: stdout: "update-demo-nautilus-6gkrw update-demo-nautilus-7dr64 "
  Jun  3 13:05:38.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-6gkrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:05:38.861: INFO: stderr: ""
  Jun  3 13:05:38.861: INFO: stdout: "true"
  Jun  3 13:05:38.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-6gkrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:05:38.970: INFO: stderr: ""
  Jun  3 13:05:38.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:05:38.970: INFO: validating pod update-demo-nautilus-6gkrw
  Jun  3 13:05:38.976: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:05:38.976: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:05:38.976: INFO: update-demo-nautilus-6gkrw is verified up and running
  Jun  3 13:05:38.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:05:39.100: INFO: stderr: ""
  Jun  3 13:05:39.100: INFO: stdout: "true"
  Jun  3 13:05:39.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods update-demo-nautilus-7dr64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:05:39.224: INFO: stderr: ""
  Jun  3 13:05:39.224: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:05:39.224: INFO: validating pod update-demo-nautilus-7dr64
  Jun  3 13:05:39.229: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:05:39.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:05:39.230: INFO: update-demo-nautilus-7dr64 is verified up and running
  STEP: using delete to clean up resources @ 06/03/23 13:05:39.23
  Jun  3 13:05:39.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 delete --grace-period=0 --force -f -'
  E0603 13:05:39.270510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:39.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 13:05:39.364: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun  3 13:05:39.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get rc,svc -l name=update-demo --no-headers'
  Jun  3 13:05:39.521: INFO: stderr: "No resources found in kubectl-7509 namespace.\n"
  Jun  3 13:05:39.521: INFO: stdout: ""
  Jun  3 13:05:39.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-7509 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun  3 13:05:39.647: INFO: stderr: ""
  Jun  3 13:05:39.647: INFO: stdout: ""
  Jun  3 13:05:39.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7509" for this suite. @ 06/03/23 13:05:39.652
• [15.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 06/03/23 13:05:39.663
  Jun  3 13:05:39.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 13:05:39.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:39.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:39.693
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:05:39.697
  E0603 13:05:40.271549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:41.271616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:42.272085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:43.272513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:05:43.726
  Jun  3 13:05:43.730: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-9242a25e-60f5-47cd-93cc-6916d7649c18 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:05:43.758
  Jun  3 13:05:43.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2995" for this suite. @ 06/03/23 13:05:43.784
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 06/03/23 13:05:43.797
  Jun  3 13:05:43.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pod-network-test @ 06/03/23 13:05:43.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:43.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:43.823
  STEP: Performing setup for networking test in namespace pod-network-test-5356 @ 06/03/23 13:05:43.828
  STEP: creating a selector @ 06/03/23 13:05:43.829
  STEP: Creating the service pods in kubernetes @ 06/03/23 13:05:43.829
  Jun  3 13:05:43.829: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0603 13:05:44.273033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:45.273138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:46.273253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:47.273492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:48.274350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:49.274527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:50.274653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:51.274800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:52.275601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:53.276195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:54.276255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:55.276374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/03/23 13:05:55.926
  E0603 13:05:56.277143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:05:57.277373      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:57.951: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun  3 13:05:57.951: INFO: Breadth first check of 192.168.118.216 on host 172.31.27.193...
  Jun  3 13:05:57.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.235:9080/dial?request=hostname&protocol=http&host=192.168.118.216&port=8083&tries=1'] Namespace:pod-network-test-5356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:05:57.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:05:57.956: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:05:57.956: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5356/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.235%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.118.216%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:05:58.036: INFO: Waiting for responses: map[]
  Jun  3 13:05:58.036: INFO: reached 192.168.118.216 after 0/1 tries
  Jun  3 13:05:58.036: INFO: Breadth first check of 192.168.192.149 on host 172.31.7.203...
  Jun  3 13:05:58.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.235:9080/dial?request=hostname&protocol=http&host=192.168.192.149&port=8083&tries=1'] Namespace:pod-network-test-5356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:05:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:05:58.043: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:05:58.043: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5356/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.235%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.192.149%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:05:58.123: INFO: Waiting for responses: map[]
  Jun  3 13:05:58.123: INFO: reached 192.168.192.149 after 0/1 tries
  Jun  3 13:05:58.123: INFO: Breadth first check of 192.168.20.69 on host 172.31.85.85...
  Jun  3 13:05:58.128: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.118.235:9080/dial?request=hostname&protocol=http&host=192.168.20.69&port=8083&tries=1'] Namespace:pod-network-test-5356 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:05:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:05:58.129: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:05:58.129: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5356/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.118.235%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.20.69%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jun  3 13:05:58.213: INFO: Waiting for responses: map[]
  Jun  3 13:05:58.213: INFO: reached 192.168.20.69 after 0/1 tries
  Jun  3 13:05:58.213: INFO: Going to retry 0 out of 3 pods....
  Jun  3 13:05:58.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5356" for this suite. @ 06/03/23 13:05:58.219
• [14.430 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 06/03/23 13:05:58.239
  Jun  3 13:05:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:05:58.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:05:58.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:05:58.271
  STEP: creating the pod @ 06/03/23 13:05:58.277
  Jun  3 13:05:58.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 create -f -'
  E0603 13:05:58.278213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:05:58.762: INFO: stderr: ""
  Jun  3 13:05:58.762: INFO: stdout: "pod/pause created\n"
  E0603 13:05:59.279106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:00.279225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 06/03/23 13:06:00.772
  Jun  3 13:06:00.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 label pods pause testing-label=testing-label-value'
  Jun  3 13:06:00.862: INFO: stderr: ""
  Jun  3 13:06:00.862: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 06/03/23 13:06:00.862
  Jun  3 13:06:00.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 get pod pause -L testing-label'
  Jun  3 13:06:00.943: INFO: stderr: ""
  Jun  3 13:06:00.943: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 06/03/23 13:06:00.944
  Jun  3 13:06:00.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 label pods pause testing-label-'
  Jun  3 13:06:01.041: INFO: stderr: ""
  Jun  3 13:06:01.041: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 06/03/23 13:06:01.041
  Jun  3 13:06:01.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 get pod pause -L testing-label'
  Jun  3 13:06:01.128: INFO: stderr: ""
  Jun  3 13:06:01.128: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 06/03/23 13:06:01.128
  Jun  3 13:06:01.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 delete --grace-period=0 --force -f -'
  Jun  3 13:06:01.220: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 13:06:01.220: INFO: stdout: "pod \"pause\" force deleted\n"
  Jun  3 13:06:01.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 get rc,svc -l name=pause --no-headers'
  E0603 13:06:01.280292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:06:01.320: INFO: stderr: "No resources found in kubectl-4497 namespace.\n"
  Jun  3 13:06:01.320: INFO: stdout: ""
  Jun  3 13:06:01.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4497 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun  3 13:06:01.405: INFO: stderr: ""
  Jun  3 13:06:01.405: INFO: stdout: ""
  Jun  3 13:06:01.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4497" for this suite. @ 06/03/23 13:06:01.411
• [3.179 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 06/03/23 13:06:01.419
  Jun  3 13:06:01.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 13:06:01.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:06:01.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:06:01.445
  STEP: Creating a pod to test downward api env vars @ 06/03/23 13:06:01.449
  E0603 13:06:02.280383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:03.280521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:04.280811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:05.280921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:06:05.476
  Jun  3 13:06:05.480: INFO: Trying to get logs from node ip-172-31-27-193 pod downward-api-4ebe0bdc-93d9-4af3-a3d7-9f9d52d204f6 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 13:06:05.489
  Jun  3 13:06:05.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3812" for this suite. @ 06/03/23 13:06:05.514
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 06/03/23 13:06:05.523
  Jun  3 13:06:05.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 13:06:05.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:06:05.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:06:05.549
  STEP: Creating a ResourceQuota with best effort scope @ 06/03/23 13:06:05.553
  STEP: Ensuring ResourceQuota status is calculated @ 06/03/23 13:06:05.56
  E0603 13:06:06.281873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:07.282545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 06/03/23 13:06:07.565
  STEP: Ensuring ResourceQuota status is calculated @ 06/03/23 13:06:07.574
  E0603 13:06:08.282989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:09.283119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 06/03/23 13:06:09.578
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 06/03/23 13:06:09.597
  E0603 13:06:10.283235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:11.283431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 06/03/23 13:06:11.602
  E0603 13:06:12.283491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:13.283863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/03/23 13:06:13.607
  STEP: Ensuring resource quota status released the pod usage @ 06/03/23 13:06:13.624
  E0603 13:06:14.284157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:15.284366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 06/03/23 13:06:15.629
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 06/03/23 13:06:15.647
  E0603 13:06:16.284978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:17.285736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 06/03/23 13:06:17.652
  E0603 13:06:18.285675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:19.285877      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 06/03/23 13:06:19.659
  STEP: Ensuring resource quota status released the pod usage @ 06/03/23 13:06:19.676
  E0603 13:06:20.286003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:21.286175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:06:21.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8847" for this suite. @ 06/03/23 13:06:21.688
• [16.175 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 06/03/23 13:06:21.699
  Jun  3 13:06:21.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 13:06:21.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:06:21.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:06:21.727
  STEP: Creating service test in namespace statefulset-1105 @ 06/03/23 13:06:21.733
  Jun  3 13:06:21.754: INFO: Found 0 stateful pods, waiting for 1
  E0603 13:06:22.287227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:23.287599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:24.288195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:25.288477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:26.288581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:27.289183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:28.289537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:29.289745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:30.290495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:31.290788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:06:31.759: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 06/03/23 13:06:31.769
  W0603 13:06:31.777625      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun  3 13:06:31.787: INFO: Found 1 stateful pods, waiting for 2
  E0603 13:06:32.291720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:33.292103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:34.292213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:35.292754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:36.293013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:37.293764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:38.294072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:39.294444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:40.294546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:41.295583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:06:41.794: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:06:41.794: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 06/03/23 13:06:41.803
  STEP: Delete all of the StatefulSets @ 06/03/23 13:06:41.807
  STEP: Verify that StatefulSets have been deleted @ 06/03/23 13:06:41.817
  Jun  3 13:06:41.821: INFO: Deleting all statefulset in ns statefulset-1105
  Jun  3 13:06:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1105" for this suite. @ 06/03/23 13:06:41.84
• [20.154 seconds]
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 06/03/23 13:06:41.853
  Jun  3 13:06:41.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 13:06:41.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:06:41.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:06:41.883
  STEP: Creating configMap with name configmap-test-upd-9fa5955f-9365-4ab7-899e-5580ecdade72 @ 06/03/23 13:06:41.895
  STEP: Creating the pod @ 06/03/23 13:06:41.902
  E0603 13:06:42.295805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:43.295927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-9fa5955f-9365-4ab7-899e-5580ecdade72 @ 06/03/23 13:06:43.937
  STEP: waiting to observe update in volume @ 06/03/23 13:06:43.943
  E0603 13:06:44.295994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:45.296101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:46.296816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:47.297431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:48.297867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:49.297977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:50.298890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:51.299005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:52.299110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:53.299211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:54.299949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:55.300034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:56.301052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:57.301310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:58.301684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:06:59.301904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:00.302768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:01.302792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:02.303420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:03.303555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:04.304338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:05.304385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:06.304867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:07.305220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:08.306009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:09.306180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:10.307128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:11.307533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:12.307619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:13.308298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:14.308436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:15.308537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:16.309274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:17.309393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:18.310394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:19.310557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:20.311191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:21.311325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:22.311367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:23.312151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:24.312333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:25.312450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:26.313087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:27.313206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:28.314227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:29.314477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:30.315063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:31.315556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:32.316085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:33.316195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:34.316648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:35.316819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:36.316919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:37.317821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:38.317931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:39.318653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:40.319274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:41.319378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:42.320488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:43.320559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:44.321254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:45.321403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:46.322490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:47.323001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:48.323367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:49.323575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:50.324567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:51.324672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:52.325198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:53.326081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:54.326971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:55.327065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:56.327256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:57.327344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:58.327372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:07:59.327473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:00.327917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:01.328065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:02.328694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:03.328824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:04.329173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:05.329296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:06.329401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:07.329542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:08.329932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:08:08.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2694" for this suite. @ 06/03/23 13:08:08.387
• [86.542 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 06/03/23 13:08:08.399
  Jun  3 13:08:08.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subpath @ 06/03/23 13:08:08.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:08.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:08.424
  STEP: Setting up data @ 06/03/23 13:08:08.429
  STEP: Creating pod pod-subpath-test-projected-w7cd @ 06/03/23 13:08:08.44
  STEP: Creating a pod to test atomic-volume-subpath @ 06/03/23 13:08:08.44
  E0603 13:08:09.330588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:10.331612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:11.331961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:12.332055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:13.332217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:14.332291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:15.332533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:16.332765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:17.332830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:18.332955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:19.333055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:20.333151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:21.333951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:22.334019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:23.334087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:24.334213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:25.334335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:26.334579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:27.334691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:28.334788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:29.334918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:30.334983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:31.335132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:32.335454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:08:32.52
  Jun  3 13:08:32.526: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-subpath-test-projected-w7cd container test-container-subpath-projected-w7cd: <nil>
  STEP: delete the pod @ 06/03/23 13:08:32.536
  STEP: Deleting pod pod-subpath-test-projected-w7cd @ 06/03/23 13:08:32.557
  Jun  3 13:08:32.557: INFO: Deleting pod "pod-subpath-test-projected-w7cd" in namespace "subpath-7667"
  Jun  3 13:08:32.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7667" for this suite. @ 06/03/23 13:08:32.566
• [24.177 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 06/03/23 13:08:32.577
  Jun  3 13:08:32.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:08:32.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:32.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:32.602
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 06/03/23 13:08:32.608
  E0603 13:08:33.336141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:34.336281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:35.336381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:36.336459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:08:36.633
  Jun  3 13:08:36.638: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-3ed3bb76-e242-429a-8cba-53752f743f1c container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:08:36.646
  Jun  3 13:08:36.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2490" for this suite. @ 06/03/23 13:08:36.668
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 06/03/23 13:08:36.68
  Jun  3 13:08:36.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:08:36.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:36.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:36.714
  STEP: Creating projection with secret that has name projected-secret-test-82215ffc-f91a-4c16-b76b-ea82ed619b24 @ 06/03/23 13:08:36.72
  STEP: Creating a pod to test consume secrets @ 06/03/23 13:08:36.727
  E0603 13:08:37.337502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:38.337642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:39.337988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:40.338038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:08:40.758
  Jun  3 13:08:40.763: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-secrets-7392bf16-baa5-47ca-a08a-c4cd7ddfc977 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 13:08:40.772
  Jun  3 13:08:40.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3833" for this suite. @ 06/03/23 13:08:40.795
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 06/03/23 13:08:40.807
  Jun  3 13:08:40.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubelet-test @ 06/03/23 13:08:40.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:40.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:40.833
  E0603 13:08:41.338355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:42.338564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:08:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-353" for this suite. @ 06/03/23 13:08:42.885
• [2.086 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 06/03/23 13:08:42.893
  Jun  3 13:08:42.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:08:42.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:42.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:42.92
  STEP: Creating configMap with name projected-configmap-test-volume-3734c30b-ea65-4a4a-bdb7-2c6327a956ce @ 06/03/23 13:08:42.926
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:08:42.932
  E0603 13:08:43.338670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:44.338814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:45.339597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:46.339711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:08:46.958
  Jun  3 13:08:46.963: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-aa8281d4-550d-4b5d-9f86-562c482a2da8 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:08:46.97
  Jun  3 13:08:46.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8528" for this suite. @ 06/03/23 13:08:46.992
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 06/03/23 13:08:47.002
  Jun  3 13:08:47.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:08:47.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:47.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:47.028
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:08:47.034
  E0603 13:08:47.339869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:48.339993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:49.340078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:50.340399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:08:51.061
  Jun  3 13:08:51.065: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-e29d549e-cbb0-4bf0-804b-be16ed864058 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:08:51.074
  Jun  3 13:08:51.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-46" for this suite. @ 06/03/23 13:08:51.096
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 06/03/23 13:08:51.106
  Jun  3 13:08:51.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 13:08:51.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:51.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:51.135
  STEP: Creating secret with name s-test-opt-del-4ab38a96-04a3-4699-85d1-cac3f10dffdb @ 06/03/23 13:08:51.149
  STEP: Creating secret with name s-test-opt-upd-1fc311e5-92b6-432a-a3b6-65686cf2327c @ 06/03/23 13:08:51.156
  STEP: Creating the pod @ 06/03/23 13:08:51.162
  E0603 13:08:51.340808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:52.340929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-4ab38a96-04a3-4699-85d1-cac3f10dffdb @ 06/03/23 13:08:53.22
  STEP: Updating secret s-test-opt-upd-1fc311e5-92b6-432a-a3b6-65686cf2327c @ 06/03/23 13:08:53.228
  STEP: Creating secret with name s-test-opt-create-3464a246-bd14-472e-aa7a-4376da32f4e9 @ 06/03/23 13:08:53.238
  STEP: waiting to observe update in volume @ 06/03/23 13:08:53.245
  E0603 13:08:53.341672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:54.341958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:55.342409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:56.342739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:08:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5791" for this suite. @ 06/03/23 13:08:57.299
• [6.203 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 06/03/23 13:08:57.31
  Jun  3 13:08:57.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 13:08:57.311
  E0603 13:08:57.343311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:08:57.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:08:57.358
  STEP: creating service in namespace services-859 @ 06/03/23 13:08:57.364
  STEP: creating service affinity-nodeport-transition in namespace services-859 @ 06/03/23 13:08:57.364
  STEP: creating replication controller affinity-nodeport-transition in namespace services-859 @ 06/03/23 13:08:57.386
  I0603 13:08:57.399035      18 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-859, replica count: 3
  E0603 13:08:58.343391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:08:59.343827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:00.344619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:09:00.449977      18 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 13:09:00.467: INFO: Creating new exec pod
  E0603 13:09:01.344996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:02.345700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:03.345608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:09:03.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jun  3 13:09:03.706: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jun  3 13:09:03.706: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:09:03.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.150 80'
  Jun  3 13:09:03.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.150 80\nConnection to 10.152.183.150 80 port [tcp/http] succeeded!\n"
  Jun  3 13:09:03.869: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:09:03.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.85.85 32436'
  Jun  3 13:09:04.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.85.85 32436\nConnection to 172.31.85.85 32436 port [tcp/*] succeeded!\n"
  Jun  3 13:09:04.050: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:09:04.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.203 32436'
  Jun  3 13:09:04.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.203 32436\nConnection to 172.31.7.203 32436 port [tcp/*] succeeded!\n"
  Jun  3 13:09:04.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:09:04.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.193:32436/ ; done'
  E0603 13:09:04.346033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:09:04.519: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n"
  Jun  3 13:09:04.520: INFO: stdout: "\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-26lgh\naffinity-nodeport-transition-26lgh\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-kxk6f\naffinity-nodeport-transition-26lgh"
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-26lgh
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-26lgh
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-kxk6f
  Jun  3 13:09:04.520: INFO: Received response from host: affinity-nodeport-transition-26lgh
  Jun  3 13:09:04.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-859 exec execpod-affinityqq4xx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.193:32436/ ; done'
  Jun  3 13:09:04.844: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32436/\n"
  Jun  3 13:09:04.844: INFO: stdout: "\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4\naffinity-nodeport-transition-jphd4"
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Received response from host: affinity-nodeport-transition-jphd4
  Jun  3 13:09:04.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 13:09:04.849: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-859, will wait for the garbage collector to delete the pods @ 06/03/23 13:09:04.87
  Jun  3 13:09:04.933: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.301668ms
  Jun  3 13:09:05.034: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.102037ms
  E0603 13:09:05.346892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:06.347688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-859" for this suite. @ 06/03/23 13:09:07.259
• [9.956 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 06/03/23 13:09:07.269
  Jun  3 13:09:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:09:07.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:09:07.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:09:07.304
  E0603 13:09:07.348542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 06/03/23 13:09:07.352
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:09:07.893
  STEP: Deploying the webhook pod @ 06/03/23 13:09:07.902
  STEP: Wait for the deployment to be ready @ 06/03/23 13:09:07.918
  Jun  3 13:09:07.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:09:08.349501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:09.349634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:09:09.944
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:09:09.963
  E0603 13:09:10.349974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:09:10.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/03/23 13:09:10.968
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 06/03/23 13:09:10.996
  STEP: Creating a dummy validating-webhook-configuration object @ 06/03/23 13:09:11.021
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 06/03/23 13:09:11.04
  STEP: Creating a dummy mutating-webhook-configuration object @ 06/03/23 13:09:11.048
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 06/03/23 13:09:11.068
  Jun  3 13:09:11.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3025" for this suite. @ 06/03/23 13:09:11.151
  STEP: Destroying namespace "webhook-markers-2854" for this suite. @ 06/03/23 13:09:11.159
• [3.900 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 06/03/23 13:09:11.17
  Jun  3 13:09:11.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 13:09:11.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:09:11.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:09:11.196
  E0603 13:09:11.351033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:12.351743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:13.352473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:14.353451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:15.353937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:16.354883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:17.355375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:18.355443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:19.355582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:20.355976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:21.356379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:22.356512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:23.357535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:24.358517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:25.358910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:26.359761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:27.360022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:28.361095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:29.361910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:30.362982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:31.363191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:32.363838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:33.364858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:34.365338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:35.366474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:36.367397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:37.368108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:38.368457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:39.368876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:40.369898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:41.370433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:42.370978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:43.371326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:44.371434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:45.371567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:46.371726      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:47.372327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:48.372458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:49.372654      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:50.372701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:51.372785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:52.372912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:53.373040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:54.373174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:55.373731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:56.373948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:57.374029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:58.374155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:09:59.375197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:00.375320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:01.375434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:02.376349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:03.376490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:04.377458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:05.377746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:06.378600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:07.378730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:08.378841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:09.379597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:10.379752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:11.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1271" for this suite. @ 06/03/23 13:10:11.226
• [60.064 seconds]
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 06/03/23 13:10:11.234
  Jun  3 13:10:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename lease-test @ 06/03/23 13:10:11.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:10:11.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:10:11.264
  Jun  3 13:10:11.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3726" for this suite. @ 06/03/23 13:10:11.348
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 06/03/23 13:10:11.359
  Jun  3 13:10:11.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:10:11.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:10:11.378
  E0603 13:10:11.379824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:10:11.383
  STEP: Setting up server cert @ 06/03/23 13:10:11.417
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:10:11.86
  STEP: Deploying the webhook pod @ 06/03/23 13:10:11.87
  STEP: Wait for the deployment to be ready @ 06/03/23 13:10:11.886
  Jun  3 13:10:11.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:10:12.380339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:13.381311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:10:13.91
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:10:13.926
  E0603 13:10:14.382178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:14.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 06/03/23 13:10:14.932
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/03/23 13:10:14.932
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 06/03/23 13:10:14.952
  E0603 13:10:15.383042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 06/03/23 13:10:15.965
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/03/23 13:10:15.965
  E0603 13:10:16.383626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 06/03/23 13:10:17.009
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/03/23 13:10:17.009
  E0603 13:10:17.383796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:18.384108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:19.384279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:20.384617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:21.384749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 06/03/23 13:10:22.053
  STEP: Registering slow webhook via the AdmissionRegistration API @ 06/03/23 13:10:22.053
  E0603 13:10:22.384915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:23.385137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:24.385478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:25.385503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:26.386551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6706" for this suite. @ 06/03/23 13:10:27.164
  STEP: Destroying namespace "webhook-markers-9583" for this suite. @ 06/03/23 13:10:27.173
• [15.834 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 06/03/23 13:10:27.194
  Jun  3 13:10:27.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:10:27.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:10:27.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:10:27.219
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 06/03/23 13:10:27.224
  E0603 13:10:27.387046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:28.387220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:29.387991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:30.388178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:10:31.247
  Jun  3 13:10:31.252: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-2d99c51c-9d6a-4807-9746-453558e42e86 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:10:31.273
  Jun  3 13:10:31.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6202" for this suite. @ 06/03/23 13:10:31.299
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 06/03/23 13:10:31.309
  Jun  3 13:10:31.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 13:10:31.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:10:31.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:10:31.335
  STEP: creating service nodeport-test with type=NodePort in namespace services-3190 @ 06/03/23 13:10:31.341
  STEP: creating replication controller nodeport-test in namespace services-3190 @ 06/03/23 13:10:31.36
  I0603 13:10:31.374011      18 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3190, replica count: 2
  E0603 13:10:31.389114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:32.389667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:33.389915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:34.390030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:10:34.425848      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 13:10:34.425: INFO: Creating new exec pod
  E0603 13:10:35.391120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:36.391632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:37.392062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:37.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun  3 13:10:37.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun  3 13:10:37.612: INFO: stdout: ""
  E0603 13:10:38.392531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:38.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jun  3 13:10:38.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jun  3 13:10:38.768: INFO: stdout: "nodeport-test-rlfs7"
  Jun  3 13:10:38.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.35 80'
  Jun  3 13:10:38.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.35 80\nConnection to 10.152.183.35 80 port [tcp/http] succeeded!\n"
  Jun  3 13:10:38.934: INFO: stdout: "nodeport-test-cn2xx"
  Jun  3 13:10:38.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.27.193 31300'
  Jun  3 13:10:39.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.27.193 31300\nConnection to 172.31.27.193 31300 port [tcp/*] succeeded!\n"
  Jun  3 13:10:39.110: INFO: stdout: "nodeport-test-cn2xx"
  Jun  3 13:10:39.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.85.85 31300'
  Jun  3 13:10:39.274: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.85.85 31300\nConnection to 172.31.85.85 31300 port [tcp/*] succeeded!\n"
  Jun  3 13:10:39.274: INFO: stdout: ""
  E0603 13:10:39.392817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:40.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-3190 exec execpodvgzpv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.85.85 31300'
  E0603 13:10:40.393146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:40.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.85.85 31300\nConnection to 172.31.85.85 31300 port [tcp/*] succeeded!\n"
  Jun  3 13:10:40.431: INFO: stdout: "nodeport-test-rlfs7"
  Jun  3 13:10:40.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3190" for this suite. @ 06/03/23 13:10:40.436
• [9.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 06/03/23 13:10:40.447
  Jun  3 13:10:40.447: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 13:10:40.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:10:40.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:10:40.475
  STEP: Creating service test in namespace statefulset-9940 @ 06/03/23 13:10:40.481
  STEP: Creating a new StatefulSet @ 06/03/23 13:10:40.488
  Jun  3 13:10:40.500: INFO: Found 0 stateful pods, waiting for 3
  E0603 13:10:41.393350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:42.393393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:43.393655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:44.394462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:45.394565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:46.395598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:47.396163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:48.396912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:49.397233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:50.397494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:10:50.506: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:10:50.506: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:10:50.506: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 06/03/23 13:10:50.519
  Jun  3 13:10:50.541: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 06/03/23 13:10:50.541
  E0603 13:10:51.397561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:52.398334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:53.398572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:54.398706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:55.398763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:56.400118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:57.400458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:58.400667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:10:59.400928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:00.401044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 06/03/23 13:11:00.563
  STEP: Performing a canary update @ 06/03/23 13:11:00.563
  Jun  3 13:11:00.585: INFO: Updating stateful set ss2
  Jun  3 13:11:00.596: INFO: Waiting for Pod statefulset-9940/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0603 13:11:01.402052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:02.402545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:03.403621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:04.403799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:05.404040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:06.404196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:07.404739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:08.405266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:09.405538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:10.405983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 06/03/23 13:11:10.606
  Jun  3 13:11:10.646: INFO: Found 1 stateful pods, waiting for 3
  E0603 13:11:11.406161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:12.406613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:13.407610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:14.407864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:15.407938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:16.408054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:17.408562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:18.408663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:19.408770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:20.408880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:20.652: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:11:20.652: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:11:20.652: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 06/03/23 13:11:20.662
  Jun  3 13:11:20.685: INFO: Updating stateful set ss2
  Jun  3 13:11:20.695: INFO: Waiting for Pod statefulset-9940/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0603 13:11:21.408996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:22.409094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:23.409218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:24.410062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:25.410461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:26.410682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:27.410793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:28.410913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:29.411041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:30.411511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:30.729: INFO: Updating stateful set ss2
  Jun  3 13:11:30.742: INFO: Waiting for StatefulSet statefulset-9940/ss2 to complete update
  Jun  3 13:11:30.742: INFO: Waiting for Pod statefulset-9940/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0603 13:11:31.411708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:32.411933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:33.412402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:34.412718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:35.412822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:36.412946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:37.413059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:38.413161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:39.413269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:40.413521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:40.752: INFO: Deleting all statefulset in ns statefulset-9940
  Jun  3 13:11:40.757: INFO: Scaling statefulset ss2 to 0
  E0603 13:11:41.414618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:42.415609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:43.416692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:44.416934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:45.417108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:46.417237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:47.418027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:48.418141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:49.418291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:50.418418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:50.776: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:11:50.780: INFO: Deleting statefulset ss2
  Jun  3 13:11:50.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9940" for this suite. @ 06/03/23 13:11:50.814
• [70.378 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 06/03/23 13:11:50.828
  Jun  3 13:11:50.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-pred @ 06/03/23 13:11:50.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:11:50.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:11:50.865
  Jun  3 13:11:50.875: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun  3 13:11:50.898: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 13:11:50.903: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-193 before test
  Jun  3 13:11:50.920: INFO: nginx-ingress-controller-kubernetes-worker-8lww8 from ingress-nginx-kubernetes-worker started at 2023-06-03 13:02:02 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.920: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:50.920: INFO: sonobuoy from sonobuoy started at 2023-06-03 12:00:20 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.920: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun  3 13:11:50.921: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-9mjpr from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:50.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:50.921: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:11:50.921: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-7-203 before test
  Jun  3 13:11:50.930: INFO: nginx-ingress-controller-kubernetes-worker-js8tg from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.930: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: calico-kube-controllers-74bc8c9977-fzgz2 from kube-system started at 2023-06-03 11:55:43 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: coredns-5c7f76ccb8-p6pwx from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container coredns ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: kube-state-metrics-5b95b4459c-wk958 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: metrics-server-v0.5.2-6cf8c8b69c-88j59 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container metrics-server ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: dashboard-metrics-scraper-6b8586b5c9-28hlk from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: kubernetes-dashboard-6869f4cd5f-dv2kl from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-7fcft from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:50.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:11:50.931: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-85-85 before test
  Jun  3 13:11:50.939: INFO: default-http-backend-kubernetes-worker-65fc475d49-ddthh from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:24 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.939: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun  3 13:11:50.939: INFO: nginx-ingress-controller-kubernetes-worker-t855p from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:50.939: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:50.939: INFO: sonobuoy-e2e-job-56763c10077449b8 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:50.939: INFO: 	Container e2e ready: true, restart count 0
  Jun  3 13:11:50.939: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:50.939: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cwkk4 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:50.939: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:50.939: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 06/03/23 13:11:50.939
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17652844ae22417d], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 06/03/23 13:11:50.97
  E0603 13:11:51.419510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:51.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6261" for this suite. @ 06/03/23 13:11:51.98
• [1.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 06/03/23 13:11:51.993
  Jun  3 13:11:51.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 13:11:51.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:11:52.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:11:52.019
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 06/03/23 13:11:52.025
  Jun  3 13:11:52.038: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0603 13:11:52.420250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:53.420475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:54.420696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:55.420794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:56.420816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:11:57.045: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 13:11:57.045
  STEP: getting scale subresource @ 06/03/23 13:11:57.045
  STEP: updating a scale subresource @ 06/03/23 13:11:57.05
  STEP: verifying the replicaset Spec.Replicas was modified @ 06/03/23 13:11:57.059
  STEP: Patch a scale subresource @ 06/03/23 13:11:57.065
  Jun  3 13:11:57.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8609" for this suite. @ 06/03/23 13:11:57.096
• [5.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 06/03/23 13:11:57.111
  Jun  3 13:11:57.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-pred @ 06/03/23 13:11:57.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:11:57.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:11:57.151
  Jun  3 13:11:57.157: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun  3 13:11:57.169: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 13:11:57.174: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-193 before test
  Jun  3 13:11:57.181: INFO: nginx-ingress-controller-kubernetes-worker-8lww8 from ingress-nginx-kubernetes-worker started at 2023-06-03 13:02:02 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.181: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:57.181: INFO: test-rs-62hzl from replicaset-8609 started at 2023-06-03 13:11:52 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.181: INFO: 	Container httpd ready: true, restart count 0
  Jun  3 13:11:57.181: INFO: sonobuoy from sonobuoy started at 2023-06-03 12:00:20 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.181: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun  3 13:11:57.181: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-9mjpr from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:57.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:57.182: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:11:57.182: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-7-203 before test
  Jun  3 13:11:57.189: INFO: nginx-ingress-controller-kubernetes-worker-js8tg from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.189: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: calico-kube-controllers-74bc8c9977-fzgz2 from kube-system started at 2023-06-03 11:55:43 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: coredns-5c7f76ccb8-p6pwx from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container coredns ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: kube-state-metrics-5b95b4459c-wk958 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: metrics-server-v0.5.2-6cf8c8b69c-88j59 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container metrics-server ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: dashboard-metrics-scraper-6b8586b5c9-28hlk from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: kubernetes-dashboard-6869f4cd5f-dv2kl from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-7fcft from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:57.190: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:11:57.190: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-85-85 before test
  Jun  3 13:11:57.197: INFO: default-http-backend-kubernetes-worker-65fc475d49-ddthh from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:24 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.198: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun  3 13:11:57.198: INFO: nginx-ingress-controller-kubernetes-worker-t855p from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.198: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:11:57.198: INFO: test-rs-j7vfx from replicaset-8609 started at 2023-06-03 13:11:57 +0000 UTC (1 container statuses recorded)
  Jun  3 13:11:57.198: INFO: 	Container httpd ready: false, restart count 0
  Jun  3 13:11:57.198: INFO: sonobuoy-e2e-job-56763c10077449b8 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:57.198: INFO: 	Container e2e ready: true, restart count 0
  Jun  3 13:11:57.198: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:57.198: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cwkk4 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:11:57.198: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:11:57.198: INFO: 	Container systemd-logs ready: true, restart count 0
  E0603 13:11:57.421206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:58.421347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:11:59.421535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:00.421721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:01.421750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:02.422026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the label node ip-172-31-27-193 @ 06/03/23 13:12:03.246
  STEP: verifying the node has the label node ip-172-31-7-203 @ 06/03/23 13:12:03.262
  STEP: verifying the node has the label node ip-172-31-85-85 @ 06/03/23 13:12:03.279
  Jun  3 13:12:03.300: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-ddthh requesting resource cpu=10m on Node ip-172-31-85-85
  Jun  3 13:12:03.300: INFO: Pod nginx-ingress-controller-kubernetes-worker-8lww8 requesting resource cpu=0m on Node ip-172-31-27-193
  Jun  3 13:12:03.301: INFO: Pod nginx-ingress-controller-kubernetes-worker-js8tg requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod nginx-ingress-controller-kubernetes-worker-t855p requesting resource cpu=0m on Node ip-172-31-85-85
  Jun  3 13:12:03.301: INFO: Pod calico-kube-controllers-74bc8c9977-fzgz2 requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod coredns-5c7f76ccb8-p6pwx requesting resource cpu=100m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod kube-state-metrics-5b95b4459c-wk958 requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-88j59 requesting resource cpu=5m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-28hlk requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.301: INFO: Pod kubernetes-dashboard-6869f4cd5f-dv2kl requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.302: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-27-193
  Jun  3 13:12:03.302: INFO: Pod sonobuoy-e2e-job-56763c10077449b8 requesting resource cpu=0m on Node ip-172-31-85-85
  Jun  3 13:12:03.302: INFO: Pod sonobuoy-systemd-logs-daemon-set-9086805e944f4091-7fcft requesting resource cpu=0m on Node ip-172-31-7-203
  Jun  3 13:12:03.302: INFO: Pod sonobuoy-systemd-logs-daemon-set-9086805e944f4091-9mjpr requesting resource cpu=0m on Node ip-172-31-27-193
  Jun  3 13:12:03.302: INFO: Pod sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cwkk4 requesting resource cpu=0m on Node ip-172-31-85-85
  STEP: Starting Pods to consume most of the cluster CPU. @ 06/03/23 13:12:03.302
  Jun  3 13:12:03.302: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-27-193
  Jun  3 13:12:03.313: INFO: Creating a pod which consumes cpu=1326m on Node ip-172-31-7-203
  Jun  3 13:12:03.322: INFO: Creating a pod which consumes cpu=1393m on Node ip-172-31-85-85
  E0603 13:12:03.422489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:04.422567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 06/03/23 13:12:05.349
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1361fc98-adbb-4ccf-a66e-893882168051.176528478f383960], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9155/filler-pod-1361fc98-adbb-4ccf-a66e-893882168051 to ip-172-31-7-203] @ 06/03/23 13:12:05.355
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1361fc98-adbb-4ccf-a66e-893882168051.17652847ba0ddd26], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/03/23 13:12:05.355
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1361fc98-adbb-4ccf-a66e-893882168051.17652847bb13b61c], Reason = [Created], Message = [Created container filler-pod-1361fc98-adbb-4ccf-a66e-893882168051] @ 06/03/23 13:12:05.355
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1361fc98-adbb-4ccf-a66e-893882168051.17652847bfb51888], Reason = [Started], Message = [Started container filler-pod-1361fc98-adbb-4ccf-a66e-893882168051] @ 06/03/23 13:12:05.355
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4.176528478e8a2a64], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9155/filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4 to ip-172-31-27-193] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4.17652847b8e119ea], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4.17652847ba058f3b], Reason = [Created], Message = [Created container filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4.17652847be9e6431], Reason = [Started], Message = [Started container filler-pod-33c01c9e-4341-4ab7-b2df-e83195d33db4] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b.176528478ffdc4d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9155/filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b to ip-172-31-85-85] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b.17652847bb281209], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b.17652847bc5c120a], Reason = [Created], Message = [Created container filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b.17652847c1435930], Reason = [Started], Message = [Started container filler-pod-970a5b8b-0f11-4b75-8ea5-2d870df76b1b] @ 06/03/23 13:12:05.356
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176528480863f750], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 06/03/23 13:12:05.37
  E0603 13:12:05.423116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-27-193 @ 06/03/23 13:12:06.369
  STEP: verifying the node doesn't have the label node @ 06/03/23 13:12:06.394
  STEP: removing the label node off the node ip-172-31-7-203 @ 06/03/23 13:12:06.405
  E0603 13:12:06.424120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node doesn't have the label node @ 06/03/23 13:12:06.433
  STEP: removing the label node off the node ip-172-31-85-85 @ 06/03/23 13:12:06.439
  STEP: verifying the node doesn't have the label node @ 06/03/23 13:12:06.455
  Jun  3 13:12:06.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9155" for this suite. @ 06/03/23 13:12:06.472
• [9.370 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 06/03/23 13:12:06.482
  Jun  3 13:12:06.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 13:12:06.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:12:06.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:12:06.529
  Jun  3 13:12:06.572: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 13:12:06.58
  Jun  3 13:12:06.588: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:06.589: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:06.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:12:06.598: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:12:07.424545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:07.604: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:07.604: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:07.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:12:07.608: INFO: Node ip-172-31-7-203 is running 0 daemon pod, expected 1
  E0603 13:12:08.424559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:08.603: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:08.603: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:08.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:12:08.607: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 06/03/23 13:12:08.627
  STEP: Check that daemon pods images are updated. @ 06/03/23 13:12:08.641
  Jun  3 13:12:08.645: INFO: Wrong image for pod: daemon-set-hzqwt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:08.646: INFO: Wrong image for pod: daemon-set-vk5mc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:08.646: INFO: Wrong image for pod: daemon-set-wgznc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:08.651: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:08.651: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:09.424707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:09.656: INFO: Pod daemon-set-h59lb is not available
  Jun  3 13:12:09.656: INFO: Wrong image for pod: daemon-set-vk5mc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:09.656: INFO: Wrong image for pod: daemon-set-wgznc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:09.661: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:09.661: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:10.425175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:10.658: INFO: Wrong image for pod: daemon-set-wgznc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:10.663: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:10.664: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:11.425651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:11.658: INFO: Pod daemon-set-m9vkm is not available
  Jun  3 13:12:11.658: INFO: Wrong image for pod: daemon-set-wgznc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:11.664: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:11.664: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:12.425761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:12.656: INFO: Pod daemon-set-m9vkm is not available
  Jun  3 13:12:12.656: INFO: Wrong image for pod: daemon-set-wgznc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jun  3 13:12:12.662: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:12.662: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:13.426034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:13.662: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:13.662: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0603 13:12:14.426107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:14.657: INFO: Pod daemon-set-92zth is not available
  Jun  3 13:12:14.662: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:14.662: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 06/03/23 13:12:14.662
  Jun  3 13:12:14.666: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:14.666: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:14.670: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:12:14.670: INFO: Node ip-172-31-7-203 is running 0 daemon pod, expected 1
  E0603 13:12:15.426534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:15.677: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:15.677: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:12:15.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:12:15.682: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 13:12:15.703
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9683, will wait for the garbage collector to delete the pods @ 06/03/23 13:12:15.703
  Jun  3 13:12:15.765: INFO: Deleting DaemonSet.extensions daemon-set took: 7.174996ms
  Jun  3 13:12:15.865: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.269041ms
  E0603 13:12:16.427197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:17.427760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:17.570: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:12:17.570: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 13:12:17.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32990"},"items":null}

  Jun  3 13:12:17.578: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32990"},"items":null}

  Jun  3 13:12:17.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9683" for this suite. @ 06/03/23 13:12:17.6
• [11.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 06/03/23 13:12:17.61
  Jun  3 13:12:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subpath @ 06/03/23 13:12:17.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:12:17.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:12:17.637
  STEP: Setting up data @ 06/03/23 13:12:17.642
  STEP: Creating pod pod-subpath-test-configmap-hz58 @ 06/03/23 13:12:17.654
  STEP: Creating a pod to test atomic-volume-subpath @ 06/03/23 13:12:17.654
  E0603 13:12:18.428678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:19.429517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:20.429640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:21.430471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:22.430535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:23.431608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:24.431719      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:25.432181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:26.432294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:27.432405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:28.432574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:29.432959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:30.433047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:31.433750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:32.434018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:33.434807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:34.435623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:35.435717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:36.435816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:37.436319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:38.436442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:39.436561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:40.436627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:41.436742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:12:41.732
  Jun  3 13:12:41.737: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-subpath-test-configmap-hz58 container test-container-subpath-configmap-hz58: <nil>
  STEP: delete the pod @ 06/03/23 13:12:41.764
  STEP: Deleting pod pod-subpath-test-configmap-hz58 @ 06/03/23 13:12:41.782
  Jun  3 13:12:41.782: INFO: Deleting pod "pod-subpath-test-configmap-hz58" in namespace "subpath-8888"
  Jun  3 13:12:41.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8888" for this suite. @ 06/03/23 13:12:41.792
• [24.190 seconds]
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 06/03/23 13:12:41.801
  Jun  3 13:12:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl-logs @ 06/03/23 13:12:41.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:12:41.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:12:41.826
  STEP: creating an pod @ 06/03/23 13:12:41.831
  Jun  3 13:12:41.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jun  3 13:12:41.918: INFO: stderr: ""
  Jun  3 13:12:41.918: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 06/03/23 13:12:41.918
  Jun  3 13:12:41.918: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0603 13:12:42.436868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:43.436980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:43.928: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 06/03/23 13:12:43.928
  Jun  3 13:12:43.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator'
  Jun  3 13:12:44.017: INFO: stderr: ""
  Jun  3 13:12:44.017: INFO: stdout: "I0603 13:12:42.702676       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/92v 298\nI0603 13:12:42.902760       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/5sq 397\nI0603 13:12:43.103282       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/5t2 542\nI0603 13:12:43.303632       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ww4 357\nI0603 13:12:43.502987       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/c4sv 396\nI0603 13:12:43.703291       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/5sp 476\nI0603 13:12:43.903637       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/95gh 570\n"
  STEP: limiting log lines @ 06/03/23 13:12:44.017
  Jun  3 13:12:44.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator --tail=1'
  Jun  3 13:12:44.110: INFO: stderr: ""
  Jun  3 13:12:44.110: INFO: stdout: "I0603 13:12:44.103006       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/z65 490\n"
  Jun  3 13:12:44.110: INFO: got output "I0603 13:12:44.103006       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/z65 490\n"
  STEP: limiting log bytes @ 06/03/23 13:12:44.11
  Jun  3 13:12:44.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator --limit-bytes=1'
  Jun  3 13:12:44.212: INFO: stderr: ""
  Jun  3 13:12:44.212: INFO: stdout: "I"
  Jun  3 13:12:44.212: INFO: got output "I"
  STEP: exposing timestamps @ 06/03/23 13:12:44.212
  Jun  3 13:12:44.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator --tail=1 --timestamps'
  Jun  3 13:12:44.301: INFO: stderr: ""
  Jun  3 13:12:44.301: INFO: stdout: "2023-06-03T13:12:44.103111647Z I0603 13:12:44.103006       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/z65 490\n"
  Jun  3 13:12:44.301: INFO: got output "2023-06-03T13:12:44.103111647Z I0603 13:12:44.103006       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/z65 490\n"
  STEP: restricting to a time range @ 06/03/23 13:12:44.301
  E0603 13:12:44.437454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:45.437542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:46.437676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:46.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator --since=1s'
  Jun  3 13:12:46.897: INFO: stderr: ""
  Jun  3 13:12:46.897: INFO: stdout: "I0603 13:12:45.903020       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/qwz 385\nI0603 13:12:46.103321       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/tqk 371\nI0603 13:12:46.303669       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/ld9z 252\nI0603 13:12:46.503027       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7kb9 252\nI0603 13:12:46.703327       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4g4q 220\n"
  Jun  3 13:12:46.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 logs logs-generator logs-generator --since=24h'
  Jun  3 13:12:46.992: INFO: stderr: ""
  Jun  3 13:12:46.992: INFO: stdout: "I0603 13:12:42.702676       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/92v 298\nI0603 13:12:42.902760       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/5sq 397\nI0603 13:12:43.103282       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/5t2 542\nI0603 13:12:43.303632       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ww4 357\nI0603 13:12:43.502987       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/c4sv 396\nI0603 13:12:43.703291       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/5sp 476\nI0603 13:12:43.903637       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/95gh 570\nI0603 13:12:44.103006       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/z65 490\nI0603 13:12:44.303297       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/8flf 372\nI0603 13:12:44.503645       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/z92 332\nI0603 13:12:44.703008       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/cq4v 292\nI0603 13:12:44.903308       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/w6nr 338\nI0603 13:12:45.103654       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/z7b7 536\nI0603 13:12:45.303011       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/6tx 465\nI0603 13:12:45.503313       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/mh4 290\nI0603 13:12:45.703660       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/dt26 433\nI0603 13:12:45.903020       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/qwz 385\nI0603 13:12:46.103321       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/tqk 371\nI0603 13:12:46.303669       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/ld9z 252\nI0603 13:12:46.503027       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7kb9 252\nI0603 13:12:46.703327       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/4g4q 220\nI0603 13:12:46.903700       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/h5c 275\n"
  Jun  3 13:12:46.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-logs-8822 delete pod logs-generator'
  E0603 13:12:47.437988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:47.902: INFO: stderr: ""
  Jun  3 13:12:47.903: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jun  3 13:12:47.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-8822" for this suite. @ 06/03/23 13:12:47.908
• [6.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 06/03/23 13:12:47.916
  Jun  3 13:12:47.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 13:12:47.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:12:47.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:12:47.941
  E0603 13:12:48.438496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:49.438754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:49.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 13:12:49.978: INFO: Deleting pod "var-expansion-416a6231-77fe-41d7-ae48-6eb38d57bb6b" in namespace "var-expansion-1065"
  Jun  3 13:12:49.986: INFO: Wait up to 5m0s for pod "var-expansion-416a6231-77fe-41d7-ae48-6eb38d57bb6b" to be fully deleted
  E0603 13:12:50.439614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:12:51.439844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-1065" for this suite. @ 06/03/23 13:12:51.996
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 06/03/23 13:12:52.007
  Jun  3 13:12:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svc-latency @ 06/03/23 13:12:52.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:12:52.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:12:52.035
  Jun  3 13:12:52.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-2804 @ 06/03/23 13:12:52.041
  I0603 13:12:52.050842      18 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2804, replica count: 1
  E0603 13:12:52.440852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:12:53.102463      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0603 13:12:53.441369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:12:54.102636      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 13:12:54.217: INFO: Created: latency-svc-gzzbn
  Jun  3 13:12:54.225: INFO: Got endpoints: latency-svc-gzzbn [22.492436ms]
  Jun  3 13:12:54.248: INFO: Created: latency-svc-d2gjh
  Jun  3 13:12:54.254: INFO: Got endpoints: latency-svc-d2gjh [27.816123ms]
  Jun  3 13:12:54.256: INFO: Created: latency-svc-8lnck
  Jun  3 13:12:54.262: INFO: Got endpoints: latency-svc-8lnck [35.845525ms]
  Jun  3 13:12:54.264: INFO: Created: latency-svc-5gkps
  Jun  3 13:12:54.273: INFO: Created: latency-svc-wtnr5
  Jun  3 13:12:54.277: INFO: Created: latency-svc-28r9m
  Jun  3 13:12:54.277: INFO: Got endpoints: latency-svc-5gkps [51.231828ms]
  Jun  3 13:12:54.281: INFO: Got endpoints: latency-svc-wtnr5 [54.336576ms]
  Jun  3 13:12:54.288: INFO: Created: latency-svc-lgvd9
  Jun  3 13:12:54.290: INFO: Got endpoints: latency-svc-28r9m [62.661857ms]
  Jun  3 13:12:54.296: INFO: Created: latency-svc-p7pps
  Jun  3 13:12:54.296: INFO: Got endpoints: latency-svc-lgvd9 [69.981627ms]
  Jun  3 13:12:54.303: INFO: Got endpoints: latency-svc-p7pps [77.111901ms]
  Jun  3 13:12:54.307: INFO: Created: latency-svc-n4gk4
  Jun  3 13:12:54.311: INFO: Got endpoints: latency-svc-n4gk4 [84.504592ms]
  Jun  3 13:12:54.314: INFO: Created: latency-svc-npjdv
  Jun  3 13:12:54.323: INFO: Got endpoints: latency-svc-npjdv [96.630674ms]
  Jun  3 13:12:54.327: INFO: Created: latency-svc-x2bmp
  Jun  3 13:12:54.332: INFO: Got endpoints: latency-svc-x2bmp [105.983487ms]
  Jun  3 13:12:54.333: INFO: Created: latency-svc-ftpxw
  Jun  3 13:12:54.340: INFO: Got endpoints: latency-svc-ftpxw [113.71919ms]
  Jun  3 13:12:54.342: INFO: Created: latency-svc-9b5w2
  Jun  3 13:12:54.345: INFO: Created: latency-svc-gmw27
  Jun  3 13:12:54.354: INFO: Got endpoints: latency-svc-9b5w2 [127.300285ms]
  Jun  3 13:12:54.356: INFO: Created: latency-svc-4mcv7
  Jun  3 13:12:54.357: INFO: Got endpoints: latency-svc-gmw27 [130.166656ms]
  Jun  3 13:12:54.363: INFO: Created: latency-svc-5glz6
  Jun  3 13:12:54.364: INFO: Got endpoints: latency-svc-4mcv7 [136.795004ms]
  Jun  3 13:12:54.370: INFO: Created: latency-svc-st6g6
  Jun  3 13:12:54.371: INFO: Got endpoints: latency-svc-5glz6 [144.085623ms]
  Jun  3 13:12:54.378: INFO: Got endpoints: latency-svc-st6g6 [124.004782ms]
  Jun  3 13:12:54.381: INFO: Created: latency-svc-kzmlc
  Jun  3 13:12:54.386: INFO: Got endpoints: latency-svc-kzmlc [124.261641ms]
  Jun  3 13:12:54.387: INFO: Created: latency-svc-65pv9
  Jun  3 13:12:54.396: INFO: Created: latency-svc-g5clx
  Jun  3 13:12:54.397: INFO: Got endpoints: latency-svc-65pv9 [120.029358ms]
  Jun  3 13:12:54.406: INFO: Got endpoints: latency-svc-g5clx [124.389454ms]
  Jun  3 13:12:54.407: INFO: Created: latency-svc-gffbc
  Jun  3 13:12:54.413: INFO: Got endpoints: latency-svc-gffbc [123.28268ms]
  Jun  3 13:12:54.415: INFO: Created: latency-svc-m4z4z
  Jun  3 13:12:54.422: INFO: Created: latency-svc-5cbvj
  Jun  3 13:12:54.423: INFO: Got endpoints: latency-svc-m4z4z [127.217703ms]
  Jun  3 13:12:54.430: INFO: Got endpoints: latency-svc-5cbvj [125.568721ms]
  Jun  3 13:12:54.441: INFO: Created: latency-svc-4zvzl
  Jun  3 13:12:54.442: INFO: Created: latency-svc-skwp7
  E0603 13:12:54.442152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:54.448: INFO: Created: latency-svc-bx68q
  Jun  3 13:12:54.448: INFO: Got endpoints: latency-svc-4zvzl [124.893401ms]
  Jun  3 13:12:54.450: INFO: Got endpoints: latency-svc-skwp7 [138.852858ms]
  Jun  3 13:12:54.456: INFO: Created: latency-svc-6c8cl
  Jun  3 13:12:54.459: INFO: Got endpoints: latency-svc-bx68q [125.986275ms]
  Jun  3 13:12:54.465: INFO: Created: latency-svc-m48hj
  Jun  3 13:12:54.466: INFO: Got endpoints: latency-svc-6c8cl [125.675395ms]
  Jun  3 13:12:54.469: INFO: Got endpoints: latency-svc-m48hj [115.278439ms]
  Jun  3 13:12:54.476: INFO: Created: latency-svc-hh75r
  Jun  3 13:12:54.482: INFO: Created: latency-svc-qmkqt
  Jun  3 13:12:54.483: INFO: Got endpoints: latency-svc-hh75r [125.950443ms]
  Jun  3 13:12:54.490: INFO: Got endpoints: latency-svc-qmkqt [125.948113ms]
  Jun  3 13:12:54.491: INFO: Created: latency-svc-s6px5
  Jun  3 13:12:54.496: INFO: Created: latency-svc-5j25w
  Jun  3 13:12:54.498: INFO: Got endpoints: latency-svc-s6px5 [127.253794ms]
  Jun  3 13:12:54.505: INFO: Got endpoints: latency-svc-5j25w [126.76618ms]
  Jun  3 13:12:54.508: INFO: Created: latency-svc-86htc
  Jun  3 13:12:54.521: INFO: Created: latency-svc-d4m9x
  Jun  3 13:12:54.522: INFO: Got endpoints: latency-svc-86htc [135.471183ms]
  Jun  3 13:12:54.529: INFO: Created: latency-svc-r49cz
  Jun  3 13:12:54.530: INFO: Got endpoints: latency-svc-d4m9x [132.687055ms]
  Jun  3 13:12:54.541: INFO: Created: latency-svc-f4hdq
  Jun  3 13:12:54.541: INFO: Got endpoints: latency-svc-r49cz [134.856623ms]
  Jun  3 13:12:54.548: INFO: Created: latency-svc-z9xw8
  Jun  3 13:12:54.548: INFO: Got endpoints: latency-svc-f4hdq [134.501912ms]
  Jun  3 13:12:54.558: INFO: Created: latency-svc-7cpbz
  Jun  3 13:12:54.562: INFO: Created: latency-svc-8vzhn
  Jun  3 13:12:54.566: INFO: Created: latency-svc-wtj2b
  Jun  3 13:12:54.571: INFO: Got endpoints: latency-svc-z9xw8 [146.939802ms]
  Jun  3 13:12:54.576: INFO: Created: latency-svc-5jtvr
  Jun  3 13:12:54.582: INFO: Created: latency-svc-kmhvl
  Jun  3 13:12:54.588: INFO: Created: latency-svc-9jblc
  Jun  3 13:12:54.595: INFO: Created: latency-svc-2kzcw
  Jun  3 13:12:54.602: INFO: Created: latency-svc-2bpkb
  Jun  3 13:12:54.607: INFO: Created: latency-svc-mxrnt
  Jun  3 13:12:54.617: INFO: Created: latency-svc-zkcw5
  Jun  3 13:12:54.622: INFO: Got endpoints: latency-svc-7cpbz [192.278276ms]
  Jun  3 13:12:54.627: INFO: Created: latency-svc-kr4pd
  Jun  3 13:12:54.632: INFO: Created: latency-svc-4lnl8
  Jun  3 13:12:54.639: INFO: Created: latency-svc-c59xq
  Jun  3 13:12:54.646: INFO: Created: latency-svc-4b6wf
  Jun  3 13:12:54.651: INFO: Created: latency-svc-pmg7c
  Jun  3 13:12:54.656: INFO: Created: latency-svc-vx6lm
  Jun  3 13:12:54.671: INFO: Got endpoints: latency-svc-8vzhn [222.471053ms]
  Jun  3 13:12:54.684: INFO: Created: latency-svc-xslbg
  Jun  3 13:12:54.722: INFO: Got endpoints: latency-svc-wtj2b [272.150333ms]
  Jun  3 13:12:54.733: INFO: Created: latency-svc-4qzsg
  Jun  3 13:12:54.770: INFO: Got endpoints: latency-svc-5jtvr [311.56927ms]
  Jun  3 13:12:54.785: INFO: Created: latency-svc-dmxmx
  Jun  3 13:12:54.821: INFO: Got endpoints: latency-svc-kmhvl [354.663293ms]
  Jun  3 13:12:54.833: INFO: Created: latency-svc-q5bjl
  Jun  3 13:12:54.872: INFO: Got endpoints: latency-svc-9jblc [402.37666ms]
  Jun  3 13:12:54.884: INFO: Created: latency-svc-269mb
  Jun  3 13:12:54.922: INFO: Got endpoints: latency-svc-2kzcw [438.848565ms]
  Jun  3 13:12:54.932: INFO: Created: latency-svc-shkdd
  Jun  3 13:12:54.972: INFO: Got endpoints: latency-svc-2bpkb [481.079751ms]
  Jun  3 13:12:54.985: INFO: Created: latency-svc-2r8jq
  Jun  3 13:12:55.022: INFO: Got endpoints: latency-svc-mxrnt [524.07926ms]
  Jun  3 13:12:55.035: INFO: Created: latency-svc-jpp8k
  Jun  3 13:12:55.071: INFO: Got endpoints: latency-svc-zkcw5 [565.996966ms]
  Jun  3 13:12:55.082: INFO: Created: latency-svc-2ncj4
  Jun  3 13:12:55.122: INFO: Got endpoints: latency-svc-kr4pd [599.803238ms]
  Jun  3 13:12:55.136: INFO: Created: latency-svc-ct5pd
  Jun  3 13:12:55.172: INFO: Got endpoints: latency-svc-4lnl8 [641.505297ms]
  Jun  3 13:12:55.184: INFO: Created: latency-svc-pq745
  Jun  3 13:12:55.221: INFO: Got endpoints: latency-svc-c59xq [679.345125ms]
  Jun  3 13:12:55.236: INFO: Created: latency-svc-6mdzf
  Jun  3 13:12:55.271: INFO: Got endpoints: latency-svc-4b6wf [722.577931ms]
  Jun  3 13:12:55.284: INFO: Created: latency-svc-dml9n
  Jun  3 13:12:55.322: INFO: Got endpoints: latency-svc-pmg7c [750.418106ms]
  Jun  3 13:12:55.335: INFO: Created: latency-svc-lg87m
  Jun  3 13:12:55.372: INFO: Got endpoints: latency-svc-vx6lm [749.172266ms]
  Jun  3 13:12:55.384: INFO: Created: latency-svc-948qx
  Jun  3 13:12:55.422: INFO: Got endpoints: latency-svc-xslbg [750.818137ms]
  Jun  3 13:12:55.436: INFO: Created: latency-svc-lv2g2
  E0603 13:12:55.442478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:55.472: INFO: Got endpoints: latency-svc-4qzsg [749.579159ms]
  Jun  3 13:12:55.484: INFO: Created: latency-svc-4bl4p
  Jun  3 13:12:55.522: INFO: Got endpoints: latency-svc-dmxmx [751.324984ms]
  Jun  3 13:12:55.534: INFO: Created: latency-svc-pv4sl
  Jun  3 13:12:55.571: INFO: Got endpoints: latency-svc-q5bjl [749.425364ms]
  Jun  3 13:12:55.585: INFO: Created: latency-svc-qwbxf
  Jun  3 13:12:55.622: INFO: Got endpoints: latency-svc-269mb [749.679282ms]
  Jun  3 13:12:55.634: INFO: Created: latency-svc-m2lhh
  Jun  3 13:12:55.671: INFO: Got endpoints: latency-svc-shkdd [748.810035ms]
  Jun  3 13:12:55.683: INFO: Created: latency-svc-gt8zv
  Jun  3 13:12:55.722: INFO: Got endpoints: latency-svc-2r8jq [749.950211ms]
  Jun  3 13:12:55.735: INFO: Created: latency-svc-jkcns
  Jun  3 13:12:55.772: INFO: Got endpoints: latency-svc-jpp8k [749.866779ms]
  Jun  3 13:12:55.784: INFO: Created: latency-svc-5thr8
  Jun  3 13:12:55.820: INFO: Got endpoints: latency-svc-2ncj4 [749.223008ms]
  Jun  3 13:12:55.834: INFO: Created: latency-svc-xkpz5
  Jun  3 13:12:55.871: INFO: Got endpoints: latency-svc-ct5pd [748.987631ms]
  Jun  3 13:12:55.886: INFO: Created: latency-svc-m7d2d
  Jun  3 13:12:55.922: INFO: Got endpoints: latency-svc-pq745 [749.584149ms]
  Jun  3 13:12:55.933: INFO: Created: latency-svc-f8xx8
  Jun  3 13:12:55.971: INFO: Got endpoints: latency-svc-6mdzf [749.637331ms]
  Jun  3 13:12:55.983: INFO: Created: latency-svc-vnvwv
  Jun  3 13:12:56.020: INFO: Got endpoints: latency-svc-dml9n [748.803074ms]
  Jun  3 13:12:56.032: INFO: Created: latency-svc-qbp8p
  Jun  3 13:12:56.072: INFO: Got endpoints: latency-svc-lg87m [749.958311ms]
  Jun  3 13:12:56.086: INFO: Created: latency-svc-dm7rb
  Jun  3 13:12:56.122: INFO: Got endpoints: latency-svc-948qx [749.977112ms]
  Jun  3 13:12:56.136: INFO: Created: latency-svc-2h7wr
  Jun  3 13:12:56.171: INFO: Got endpoints: latency-svc-lv2g2 [748.379861ms]
  Jun  3 13:12:56.184: INFO: Created: latency-svc-cwl9n
  Jun  3 13:12:56.225: INFO: Got endpoints: latency-svc-4bl4p [752.620025ms]
  Jun  3 13:12:56.240: INFO: Created: latency-svc-rm7sz
  Jun  3 13:12:56.272: INFO: Got endpoints: latency-svc-pv4sl [750.438136ms]
  Jun  3 13:12:56.284: INFO: Created: latency-svc-k46cx
  Jun  3 13:12:56.322: INFO: Got endpoints: latency-svc-qwbxf [751.148268ms]
  Jun  3 13:12:56.335: INFO: Created: latency-svc-7fcqd
  Jun  3 13:12:56.371: INFO: Got endpoints: latency-svc-m2lhh [749.168557ms]
  Jun  3 13:12:56.383: INFO: Created: latency-svc-r72x9
  Jun  3 13:12:56.420: INFO: Got endpoints: latency-svc-gt8zv [749.111854ms]
  Jun  3 13:12:56.435: INFO: Created: latency-svc-wcznc
  E0603 13:12:56.444982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:56.470: INFO: Got endpoints: latency-svc-jkcns [747.69175ms]
  Jun  3 13:12:56.483: INFO: Created: latency-svc-4hwjk
  Jun  3 13:12:56.524: INFO: Got endpoints: latency-svc-5thr8 [751.289903ms]
  Jun  3 13:12:56.556: INFO: Created: latency-svc-plprq
  Jun  3 13:12:56.572: INFO: Got endpoints: latency-svc-xkpz5 [751.21278ms]
  Jun  3 13:12:56.585: INFO: Created: latency-svc-45lvv
  Jun  3 13:12:56.621: INFO: Got endpoints: latency-svc-m7d2d [749.892249ms]
  Jun  3 13:12:56.636: INFO: Created: latency-svc-gw6bq
  Jun  3 13:12:56.671: INFO: Got endpoints: latency-svc-f8xx8 [749.27795ms]
  Jun  3 13:12:56.685: INFO: Created: latency-svc-6jqqr
  Jun  3 13:12:56.722: INFO: Got endpoints: latency-svc-vnvwv [751.119587ms]
  Jun  3 13:12:56.734: INFO: Created: latency-svc-xrnvk
  Jun  3 13:12:56.771: INFO: Got endpoints: latency-svc-qbp8p [751.287053ms]
  Jun  3 13:12:56.785: INFO: Created: latency-svc-s6rmd
  Jun  3 13:12:56.822: INFO: Got endpoints: latency-svc-dm7rb [749.90889ms]
  Jun  3 13:12:56.836: INFO: Created: latency-svc-7lmtq
  Jun  3 13:12:56.871: INFO: Got endpoints: latency-svc-2h7wr [749.088043ms]
  Jun  3 13:12:56.884: INFO: Created: latency-svc-mfwdh
  Jun  3 13:12:56.921: INFO: Got endpoints: latency-svc-cwl9n [749.630411ms]
  Jun  3 13:12:56.934: INFO: Created: latency-svc-8d8b9
  Jun  3 13:12:56.972: INFO: Got endpoints: latency-svc-rm7sz [746.73208ms]
  Jun  3 13:12:56.984: INFO: Created: latency-svc-78bj7
  Jun  3 13:12:57.022: INFO: Got endpoints: latency-svc-k46cx [749.749554ms]
  Jun  3 13:12:57.035: INFO: Created: latency-svc-qqt9w
  Jun  3 13:12:57.072: INFO: Got endpoints: latency-svc-7fcqd [750.122296ms]
  Jun  3 13:12:57.087: INFO: Created: latency-svc-zzkvk
  Jun  3 13:12:57.125: INFO: Got endpoints: latency-svc-r72x9 [753.74268ms]
  Jun  3 13:12:57.138: INFO: Created: latency-svc-8g9k4
  Jun  3 13:12:57.174: INFO: Got endpoints: latency-svc-wcznc [753.393769ms]
  Jun  3 13:12:57.190: INFO: Created: latency-svc-g548v
  Jun  3 13:12:57.220: INFO: Got endpoints: latency-svc-4hwjk [749.322001ms]
  Jun  3 13:12:57.238: INFO: Created: latency-svc-c68cj
  Jun  3 13:12:57.274: INFO: Got endpoints: latency-svc-plprq [750.24724ms]
  Jun  3 13:12:57.285: INFO: Created: latency-svc-s4skb
  Jun  3 13:12:57.321: INFO: Got endpoints: latency-svc-45lvv [749.110074ms]
  Jun  3 13:12:57.334: INFO: Created: latency-svc-7hk7f
  Jun  3 13:12:57.371: INFO: Got endpoints: latency-svc-gw6bq [749.29284ms]
  Jun  3 13:12:57.384: INFO: Created: latency-svc-cct9g
  Jun  3 13:12:57.421: INFO: Got endpoints: latency-svc-6jqqr [749.953611ms]
  Jun  3 13:12:57.440: INFO: Created: latency-svc-g7bgm
  E0603 13:12:57.444948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:57.473: INFO: Got endpoints: latency-svc-xrnvk [750.719545ms]
  Jun  3 13:12:57.486: INFO: Created: latency-svc-pk97c
  Jun  3 13:12:57.522: INFO: Got endpoints: latency-svc-s6rmd [750.841409ms]
  Jun  3 13:12:57.534: INFO: Created: latency-svc-f42cv
  Jun  3 13:12:57.572: INFO: Got endpoints: latency-svc-7lmtq [749.898649ms]
  Jun  3 13:12:57.586: INFO: Created: latency-svc-bd95z
  Jun  3 13:12:57.620: INFO: Got endpoints: latency-svc-mfwdh [748.97237ms]
  Jun  3 13:12:57.634: INFO: Created: latency-svc-94thn
  Jun  3 13:12:57.670: INFO: Got endpoints: latency-svc-8d8b9 [748.948289ms]
  Jun  3 13:12:57.684: INFO: Created: latency-svc-9x5tk
  Jun  3 13:12:57.722: INFO: Got endpoints: latency-svc-78bj7 [749.59701ms]
  Jun  3 13:12:57.733: INFO: Created: latency-svc-2t8d5
  Jun  3 13:12:57.772: INFO: Got endpoints: latency-svc-qqt9w [749.349082ms]
  Jun  3 13:12:57.785: INFO: Created: latency-svc-6hgrc
  Jun  3 13:12:57.822: INFO: Got endpoints: latency-svc-zzkvk [749.540737ms]
  Jun  3 13:12:57.836: INFO: Created: latency-svc-tqzfx
  Jun  3 13:12:57.872: INFO: Got endpoints: latency-svc-8g9k4 [746.186952ms]
  Jun  3 13:12:57.884: INFO: Created: latency-svc-cn7rm
  Jun  3 13:12:57.922: INFO: Got endpoints: latency-svc-g548v [747.980759ms]
  Jun  3 13:12:57.937: INFO: Created: latency-svc-bq6g4
  Jun  3 13:12:57.971: INFO: Got endpoints: latency-svc-c68cj [751.090126ms]
  Jun  3 13:12:57.986: INFO: Created: latency-svc-5x8qj
  Jun  3 13:12:58.021: INFO: Got endpoints: latency-svc-s4skb [746.803892ms]
  Jun  3 13:12:58.033: INFO: Created: latency-svc-pplgm
  Jun  3 13:12:58.074: INFO: Got endpoints: latency-svc-7hk7f [752.585924ms]
  Jun  3 13:12:58.088: INFO: Created: latency-svc-bkf4z
  Jun  3 13:12:58.123: INFO: Got endpoints: latency-svc-cct9g [752.449029ms]
  Jun  3 13:12:58.134: INFO: Created: latency-svc-8qcfv
  Jun  3 13:12:58.172: INFO: Got endpoints: latency-svc-g7bgm [750.5549ms]
  Jun  3 13:12:58.185: INFO: Created: latency-svc-xdm5k
  Jun  3 13:12:58.221: INFO: Got endpoints: latency-svc-pk97c [747.68707ms]
  Jun  3 13:12:58.236: INFO: Created: latency-svc-btd7v
  Jun  3 13:12:58.272: INFO: Got endpoints: latency-svc-f42cv [749.378243ms]
  Jun  3 13:12:58.284: INFO: Created: latency-svc-q4xvj
  Jun  3 13:12:58.321: INFO: Got endpoints: latency-svc-bd95z [749.374482ms]
  Jun  3 13:12:58.332: INFO: Created: latency-svc-bxfpg
  Jun  3 13:12:58.372: INFO: Got endpoints: latency-svc-94thn [750.965843ms]
  Jun  3 13:12:58.386: INFO: Created: latency-svc-5gtz9
  Jun  3 13:12:58.420: INFO: Got endpoints: latency-svc-9x5tk [749.527868ms]
  Jun  3 13:12:58.432: INFO: Created: latency-svc-5flqb
  E0603 13:12:58.446020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:58.470: INFO: Got endpoints: latency-svc-2t8d5 [747.485883ms]
  Jun  3 13:12:58.483: INFO: Created: latency-svc-f7wkw
  Jun  3 13:12:58.521: INFO: Got endpoints: latency-svc-6hgrc [748.945329ms]
  Jun  3 13:12:58.534: INFO: Created: latency-svc-5kb7f
  Jun  3 13:12:58.571: INFO: Got endpoints: latency-svc-tqzfx [748.517596ms]
  Jun  3 13:12:58.590: INFO: Created: latency-svc-vc4md
  Jun  3 13:12:58.620: INFO: Got endpoints: latency-svc-cn7rm [747.998379ms]
  Jun  3 13:12:58.632: INFO: Created: latency-svc-phhw7
  Jun  3 13:12:58.671: INFO: Got endpoints: latency-svc-bq6g4 [748.900098ms]
  Jun  3 13:12:58.684: INFO: Created: latency-svc-2btss
  Jun  3 13:12:58.721: INFO: Got endpoints: latency-svc-5x8qj [749.397713ms]
  Jun  3 13:12:58.735: INFO: Created: latency-svc-plmrr
  Jun  3 13:12:58.771: INFO: Got endpoints: latency-svc-pplgm [750.550839ms]
  Jun  3 13:12:58.782: INFO: Created: latency-svc-4qkwk
  Jun  3 13:12:58.824: INFO: Got endpoints: latency-svc-bkf4z [749.322761ms]
  Jun  3 13:12:58.842: INFO: Created: latency-svc-2qqbx
  Jun  3 13:12:58.871: INFO: Got endpoints: latency-svc-8qcfv [747.195634ms]
  Jun  3 13:12:58.884: INFO: Created: latency-svc-bdq4z
  Jun  3 13:12:58.921: INFO: Got endpoints: latency-svc-xdm5k [748.62895ms]
  Jun  3 13:12:58.932: INFO: Created: latency-svc-mvcdn
  Jun  3 13:12:58.971: INFO: Got endpoints: latency-svc-btd7v [749.196397ms]
  Jun  3 13:12:58.982: INFO: Created: latency-svc-p6s5z
  Jun  3 13:12:59.021: INFO: Got endpoints: latency-svc-q4xvj [748.823375ms]
  Jun  3 13:12:59.033: INFO: Created: latency-svc-s294m
  Jun  3 13:12:59.071: INFO: Got endpoints: latency-svc-bxfpg [749.738784ms]
  Jun  3 13:12:59.084: INFO: Created: latency-svc-hqblm
  Jun  3 13:12:59.123: INFO: Got endpoints: latency-svc-5gtz9 [751.410297ms]
  Jun  3 13:12:59.137: INFO: Created: latency-svc-9zxr6
  Jun  3 13:12:59.170: INFO: Got endpoints: latency-svc-5flqb [750.281191ms]
  Jun  3 13:12:59.184: INFO: Created: latency-svc-78m2z
  Jun  3 13:12:59.221: INFO: Got endpoints: latency-svc-f7wkw [751.299203ms]
  Jun  3 13:12:59.236: INFO: Created: latency-svc-htbsr
  Jun  3 13:12:59.291: INFO: Got endpoints: latency-svc-5kb7f [769.68295ms]
  Jun  3 13:12:59.303: INFO: Created: latency-svc-trbrq
  Jun  3 13:12:59.321: INFO: Got endpoints: latency-svc-vc4md [749.946081ms]
  Jun  3 13:12:59.334: INFO: Created: latency-svc-k8vpj
  Jun  3 13:12:59.370: INFO: Got endpoints: latency-svc-phhw7 [750.23853ms]
  Jun  3 13:12:59.383: INFO: Created: latency-svc-rpm8d
  Jun  3 13:12:59.421: INFO: Got endpoints: latency-svc-2btss [749.948581ms]
  Jun  3 13:12:59.434: INFO: Created: latency-svc-l77cq
  E0603 13:12:59.447031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:12:59.472: INFO: Got endpoints: latency-svc-plmrr [749.716224ms]
  Jun  3 13:12:59.485: INFO: Created: latency-svc-4t7xl
  Jun  3 13:12:59.522: INFO: Got endpoints: latency-svc-4qkwk [750.734655ms]
  Jun  3 13:12:59.534: INFO: Created: latency-svc-vl2nc
  Jun  3 13:12:59.572: INFO: Got endpoints: latency-svc-2qqbx [747.490943ms]
  Jun  3 13:12:59.584: INFO: Created: latency-svc-fplkp
  Jun  3 13:12:59.620: INFO: Got endpoints: latency-svc-bdq4z [749.715983ms]
  Jun  3 13:12:59.633: INFO: Created: latency-svc-64jpw
  Jun  3 13:12:59.671: INFO: Got endpoints: latency-svc-mvcdn [750.173508ms]
  Jun  3 13:12:59.683: INFO: Created: latency-svc-cdkjh
  Jun  3 13:12:59.721: INFO: Got endpoints: latency-svc-p6s5z [749.851198ms]
  Jun  3 13:12:59.733: INFO: Created: latency-svc-wrq5g
  Jun  3 13:12:59.771: INFO: Got endpoints: latency-svc-s294m [749.689882ms]
  Jun  3 13:12:59.786: INFO: Created: latency-svc-hl6kf
  Jun  3 13:12:59.828: INFO: Got endpoints: latency-svc-hqblm [756.659052ms]
  Jun  3 13:12:59.845: INFO: Created: latency-svc-k8g6d
  Jun  3 13:12:59.871: INFO: Got endpoints: latency-svc-9zxr6 [747.782923ms]
  Jun  3 13:12:59.886: INFO: Created: latency-svc-9pzr4
  Jun  3 13:12:59.921: INFO: Got endpoints: latency-svc-78m2z [750.984963ms]
  Jun  3 13:12:59.932: INFO: Created: latency-svc-rltq9
  Jun  3 13:12:59.970: INFO: Got endpoints: latency-svc-htbsr [748.866447ms]
  Jun  3 13:12:59.984: INFO: Created: latency-svc-4jqkz
  Jun  3 13:13:00.021: INFO: Got endpoints: latency-svc-trbrq [730.119628ms]
  Jun  3 13:13:00.035: INFO: Created: latency-svc-8h278
  Jun  3 13:13:00.070: INFO: Got endpoints: latency-svc-k8vpj [748.899649ms]
  Jun  3 13:13:00.082: INFO: Created: latency-svc-xh9w6
  Jun  3 13:13:00.123: INFO: Got endpoints: latency-svc-rpm8d [752.460989ms]
  Jun  3 13:13:00.135: INFO: Created: latency-svc-ssr74
  Jun  3 13:13:00.172: INFO: Got endpoints: latency-svc-l77cq [750.266541ms]
  Jun  3 13:13:00.186: INFO: Created: latency-svc-69kgn
  Jun  3 13:13:00.220: INFO: Got endpoints: latency-svc-4t7xl [748.215317ms]
  Jun  3 13:13:00.245: INFO: Created: latency-svc-88xg9
  Jun  3 13:13:00.271: INFO: Got endpoints: latency-svc-vl2nc [749.183037ms]
  Jun  3 13:13:00.285: INFO: Created: latency-svc-lc7m9
  Jun  3 13:13:00.322: INFO: Got endpoints: latency-svc-fplkp [749.696463ms]
  Jun  3 13:13:00.335: INFO: Created: latency-svc-7mq4w
  Jun  3 13:13:00.370: INFO: Got endpoints: latency-svc-64jpw [749.966531ms]
  Jun  3 13:13:00.382: INFO: Created: latency-svc-9zp8g
  Jun  3 13:13:00.422: INFO: Got endpoints: latency-svc-cdkjh [750.88807ms]
  Jun  3 13:13:00.434: INFO: Created: latency-svc-fpfpw
  E0603 13:13:00.447742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:00.473: INFO: Got endpoints: latency-svc-wrq5g [752.295284ms]
  Jun  3 13:13:00.486: INFO: Created: latency-svc-hps8c
  Jun  3 13:13:00.520: INFO: Got endpoints: latency-svc-hl6kf [749.28981ms]
  Jun  3 13:13:00.531: INFO: Created: latency-svc-vxq8q
  Jun  3 13:13:00.573: INFO: Got endpoints: latency-svc-k8g6d [744.724887ms]
  Jun  3 13:13:00.585: INFO: Created: latency-svc-hrmvb
  Jun  3 13:13:00.624: INFO: Got endpoints: latency-svc-9pzr4 [753.048347ms]
  Jun  3 13:13:00.639: INFO: Created: latency-svc-xn58m
  Jun  3 13:13:00.670: INFO: Got endpoints: latency-svc-rltq9 [748.952499ms]
  Jun  3 13:13:00.684: INFO: Created: latency-svc-6q7h9
  Jun  3 13:13:00.721: INFO: Got endpoints: latency-svc-4jqkz [750.183148ms]
  Jun  3 13:13:00.734: INFO: Created: latency-svc-fj8nm
  Jun  3 13:13:00.771: INFO: Got endpoints: latency-svc-8h278 [749.802916ms]
  Jun  3 13:13:00.784: INFO: Created: latency-svc-72rl5
  Jun  3 13:13:00.822: INFO: Got endpoints: latency-svc-xh9w6 [751.036874ms]
  Jun  3 13:13:00.834: INFO: Created: latency-svc-bgbg9
  Jun  3 13:13:00.872: INFO: Got endpoints: latency-svc-ssr74 [748.473074ms]
  Jun  3 13:13:00.884: INFO: Created: latency-svc-wl62f
  Jun  3 13:13:00.922: INFO: Got endpoints: latency-svc-69kgn [749.665122ms]
  Jun  3 13:13:00.935: INFO: Created: latency-svc-22mkp
  Jun  3 13:13:00.970: INFO: Got endpoints: latency-svc-88xg9 [749.371753ms]
  Jun  3 13:13:00.983: INFO: Created: latency-svc-pmj7x
  Jun  3 13:13:01.021: INFO: Got endpoints: latency-svc-lc7m9 [749.340591ms]
  Jun  3 13:13:01.035: INFO: Created: latency-svc-6gtnd
  Jun  3 13:13:01.071: INFO: Got endpoints: latency-svc-7mq4w [749.504827ms]
  Jun  3 13:13:01.090: INFO: Created: latency-svc-xk99t
  Jun  3 13:13:01.123: INFO: Got endpoints: latency-svc-9zp8g [752.249173ms]
  Jun  3 13:13:01.140: INFO: Created: latency-svc-cxxlx
  Jun  3 13:13:01.174: INFO: Got endpoints: latency-svc-fpfpw [751.330774ms]
  Jun  3 13:13:01.190: INFO: Created: latency-svc-gpc9v
  Jun  3 13:13:01.222: INFO: Got endpoints: latency-svc-hps8c [749.500377ms]
  Jun  3 13:13:01.253: INFO: Created: latency-svc-qcpxb
  Jun  3 13:13:01.273: INFO: Got endpoints: latency-svc-vxq8q [752.507561ms]
  Jun  3 13:13:01.286: INFO: Created: latency-svc-nh8hk
  Jun  3 13:13:01.322: INFO: Got endpoints: latency-svc-hrmvb [749.332501ms]
  Jun  3 13:13:01.334: INFO: Created: latency-svc-k5vm2
  Jun  3 13:13:01.372: INFO: Got endpoints: latency-svc-xn58m [746.999458ms]
  Jun  3 13:13:01.387: INFO: Created: latency-svc-t9j5p
  Jun  3 13:13:01.423: INFO: Got endpoints: latency-svc-6q7h9 [751.695465ms]
  Jun  3 13:13:01.439: INFO: Created: latency-svc-klsct
  E0603 13:13:01.448636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:01.471: INFO: Got endpoints: latency-svc-fj8nm [749.816336ms]
  Jun  3 13:13:01.485: INFO: Created: latency-svc-lrx6s
  Jun  3 13:13:01.521: INFO: Got endpoints: latency-svc-72rl5 [749.808846ms]
  Jun  3 13:13:01.535: INFO: Created: latency-svc-j2sxk
  Jun  3 13:13:01.574: INFO: Got endpoints: latency-svc-bgbg9 [751.83436ms]
  Jun  3 13:13:01.586: INFO: Created: latency-svc-rj2kp
  Jun  3 13:13:01.623: INFO: Got endpoints: latency-svc-wl62f [750.702015ms]
  Jun  3 13:13:01.635: INFO: Created: latency-svc-6f4l5
  Jun  3 13:13:01.672: INFO: Got endpoints: latency-svc-22mkp [750.110055ms]
  Jun  3 13:13:01.685: INFO: Created: latency-svc-nzjmg
  Jun  3 13:13:01.721: INFO: Got endpoints: latency-svc-pmj7x [750.522679ms]
  Jun  3 13:13:01.734: INFO: Created: latency-svc-g7hnh
  Jun  3 13:13:01.771: INFO: Got endpoints: latency-svc-6gtnd [749.813776ms]
  Jun  3 13:13:01.785: INFO: Created: latency-svc-zd4rx
  Jun  3 13:13:01.827: INFO: Got endpoints: latency-svc-xk99t [755.0059ms]
  Jun  3 13:13:01.841: INFO: Created: latency-svc-5zv2h
  Jun  3 13:13:01.872: INFO: Got endpoints: latency-svc-cxxlx [749.435044ms]
  Jun  3 13:13:01.883: INFO: Created: latency-svc-nblz6
  Jun  3 13:13:01.921: INFO: Got endpoints: latency-svc-gpc9v [747.69974ms]
  Jun  3 13:13:01.934: INFO: Created: latency-svc-45bjx
  Jun  3 13:13:01.973: INFO: Got endpoints: latency-svc-qcpxb [750.155207ms]
  Jun  3 13:13:01.988: INFO: Created: latency-svc-cdhxp
  Jun  3 13:13:02.022: INFO: Got endpoints: latency-svc-nh8hk [748.602928ms]
  Jun  3 13:13:02.034: INFO: Created: latency-svc-s5vh8
  Jun  3 13:13:02.071: INFO: Got endpoints: latency-svc-k5vm2 [748.767244ms]
  Jun  3 13:13:02.122: INFO: Got endpoints: latency-svc-t9j5p [750.080475ms]
  Jun  3 13:13:02.170: INFO: Got endpoints: latency-svc-klsct [747.605927ms]
  Jun  3 13:13:02.224: INFO: Got endpoints: latency-svc-lrx6s [752.639345ms]
  Jun  3 13:13:02.274: INFO: Got endpoints: latency-svc-j2sxk [752.631864ms]
  Jun  3 13:13:02.321: INFO: Got endpoints: latency-svc-rj2kp [746.490822ms]
  Jun  3 13:13:02.370: INFO: Got endpoints: latency-svc-6f4l5 [747.392931ms]
  Jun  3 13:13:02.424: INFO: Got endpoints: latency-svc-nzjmg [751.369355ms]
  E0603 13:13:02.449548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:02.470: INFO: Got endpoints: latency-svc-g7hnh [749.194097ms]
  Jun  3 13:13:02.522: INFO: Got endpoints: latency-svc-zd4rx [750.919591ms]
  Jun  3 13:13:02.570: INFO: Got endpoints: latency-svc-5zv2h [743.600031ms]
  Jun  3 13:13:02.621: INFO: Got endpoints: latency-svc-nblz6 [748.526917ms]
  Jun  3 13:13:02.670: INFO: Got endpoints: latency-svc-45bjx [748.863767ms]
  Jun  3 13:13:02.721: INFO: Got endpoints: latency-svc-cdhxp [747.818274ms]
  Jun  3 13:13:02.770: INFO: Got endpoints: latency-svc-s5vh8 [748.398342ms]
  Jun  3 13:13:02.770: INFO: Latencies: [27.816123ms 35.845525ms 51.231828ms 54.336576ms 62.661857ms 69.981627ms 77.111901ms 84.504592ms 96.630674ms 105.983487ms 113.71919ms 115.278439ms 120.029358ms 123.28268ms 124.004782ms 124.261641ms 124.389454ms 124.893401ms 125.568721ms 125.675395ms 125.948113ms 125.950443ms 125.986275ms 126.76618ms 127.217703ms 127.253794ms 127.300285ms 130.166656ms 132.687055ms 134.501912ms 134.856623ms 135.471183ms 136.795004ms 138.852858ms 144.085623ms 146.939802ms 192.278276ms 222.471053ms 272.150333ms 311.56927ms 354.663293ms 402.37666ms 438.848565ms 481.079751ms 524.07926ms 565.996966ms 599.803238ms 641.505297ms 679.345125ms 722.577931ms 730.119628ms 743.600031ms 744.724887ms 746.186952ms 746.490822ms 746.73208ms 746.803892ms 746.999458ms 747.195634ms 747.392931ms 747.485883ms 747.490943ms 747.605927ms 747.68707ms 747.69175ms 747.69974ms 747.782923ms 747.818274ms 747.980759ms 747.998379ms 748.215317ms 748.379861ms 748.398342ms 748.473074ms 748.517596ms 748.526917ms 748.602928ms 748.62895ms 748.767244ms 748.803074ms 748.810035ms 748.823375ms 748.863767ms 748.866447ms 748.899649ms 748.900098ms 748.945329ms 748.948289ms 748.952499ms 748.97237ms 748.987631ms 749.088043ms 749.110074ms 749.111854ms 749.168557ms 749.172266ms 749.183037ms 749.194097ms 749.196397ms 749.223008ms 749.27795ms 749.28981ms 749.29284ms 749.322001ms 749.322761ms 749.332501ms 749.340591ms 749.349082ms 749.371753ms 749.374482ms 749.378243ms 749.397713ms 749.425364ms 749.435044ms 749.500377ms 749.504827ms 749.527868ms 749.540737ms 749.579159ms 749.584149ms 749.59701ms 749.630411ms 749.637331ms 749.665122ms 749.679282ms 749.689882ms 749.696463ms 749.715983ms 749.716224ms 749.738784ms 749.749554ms 749.802916ms 749.808846ms 749.813776ms 749.816336ms 749.851198ms 749.866779ms 749.892249ms 749.898649ms 749.90889ms 749.946081ms 749.948581ms 749.950211ms 749.953611ms 749.958311ms 749.966531ms 749.977112ms 750.080475ms 750.110055ms 750.122296ms 750.155207ms 750.173508ms 750.183148ms 750.23853ms 750.24724ms 750.266541ms 750.281191ms 750.418106ms 750.438136ms 750.522679ms 750.550839ms 750.5549ms 750.702015ms 750.719545ms 750.734655ms 750.818137ms 750.841409ms 750.88807ms 750.919591ms 750.965843ms 750.984963ms 751.036874ms 751.090126ms 751.119587ms 751.148268ms 751.21278ms 751.287053ms 751.289903ms 751.299203ms 751.324984ms 751.330774ms 751.369355ms 751.410297ms 751.695465ms 751.83436ms 752.249173ms 752.295284ms 752.449029ms 752.460989ms 752.507561ms 752.585924ms 752.620025ms 752.631864ms 752.639345ms 753.048347ms 753.393769ms 753.74268ms 755.0059ms 756.659052ms 769.68295ms]
  Jun  3 13:13:02.771: INFO: 50 %ile: 749.27795ms
  Jun  3 13:13:02.771: INFO: 90 %ile: 751.330774ms
  Jun  3 13:13:02.771: INFO: 99 %ile: 756.659052ms
  Jun  3 13:13:02.771: INFO: Total sample count: 200
  Jun  3 13:13:02.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-2804" for this suite. @ 06/03/23 13:13:02.778
• [10.780 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 06/03/23 13:13:02.789
  Jun  3 13:13:02.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 13:13:02.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:13:02.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:13:02.82
  STEP: Creating a pod to test substitution in container's command @ 06/03/23 13:13:02.826
  E0603 13:13:03.449665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:04.449791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:05.450050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:06.450242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:13:06.855
  Jun  3 13:13:06.860: INFO: Trying to get logs from node ip-172-31-27-193 pod var-expansion-4e0ea214-e0be-426b-acc8-a4be64d454f8 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 13:13:06.878
  Jun  3 13:13:06.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5919" for this suite. @ 06/03/23 13:13:06.908
• [4.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 06/03/23 13:13:06.918
  Jun  3 13:13:06.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 13:13:06.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:13:06.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:13:06.946
  Jun  3 13:13:06.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:13:07.451089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:08.451346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 06/03/23 13:13:08.772
  Jun  3 13:13:08.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-2837 --namespace=crd-publish-openapi-2837 create -f -'
  E0603 13:13:09.452204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:10.199: INFO: stderr: ""
  Jun  3 13:13:10.199: INFO: stdout: "e2e-test-crd-publish-openapi-8599-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun  3 13:13:10.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-2837 --namespace=crd-publish-openapi-2837 delete e2e-test-crd-publish-openapi-8599-crds test-cr'
  Jun  3 13:13:10.304: INFO: stderr: ""
  Jun  3 13:13:10.304: INFO: stdout: "e2e-test-crd-publish-openapi-8599-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jun  3 13:13:10.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-2837 --namespace=crd-publish-openapi-2837 apply -f -'
  E0603 13:13:10.452536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:11.453188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:11.872: INFO: stderr: ""
  Jun  3 13:13:11.872: INFO: stdout: "e2e-test-crd-publish-openapi-8599-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jun  3 13:13:11.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-2837 --namespace=crd-publish-openapi-2837 delete e2e-test-crd-publish-openapi-8599-crds test-cr'
  Jun  3 13:13:11.989: INFO: stderr: ""
  Jun  3 13:13:11.989: INFO: stdout: "e2e-test-crd-publish-openapi-8599-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 06/03/23 13:13:11.989
  Jun  3 13:13:11.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=crd-publish-openapi-2837 explain e2e-test-crd-publish-openapi-8599-crds'
  Jun  3 13:13:12.272: INFO: stderr: ""
  Jun  3 13:13:12.272: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8599-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0603 13:13:12.453242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:13.454027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:13:13.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2837" for this suite. @ 06/03/23 13:13:13.877
• [6.970 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 06/03/23 13:13:13.889
  Jun  3 13:13:13.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:13:13.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:13:13.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:13:13.934
  STEP: Creating configMap with name projected-configmap-test-volume-map-ff3a799c-edab-4e63-942d-b39aedd0868e @ 06/03/23 13:13:13.94
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:13:13.948
  E0603 13:13:14.454769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:15.454885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:16.455598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:17.455737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:13:17.981
  Jun  3 13:13:17.986: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-5ba1b551-05d6-43cc-bd71-13fd59705fde container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:13:17.995
  Jun  3 13:13:18.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3376" for this suite. @ 06/03/23 13:13:18.019
• [4.138 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 06/03/23 13:13:18.028
  Jun  3 13:13:18.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-watch @ 06/03/23 13:13:18.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:13:18.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:13:18.052
  Jun  3 13:13:18.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:13:18.456748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:19.458058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:20.458192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 06/03/23 13:13:20.618
  Jun  3 13:13:20.624: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:20Z]] name:name1 resourceVersion:35120 uid:b1865f22-540e-42e2-92ef-8896fc20f07c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:13:21.458219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:22.458550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:23.459628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:24.459822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:25.459961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:26.460073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:27.460445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:28.460556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:29.460845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:30.460973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 06/03/23 13:13:30.625
  Jun  3 13:13:30.638: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:30Z]] name:name2 resourceVersion:35151 uid:35bec7ad-0dc6-421a-ba52-cd5aa670a2bb] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:13:31.461212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:32.461367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:33.461878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:34.462461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:35.462645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:36.462667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:37.463275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:38.463394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:39.463513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:40.463665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 06/03/23 13:13:40.64
  Jun  3 13:13:40.651: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:40Z]] name:name1 resourceVersion:35170 uid:b1865f22-540e-42e2-92ef-8896fc20f07c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:13:41.463832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:42.463946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:43.463975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:44.464298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:45.465018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:46.465235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:47.465920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:48.465991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:49.466119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:50.466504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 06/03/23 13:13:50.652
  Jun  3 13:13:50.660: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:50Z]] name:name2 resourceVersion:35190 uid:35bec7ad-0dc6-421a-ba52-cd5aa670a2bb] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:13:51.467595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:52.467981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:53.468307      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:54.468628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:55.468708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:56.468802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:57.468902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:58.469062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:13:59.469173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:00.469844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 06/03/23 13:14:00.661
  Jun  3 13:14:00.670: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:40Z]] name:name1 resourceVersion:35210 uid:b1865f22-540e-42e2-92ef-8896fc20f07c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:14:01.470228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:02.470540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:03.470677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:04.470789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:05.470989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:06.471111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:07.471587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:08.471827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:09.471986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:10.472845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 06/03/23 13:14:10.671
  Jun  3 13:14:10.680: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-03T13:13:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-03T13:13:50Z]] name:name2 resourceVersion:35230 uid:35bec7ad-0dc6-421a-ba52-cd5aa670a2bb] num:map[num1:9223372036854775807 num2:1000000]]}
  E0603 13:14:11.473356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:12.473744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:13.473916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:14.474017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:15.474139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:16.474692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:17.474792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:18.474901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:19.475002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:20.475114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:14:21.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-666" for this suite. @ 06/03/23 13:14:21.217
• [63.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 06/03/23 13:14:21.242
  Jun  3 13:14:21.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:14:21.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:21.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:21.291
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/03/23 13:14:21.297
  E0603 13:14:21.476271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:22.476864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:23.477332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:24.477497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:14:25.334
  Jun  3 13:14:25.339: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-3a9ad7c3-d30c-4586-8793-f92f4318c8e6 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:14:25.347
  Jun  3 13:14:25.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8646" for this suite. @ 06/03/23 13:14:25.373
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 06/03/23 13:14:25.386
  Jun  3 13:14:25.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir-wrapper @ 06/03/23 13:14:25.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:25.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:25.417
  E0603 13:14:25.477847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:26.478090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:14:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 06/03/23 13:14:27.466
  STEP: Cleaning up the configmap @ 06/03/23 13:14:27.473
  E0603 13:14:27.478547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the pod @ 06/03/23 13:14:27.481
  STEP: Destroying namespace "emptydir-wrapper-5672" for this suite. @ 06/03/23 13:14:27.495
• [2.118 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 06/03/23 13:14:27.504
  Jun  3 13:14:27.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename conformance-tests @ 06/03/23 13:14:27.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:27.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:27.528
  STEP: Getting node addresses @ 06/03/23 13:14:27.535
  Jun  3 13:14:27.535: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jun  3 13:14:27.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3749" for this suite. @ 06/03/23 13:14:27.549
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 06/03/23 13:14:27.56
  Jun  3 13:14:27.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 13:14:27.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:27.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:27.587
  STEP: Creating secret with name secret-test-map-2ba6d305-da4e-4f5c-8d33-c6a56116e05f @ 06/03/23 13:14:27.593
  STEP: Creating a pod to test consume secrets @ 06/03/23 13:14:27.599
  E0603 13:14:28.479277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:29.479570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:30.479668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:31.480702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:14:31.625
  Jun  3 13:14:31.630: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-c6de3c4b-5b2a-4cd4-9c41-87265f1de748 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 13:14:31.638
  Jun  3 13:14:31.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4597" for this suite. @ 06/03/23 13:14:31.659
• [4.106 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 06/03/23 13:14:31.667
  Jun  3 13:14:31.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-runtime @ 06/03/23 13:14:31.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:31.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:31.692
  STEP: create the container @ 06/03/23 13:14:31.696
  W0603 13:14:31.705652      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/03/23 13:14:31.705
  E0603 13:14:32.481822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:33.481931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:34.482847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/03/23 13:14:34.726
  STEP: the container should be terminated @ 06/03/23 13:14:34.73
  STEP: the termination message should be set @ 06/03/23 13:14:34.73
  Jun  3 13:14:34.730: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 06/03/23 13:14:34.731
  Jun  3 13:14:34.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3478" for this suite. @ 06/03/23 13:14:34.761
• [3.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 06/03/23 13:14:34.772
  Jun  3 13:14:34.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 13:14:34.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:14:34.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:14:34.796
  Jun  3 13:14:34.817: INFO: created pod
  E0603 13:14:35.483583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:36.483692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:37.484742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:38.484857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:14:38.835
  E0603 13:14:39.485290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:40.485413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:41.485516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:42.485874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:43.486000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:44.486066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:45.486293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:46.486575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:47.486683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:48.487364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:49.487443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:50.487705      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:51.487944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:52.488501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:53.488904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:54.489228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:55.489753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:56.490207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:57.490530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:58.490828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:14:59.491587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:00.491673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:01.491816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:02.492223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:03.492552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:04.493504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:05.493790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:06.493917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:07.494508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:08.494802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:08.836: INFO: polling logs
  Jun  3 13:15:08.847: INFO: Pod logs: 
  I0603 13:14:35.609268       1 log.go:198] OK: Got token
  I0603 13:14:35.609322       1 log.go:198] validating with in-cluster discovery
  I0603 13:14:35.609685       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0603 13:14:35.609732       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-860:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1685798675, NotBefore:1685798075, IssuedAt:1685798075, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-860", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"517d2106-2014-45f8-ba86-b5c4ed744eb4"}}}
  I0603 13:14:35.622020       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0603 13:14:35.629697       1 log.go:198] OK: Validated signature on JWT
  I0603 13:14:35.629964       1 log.go:198] OK: Got valid claims from token!
  I0603 13:14:35.630103       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-860:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1685798675, NotBefore:1685798075, IssuedAt:1685798075, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-860", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"517d2106-2014-45f8-ba86-b5c4ed744eb4"}}}

  Jun  3 13:15:08.853: INFO: completed pod
  Jun  3 13:15:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-860" for this suite. @ 06/03/23 13:15:08.873
• [34.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 06/03/23 13:15:08.883
  Jun  3 13:15:08.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 13:15:08.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:15:08.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:15:08.912
  Jun  3 13:15:08.927: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0603 13:15:09.494934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:10.495069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:11.495195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:12.495301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:13.495457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:13.933: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 13:15:13.933
  Jun  3 13:15:13.934: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0603 13:15:14.495565      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:15.495689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:15.938: INFO: Creating deployment "test-rollover-deployment"
  Jun  3 13:15:15.950: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0603 13:15:16.496553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:17.496672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:17.959: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jun  3 13:15:17.968: INFO: Ensure that both replica sets have 1 created replica
  Jun  3 13:15:17.977: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jun  3 13:15:17.988: INFO: Updating deployment test-rollover-deployment
  Jun  3 13:15:17.988: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0603 13:15:18.496759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:19.496938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:20.000: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jun  3 13:15:20.010: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jun  3 13:15:20.023: INFO: all replica sets need to contain the pod-template-hash label
  Jun  3 13:15:20.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:15:20.496991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:21.497092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:22.033: INFO: all replica sets need to contain the pod-template-hash label
  Jun  3 13:15:22.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:15:22.497833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:23.498088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:24.033: INFO: all replica sets need to contain the pod-template-hash label
  Jun  3 13:15:24.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:15:24.498164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:25.498297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:26.032: INFO: all replica sets need to contain the pod-template-hash label
  Jun  3 13:15:26.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:15:26.498545      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:27.499009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:28.032: INFO: all replica sets need to contain the pod-template-hash label
  Jun  3 13:15:28.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 15, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 15, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:15:28.500027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:29.500112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:15:30.034: INFO: 
  Jun  3 13:15:30.035: INFO: Ensure that both old replica sets have no replicas
  Jun  3 13:15:30.046: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2926  c8705512-5c06-4263-9e71-6cbe689fd24f 35667 2 2023-06-03 13:15:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-03 13:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055363d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-03 13:15:15 +0000 UTC,LastTransitionTime:2023-06-03 13:15:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-06-03 13:15:29 +0000 UTC,LastTransitionTime:2023-06-03 13:15:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun  3 13:15:30.051: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2926  2cc43699-0407-40b7-a5e5-cd6a230ddebd 35657 2 2023-06-03 13:15:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c8705512-5c06-4263-9e71-6cbe689fd24f 0xc004914307 0xc004914308}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8705512-5c06-4263-9e71-6cbe689fd24f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:15:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049143b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 13:15:30.051: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jun  3 13:15:30.052: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2926  b211ed55-3807-4d9a-9019-73b74cc0a421 35666 2 2023-06-03 13:15:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c8705512-5c06-4263-9e71-6cbe689fd24f 0xc0049141d7 0xc0049141d8}] [] [{e2e.test Update apps/v1 2023-06-03 13:15:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8705512-5c06-4263-9e71-6cbe689fd24f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:15:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004914298 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 13:15:30.052: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2926  8e8d9469-5802-447d-93a8-07022bcab081 35617 2 2023-06-03 13:15:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c8705512-5c06-4263-9e71-6cbe689fd24f 0xc004914427 0xc004914428}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8705512-5c06-4263-9e71-6cbe689fd24f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:15:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049144d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 13:15:30.056: INFO: Pod "test-rollover-deployment-57777854c9-vv7qk" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-vv7qk test-rollover-deployment-57777854c9- deployment-2926  e3a64bd4-8938-4d0f-921a-caf65e8dc949 35635 0 2023-06-03 13:15:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 2cc43699-0407-40b7-a5e5-cd6a230ddebd 0xc005536787 0xc005536788}] [] [{kube-controller-manager Update v1 2023-06-03 13:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2cc43699-0407-40b7-a5e5-cd6a230ddebd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqd5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqd5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.240,StartTime:2023-06-03 13:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:15:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://8eafc92cd7f42dcffb619820b4d16ec0d06b254d266785a09272f0d970ec1bba,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.240,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 13:15:30.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2926" for this suite. @ 06/03/23 13:15:30.062
• [21.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 06/03/23 13:15:30.071
  Jun  3 13:15:30.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 13:15:30.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:15:30.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:15:30.097
  STEP: Creating a job @ 06/03/23 13:15:30.102
  STEP: Ensuring active pods == parallelism @ 06/03/23 13:15:30.108
  E0603 13:15:30.500865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:31.500827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:32.501388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:33.501838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 06/03/23 13:15:34.113
  STEP: deleting Job.batch foo in namespace job-4754, will wait for the garbage collector to delete the pods @ 06/03/23 13:15:34.113
  Jun  3 13:15:34.174: INFO: Deleting Job.batch foo took: 7.040655ms
  Jun  3 13:15:34.275: INFO: Terminating Job.batch foo pods took: 100.46532ms
  E0603 13:15:34.502101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:35.502905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:36.503493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:37.504289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:38.504454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:39.505136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:40.505920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:41.506795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:42.507737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:43.508218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:44.508898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:45.509661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:46.510440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:47.511402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:48.512159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:49.512788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:50.513604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:51.514332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:52.515026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:53.515937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:54.516661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:55.517603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:56.518437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:57.519314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:58.519927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:15:59.520723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:00.521686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:01.522458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:02.523402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:03.524142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:04.525759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 06/03/23 13:16:05.176
  Jun  3 13:16:05.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4754" for this suite. @ 06/03/23 13:16:05.186
• [35.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 06/03/23 13:16:05.196
  Jun  3 13:16:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 13:16:05.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:16:05.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:16:05.222
  STEP: Creating configMap with name configmap-test-volume-e394debb-4d24-4438-b9f9-61f0eab3d77f @ 06/03/23 13:16:05.226
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:16:05.232
  E0603 13:16:05.525895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:06.526099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:07.526166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:08.526355      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:16:09.26
  Jun  3 13:16:09.265: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-76e8fcdb-b973-4dda-8863-3dae0def6f77 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:16:09.273
  Jun  3 13:16:09.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5992" for this suite. @ 06/03/23 13:16:09.3
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 06/03/23 13:16:09.309
  Jun  3 13:16:09.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:16:09.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:16:09.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:16:09.335
  STEP: Creating a test headless service @ 06/03/23 13:16:09.34
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.173_tcp@PTR;sleep 1; done
   @ 06/03/23 13:16:09.359
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8795.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8795.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8795.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8795.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 173.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.173_tcp@PTR;sleep 1; done
   @ 06/03/23 13:16:09.359
  STEP: creating a pod to probe DNS @ 06/03/23 13:16:09.36
  STEP: submitting the pod to kubernetes @ 06/03/23 13:16:09.36
  E0603 13:16:09.526503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:10.526532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:16:11.386
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:16:11.391
  Jun  3 13:16:11.397: INFO: Unable to read wheezy_udp@dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.402: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.414: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.440: INFO: Unable to read jessie_udp@dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.444: INFO: Unable to read jessie_tcp@dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.450: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.457: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local from pod dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316: the server could not find the requested resource (get pods dns-test-0e97d0fa-9391-40d6-be24-f76c37326316)
  Jun  3 13:16:11.478: INFO: Lookups using dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316 failed for: [wheezy_udp@dns-test-service.dns-8795.svc.cluster.local wheezy_tcp@dns-test-service.dns-8795.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local jessie_udp@dns-test-service.dns-8795.svc.cluster.local jessie_tcp@dns-test-service.dns-8795.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8795.svc.cluster.local]

  E0603 13:16:11.527007      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:12.527468      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:13.527591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:14.527695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:15.527805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:16.528743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:16:16.570: INFO: DNS probes using dns-8795/dns-test-0e97d0fa-9391-40d6-be24-f76c37326316 succeeded

  Jun  3 13:16:16.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:16:16.58
  STEP: deleting the test service @ 06/03/23 13:16:16.619
  STEP: deleting the test headless service @ 06/03/23 13:16:16.65
  STEP: Destroying namespace "dns-8795" for this suite. @ 06/03/23 13:16:16.67
• [7.378 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 06/03/23 13:16:16.688
  Jun  3 13:16:16.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename svcaccounts @ 06/03/23 13:16:16.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:16:16.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:16:16.721
  STEP: Creating ServiceAccount "e2e-sa-5wnvc"  @ 06/03/23 13:16:16.727
  Jun  3 13:16:16.733: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-5wnvc"  @ 06/03/23 13:16:16.733
  Jun  3 13:16:16.744: INFO: AutomountServiceAccountToken: true
  Jun  3 13:16:16.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7837" for this suite. @ 06/03/23 13:16:16.749
• [0.069 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 06/03/23 13:16:16.758
  Jun  3 13:16:16.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename crd-publish-openapi @ 06/03/23 13:16:16.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:16:16.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:16:16.79
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 06/03/23 13:16:16.795
  Jun  3 13:16:16.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:16:17.529310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:16:18.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:16:18.530229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:19.530551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:20.530770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:21.531217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:22.541990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:23.542094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:24.543128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:16:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2342" for this suite. @ 06/03/23 13:16:24.826
• [8.075 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 06/03/23 13:16:24.835
  Jun  3 13:16:24.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename cronjob @ 06/03/23 13:16:24.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:16:24.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:16:24.858
  STEP: Creating a suspended cronjob @ 06/03/23 13:16:24.862
  STEP: Ensuring no jobs are scheduled @ 06/03/23 13:16:24.871
  E0603 13:16:25.543668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:26.543791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:27.544650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:28.544912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:29.545020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:30.545689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:31.545859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:32.545957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:33.546902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:34.547073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:35.547171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:36.547610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:37.548593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:38.548875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:39.548969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:40.549084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:41.549215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:42.549298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:43.549387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:44.549642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:45.549688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:46.549802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:47.549904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:48.550023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:49.550129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:50.550234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:51.550397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:52.550562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:53.550697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:54.550788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:55.551655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:56.552183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:57.552278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:58.552397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:16:59.552472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:00.552604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:01.552744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:02.552988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:03.553619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:04.553713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:05.553863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:06.553931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:07.554020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:08.554130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:09.554519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:10.554725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:11.554841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:12.554971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:13.555598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:14.555699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:15.556042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:16.556141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:17.556376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:18.556456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:19.557632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:20.557920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:21.557975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:22.558085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:23.558925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:24.559038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:25.559957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:26.560333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:27.560494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:28.560764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:29.561121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:30.561399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:31.561584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:32.562170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:33.563240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:34.563306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:35.563436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:36.563505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:37.564483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:38.564596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:39.564727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:40.564821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:41.565166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:42.565792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:43.566567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:44.566680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:45.567619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:46.567681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:47.567788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:48.567964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:49.568138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:50.568809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:51.568895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:52.569511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:53.570168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:54.570539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:55.571581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:56.571676      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:57.572606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:58.572639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:17:59.572718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:00.572830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:01.572927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:02.573176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:03.573353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:04.573581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:05.574632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:06.575612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:07.575710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:08.575829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:09.576452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:10.576829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:11.576954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:12.577490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:13.578180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:14.578348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:15.578532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:16.578619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:17.578769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:18.578856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:19.578999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:20.579992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:21.580336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:22.580910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:23.581583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:24.581639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:25.581716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:26.581839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:27.582529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:28.582880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:29.583004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:30.583112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:31.583586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:32.583699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:33.584695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:34.584807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:35.584897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:36.585012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:37.585133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:38.585243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:39.585354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:40.585497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:41.586510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:42.586627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:43.586737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:44.586848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:45.586970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:46.587084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:47.587580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:48.587686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:49.587995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:50.588995      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:51.589133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:52.589477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:53.589598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:54.589939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:55.590312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:56.590560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:57.590855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:58.591030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:18:59.591253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:00.591360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:01.591564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:02.592155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:03.592398      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:04.592513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:05.593457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:06.593568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:07.594596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:08.594692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:09.595581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:10.595700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:11.596681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:12.596807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:13.596926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:14.597043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:15.597657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:16.597779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:17.597865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:18.598194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:19.598536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:20.598626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:21.598765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:22.598867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:23.598983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:24.599107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:25.600039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:26.600292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:27.600340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:28.600469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:29.601495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:30.601657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:31.602146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:32.602537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:33.602929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:34.603043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:35.603162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:36.603274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:37.603819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:38.604083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:39.604228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:40.604543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:41.605040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:42.605583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:43.605688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:44.605795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:45.606505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:46.606621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:47.607584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:48.607942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:49.608053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:50.608194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:51.608302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:52.608399      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:53.608870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:54.608975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:55.609243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:56.609388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:57.610492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:58.610747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:19:59.610994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:00.611100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:01.611320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:02.611647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:03.611724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:04.611835      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:05.612794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:06.612879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:07.613053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:08.613499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:09.614571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:10.615628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:11.616574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:12.617483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:13.617776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:14.617887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:15.618805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:16.619575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:17.619675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:18.619783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:19.619900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:20.620010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:21.620119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:22.620235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:23.621387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:24.622162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:25.622493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:26.622602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:27.622706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:28.622814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:29.623619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:30.623943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:31.624908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:32.625015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:33.625610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:34.625938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:35.626258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:36.626476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:37.627520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:38.627859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:39.628371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:40.628733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:41.628850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:42.628945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:43.629423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:44.630509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:45.631568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:46.632026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:47.632432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:48.632808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:49.633047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:50.633523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:51.634015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:52.634322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:53.634476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:54.634526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:55.635602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:56.635964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:57.636091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:58.636216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:20:59.636608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:00.637073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:01.637074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:02.637222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:03.637444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:04.637723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:05.638018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:06.638127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:07.638491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:08.638525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:09.639451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:10.639625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:11.640338      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:12.640710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:13.640793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:14.640910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:15.641861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:16.642179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:17.642406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:18.642494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:19.643575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:20.643900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:21.643974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:22.644056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:23.645302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:24.645405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 06/03/23 13:21:24.879
  STEP: Removing cronjob @ 06/03/23 13:21:24.883
  Jun  3 13:21:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1602" for this suite. @ 06/03/23 13:21:24.894
• [300.066 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 06/03/23 13:21:24.904
  Jun  3 13:21:24.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename proxy @ 06/03/23 13:21:24.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:24.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:24.931
  Jun  3 13:21:24.934: INFO: Creating pod...
  E0603 13:21:25.646408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:26.646590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:21:26.956: INFO: Creating service...
  Jun  3 13:21:26.969: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/DELETE
  Jun  3 13:21:26.986: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun  3 13:21:26.986: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/GET
  Jun  3 13:21:26.990: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun  3 13:21:26.991: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/HEAD
  Jun  3 13:21:26.995: INFO: http.Client request:HEAD | StatusCode:200
  Jun  3 13:21:26.995: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/OPTIONS
  Jun  3 13:21:27.001: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun  3 13:21:27.001: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/PATCH
  Jun  3 13:21:27.007: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun  3 13:21:27.007: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/POST
  Jun  3 13:21:27.013: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun  3 13:21:27.013: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/pods/agnhost/proxy/some/path/with/PUT
  Jun  3 13:21:27.018: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun  3 13:21:27.018: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/DELETE
  Jun  3 13:21:27.024: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jun  3 13:21:27.024: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/GET
  Jun  3 13:21:27.029: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jun  3 13:21:27.029: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/HEAD
  Jun  3 13:21:27.036: INFO: http.Client request:HEAD | StatusCode:200
  Jun  3 13:21:27.036: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/OPTIONS
  Jun  3 13:21:27.041: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jun  3 13:21:27.041: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/PATCH
  Jun  3 13:21:27.047: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jun  3 13:21:27.047: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/POST
  Jun  3 13:21:27.053: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jun  3 13:21:27.053: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-6473/services/test-service/proxy/some/path/with/PUT
  Jun  3 13:21:27.059: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jun  3 13:21:27.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6473" for this suite. @ 06/03/23 13:21:27.064
• [2.167 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 06/03/23 13:21:27.072
  Jun  3 13:21:27.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 13:21:27.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:27.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:27.096
  STEP: Read namespace status @ 06/03/23 13:21:27.101
  Jun  3 13:21:27.105: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 06/03/23 13:21:27.105
  Jun  3 13:21:27.112: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 06/03/23 13:21:27.113
  Jun  3 13:21:27.122: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jun  3 13:21:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6328" for this suite. @ 06/03/23 13:21:27.127
• [0.063 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 06/03/23 13:21:27.138
  Jun  3 13:21:27.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:21:27.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:27.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:27.16
  STEP: Setting up server cert @ 06/03/23 13:21:27.191
  E0603 13:21:27.647586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:21:27.726
  STEP: Deploying the webhook pod @ 06/03/23 13:21:27.735
  STEP: Wait for the deployment to be ready @ 06/03/23 13:21:27.749
  Jun  3 13:21:27.758: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:21:28.647702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:29.647812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:21:29.771
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:21:29.788
  E0603 13:21:30.647940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:21:30.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 06/03/23 13:21:30.793
  STEP: create a pod @ 06/03/23 13:21:30.819
  E0603 13:21:31.648212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:32.648356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 06/03/23 13:21:32.837
  Jun  3 13:21:32.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=webhook-3943 attach --namespace=webhook-3943 to-be-attached-pod -i -c=container1'
  Jun  3 13:21:32.940: INFO: rc: 1
  Jun  3 13:21:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3943" for this suite. @ 06/03/23 13:21:32.999
  STEP: Destroying namespace "webhook-markers-4451" for this suite. @ 06/03/23 13:21:33.008
• [5.878 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 06/03/23 13:21:33.016
  Jun  3 13:21:33.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 13:21:33.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:33.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:33.037
  Jun  3 13:21:33.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2995" for this suite. @ 06/03/23 13:21:33.09
• [0.082 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 06/03/23 13:21:33.099
  Jun  3 13:21:33.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename cronjob @ 06/03/23 13:21:33.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:33.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:33.121
  STEP: Creating a cronjob @ 06/03/23 13:21:33.126
  STEP: creating @ 06/03/23 13:21:33.126
  STEP: getting @ 06/03/23 13:21:33.131
  STEP: listing @ 06/03/23 13:21:33.134
  STEP: watching @ 06/03/23 13:21:33.138
  Jun  3 13:21:33.138: INFO: starting watch
  STEP: cluster-wide listing @ 06/03/23 13:21:33.139
  STEP: cluster-wide watching @ 06/03/23 13:21:33.143
  Jun  3 13:21:33.143: INFO: starting watch
  STEP: patching @ 06/03/23 13:21:33.145
  STEP: updating @ 06/03/23 13:21:33.151
  Jun  3 13:21:33.161: INFO: waiting for watch events with expected annotations
  Jun  3 13:21:33.161: INFO: saw patched and updated annotations
  STEP: patching /status @ 06/03/23 13:21:33.162
  STEP: updating /status @ 06/03/23 13:21:33.172
  STEP: get /status @ 06/03/23 13:21:33.182
  STEP: deleting @ 06/03/23 13:21:33.186
  STEP: deleting a collection @ 06/03/23 13:21:33.204
  Jun  3 13:21:33.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8970" for this suite. @ 06/03/23 13:21:33.22
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 06/03/23 13:21:33.232
  Jun  3 13:21:33.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 13:21:33.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:33.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:33.253
  STEP: creating service in namespace services-9722 @ 06/03/23 13:21:33.257
  STEP: creating service affinity-clusterip in namespace services-9722 @ 06/03/23 13:21:33.257
  STEP: creating replication controller affinity-clusterip in namespace services-9722 @ 06/03/23 13:21:33.269
  I0603 13:21:33.277570      18 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9722, replica count: 3
  E0603 13:21:33.648421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:34.649374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:35.649542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:21:36.329642      18 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 13:21:36.337: INFO: Creating new exec pod
  E0603 13:21:36.650570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:37.651017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:38.651697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:21:39.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9722 exec execpod-affinityhzg4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jun  3 13:21:39.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jun  3 13:21:39.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:21:39.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9722 exec execpod-affinityhzg4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.135 80'
  E0603 13:21:39.652367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:21:39.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.135 80\nConnection to 10.152.183.135 80 port [tcp/http] succeeded!\n"
  Jun  3 13:21:39.674: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:21:39.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-9722 exec execpod-affinityhzg4b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.135:80/ ; done'
  Jun  3 13:21:39.926: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.135:80/\n"
  Jun  3 13:21:39.926: INFO: stdout: "\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t\naffinity-clusterip-9x66t"
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Received response from host: affinity-clusterip-9x66t
  Jun  3 13:21:39.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 13:21:39.931: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9722, will wait for the garbage collector to delete the pods @ 06/03/23 13:21:39.943
  Jun  3 13:21:40.008: INFO: Deleting ReplicationController affinity-clusterip took: 9.013226ms
  Jun  3 13:21:40.109: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.885386ms
  E0603 13:21:40.653301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:41.653330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9722" for this suite. @ 06/03/23 13:21:41.828
• [8.602 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 06/03/23 13:21:41.84
  Jun  3 13:21:41.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 13:21:41.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:41.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:41.861
  Jun  3 13:21:41.865: INFO: Creating simple deployment test-new-deployment
  Jun  3 13:21:41.880: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0603 13:21:42.653614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:43.654116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 06/03/23 13:21:43.894
  STEP: updating a scale subresource @ 06/03/23 13:21:43.897
  STEP: verifying the deployment Spec.Replicas was modified @ 06/03/23 13:21:43.904
  STEP: Patch a scale subresource @ 06/03/23 13:21:43.908
  Jun  3 13:21:43.948: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-9664  bf2849a3-5ac7-4641-848b-529d2977bc45 37013 3 2023-06-03 13:21:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-03 13:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ec02a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-03 13:21:43 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-06-03 13:21:43 +0000 UTC,LastTransitionTime:2023-06-03 13:21:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun  3 13:21:43.978: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-9664  4a781a75-139d-484b-bab7-69c2c71b2bcf 37015 2 2023-06-03 13:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment bf2849a3-5ac7-4641-848b-529d2977bc45 0xc0054526a7 0xc0054526a8}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf2849a3-5ac7-4641-848b-529d2977bc45\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005452758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 13:21:43.991: INFO: Pod "test-new-deployment-67bd4bf6dc-4wgw7" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-4wgw7 test-new-deployment-67bd4bf6dc- deployment-9664  7ca48373-05b4-4e5e-9b03-5385ad37ec7b 37007 0 2023-06-03 13:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 4a781a75-139d-484b-bab7-69c2c71b2bcf 0xc005452b87 0xc005452b88}] [] [{kube-controller-manager Update v1 2023-06-03 13:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a781a75-139d-484b-bab7-69c2c71b2bcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mxsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mxsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.239,StartTime:2023-06-03 13:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:21:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d64740158a5b3ab94bf9aa29f72b3e40b1978dadaab11f8f0c02447e68b3c783,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.239,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 13:21:43.991: INFO: Pod "test-new-deployment-67bd4bf6dc-m4lrg" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-m4lrg test-new-deployment-67bd4bf6dc- deployment-9664  547a1f84-22dc-419a-88b2-fb3393f7eba4 37019 0 2023-06-03 13:21:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 4a781a75-139d-484b-bab7-69c2c71b2bcf 0xc005452d87 0xc005452d88}] [] [{kube-controller-manager Update v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a781a75-139d-484b-bab7-69c2c71b2bcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgl6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgl6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:21:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:,StartTime:2023-06-03 13:21:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 13:21:43.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9664" for this suite. @ 06/03/23 13:21:44.005
• [2.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 06/03/23 13:21:44.039
  Jun  3 13:21:44.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename downward-api @ 06/03/23 13:21:44.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:44.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:44.105
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:21:44.109
  E0603 13:21:44.654787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:45.655620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:46.655760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:47.656724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:21:48.134
  Jun  3 13:21:48.138: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-ca7d1d30-b16f-4170-a616-979533ee227a container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:21:48.163
  Jun  3 13:21:48.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-633" for this suite. @ 06/03/23 13:21:48.186
• [4.153 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 06/03/23 13:21:48.194
  Jun  3 13:21:48.194: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename runtimeclass @ 06/03/23 13:21:48.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:48.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:48.215
  E0603 13:21:48.657363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:49.657492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:21:50.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-299" for this suite. @ 06/03/23 13:21:50.254
• [2.068 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 06/03/23 13:21:50.262
  Jun  3 13:21:50.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 13:21:50.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:50.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:21:50.283
  STEP: Creating a test namespace @ 06/03/23 13:21:50.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:21:50.303
  STEP: Creating a pod in the namespace @ 06/03/23 13:21:50.308
  STEP: Waiting for the pod to have running status @ 06/03/23 13:21:50.318
  E0603 13:21:50.657582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:51.657795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 06/03/23 13:21:52.329
  STEP: Waiting for the namespace to be removed. @ 06/03/23 13:21:52.338
  E0603 13:21:52.658691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:53.659445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:54.659559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:55.659666      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:56.660290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:57.660429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:58.661214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:21:59.661760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:00.662130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:01.663143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:02.663626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 06/03/23 13:22:03.342
  STEP: Verifying there are no pods in the namespace @ 06/03/23 13:22:03.358
  Jun  3 13:22:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8752" for this suite. @ 06/03/23 13:22:03.367
  STEP: Destroying namespace "nsdeletetest-1451" for this suite. @ 06/03/23 13:22:03.375
  Jun  3 13:22:03.378: INFO: Namespace nsdeletetest-1451 was already deleted
  STEP: Destroying namespace "nsdeletetest-7146" for this suite. @ 06/03/23 13:22:03.378
• [13.124 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 06/03/23 13:22:03.387
  Jun  3 13:22:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:22:03.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:03.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:03.409
  STEP: Setting up server cert @ 06/03/23 13:22:03.439
  E0603 13:22:03.664065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:22:04
  STEP: Deploying the webhook pod @ 06/03/23 13:22:04.01
  STEP: Wait for the deployment to be ready @ 06/03/23 13:22:04.024
  Jun  3 13:22:04.032: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:22:04.664266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:05.664417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:06.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 3, 13, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 22, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 22, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 22, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0603 13:22:06.664583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:07.664704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:22:08.05
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:22:08.064
  E0603 13:22:08.664816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:09.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun  3 13:22:09.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3219-crds.webhook.example.com via the AdmissionRegistration API @ 06/03/23 13:22:09.582
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/03/23 13:22:09.6
  E0603 13:22:09.665351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:10.665589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:11.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0603 13:22:11.666222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9312" for this suite. @ 06/03/23 13:22:12.269
  STEP: Destroying namespace "webhook-markers-1524" for this suite. @ 06/03/23 13:22:12.277
• [8.898 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 06/03/23 13:22:12.288
  Jun  3 13:22:12.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 13:22:12.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:12.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:12.313
  STEP: Creating a ResourceQuota @ 06/03/23 13:22:12.319
  STEP: Getting a ResourceQuota @ 06/03/23 13:22:12.325
  STEP: Updating a ResourceQuota @ 06/03/23 13:22:12.328
  STEP: Verifying a ResourceQuota was modified @ 06/03/23 13:22:12.34
  STEP: Deleting a ResourceQuota @ 06/03/23 13:22:12.343
  STEP: Verifying the deleted ResourceQuota @ 06/03/23 13:22:12.351
  Jun  3 13:22:12.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9027" for this suite. @ 06/03/23 13:22:12.359
• [0.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 06/03/23 13:22:12.369
  Jun  3 13:22:12.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 13:22:12.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:12.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:12.392
  STEP: apply creating a deployment @ 06/03/23 13:22:12.396
  Jun  3 13:22:12.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9022" for this suite. @ 06/03/23 13:22:12.424
• [0.064 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 06/03/23 13:22:12.435
  Jun  3 13:22:12.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 13:22:12.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:12.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:12.462
  STEP: Creating a pod to test substitution in volume subpath @ 06/03/23 13:22:12.466
  E0603 13:22:12.667156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:13.667469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:14.667626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:15.667785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:22:16.495
  Jun  3 13:22:16.499: INFO: Trying to get logs from node ip-172-31-27-193 pod var-expansion-ab120d4f-7704-4e1a-9177-ce32d0f3f629 container dapi-container: <nil>
  STEP: delete the pod @ 06/03/23 13:22:16.507
  Jun  3 13:22:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4646" for this suite. @ 06/03/23 13:22:16.531
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 06/03/23 13:22:16.54
  Jun  3 13:22:16.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:22:16.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:16.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:16.561
  STEP: validating cluster-info @ 06/03/23 13:22:16.565
  Jun  3 13:22:16.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2786 cluster-info'
  Jun  3 13:22:16.648: INFO: stderr: ""
  Jun  3 13:22:16.648: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jun  3 13:22:16.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2786" for this suite. @ 06/03/23 13:22:16.654
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 06/03/23 13:22:16.663
  Jun  3 13:22:16.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 13:22:16.664
  E0603 13:22:16.668088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:16.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:16.685
  STEP: Create a Replicaset @ 06/03/23 13:22:16.694
  STEP: Verify that the required pods have come up. @ 06/03/23 13:22:16.701
  Jun  3 13:22:16.705: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0603 13:22:17.668281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:18.668358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:19.668772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:20.669248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:21.669310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:21.710: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 06/03/23 13:22:21.711
  STEP: Getting /status @ 06/03/23 13:22:21.711
  Jun  3 13:22:21.715: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 06/03/23 13:22:21.715
  Jun  3 13:22:21.730: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 06/03/23 13:22:21.73
  Jun  3 13:22:21.732: INFO: Observed &ReplicaSet event: ADDED
  Jun  3 13:22:21.733: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.733: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.733: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.733: INFO: Found replicaset test-rs in namespace replicaset-9270 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jun  3 13:22:21.733: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 06/03/23 13:22:21.733
  Jun  3 13:22:21.733: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun  3 13:22:21.743: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 06/03/23 13:22:21.744
  Jun  3 13:22:21.746: INFO: Observed &ReplicaSet event: ADDED
  Jun  3 13:22:21.747: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.747: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.747: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.747: INFO: Observed replicaset test-rs in namespace replicaset-9270 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 13:22:21.748: INFO: Observed &ReplicaSet event: MODIFIED
  Jun  3 13:22:21.748: INFO: Found replicaset test-rs in namespace replicaset-9270 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jun  3 13:22:21.748: INFO: Replicaset test-rs has a patched status
  Jun  3 13:22:21.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9270" for this suite. @ 06/03/23 13:22:21.755
• [5.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 06/03/23 13:22:21.779
  Jun  3 13:22:21.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename namespaces @ 06/03/23 13:22:21.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:21.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:21.808
  STEP: Updating Namespace "namespaces-6506" @ 06/03/23 13:22:21.813
  Jun  3 13:22:21.826: INFO: Namespace "namespaces-6506" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"890fff61-bdc4-4aa7-8c13-2e2931076cd1", "kubernetes.io/metadata.name":"namespaces-6506", "namespaces-6506":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jun  3 13:22:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6506" for this suite. @ 06/03/23 13:22:21.832
• [0.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 06/03/23 13:22:21.845
  Jun  3 13:22:21.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:22:21.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:21.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:21.87
  STEP: Creating a test headless service @ 06/03/23 13:22:21.873
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4658.svc.cluster.local;sleep 1; done
   @ 06/03/23 13:22:21.881
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4658.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4658.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4658.svc.cluster.local;sleep 1; done
   @ 06/03/23 13:22:21.881
  STEP: creating a pod to probe DNS @ 06/03/23 13:22:21.881
  STEP: submitting the pod to kubernetes @ 06/03/23 13:22:21.882
  E0603 13:22:22.670297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:23.670715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:22:23.905
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:22:23.909
  Jun  3 13:22:23.916: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.921: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.927: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.931: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.942: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.946: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.950: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4658.svc.cluster.local from pod dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4: the server could not find the requested resource (get pods dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4)
  Jun  3 13:22:23.950: INFO: Lookups using dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4658.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4658.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4658.svc.cluster.local jessie_udp@dns-test-service-2.dns-4658.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4658.svc.cluster.local]

  E0603 13:22:24.671657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:25.671717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:26.671823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:27.671935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:28.672047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:28.986: INFO: DNS probes using dns-4658/dns-test-7c80e0d5-2bd2-4608-b61c-8d2127978fe4 succeeded

  Jun  3 13:22:28.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:22:28.991
  STEP: deleting the test headless service @ 06/03/23 13:22:29.006
  STEP: Destroying namespace "dns-4658" for this suite. @ 06/03/23 13:22:29.038
• [7.206 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 06/03/23 13:22:29.051
  Jun  3 13:22:29.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 13:22:29.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:29.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:29.075
  Jun  3 13:22:29.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:22:29.672362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:30.672516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0603 13:22:31.648139      18 warnings.go:70] unknown field "alpha"
  W0603 13:22:31.648161      18 warnings.go:70] unknown field "beta"
  W0603 13:22:31.648167      18 warnings.go:70] unknown field "delta"
  W0603 13:22:31.648173      18 warnings.go:70] unknown field "epsilon"
  W0603 13:22:31.648178      18 warnings.go:70] unknown field "gamma"
  Jun  3 13:22:31.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0603 13:22:31.672762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-5739" for this suite. @ 06/03/23 13:22:31.696
• [2.653 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 06/03/23 13:22:31.705
  Jun  3 13:22:31.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 13:22:31.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:22:31.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:22:31.735
  STEP: Creating pod test-grpc-88fec16a-e66a-4c18-ae93-0d11984706fc in namespace container-probe-8232 @ 06/03/23 13:22:31.741
  E0603 13:22:32.673115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:33.673476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:22:33.769: INFO: Started pod test-grpc-88fec16a-e66a-4c18-ae93-0d11984706fc in namespace container-probe-8232
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 13:22:33.769
  Jun  3 13:22:33.773: INFO: Initial restart count of pod test-grpc-88fec16a-e66a-4c18-ae93-0d11984706fc is 0
  E0603 13:22:34.673592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:35.674091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:36.674730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:37.674813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:38.674932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:39.675126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:40.675168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:41.675521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:42.676047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:43.676278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:44.676350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:45.676620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:46.676800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:47.676842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:48.677150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:49.678198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:50.678284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:51.678569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:52.678701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:53.678819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:54.678922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:55.679043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:56.679579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:57.679691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:58.679872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:22:59.680120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:00.680232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:01.681135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:02.681209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:03.682026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:04.682259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:05.682391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:06.682786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:07.682907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:08.683011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:09.683128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:10.683236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:11.683353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:12.683457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:13.683579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:14.684030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:15.684185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:16.684309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:17.684389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:18.684499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:19.684580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:20.684722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:21.685033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:22.685142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:23.685262      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:24.685386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:25.685497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:26.685695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:27.686193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:28.686347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:29.686526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:30.686661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:31.686802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:32.686896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:33.686991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:34.687554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:35.687661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:36.687787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:37.689052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:38.689138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:39.689573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:23:39.935: INFO: Restart count of pod container-probe-8232/test-grpc-88fec16a-e66a-4c18-ae93-0d11984706fc is now 1 (1m6.161479277s elapsed)
  Jun  3 13:23:39.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:23:39.939
  STEP: Destroying namespace "container-probe-8232" for this suite. @ 06/03/23 13:23:39.951
• [68.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 06/03/23 13:23:39.963
  Jun  3 13:23:39.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:23:39.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:39.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:39.989
  STEP: Setting up server cert @ 06/03/23 13:23:40.017
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:23:40.292
  STEP: Deploying the webhook pod @ 06/03/23 13:23:40.301
  STEP: Wait for the deployment to be ready @ 06/03/23 13:23:40.318
  Jun  3 13:23:40.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:23:40.690493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:41.690552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:23:42.341
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:23:42.351
  E0603 13:23:42.691097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:23:43.352: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jun  3 13:23:43.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:23:43.691647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2463-crds.webhook.example.com via the AdmissionRegistration API @ 06/03/23 13:23:43.869
  STEP: Creating a custom resource that should be mutated by the webhook @ 06/03/23 13:23:43.887
  E0603 13:23:44.692657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:45.692750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:23:45.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9966" for this suite. @ 06/03/23 13:23:46.52
  STEP: Destroying namespace "webhook-markers-7360" for this suite. @ 06/03/23 13:23:46.527
• [6.572 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 06/03/23 13:23:46.535
  Jun  3 13:23:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:23:46.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:46.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:46.556
  STEP: Creating a test headless service @ 06/03/23 13:23:46.561
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1027.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1027.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 06/03/23 13:23:46.567
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1027.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1027.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 06/03/23 13:23:46.568
  STEP: creating a pod to probe DNS @ 06/03/23 13:23:46.568
  STEP: submitting the pod to kubernetes @ 06/03/23 13:23:46.568
  E0603 13:23:46.693185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:47.693495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:23:48.592
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:23:48.596
  Jun  3 13:23:48.615: INFO: DNS probes using dns-1027/dns-test-1daa01f5-a40f-472b-a360-071606a0e089 succeeded

  Jun  3 13:23:48.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:23:48.621
  STEP: deleting the test headless service @ 06/03/23 13:23:48.634
  STEP: Destroying namespace "dns-1027" for this suite. @ 06/03/23 13:23:48.655
• [2.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 06/03/23 13:23:48.666
  Jun  3 13:23:48.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:23:48.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:48.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:48.689
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5291.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5291.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 06/03/23 13:23:48.693
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5291.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5291.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 06/03/23 13:23:48.693
  STEP: creating a pod to probe /etc/hosts @ 06/03/23 13:23:48.693
  STEP: submitting the pod to kubernetes @ 06/03/23 13:23:48.693
  E0603 13:23:48.693893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:49.694012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:50.694168      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:23:50.713
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:23:50.717
  Jun  3 13:23:50.734: INFO: DNS probes using dns-5291/dns-test-9853e07f-9367-4c25-bb58-026dce49eebd succeeded

  Jun  3 13:23:50.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:23:50.739
  STEP: Destroying namespace "dns-5291" for this suite. @ 06/03/23 13:23:50.753
• [2.094 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 06/03/23 13:23:50.761
  Jun  3 13:23:50.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 13:23:50.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:50.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:50.782
  STEP: creating the pod @ 06/03/23 13:23:50.785
  STEP: submitting the pod to kubernetes @ 06/03/23 13:23:50.785
  STEP: verifying QOS class is set on the pod @ 06/03/23 13:23:50.795
  Jun  3 13:23:50.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8344" for this suite. @ 06/03/23 13:23:50.806
• [0.053 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 06/03/23 13:23:50.815
  Jun  3 13:23:50.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:23:50.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:50.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:50.837
  STEP: Setting up server cert @ 06/03/23 13:23:50.863
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:23:51.426
  STEP: Deploying the webhook pod @ 06/03/23 13:23:51.432
  STEP: Wait for the deployment to be ready @ 06/03/23 13:23:51.444
  Jun  3 13:23:51.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:23:51.694524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:52.694619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:23:53.465
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:23:53.475
  E0603 13:23:53.695675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:23:54.475: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 06/03/23 13:23:54.48
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/03/23 13:23:54.498
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 06/03/23 13:23:54.509
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/03/23 13:23:54.521
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 06/03/23 13:23:54.533
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 06/03/23 13:23:54.541
  Jun  3 13:23:54.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9584" for this suite. @ 06/03/23 13:23:54.608
  STEP: Destroying namespace "webhook-markers-8765" for this suite. @ 06/03/23 13:23:54.618
• [3.812 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 06/03/23 13:23:54.631
  Jun  3 13:23:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:23:54.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:54.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:54.653
  STEP: Creating secret with name projected-secret-test-1e0b072b-ea53-47cf-923a-15f924d2a446 @ 06/03/23 13:23:54.657
  STEP: Creating a pod to test consume secrets @ 06/03/23 13:23:54.662
  E0603 13:23:54.696580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:55.696549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:56.697116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:23:57.697873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:23:58.687
  Jun  3 13:23:58.691: INFO: Trying to get logs from node ip-172-31-85-85 pod pod-projected-secrets-de26c786-3df7-4f11-9d32-e9553d1a3a82 container secret-volume-test: <nil>
  E0603 13:23:58.698505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 06/03/23 13:23:58.715
  Jun  3 13:23:58.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5289" for this suite. @ 06/03/23 13:23:58.737
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 06/03/23 13:23:58.747
  Jun  3 13:23:58.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/03/23 13:23:58.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:58.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:58.77
  Jun  3 13:23:58.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:23:59.699345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:23:59.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5189" for this suite. @ 06/03/23 13:23:59.812
• [1.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 06/03/23 13:23:59.821
  Jun  3 13:23:59.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 13:23:59.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:23:59.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:23:59.849
  STEP: Discovering how many secrets are in namespace by default @ 06/03/23 13:23:59.853
  E0603 13:24:00.699477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:01.699500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:02.699603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:03.700061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:04.700925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/03/23 13:24:04.858
  E0603 13:24:05.701104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:06.701240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:07.701360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:08.701481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:09.702237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/03/23 13:24:09.863
  STEP: Ensuring resource quota status is calculated @ 06/03/23 13:24:09.869
  E0603 13:24:10.702825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:11.702956      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 06/03/23 13:24:11.873
  STEP: Ensuring resource quota status captures secret creation @ 06/03/23 13:24:11.884
  E0603 13:24:12.703931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:13.704069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 06/03/23 13:24:13.89
  STEP: Ensuring resource quota status released usage @ 06/03/23 13:24:13.896
  E0603 13:24:14.704166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:15.704294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:15.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7573" for this suite. @ 06/03/23 13:24:15.906
• [16.093 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 06/03/23 13:24:15.915
  Jun  3 13:24:15.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replication-controller @ 06/03/23 13:24:15.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:24:15.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:24:15.939
  STEP: Creating ReplicationController "e2e-rc-wgnsx" @ 06/03/23 13:24:15.945
  Jun  3 13:24:15.953: INFO: Get Replication Controller "e2e-rc-wgnsx" to confirm replicas
  E0603 13:24:16.704445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:16.959: INFO: Get Replication Controller "e2e-rc-wgnsx" to confirm replicas
  Jun  3 13:24:16.963: INFO: Found 1 replicas for "e2e-rc-wgnsx" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-wgnsx" @ 06/03/23 13:24:16.963
  STEP: Updating a scale subresource @ 06/03/23 13:24:16.967
  STEP: Verifying replicas where modified for replication controller "e2e-rc-wgnsx" @ 06/03/23 13:24:16.974
  Jun  3 13:24:16.974: INFO: Get Replication Controller "e2e-rc-wgnsx" to confirm replicas
  E0603 13:24:17.704621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:17.978: INFO: Get Replication Controller "e2e-rc-wgnsx" to confirm replicas
  Jun  3 13:24:17.983: INFO: Found 2 replicas for "e2e-rc-wgnsx" replication controller
  Jun  3 13:24:17.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7151" for this suite. @ 06/03/23 13:24:17.988
• [2.081 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 06/03/23 13:24:17.996
  Jun  3 13:24:17.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:24:17.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:24:18.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:24:18.018
  STEP: creating Agnhost RC @ 06/03/23 13:24:18.025
  Jun  3 13:24:18.025: INFO: namespace kubectl-2252
  Jun  3 13:24:18.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2252 create -f -'
  E0603 13:24:18.704820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:19.425: INFO: stderr: ""
  Jun  3 13:24:19.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/03/23 13:24:19.425
  E0603 13:24:19.704879      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:20.429: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:24:20.429: INFO: Found 0 / 1
  E0603 13:24:20.704954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:21.430: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:24:21.430: INFO: Found 1 / 1
  Jun  3 13:24:21.430: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun  3 13:24:21.434: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:24:21.434: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun  3 13:24:21.434: INFO: wait on agnhost-primary startup in kubectl-2252 
  Jun  3 13:24:21.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2252 logs agnhost-primary-f42n4 agnhost-primary'
  Jun  3 13:24:21.522: INFO: stderr: ""
  Jun  3 13:24:21.522: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 06/03/23 13:24:21.522
  Jun  3 13:24:21.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2252 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jun  3 13:24:21.635: INFO: stderr: ""
  Jun  3 13:24:21.635: INFO: stdout: "service/rm2 exposed\n"
  Jun  3 13:24:21.639: INFO: Service rm2 in namespace kubectl-2252 found.
  E0603 13:24:21.705945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:22.706073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 06/03/23 13:24:23.647
  Jun  3 13:24:23.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-2252 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  E0603 13:24:23.706919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:23.867: INFO: stderr: ""
  Jun  3 13:24:23.867: INFO: stdout: "service/rm3 exposed\n"
  Jun  3 13:24:23.877: INFO: Service rm3 in namespace kubectl-2252 found.
  E0603 13:24:24.707038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:25.707133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:24:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2252" for this suite. @ 06/03/23 13:24:25.889
• [7.900 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 06/03/23 13:24:25.902
  Jun  3 13:24:25.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-runtime @ 06/03/23 13:24:25.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:24:25.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:24:25.922
  STEP: create the container @ 06/03/23 13:24:25.927
  W0603 13:24:25.935929      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 06/03/23 13:24:25.936
  E0603 13:24:26.707245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:27.707575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:28.707717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/03/23 13:24:28.955
  STEP: the container should be terminated @ 06/03/23 13:24:28.959
  STEP: the termination message should be set @ 06/03/23 13:24:28.959
  Jun  3 13:24:28.959: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 06/03/23 13:24:28.959
  Jun  3 13:24:28.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-189" for this suite. @ 06/03/23 13:24:28.985
• [3.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 06/03/23 13:24:28.996
  Jun  3 13:24:28.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename sched-pred @ 06/03/23 13:24:28.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:24:29.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:24:29.019
  Jun  3 13:24:29.029: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jun  3 13:24:29.039: INFO: Waiting for terminating namespaces to be deleted...
  Jun  3 13:24:29.044: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-27-193 before test
  Jun  3 13:24:29.050: INFO: nginx-ingress-controller-kubernetes-worker-8lww8 from ingress-nginx-kubernetes-worker started at 2023-06-03 13:02:02 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.050: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:24:29.050: INFO: pod-qos-class-4d0cca53-d684-4a19-9af2-2ea6b0308cfb from pods-8344 started at 2023-06-03 13:23:50 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.050: INFO: 	Container agnhost ready: false, restart count 0
  Jun  3 13:24:29.050: INFO: sonobuoy from sonobuoy started at 2023-06-03 12:00:20 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.050: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jun  3 13:24:29.050: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-9mjpr from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:24:29.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:24:29.050: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:24:29.050: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-7-203 before test
  Jun  3 13:24:29.057: INFO: nginx-ingress-controller-kubernetes-worker-js8tg from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: calico-kube-controllers-74bc8c9977-fzgz2 from kube-system started at 2023-06-03 11:55:43 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: coredns-5c7f76ccb8-p6pwx from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container coredns ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: kube-state-metrics-5b95b4459c-wk958 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: metrics-server-v0.5.2-6cf8c8b69c-88j59 from kube-system started at 2023-06-03 11:55:15 +0000 UTC (2 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container metrics-server ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: dashboard-metrics-scraper-6b8586b5c9-28hlk from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: kubernetes-dashboard-6869f4cd5f-dv2kl from kubernetes-dashboard started at 2023-06-03 11:55:15 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jun  3 13:24:29.057: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-7fcft from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:24:29.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:24:29.058: INFO: 	Container systemd-logs ready: true, restart count 0
  Jun  3 13:24:29.058: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-85-85 before test
  Jun  3 13:24:29.064: INFO: default-http-backend-kubernetes-worker-65fc475d49-ddthh from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:24 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.064: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: nginx-ingress-controller-kubernetes-worker-t855p from ingress-nginx-kubernetes-worker started at 2023-06-03 11:55:23 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.064: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: agnhost-primary-f42n4 from kubectl-2252 started at 2023-06-03 13:24:19 +0000 UTC (1 container statuses recorded)
  Jun  3 13:24:29.064: INFO: 	Container agnhost-primary ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: sonobuoy-e2e-job-56763c10077449b8 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:24:29.064: INFO: 	Container e2e ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cwkk4 from sonobuoy started at 2023-06-03 12:00:23 +0000 UTC (2 container statuses recorded)
  Jun  3 13:24:29.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jun  3 13:24:29.064: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 06/03/23 13:24:29.064
  E0603 13:24:29.708112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:30.708295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 06/03/23 13:24:31.089
  STEP: Trying to apply a random label on the found node. @ 06/03/23 13:24:31.111
  STEP: verifying the node has the label kubernetes.io/e2e-bcc47dfd-6267-4a49-97a7-31bde138b9cb 95 @ 06/03/23 13:24:31.122
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 06/03/23 13:24:31.128
  E0603 13:24:31.708417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:32.708881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.27.193 on the node which pod4 resides and expect not scheduled @ 06/03/23 13:24:33.147
  E0603 13:24:33.709056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:34.709136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:35.710051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:36.710140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:37.710963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:38.711205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:39.712229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:40.712334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:41.712504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:42.712665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:43.712958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:44.713074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:45.713257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:46.713325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:47.714448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:48.714574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:49.714688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:50.714898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:51.715698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:52.716093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:53.716190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:54.716803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:55.716971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:56.717066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:57.718072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:58.718248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:24:59.718527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:00.718637      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:01.718794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:02.718861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:03.718993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:04.719100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:05.719275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:06.719361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:07.719730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:08.719845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:09.719974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:10.720028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:11.720698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:12.721763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:13.721886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:14.722017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:15.722624      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:16.723224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:17.723594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:18.724219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:19.724414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:20.724646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:21.724742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:22.724844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:23.725489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:24.725587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:25.726171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:26.726288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:27.726569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:28.726625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:29.726750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:30.726857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:31.727122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:32.727692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:33.728615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:34.728727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:35.728859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:36.729415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:37.729523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:38.729873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:39.730953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:40.731166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:41.732184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:42.732263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:43.732403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:44.732685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:45.732710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:46.732803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:47.732889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:48.732986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:49.733102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:50.733240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:51.733930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:52.734509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:53.735324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:54.735682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:55.736630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:56.737010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:57.737086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:58.737188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:25:59.738330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:00.738494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:01.739412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:02.739609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:03.739749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:04.740778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:05.740964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:06.741071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:07.741689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:08.742038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:09.742173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:10.742253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:11.742394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:12.742617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:13.743390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:14.743508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:15.743621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:16.743690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:17.743856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:18.744226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:19.744533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:20.744599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:21.745687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:22.745798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:23.745924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:24.746004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:25.746147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:26.746218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:27.746782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:28.746906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:29.747890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:30.748001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:31.748115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:32.748237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:33.749013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:34.749149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:35.749250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:36.749784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:37.750720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:38.750852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:39.751577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:40.751695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:41.751846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:42.752588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:43.752811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:44.753239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:45.754086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:46.754217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:47.754774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:48.755588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:49.756198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:50.756253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:51.756800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:52.756941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:53.757047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:54.757164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:55.757304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:56.757392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:57.758454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:58.759346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:26:59.759570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:00.759683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:01.760526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:02.760638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:03.761720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:04.761824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:05.762721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:06.762819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:07.763283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:08.763385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:09.764109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:10.764341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:11.764747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:12.765170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:13.765280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:14.765392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:15.766098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:16.766218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:17.766980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:18.767096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:19.768086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:20.768207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:21.768499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:22.768617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:23.769613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:24.769737      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:25.770258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:26.770519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:27.771107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:28.771233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:29.771341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:30.771501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:31.772523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:32.772971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:33.773853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:34.774039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:35.775117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:36.775204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:37.776473      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:38.776582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:39.777588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:40.777696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:41.777809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:42.777928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:43.778917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:44.779589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:45.780563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:46.781489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:47.782092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:48.782330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:49.782625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:50.783576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:51.784213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:52.784608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:53.785672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:54.785858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:55.785983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:56.786244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:57.787000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:58.787128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:27:59.788181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:00.788169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:01.789107      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:02.789224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:03.789801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:04.790834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:05.791204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:06.792177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:07.792756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:08.793033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:09.793905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:10.794251      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:11.795177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:12.795515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:13.795776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:14.795886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:15.796696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:16.796803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:17.797629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:18.797728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:19.798072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:20.798192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:21.798723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:22.798824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:23.798889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:24.800692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:25.801708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:26.801994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:27.802475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:28.802520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:29.803440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:30.803552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:31.804339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:32.804829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:33.805787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:34.806005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:35.806112      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:36.807199      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:37.807454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:38.807563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:39.807788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:40.808762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:41.809361      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:42.809475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:43.809517      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:44.809601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:45.810510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:46.811022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:47.811077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:48.811153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:49.811810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:50.812673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:51.813440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:52.813573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:53.814611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:54.815569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:55.815932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:56.815998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:57.816383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:58.816495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:28:59.817058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:00.817189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:01.817936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:02.818250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:03.819097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:04.819209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:05.819570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:06.820097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:07.820528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:08.820776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:09.821041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:10.821158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:11.821376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:12.821527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:13.822188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:14.822273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:15.822619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:16.823515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:17.823893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:18.824202      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:19.824920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:20.825041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:21.825736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:22.826513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:23.827830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:24.828192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:25.828301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:26.829270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:27.829928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:28.830082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:29.830220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:30.830556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:31.831114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:32.831721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-bcc47dfd-6267-4a49-97a7-31bde138b9cb off the node ip-172-31-27-193 @ 06/03/23 13:29:33.157
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-bcc47dfd-6267-4a49-97a7-31bde138b9cb @ 06/03/23 13:29:33.171
  Jun  3 13:29:33.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7991" for this suite. @ 06/03/23 13:29:33.181
• [304.193 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 06/03/23 13:29:33.189
  Jun  3 13:29:33.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:29:33.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:33.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:33.227
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 06/03/23 13:29:33.231
  Jun  3 13:29:33.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6775 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jun  3 13:29:33.336: INFO: stderr: ""
  Jun  3 13:29:33.336: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 06/03/23 13:29:33.336
  Jun  3 13:29:33.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6775 delete pods e2e-test-httpd-pod'
  E0603 13:29:33.831839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:34.832328      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:29:35.678: INFO: stderr: ""
  Jun  3 13:29:35.678: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jun  3 13:29:35.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6775" for this suite. @ 06/03/23 13:29:35.683
• [2.502 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 06/03/23 13:29:35.694
  Jun  3 13:29:35.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename endpointslice @ 06/03/23 13:29:35.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:35.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:35.716
  E0603 13:29:35.832845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:36.833183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:37.833457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:38.833599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:29:39.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6301" for this suite. @ 06/03/23 13:29:39.785
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 06/03/23 13:29:39.795
  Jun  3 13:29:39.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 13:29:39.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:39.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:39.816
  STEP: fetching services @ 06/03/23 13:29:39.821
  Jun  3 13:29:39.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-95" for this suite. @ 06/03/23 13:29:39.83
  E0603 13:29:39.834056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.042 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 06/03/23 13:29:39.837
  Jun  3 13:29:39.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 13:29:39.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:39.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:39.863
  STEP: Creating configMap with name configmap-test-volume-map-26158099-23f1-4658-9a89-f2e706a72bac @ 06/03/23 13:29:39.867
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:29:39.872
  E0603 13:29:40.835060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:41.835148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:42.835285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:43.835420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:29:43.895
  Jun  3 13:29:43.899: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-64d680c2-c687-4d33-a404-21b7d59cc217 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:29:43.925
  Jun  3 13:29:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1046" for this suite. @ 06/03/23 13:29:43.946
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 06/03/23 13:29:43.959
  Jun  3 13:29:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename watch @ 06/03/23 13:29:43.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:43.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:43.98
  STEP: creating a watch on configmaps @ 06/03/23 13:29:43.984
  STEP: creating a new configmap @ 06/03/23 13:29:43.985
  STEP: modifying the configmap once @ 06/03/23 13:29:43.991
  STEP: closing the watch once it receives two notifications @ 06/03/23 13:29:44
  Jun  3 13:29:44.000: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-579  175aafbd-3f43-4be3-82ea-cc6c35aa2337 39141 0 2023-06-03 13:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-03 13:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 13:29:44.000: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-579  175aafbd-3f43-4be3-82ea-cc6c35aa2337 39142 0 2023-06-03 13:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-03 13:29:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 06/03/23 13:29:44
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 06/03/23 13:29:44.009
  STEP: deleting the configmap @ 06/03/23 13:29:44.011
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 06/03/23 13:29:44.018
  Jun  3 13:29:44.018: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-579  175aafbd-3f43-4be3-82ea-cc6c35aa2337 39143 0 2023-06-03 13:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-03 13:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 13:29:44.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-579  175aafbd-3f43-4be3-82ea-cc6c35aa2337 39144 0 2023-06-03 13:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-03 13:29:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jun  3 13:29:44.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-579" for this suite. @ 06/03/23 13:29:44.024
• [0.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 06/03/23 13:29:44.033
  Jun  3 13:29:44.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subpath @ 06/03/23 13:29:44.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:29:44.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:29:44.054
  STEP: Setting up data @ 06/03/23 13:29:44.058
  STEP: Creating pod pod-subpath-test-configmap-4rpk @ 06/03/23 13:29:44.068
  STEP: Creating a pod to test atomic-volume-subpath @ 06/03/23 13:29:44.068
  E0603 13:29:44.835450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:45.835556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:46.836266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:47.836403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:48.836461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:49.836587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:50.836658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:51.837329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:52.837437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:53.837651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:54.837800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:55.837949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:56.838286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:57.838557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:58.838663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:29:59.839577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:00.839799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:01.840243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:02.840453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:03.840770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:04.841206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:05.841464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:06.842276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:07.842521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:30:08.146
  Jun  3 13:30:08.150: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-subpath-test-configmap-4rpk container test-container-subpath-configmap-4rpk: <nil>
  STEP: delete the pod @ 06/03/23 13:30:08.158
  STEP: Deleting pod pod-subpath-test-configmap-4rpk @ 06/03/23 13:30:08.173
  Jun  3 13:30:08.173: INFO: Deleting pod "pod-subpath-test-configmap-4rpk" in namespace "subpath-2377"
  Jun  3 13:30:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2377" for this suite. @ 06/03/23 13:30:08.183
• [24.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 06/03/23 13:30:08.195
  Jun  3 13:30:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename field-validation @ 06/03/23 13:30:08.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:30:08.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:30:08.217
  Jun  3 13:30:08.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:30:08.843354      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:09.843851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0603 13:30:10.784372      18 warnings.go:70] unknown field "alpha"
  W0603 13:30:10.784413      18 warnings.go:70] unknown field "beta"
  W0603 13:30:10.784420      18 warnings.go:70] unknown field "delta"
  W0603 13:30:10.784426      18 warnings.go:70] unknown field "epsilon"
  W0603 13:30:10.784433      18 warnings.go:70] unknown field "gamma"
  Jun  3 13:30:10.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7459" for this suite. @ 06/03/23 13:30:10.824
• [2.637 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 06/03/23 13:30:10.834
  Jun  3 13:30:10.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 13:30:10.835
  E0603 13:30:10.844044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:30:10.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:30:10.862
  STEP: creating the pod @ 06/03/23 13:30:10.869
  STEP: waiting for pod running @ 06/03/23 13:30:10.878
  E0603 13:30:11.844797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:12.844884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 06/03/23 13:30:12.889
  Jun  3 13:30:12.892: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5630 PodName:var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:30:12.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:30:12.893: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:30:12.893: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-5630/pods/var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 06/03/23 13:30:12.97
  Jun  3 13:30:12.975: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5630 PodName:var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:30:12.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:30:12.976: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:30:12.976: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-5630/pods/var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 06/03/23 13:30:13.043
  Jun  3 13:30:13.557: INFO: Successfully updated pod "var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2"
  STEP: waiting for annotated pod running @ 06/03/23 13:30:13.557
  STEP: deleting the pod gracefully @ 06/03/23 13:30:13.562
  Jun  3 13:30:13.562: INFO: Deleting pod "var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2" in namespace "var-expansion-5630"
  Jun  3 13:30:13.571: INFO: Wait up to 5m0s for pod "var-expansion-01665111-9b2e-4e3a-830f-d74f6ff885d2" to be fully deleted
  E0603 13:30:13.845670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:14.845771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:15.846563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:16.847429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:17.847506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:18.847553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:19.848653      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:20.849123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:21.849689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:22.849781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:23.849932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:24.850006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:25.850708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:26.851203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:27.852045      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:28.852154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:29.852374      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:30.852765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:31.853663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:32.853759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:33.853894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:34.854885      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:35.855114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:36.855652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:37.856495      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:38.856612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:39.856668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:40.856748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:41.857446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:42.857543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:43.858439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:44.858555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:30:45.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5630" for this suite. @ 06/03/23 13:30:45.659
• [34.833 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 06/03/23 13:30:45.671
  Jun  3 13:30:45.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 13:30:45.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:30:45.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:30:45.695
  STEP: Counting existing ResourceQuota @ 06/03/23 13:30:45.702
  E0603 13:30:45.858923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:46.859814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:47.860620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:48.861530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:49.862430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/03/23 13:30:50.706
  STEP: Ensuring resource quota status is calculated @ 06/03/23 13:30:50.714
  E0603 13:30:50.862445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:51.863256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 06/03/23 13:30:52.718
  STEP: Ensuring resource quota status captures replication controller creation @ 06/03/23 13:30:52.733
  E0603 13:30:52.863278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:53.863390      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 06/03/23 13:30:54.74
  STEP: Ensuring resource quota status released usage @ 06/03/23 13:30:54.755
  E0603 13:30:54.864165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:55.864272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:30:56.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5465" for this suite. @ 06/03/23 13:30:56.765
• [11.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 06/03/23 13:30:56.775
  Jun  3 13:30:56.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:30:56.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:30:56.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:30:56.797
  STEP: Creating configMap with name projected-configmap-test-volume-map-76c6d416-10af-4017-822c-dea1fba84651 @ 06/03/23 13:30:56.801
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:30:56.807
  E0603 13:30:56.865114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:57.865782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:58.866356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:30:59.866571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:31:00.83
  Jun  3 13:31:00.834: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-projected-configmaps-a90dd241-3107-4a9b-ae13-304ffe74d5a9 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:31:00.84
  Jun  3 13:31:00.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5293" for this suite. @ 06/03/23 13:31:00.859
• [4.091 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 06/03/23 13:31:00.867
  Jun  3 13:31:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:31:00.867488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename pod-network-test @ 06/03/23 13:31:00.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:31:00.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:31:00.889
  STEP: Performing setup for networking test in namespace pod-network-test-1531 @ 06/03/23 13:31:00.894
  STEP: creating a selector @ 06/03/23 13:31:00.894
  STEP: Creating the service pods in kubernetes @ 06/03/23 13:31:00.894
  Jun  3 13:31:00.894: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0603 13:31:01.868292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:02.868605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:03.868683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:04.868798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:05.868970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:06.869812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:07.869986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:08.871021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:09.871173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:10.871252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:11.871619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:12.871949      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 06/03/23 13:31:12.98
  E0603 13:31:13.872861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:14.873370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:15.015: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jun  3 13:31:15.015: INFO: Going to poll 192.168.118.251 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 13:31:15.018: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.118.251 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:31:15.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:31:15.019: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:31:15.019: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.118.251+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0603 13:31:15.873422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:16.097: INFO: Found all 1 expected endpoints: [netserver-0]
  Jun  3 13:31:16.097: INFO: Going to poll 192.168.192.160 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 13:31:16.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.192.160 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:31:16.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:31:16.102: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:31:16.102: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.192.160+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0603 13:31:16.874185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:17.171: INFO: Found all 1 expected endpoints: [netserver-1]
  Jun  3 13:31:17.172: INFO: Going to poll 192.168.20.93 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jun  3 13:31:17.176: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.20.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1531 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jun  3 13:31:17.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:31:17.177: INFO: ExecWithOptions: Clientset creation
  Jun  3 13:31:17.177: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1531/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.20.93+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0603 13:31:17.874510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:18.245: INFO: Found all 1 expected endpoints: [netserver-2]
  Jun  3 13:31:18.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1531" for this suite. @ 06/03/23 13:31:18.25
• [17.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 06/03/23 13:31:18.258
  Jun  3 13:31:18.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename services @ 06/03/23 13:31:18.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:31:18.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:31:18.282
  STEP: creating service in namespace services-7288 @ 06/03/23 13:31:18.286
  STEP: creating service affinity-nodeport in namespace services-7288 @ 06/03/23 13:31:18.286
  STEP: creating replication controller affinity-nodeport in namespace services-7288 @ 06/03/23 13:31:18.31
  I0603 13:31:18.324467      18 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-7288, replica count: 3
  E0603 13:31:18.875163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:19.875913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:20.876000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0603 13:31:21.375487      18 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jun  3 13:31:21.389: INFO: Creating new exec pod
  E0603 13:31:21.876417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:22.876491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:23.877415      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:24.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7288 exec execpod-affinitys2cg5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jun  3 13:31:24.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jun  3 13:31:24.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:31:24.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7288 exec execpod-affinitys2cg5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.173 80'
  Jun  3 13:31:24.753: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.173 80\nConnection to 10.152.183.173 80 port [tcp/http] succeeded!\n"
  Jun  3 13:31:24.753: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:31:24.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7288 exec execpod-affinitys2cg5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.85.85 32166'
  E0603 13:31:24.878170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:24.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.85.85 32166\nConnection to 172.31.85.85 32166 port [tcp/*] succeeded!\n"
  Jun  3 13:31:24.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:31:24.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7288 exec execpod-affinitys2cg5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.203 32166'
  Jun  3 13:31:25.088: INFO: stderr: "+ nc -v -t -w 2 172.31.7.203 32166\n+ echo hostName\nConnection to 172.31.7.203 32166 port [tcp/*] succeeded!\n"
  Jun  3 13:31:25.088: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jun  3 13:31:25.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=services-7288 exec execpod-affinitys2cg5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.27.193:32166/ ; done'
  Jun  3 13:31:25.381: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.27.193:32166/\n"
  Jun  3 13:31:25.381: INFO: stdout: "\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b\naffinity-nodeport-zwh5b"
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Received response from host: affinity-nodeport-zwh5b
  Jun  3 13:31:25.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jun  3 13:31:25.387: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-7288, will wait for the garbage collector to delete the pods @ 06/03/23 13:31:25.401
  Jun  3 13:31:25.463: INFO: Deleting ReplicationController affinity-nodeport took: 7.32445ms
  Jun  3 13:31:25.563: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.698206ms
  E0603 13:31:25.879147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:26.879270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:27.879987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7288" for this suite. @ 06/03/23 13:31:27.999
• [9.749 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 06/03/23 13:31:28.007
  Jun  3 13:31:28.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 13:31:28.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:31:28.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:31:28.031
  STEP: Create set of pods @ 06/03/23 13:31:28.036
  Jun  3 13:31:28.049: INFO: created test-pod-1
  Jun  3 13:31:28.058: INFO: created test-pod-2
  Jun  3 13:31:28.070: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 06/03/23 13:31:28.07
  E0603 13:31:28.880304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:29.880248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 06/03/23 13:31:30.121
  Jun  3 13:31:30.129: INFO: Pod quantity 3 is different from expected quantity 0
  E0603 13:31:30.880358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:31.133: INFO: Pod quantity 2 is different from expected quantity 0
  E0603 13:31:31.881087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:31:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6510" for this suite. @ 06/03/23 13:31:32.137
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 06/03/23 13:31:32.149
  Jun  3 13:31:32.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subpath @ 06/03/23 13:31:32.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:31:32.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:31:32.175
  STEP: Setting up data @ 06/03/23 13:31:32.18
  STEP: Creating pod pod-subpath-test-secret-bmvb @ 06/03/23 13:31:32.19
  STEP: Creating a pod to test atomic-volume-subpath @ 06/03/23 13:31:32.19
  E0603 13:31:32.881566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:33.881593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:34.882620      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:35.882738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:36.883034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:37.883628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:38.884291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:39.884455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:40.885244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:41.886235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:42.886588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:43.886695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:44.887600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:45.887898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:46.888134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:47.888268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:48.888357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:49.888477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:50.888762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:51.889453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:52.889590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:53.889979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:54.890109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:55.890163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:31:56.268
  Jun  3 13:31:56.271: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-subpath-test-secret-bmvb container test-container-subpath-secret-bmvb: <nil>
  STEP: delete the pod @ 06/03/23 13:31:56.279
  STEP: Deleting pod pod-subpath-test-secret-bmvb @ 06/03/23 13:31:56.294
  Jun  3 13:31:56.294: INFO: Deleting pod "pod-subpath-test-secret-bmvb" in namespace "subpath-4655"
  Jun  3 13:31:56.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4655" for this suite. @ 06/03/23 13:31:56.301
• [24.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 06/03/23 13:31:56.311
  Jun  3 13:31:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename security-context-test @ 06/03/23 13:31:56.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:31:56.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:31:56.332
  E0603 13:31:56.891217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:57.891233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:58.891601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:31:59.891875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:00.363: INFO: Got logs for pod "busybox-privileged-false-ee1537b9-2110-4cf2-b9f9-3f9000386d24": "ip: RTNETLINK answers: Operation not permitted\n"
  Jun  3 13:32:00.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3476" for this suite. @ 06/03/23 13:32:00.367
• [4.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 06/03/23 13:32:00.377
  Jun  3 13:32:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubelet-test @ 06/03/23 13:32:00.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:00.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:00.4
  Jun  3 13:32:00.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7316" for this suite. @ 06/03/23 13:32:00.446
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 06/03/23 13:32:00.456
  Jun  3 13:32:00.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename replicaset @ 06/03/23 13:32:00.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:00.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:00.479
  STEP: Create a ReplicaSet @ 06/03/23 13:32:00.484
  STEP: Verify that the required pods have come up @ 06/03/23 13:32:00.491
  Jun  3 13:32:00.496: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0603 13:32:00.892886      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:01.893205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:02.893316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:03.893428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:04.893627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:05.501: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 06/03/23 13:32:05.502
  Jun  3 13:32:05.508: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 06/03/23 13:32:05.508
  STEP: DeleteCollection of the ReplicaSets @ 06/03/23 13:32:05.512
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 06/03/23 13:32:05.523
  Jun  3 13:32:05.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7545" for this suite. @ 06/03/23 13:32:05.535
• [5.094 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 06/03/23 13:32:05.551
  Jun  3 13:32:05.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:32:05.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:05.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:05.598
  STEP: Creating configMap with name configmap-projected-all-test-volume-c9ebd3fd-a29e-4c89-be8b-52f9ec9f478d @ 06/03/23 13:32:05.602
  STEP: Creating secret with name secret-projected-all-test-volume-d2fee306-585a-4dac-91fe-b9e0898075a3 @ 06/03/23 13:32:05.608
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 06/03/23 13:32:05.618
  E0603 13:32:05.894641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:06.895175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:07.896163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:08.896289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:32:09.656
  Jun  3 13:32:09.660: INFO: Trying to get logs from node ip-172-31-27-193 pod projected-volume-9e04a6c0-805f-49ab-a810-709f836f27dd container projected-all-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 13:32:09.668
  Jun  3 13:32:09.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9238" for this suite. @ 06/03/23 13:32:09.688
• [4.145 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 06/03/23 13:32:09.696
  Jun  3 13:32:09.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/03/23 13:32:09.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:09.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:09.72
  Jun  3 13:32:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  E0603 13:32:09.897288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:10.897510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:11.898513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:12.899128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:13.900108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:14.900582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:15.901244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:16.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6476" for this suite. @ 06/03/23 13:32:16.032
• [6.345 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 06/03/23 13:32:16.042
  Jun  3 13:32:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:32:16.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:16.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:16.063
  STEP: Creating the pod @ 06/03/23 13:32:16.067
  E0603 13:32:16.902121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:17.902553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:18.612: INFO: Successfully updated pod "labelsupdatec702ecca-ab50-4337-ab07-98643a38fedc"
  E0603 13:32:18.902699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:19.902829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:20.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8657" for this suite. @ 06/03/23 13:32:20.637
• [4.603 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 06/03/23 13:32:20.647
  Jun  3 13:32:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:32:20.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:20.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:20.669
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:32:20.673
  E0603 13:32:20.902891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:21.903184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:22.903766      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:23.903868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:32:24.7
  Jun  3 13:32:24.704: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-005cb627-e260-48bb-bf5f-3981f7e2a994 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:32:24.712
  Jun  3 13:32:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7916" for this suite. @ 06/03/23 13:32:24.734
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 06/03/23 13:32:24.746
  Jun  3 13:32:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 13:32:24.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:24.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:24.811
  STEP: Creating a simple DaemonSet "daemon-set" @ 06/03/23 13:32:24.851
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 13:32:24.861
  Jun  3 13:32:24.874: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:24.875: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:24.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:32:24.889: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:32:24.909593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:25.898: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:25.898: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:25.903: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:32:25.903: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:32:25.909864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:26.896: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:26.896: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:26.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:32:26.901: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:32:26.910454      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:27.895: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:27.896: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:27.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:32:27.901: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 06/03/23 13:32:27.904
  E0603 13:32:27.911305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:27.930: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:27.931: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:27.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:32:27.936: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:32:28.911479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:28.941: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:28.941: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:28.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:32:28.945: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:32:29.912481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:29.942: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:29.942: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:32:29.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:32:29.947: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 06/03/23 13:32:29.947
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 13:32:29.957
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7973, will wait for the garbage collector to delete the pods @ 06/03/23 13:32:29.957
  Jun  3 13:32:30.024: INFO: Deleting DaemonSet.extensions daemon-set took: 9.724599ms
  Jun  3 13:32:30.124: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.281259ms
  E0603 13:32:30.913357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:31.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:32:31.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 13:32:31.233: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40445"},"items":null}

  Jun  3 13:32:31.237: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40445"},"items":null}

  Jun  3 13:32:31.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7973" for this suite. @ 06/03/23 13:32:31.26
• [6.523 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 06/03/23 13:32:31.271
  Jun  3 13:32:31.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 13:32:31.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:31.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:31.295
  STEP: Creating secret with name secret-test-795a6688-50f6-4037-9c3a-1b249b791dd7 @ 06/03/23 13:32:31.3
  STEP: Creating a pod to test consume secrets @ 06/03/23 13:32:31.307
  E0603 13:32:31.914300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:32.914507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:33.914613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:34.914745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:32:35.334
  Jun  3 13:32:35.338: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-72cc078e-f094-4751-95a0-83475cfa3ec6 container secret-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 13:32:35.347
  Jun  3 13:32:35.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6315" for this suite. @ 06/03/23 13:32:35.367
• [4.104 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 06/03/23 13:32:35.376
  Jun  3 13:32:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename disruption @ 06/03/23 13:32:35.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:35.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:35.397
  STEP: Creating a pdb that targets all three pods in a test replica set @ 06/03/23 13:32:35.402
  STEP: Waiting for the pdb to be processed @ 06/03/23 13:32:35.41
  E0603 13:32:35.915742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:36.916156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 06/03/23 13:32:37.429
  STEP: Waiting for all pods to be running @ 06/03/23 13:32:37.429
  Jun  3 13:32:37.432: INFO: pods: 0 < 3
  E0603 13:32:37.916294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:38.916516      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:39.437: INFO: running pods: 2 < 3
  E0603 13:32:39.917394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:40.917503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 06/03/23 13:32:41.437
  STEP: Updating the pdb to allow a pod to be evicted @ 06/03/23 13:32:41.451
  STEP: Waiting for the pdb to be processed @ 06/03/23 13:32:41.461
  E0603 13:32:41.918298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:42.918740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/03/23 13:32:43.469
  STEP: Waiting for all pods to be running @ 06/03/23 13:32:43.469
  STEP: Waiting for the pdb to observed all healthy pods @ 06/03/23 13:32:43.473
  STEP: Patching the pdb to disallow a pod to be evicted @ 06/03/23 13:32:43.505
  STEP: Waiting for the pdb to be processed @ 06/03/23 13:32:43.53
  E0603 13:32:43.918818      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:44.919694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 06/03/23 13:32:45.545
  STEP: locating a running pod @ 06/03/23 13:32:45.55
  STEP: Deleting the pdb to allow a pod to be evicted @ 06/03/23 13:32:45.561
  STEP: Waiting for the pdb to be deleted @ 06/03/23 13:32:45.568
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 06/03/23 13:32:45.572
  STEP: Waiting for all pods to be running @ 06/03/23 13:32:45.572
  Jun  3 13:32:45.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3666" for this suite. @ 06/03/23 13:32:45.606
• [10.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 06/03/23 13:32:45.625
  Jun  3 13:32:45.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-probe @ 06/03/23 13:32:45.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:32:45.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:32:45.656
  STEP: Creating pod liveness-d21fa08a-edea-460d-b9b4-fda1af37de84 in namespace container-probe-1794 @ 06/03/23 13:32:45.662
  E0603 13:32:45.920442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:46.921041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:32:47.686: INFO: Started pod liveness-d21fa08a-edea-460d-b9b4-fda1af37de84 in namespace container-probe-1794
  STEP: checking the pod's current state and verifying that restartCount is present @ 06/03/23 13:32:47.686
  Jun  3 13:32:47.691: INFO: Initial restart count of pod liveness-d21fa08a-edea-460d-b9b4-fda1af37de84 is 0
  E0603 13:32:47.921672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:48.922192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:49.923053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:50.923131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:51.923573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:52.923668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:53.924293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:54.924416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:55.925467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:56.926012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:57.926544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:58.926615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:32:59.927442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:00.927571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:01.928417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:02.928502      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:03.929298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:04.930194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:05.930276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:06.930972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:07.932076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:08.932651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:09.932744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:10.932817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:11.933554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:12.933643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:13.933797      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:14.933862      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:15.934871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:16.935137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:17.935504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:18.935605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:19.936591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:20.937082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:21.938039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:22.938731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:23.939157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:24.939302      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:25.939410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:26.940034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:27.940333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:28.940685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:29.940704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:30.940927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:31.941590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:32.941669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:33.942016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:34.942133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:35.943144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:36.944166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:37.944935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:38.944990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:39.945947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:40.946008      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:41.946123      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:42.946233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:43.946663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:44.946802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:45.947137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:46.947270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:47.948247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:48.948539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:49.949503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:50.949876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:51.950287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:52.950520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:53.951349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:54.951837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:55.951929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:56.952568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:57.953098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:58.953661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:33:59.953997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:00.954696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:01.955119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:02.955223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:03.955941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:04.956034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:05.956943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:06.957253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:07.957512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:08.957628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:09.958506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:10.958601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:11.959116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:12.959181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:13.960026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:14.960084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:15.960742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:16.960902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:17.961148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:18.961371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:19.961530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:20.961738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:21.962155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:22.962657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:23.963544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:24.966512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:25.967120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:26.967268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:27.967440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:28.967557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:29.967930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:30.968053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:31.968156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:32.968237      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:33.968499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:34.968614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:35.968945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:36.968996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:37.969740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:38.969849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:39.970570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:40.970690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:41.970898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:42.971000      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:43.971085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:44.971214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:45.971444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:46.972239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:47.972347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:48.972444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:49.972554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:50.972721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:51.973529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:52.973638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:53.973750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:54.973863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:55.973959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:56.974504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:57.974513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:58.974701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:34:59.975607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:00.975780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:01.976325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:02.976690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:03.976839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:04.977124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:05.977508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:06.978136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:07.978194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:08.978512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:09.978973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:10.979120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:11.979215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:12.979576      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:13.980159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:14.980996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:15.981978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:16.982177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:17.982527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:18.982713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:19.982836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:20.982918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:21.983616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:22.984333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:23.984512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:24.984847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:25.985441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:26.986232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:27.986353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:28.990560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:29.991594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:30.991776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:31.992429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:32.992541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:33.992644      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:34.992960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:35.993083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:36.994038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:37.994120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:38.994255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:39.994513      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:40.994646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:41.995225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:42.995368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:43.995460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:44.995595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:45.995688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:46.996140      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:47.996236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:48.996367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:49.996464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:50.997024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:51.997923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:52.998054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:53.998174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:54.998552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:55.998647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:56.999547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:57.999838      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:35:58.999980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:00.000694      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:01.001490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:02.002162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:03.002568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:04.002640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:05.002747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:06.003137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:07.003240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:08.003585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:09.004599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:10.004647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:11.005435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:12.006413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:13.006926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:14.007031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:15.007150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:16.007236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:17.008218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:18.008531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:19.008649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:20.008742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:21.008830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:22.009739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:23.009872      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:24.009967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:25.010967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:26.011087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:27.011205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:28.011294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:29.011482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:30.011663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:31.011889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:32.012643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:33.012772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:34.012884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:35.013864      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:36.014194      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:37.014330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:38.014490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:39.014612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:40.014710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:41.015444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:42.015897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:43.016849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:44.017043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:45.017765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:46.017951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:47.018598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:48.019570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:36:48.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:36:48.269
  STEP: Destroying namespace "container-probe-1794" for this suite. @ 06/03/23 13:36:48.283
• [242.665 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 06/03/23 13:36:48.291
  Jun  3 13:36:48.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 13:36:48.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:36:48.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:36:48.316
  STEP: Creating a job @ 06/03/23 13:36:48.321
  STEP: Ensuring active pods == parallelism @ 06/03/23 13:36:48.329
  E0603 13:36:49.019731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:50.019854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 06/03/23 13:36:50.334
  Jun  3 13:36:50.853: INFO: Successfully updated pod "adopt-release-cxbqs"
  STEP: Checking that the Job readopts the Pod @ 06/03/23 13:36:50.853
  E0603 13:36:51.020893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:52.021441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 06/03/23 13:36:52.863
  E0603 13:36:53.022026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:36:53.381: INFO: Successfully updated pod "adopt-release-cxbqs"
  STEP: Checking that the Job releases the Pod @ 06/03/23 13:36:53.381
  E0603 13:36:54.022525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:55.022701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:36:55.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3804" for this suite. @ 06/03/23 13:36:55.396
• [7.112 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 06/03/23 13:36:55.404
  Jun  3 13:36:55.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:36:55.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:36:55.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:36:55.429
  STEP: creating a replication controller @ 06/03/23 13:36:55.433
  Jun  3 13:36:55.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 create -f -'
  Jun  3 13:36:55.804: INFO: stderr: ""
  Jun  3 13:36:55.804: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 06/03/23 13:36:55.804
  Jun  3 13:36:55.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jun  3 13:36:55.899: INFO: stderr: ""
  Jun  3 13:36:55.899: INFO: stdout: "update-demo-nautilus-8z8x2 update-demo-nautilus-nrck7 "
  Jun  3 13:36:55.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods update-demo-nautilus-8z8x2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:36:55.983: INFO: stderr: ""
  Jun  3 13:36:55.983: INFO: stdout: ""
  Jun  3 13:36:55.983: INFO: update-demo-nautilus-8z8x2 is created but not running
  E0603 13:36:56.023151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:57.024054      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:58.024144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:36:59.024260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:00.024370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:37:00.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0603 13:37:01.025710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:37:01.072: INFO: stderr: ""
  Jun  3 13:37:01.073: INFO: stdout: "update-demo-nautilus-8z8x2 update-demo-nautilus-nrck7 "
  Jun  3 13:37:01.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods update-demo-nautilus-8z8x2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:37:01.156: INFO: stderr: ""
  Jun  3 13:37:01.157: INFO: stdout: "true"
  Jun  3 13:37:01.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods update-demo-nautilus-8z8x2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:37:01.239: INFO: stderr: ""
  Jun  3 13:37:01.239: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:37:01.239: INFO: validating pod update-demo-nautilus-8z8x2
  Jun  3 13:37:01.245: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:37:01.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:37:01.245: INFO: update-demo-nautilus-8z8x2 is verified up and running
  Jun  3 13:37:01.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods update-demo-nautilus-nrck7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jun  3 13:37:01.330: INFO: stderr: ""
  Jun  3 13:37:01.330: INFO: stdout: "true"
  Jun  3 13:37:01.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods update-demo-nautilus-nrck7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jun  3 13:37:01.417: INFO: stderr: ""
  Jun  3 13:37:01.417: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jun  3 13:37:01.417: INFO: validating pod update-demo-nautilus-nrck7
  Jun  3 13:37:01.426: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jun  3 13:37:01.426: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jun  3 13:37:01.426: INFO: update-demo-nautilus-nrck7 is verified up and running
  STEP: using delete to clean up resources @ 06/03/23 13:37:01.426
  Jun  3 13:37:01.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 delete --grace-period=0 --force -f -'
  Jun  3 13:37:01.516: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jun  3 13:37:01.516: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jun  3 13:37:01.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get rc,svc -l name=update-demo --no-headers'
  Jun  3 13:37:01.643: INFO: stderr: "No resources found in kubectl-6878 namespace.\n"
  Jun  3 13:37:01.643: INFO: stdout: ""
  Jun  3 13:37:01.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-6878 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jun  3 13:37:01.785: INFO: stderr: ""
  Jun  3 13:37:01.785: INFO: stdout: ""
  Jun  3 13:37:01.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6878" for this suite. @ 06/03/23 13:37:01.791
• [6.395 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 06/03/23 13:37:01.799
  Jun  3 13:37:01.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:37:01.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:01.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:01.829
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:37:01.834
  E0603 13:37:02.026671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:03.026734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:04.027636      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:05.027759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:37:05.864
  Jun  3 13:37:05.869: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-c0545afd-534a-4e40-b979-1eab10aa8621 container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:37:05.901
  Jun  3 13:37:05.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7173" for this suite. @ 06/03/23 13:37:05.926
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 06/03/23 13:37:05.937
  Jun  3 13:37:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-runtime @ 06/03/23 13:37:05.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:05.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:05.966
  STEP: create the container @ 06/03/23 13:37:05.973
  W0603 13:37:05.987598      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 06/03/23 13:37:05.987
  E0603 13:37:06.028569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:07.029524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:08.030499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:09.030602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 06/03/23 13:37:10.015
  STEP: the container should be terminated @ 06/03/23 13:37:10.019
  STEP: the termination message should be set @ 06/03/23 13:37:10.019
  Jun  3 13:37:10.019: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 06/03/23 13:37:10.019
  E0603 13:37:10.031264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:37:10.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6460" for this suite. @ 06/03/23 13:37:10.041
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 06/03/23 13:37:10.052
  Jun  3 13:37:10.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename endpointslice @ 06/03/23 13:37:10.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:10.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:10.075
  Jun  3 13:37:10.091: INFO: Endpoints addresses: [172.31.14.110 172.31.94.46] , ports: [6443]
  Jun  3 13:37:10.091: INFO: EndpointSlices addresses: [172.31.14.110 172.31.94.46] , ports: [6443]
  Jun  3 13:37:10.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9160" for this suite. @ 06/03/23 13:37:10.097
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 06/03/23 13:37:10.112
  Jun  3 13:37:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 13:37:10.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:10.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:10.143
  STEP: creating a Deployment @ 06/03/23 13:37:10.152
  Jun  3 13:37:10.152: INFO: Creating simple deployment test-deployment-bbd6s
  Jun  3 13:37:10.168: INFO: new replicaset for deployment "test-deployment-bbd6s" is yet to be created
  E0603 13:37:11.031970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:12.032315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 06/03/23 13:37:12.185
  Jun  3 13:37:12.191: INFO: Deployment test-deployment-bbd6s has Conditions: [{Available True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bbd6s-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 06/03/23 13:37:12.191
  Jun  3 13:37:12.203: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 37, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 3, 13, 37, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 3, 13, 37, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-bbd6s-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 06/03/23 13:37:12.203
  Jun  3 13:37:12.205: INFO: Observed &Deployment event: ADDED
  Jun  3 13:37:12.205: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bbd6s-5994cf9475"}
  Jun  3 13:37:12.206: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.206: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bbd6s-5994cf9475"}
  Jun  3 13:37:12.206: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun  3 13:37:12.206: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.206: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun  3 13:37:12.206: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bbd6s-5994cf9475" is progressing.}
  Jun  3 13:37:12.207: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.207: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun  3 13:37:12.207: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bbd6s-5994cf9475" has successfully progressed.}
  Jun  3 13:37:12.207: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.207: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun  3 13:37:12.207: INFO: Observed Deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bbd6s-5994cf9475" has successfully progressed.}
  Jun  3 13:37:12.207: INFO: Found Deployment test-deployment-bbd6s in namespace deployment-1663 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 13:37:12.207: INFO: Deployment test-deployment-bbd6s has an updated status
  STEP: patching the Statefulset Status @ 06/03/23 13:37:12.207
  Jun  3 13:37:12.208: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jun  3 13:37:12.218: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 06/03/23 13:37:12.218
  Jun  3 13:37:12.221: INFO: Observed &Deployment event: ADDED
  Jun  3 13:37:12.221: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bbd6s-5994cf9475"}
  Jun  3 13:37:12.222: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.222: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-bbd6s-5994cf9475"}
  Jun  3 13:37:12.222: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun  3 13:37:12.222: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.222: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jun  3 13:37:12.222: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:10 +0000 UTC 2023-06-03 13:37:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-bbd6s-5994cf9475" is progressing.}
  Jun  3 13:37:12.223: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.223: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun  3 13:37:12.223: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bbd6s-5994cf9475" has successfully progressed.}
  Jun  3 13:37:12.223: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.223: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jun  3 13:37:12.224: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-03 13:37:11 +0000 UTC 2023-06-03 13:37:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-bbd6s-5994cf9475" has successfully progressed.}
  Jun  3 13:37:12.224: INFO: Observed deployment test-deployment-bbd6s in namespace deployment-1663 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jun  3 13:37:12.224: INFO: Observed &Deployment event: MODIFIED
  Jun  3 13:37:12.224: INFO: Found deployment test-deployment-bbd6s in namespace deployment-1663 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jun  3 13:37:12.224: INFO: Deployment test-deployment-bbd6s has a patched status
  Jun  3 13:37:12.229: INFO: Deployment "test-deployment-bbd6s":
  &Deployment{ObjectMeta:{test-deployment-bbd6s  deployment-1663  4997eb46-2314-4b25-974f-7e19959116c2 41495 1 2023-06-03 13:37:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-03 13:37:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-03 13:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-03 13:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c25d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-bbd6s-5994cf9475",LastUpdateTime:2023-06-03 13:37:12 +0000 UTC,LastTransitionTime:2023-06-03 13:37:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jun  3 13:37:12.234: INFO: New ReplicaSet "test-deployment-bbd6s-5994cf9475" of Deployment "test-deployment-bbd6s":
  &ReplicaSet{ObjectMeta:{test-deployment-bbd6s-5994cf9475  deployment-1663  2bdd6b90-eaa3-4d0e-a868-2a33f1fe3863 41491 1 2023-06-03 13:37:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-bbd6s 4997eb46-2314-4b25-974f-7e19959116c2 0xc003c2c1f0 0xc003c2c1f1}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:37:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4997eb46-2314-4b25-974f-7e19959116c2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:37:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c2c298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jun  3 13:37:12.243: INFO: Pod "test-deployment-bbd6s-5994cf9475-74n2z" is available:
  &Pod{ObjectMeta:{test-deployment-bbd6s-5994cf9475-74n2z test-deployment-bbd6s-5994cf9475- deployment-1663  0191e689-0357-4853-a245-0c44c4e5572a 41490 0 2023-06-03 13:37:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-bbd6s-5994cf9475 2bdd6b90-eaa3-4d0e-a868-2a33f1fe3863 0xc003c2c680 0xc003c2c681}] [] [{kube-controller-manager Update v1 2023-06-03 13:37:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bdd6b90-eaa3-4d0e-a868-2a33f1fe3863\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8zdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8zdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:37:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.212,StartTime:2023-06-03 13:37:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e7fdd5b393bf59f23c8dcd8377e2b894390ff94dd3331f808ff9bc205ed46ec8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.212,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jun  3 13:37:12.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1663" for this suite. @ 06/03/23 13:37:12.249
• [2.146 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 06/03/23 13:37:12.258
  Jun  3 13:37:12.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:37:12.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:12.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:12.282
  STEP: Setting up server cert @ 06/03/23 13:37:12.321
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:37:12.725
  STEP: Deploying the webhook pod @ 06/03/23 13:37:12.734
  STEP: Wait for the deployment to be ready @ 06/03/23 13:37:12.748
  Jun  3 13:37:12.760: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:37:13.032348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:14.032460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:37:14.772
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:37:14.784
  E0603 13:37:15.032586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:37:15.784: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 06/03/23 13:37:15.787
  STEP: create a configmap that should be updated by the webhook @ 06/03/23 13:37:15.807
  Jun  3 13:37:15.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-159" for this suite. @ 06/03/23 13:37:15.888
  STEP: Destroying namespace "webhook-markers-4526" for this suite. @ 06/03/23 13:37:15.899
• [3.650 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 06/03/23 13:37:15.91
  Jun  3 13:37:15.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename subjectreview @ 06/03/23 13:37:15.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:15.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:15.933
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-1764" @ 06/03/23 13:37:15.938
  Jun  3 13:37:15.943: INFO: saUsername: "system:serviceaccount:subjectreview-1764:e2e"
  Jun  3 13:37:15.943: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-1764"}
  Jun  3 13:37:15.943: INFO: saUID: "1a2c1c61-0fc5-4213-a177-020ffd665d0e"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-1764:e2e" @ 06/03/23 13:37:15.943
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-1764:e2e" @ 06/03/23 13:37:15.944
  Jun  3 13:37:15.946: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-1764:e2e" api 'list' configmaps in "subjectreview-1764" namespace @ 06/03/23 13:37:15.946
  Jun  3 13:37:15.949: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-1764:e2e" @ 06/03/23 13:37:15.949
  Jun  3 13:37:15.953: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jun  3 13:37:15.953: INFO: LocalSubjectAccessReview has been verified
  Jun  3 13:37:15.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-1764" for this suite. @ 06/03/23 13:37:15.958
• [0.058 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 06/03/23 13:37:15.969
  Jun  3 13:37:15.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename events @ 06/03/23 13:37:15.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:15.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:15.991
  STEP: Create set of events @ 06/03/23 13:37:15.996
  STEP: get a list of Events with a label in the current namespace @ 06/03/23 13:37:16.015
  STEP: delete a list of events @ 06/03/23 13:37:16.02
  Jun  3 13:37:16.020: INFO: requesting DeleteCollection of events
  E0603 13:37:16.033509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check that the list of events matches the requested quantity @ 06/03/23 13:37:16.043
  Jun  3 13:37:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7393" for this suite. @ 06/03/23 13:37:16.052
• [0.092 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 06/03/23 13:37:16.061
  Jun  3 13:37:16.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename custom-resource-definition @ 06/03/23 13:37:16.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:16.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:16.091
  Jun  3 13:37:16.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  Jun  3 13:37:16.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9298" for this suite. @ 06/03/23 13:37:16.654
• [0.603 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 06/03/23 13:37:16.665
  Jun  3 13:37:16.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename var-expansion @ 06/03/23 13:37:16.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:37:16.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:37:16.69
  STEP: creating the pod with failed condition @ 06/03/23 13:37:16.695
  E0603 13:37:17.033807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:18.033835      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:19.034515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:20.034693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:21.035205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:22.035469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:23.036392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:24.036707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:25.037402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:26.037718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:27.037985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:28.038499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:29.039106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:30.039244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:31.039671      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:32.040405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:33.040525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:34.040807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:35.041519      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:36.041639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:37.042018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:38.042153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:39.042205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:40.042626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:41.043564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:42.043609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:43.044482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:44.044583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:45.045407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:46.045962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:47.046474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:48.046562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:49.047527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:50.047645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:51.048159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:52.048273      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:53.048958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:54.049078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:55.049993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:56.050331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:57.050486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:58.050629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:37:59.051270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:00.051335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:01.051675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:02.052417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:03.053366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:04.053508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:05.054349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:06.054557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:07.055016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:08.055150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:09.056006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:10.056539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:11.057347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:12.057434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:13.057523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:14.057589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:15.057939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:16.058067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:17.058609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:18.058732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:19.058840      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:20.059060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:21.059396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:22.059894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:23.060341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:24.060649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:25.061206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:26.060939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:27.061660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:28.062219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:29.062827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:30.063642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:31.063993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:32.064723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:33.065329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:34.065622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:35.066523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:36.067574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:37.068146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:38.068623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:39.068734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:40.068855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:41.068942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:42.069425      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:43.070475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:44.070525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:45.071586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:46.071938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:47.072391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:48.072464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:49.072770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:50.072888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:51.073768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:52.074051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:53.074861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:54.075580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:55.075852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:56.076157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:57.076220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:58.076548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:38:59.077190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:00.077471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:01.078323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:02.078769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:03.079157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:04.079282      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:05.079658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:06.079944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:07.080464      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:08.080556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:09.081493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:10.081587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:11.082178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:12.082279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:13.082756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:14.082881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:15.083472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:16.083583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 06/03/23 13:39:16.707
  E0603 13:39:17.083687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:17.222: INFO: Successfully updated pod "var-expansion-9784409f-c20f-4bb6-a79e-83241d61682a"
  STEP: waiting for pod running @ 06/03/23 13:39:17.222
  E0603 13:39:18.084291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:19.084463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 06/03/23 13:39:19.23
  Jun  3 13:39:19.230: INFO: Deleting pod "var-expansion-9784409f-c20f-4bb6-a79e-83241d61682a" in namespace "var-expansion-2778"
  Jun  3 13:39:19.239: INFO: Wait up to 5m0s for pod "var-expansion-9784409f-c20f-4bb6-a79e-83241d61682a" to be fully deleted
  E0603 13:39:20.084551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:21.084645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:22.085646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:23.086016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:24.086511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:25.086673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:26.087625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:27.088641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:28.088751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:29.088915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:30.089012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:31.089117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:32.090051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:33.090613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:34.090792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:35.090925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:36.091084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:37.091203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:38.091305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:39.091738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:40.091839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:41.092553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:42.093174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:43.093483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:44.094403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:45.094555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:46.095639      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:47.096225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:48.096416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:49.096500      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:50.096626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:51.096944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:51.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2778" for this suite. @ 06/03/23 13:39:51.33
• [154.672 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 06/03/23 13:39:51.34
  Jun  3 13:39:51.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename daemonsets @ 06/03/23 13:39:51.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:39:51.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:39:51.362
  STEP: Creating simple DaemonSet "daemon-set" @ 06/03/23 13:39:51.39
  STEP: Check that daemon pods launch on every node of the cluster. @ 06/03/23 13:39:51.396
  Jun  3 13:39:51.401: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:51.402: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:51.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:39:51.406: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:39:52.097486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:52.412: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:52.412: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:52.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:39:52.416: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:39:53.098057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:53.411: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:53.411: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:53.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:39:53.415: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 06/03/23 13:39:53.422
  Jun  3 13:39:53.441: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:53.441: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:53.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:39:53.446: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:39:54.098550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:54.452: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:54.452: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:54.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:39:54.456: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:39:55.099606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:55.451: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:55.451: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:55.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jun  3 13:39:55.455: INFO: Node ip-172-31-27-193 is running 0 daemon pod, expected 1
  E0603 13:39:56.100650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:56.453: INFO: DaemonSet pods can't tolerate node ip-172-31-14-110 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:56.453: INFO: DaemonSet pods can't tolerate node ip-172-31-94-46 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jun  3 13:39:56.459: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jun  3 13:39:56.459: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 06/03/23 13:39:56.463
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9754, will wait for the garbage collector to delete the pods @ 06/03/23 13:39:56.463
  Jun  3 13:39:56.525: INFO: Deleting DaemonSet.extensions daemon-set took: 7.227736ms
  Jun  3 13:39:56.627: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.340512ms
  E0603 13:39:57.100890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:39:58.101802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:39:58.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jun  3 13:39:58.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jun  3 13:39:58.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42167"},"items":null}

  Jun  3 13:39:58.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42167"},"items":null}

  Jun  3 13:39:58.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9754" for this suite. @ 06/03/23 13:39:58.164
• [6.838 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 06/03/23 13:39:58.181
  Jun  3 13:39:58.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 13:39:58.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:39:58.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:39:58.205
  STEP: Creating service test in namespace statefulset-4155 @ 06/03/23 13:39:58.209
  STEP: Looking for a node to schedule stateful set and pod @ 06/03/23 13:39:58.217
  STEP: Creating pod with conflicting port in namespace statefulset-4155 @ 06/03/23 13:39:58.227
  STEP: Waiting until pod test-pod will start running in namespace statefulset-4155 @ 06/03/23 13:39:58.239
  E0603 13:39:59.102055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:00.102138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-4155 @ 06/03/23 13:40:00.249
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4155 @ 06/03/23 13:40:00.256
  Jun  3 13:40:00.269: INFO: Observed stateful pod in namespace: statefulset-4155, name: ss-0, uid: a2a9ca74-03e3-4e87-b6ea-022427b0b1e9, status phase: Pending. Waiting for statefulset controller to delete.
  Jun  3 13:40:00.293: INFO: Observed stateful pod in namespace: statefulset-4155, name: ss-0, uid: a2a9ca74-03e3-4e87-b6ea-022427b0b1e9, status phase: Failed. Waiting for statefulset controller to delete.
  Jun  3 13:40:00.302: INFO: Observed stateful pod in namespace: statefulset-4155, name: ss-0, uid: a2a9ca74-03e3-4e87-b6ea-022427b0b1e9, status phase: Failed. Waiting for statefulset controller to delete.
  Jun  3 13:40:00.306: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4155
  STEP: Removing pod with conflicting port in namespace statefulset-4155 @ 06/03/23 13:40:00.306
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4155 and will be in running state @ 06/03/23 13:40:00.323
  E0603 13:40:01.102557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:02.103532      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:02.336: INFO: Deleting all statefulset in ns statefulset-4155
  Jun  3 13:40:02.340: INFO: Scaling statefulset ss to 0
  E0603 13:40:03.103691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:04.103809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:05.104001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:06.104174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:07.104292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:08.104622      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:09.104621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:10.104779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:11.104901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:12.105458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:12.366: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:40:12.370: INFO: Deleting statefulset ss
  Jun  3 13:40:12.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4155" for this suite. @ 06/03/23 13:40:12.391
• [14.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 06/03/23 13:40:12.409
  Jun  3 13:40:12.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename podtemplate @ 06/03/23 13:40:12.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:40:12.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:40:12.432
  STEP: Create set of pod templates @ 06/03/23 13:40:12.436
  Jun  3 13:40:12.441: INFO: created test-podtemplate-1
  Jun  3 13:40:12.453: INFO: created test-podtemplate-2
  Jun  3 13:40:12.458: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 06/03/23 13:40:12.459
  STEP: delete collection of pod templates @ 06/03/23 13:40:12.462
  Jun  3 13:40:12.462: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 06/03/23 13:40:12.48
  Jun  3 13:40:12.481: INFO: requesting list of pod templates to confirm quantity
  Jun  3 13:40:12.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1986" for this suite. @ 06/03/23 13:40:12.491
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 06/03/23 13:40:12.503
  Jun  3 13:40:12.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 13:40:12.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:40:12.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:40:12.53
  STEP: Creating configMap with name configmap-test-volume-b2079718-f1d8-4268-9361-3692ef282176 @ 06/03/23 13:40:12.533
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:40:12.54
  E0603 13:40:13.105546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:14.105708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:15.106501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:16.106673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:40:16.563
  Jun  3 13:40:16.567: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-1283b9ee-e0b2-47aa-9f55-8b70143a6049 container agnhost-container: <nil>
  STEP: delete the pod @ 06/03/23 13:40:16.587
  Jun  3 13:40:16.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6160" for this suite. @ 06/03/23 13:40:16.607
• [4.113 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 06/03/23 13:40:16.617
  Jun  3 13:40:16.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename statefulset @ 06/03/23 13:40:16.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:40:16.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:40:16.643
  STEP: Creating service test in namespace statefulset-8709 @ 06/03/23 13:40:16.646
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 06/03/23 13:40:16.654
  STEP: Creating stateful set ss in namespace statefulset-8709 @ 06/03/23 13:40:16.659
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8709 @ 06/03/23 13:40:16.665
  Jun  3 13:40:16.673: INFO: Found 0 stateful pods, waiting for 1
  E0603 13:40:17.106761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:18.106913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:19.107600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:20.107749      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:21.107834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:22.108364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:23.108520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:24.108894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:25.109274      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:26.109523      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:26.680: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 06/03/23 13:40:26.68
  Jun  3 13:40:26.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 13:40:26.852: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 13:40:26.852: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 13:40:26.852: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 13:40:26.856: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0603 13:40:27.110590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:28.110897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:29.111746      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:30.111853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:31.111968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:32.112851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:33.112961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:34.113072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:35.113192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:36.113492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:36.863: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 13:40:36.863: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:40:36.882: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999967s
  E0603 13:40:37.113748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:37.886: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995644681s
  E0603 13:40:38.114314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:38.890: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990805677s
  E0603 13:40:39.115122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:39.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986582913s
  E0603 13:40:40.115231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:40.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982584556s
  E0603 13:40:41.115788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:41.904: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977962138s
  E0603 13:40:42.116510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:42.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973795256s
  E0603 13:40:43.116536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:43.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969592638s
  E0603 13:40:44.117051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:44.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965230951s
  E0603 13:40:45.118025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:45.920: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.413342ms
  E0603 13:40:46.118686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8709 @ 06/03/23 13:40:46.92
  Jun  3 13:40:46.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 13:40:47.081: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 13:40:47.081: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 13:40:47.081: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 13:40:47.086: INFO: Found 1 stateful pods, waiting for 3
  E0603 13:40:47.119337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:48.119758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:49.120492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:50.120489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:51.120736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:52.121491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:53.122098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:54.122212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:55.122456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:56.122652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:57.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:40:57.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jun  3 13:40:57.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 06/03/23 13:40:57.092
  STEP: Scale down will halt with unhealthy stateful pod @ 06/03/23 13:40:57.092
  Jun  3 13:40:57.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0603 13:40:57.123805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:40:57.246: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 13:40:57.246: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 13:40:57.246: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 13:40:57.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 13:40:57.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 13:40:57.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 13:40:57.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 13:40:57.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jun  3 13:40:57.588: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jun  3 13:40:57.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jun  3 13:40:57.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jun  3 13:40:57.588: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:40:57.592: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0603 13:40:58.124656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:40:59.124789      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:00.124868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:01.124975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:02.125541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:03.125762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:04.126030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:05.126226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:06.126496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:07.127038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:07.601: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 13:41:07.601: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 13:41:07.601: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jun  3 13:41:07.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999967s
  E0603 13:41:08.127403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:08.623: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994671283s
  E0603 13:41:09.127698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:09.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990068471s
  E0603 13:41:10.127579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:10.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985095307s
  E0603 13:41:11.128453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:11.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980503796s
  E0603 13:41:12.128571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:12.642: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975116282s
  E0603 13:41:13.128660      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:13.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970031388s
  E0603 13:41:14.128778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:14.654: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964936633s
  E0603 13:41:15.129548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:15.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959285983s
  E0603 13:41:16.129643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:16.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.113469ms
  E0603 13:41:17.129708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8709 @ 06/03/23 13:41:17.663
  Jun  3 13:41:17.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 13:41:17.833: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 13:41:17.833: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 13:41:17.833: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 13:41:17.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jun  3 13:41:17.996: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 13:41:17.996: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 13:41:17.996: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 13:41:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=statefulset-8709 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0603 13:41:18.130585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:41:18.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jun  3 13:41:18.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jun  3 13:41:18.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jun  3 13:41:18.164: INFO: Scaling statefulset ss to 0
  E0603 13:41:19.130944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:20.131863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:21.132706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:22.133411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:23.133589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:24.133717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:25.133990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:26.134059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:27.134630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:28.134733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 06/03/23 13:41:28.184
  Jun  3 13:41:28.184: INFO: Deleting all statefulset in ns statefulset-8709
  Jun  3 13:41:28.188: INFO: Scaling statefulset ss to 0
  Jun  3 13:41:28.201: INFO: Waiting for statefulset status.replicas updated to 0
  Jun  3 13:41:28.205: INFO: Deleting statefulset ss
  Jun  3 13:41:28.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8709" for this suite. @ 06/03/23 13:41:28.225
• [71.617 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 06/03/23 13:41:28.237
  Jun  3 13:41:28.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:41:28.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:41:28.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:41:28.259
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 06/03/23 13:41:28.264
  E0603 13:41:29.135675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:30.136224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:31.136401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:32.136478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:41:32.287
  Jun  3 13:41:32.290: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-87166554-7531-4fea-8271-6a2d1a8327c5 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:41:32.298
  Jun  3 13:41:32.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6303" for this suite. @ 06/03/23 13:41:32.318
• [4.089 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 06/03/23 13:41:32.327
  Jun  3 13:41:32.327: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:41:32.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:41:32.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:41:32.351
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 06/03/23 13:41:32.354
  E0603 13:41:33.136941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:34.137059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:35.137218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:36.138146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:41:36.377
  Jun  3 13:41:36.381: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-1c41ed77-ca62-4e53-a4f5-90615876eaf1 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:41:36.388
  Jun  3 13:41:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6904" for this suite. @ 06/03/23 13:41:36.408
• [4.088 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 06/03/23 13:41:36.416
  Jun  3 13:41:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename projected @ 06/03/23 13:41:36.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:41:36.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:41:36.437
  STEP: Creating a pod to test downward API volume plugin @ 06/03/23 13:41:36.44
  E0603 13:41:37.138226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:38.138553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:39.138915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:40.138954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:41:40.469
  Jun  3 13:41:40.473: INFO: Trying to get logs from node ip-172-31-27-193 pod downwardapi-volume-af86ae9e-13f5-4a5c-aced-842fdea166fe container client-container: <nil>
  STEP: delete the pod @ 06/03/23 13:41:40.48
  Jun  3 13:41:40.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2400" for this suite. @ 06/03/23 13:41:40.5
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 06/03/23 13:41:40.518
  Jun  3 13:41:40.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename resourcequota @ 06/03/23 13:41:40.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:41:40.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:41:40.54
  E0603 13:41:41.139105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:42.139983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:43.140775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:44.141507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:45.142316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:46.142621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:47.143488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:48.143607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:49.143716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:50.144530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:51.144649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:52.144815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:53.144883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:54.145011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:55.145172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:56.145980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:57.146098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 06/03/23 13:41:57.547
  E0603 13:41:58.146691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:41:59.147210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:00.148209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:01.149166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:02.149526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 06/03/23 13:42:02.557
  STEP: Ensuring resource quota status is calculated @ 06/03/23 13:42:02.563
  E0603 13:42:03.149610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:04.149738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 06/03/23 13:42:04.567
  STEP: Ensuring resource quota status captures configMap creation @ 06/03/23 13:42:04.579
  E0603 13:42:05.150616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:06.150841      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 06/03/23 13:42:06.585
  STEP: Ensuring resource quota status released usage @ 06/03/23 13:42:06.592
  E0603 13:42:07.151343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:08.151483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:08.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4070" for this suite. @ 06/03/23 13:42:08.604
• [28.093 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 06/03/23 13:42:08.611
  Jun  3 13:42:08.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename pods @ 06/03/23 13:42:08.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:08.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:08.634
  STEP: Create a pod @ 06/03/23 13:42:08.643
  E0603 13:42:09.151597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:10.151743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 06/03/23 13:42:10.662
  Jun  3 13:42:10.671: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jun  3 13:42:10.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1152" for this suite. @ 06/03/23 13:42:10.677
• [2.074 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 06/03/23 13:42:10.686
  Jun  3 13:42:10.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename gc @ 06/03/23 13:42:10.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:10.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:10.71
  Jun  3 13:42:10.749: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3e906704-7312-4f9e-a4a1-e418c69cbd43", Controller:(*bool)(0xc005451086), BlockOwnerDeletion:(*bool)(0xc005451087)}}
  Jun  3 13:42:10.759: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"05c4d843-5543-49c4-82ac-acc2c29669a1", Controller:(*bool)(0xc0054512ba), BlockOwnerDeletion:(*bool)(0xc0054512bb)}}
  Jun  3 13:42:10.770: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5ba05ceb-eb78-4c42-82d3-c4c9d68fbbff", Controller:(*bool)(0xc00553658e), BlockOwnerDeletion:(*bool)(0xc00553658f)}}
  E0603 13:42:11.152272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:12.152315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:13.152508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:14.152599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:15.152656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8319" for this suite. @ 06/03/23 13:42:15.789
• [5.111 seconds]
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 06/03/23 13:42:15.798
  Jun  3 13:42:15.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename dns @ 06/03/23 13:42:15.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:15.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:15.821
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/03/23 13:42:15.825
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 06/03/23 13:42:15.825
  STEP: creating a pod to probe DNS @ 06/03/23 13:42:15.825
  STEP: submitting the pod to kubernetes @ 06/03/23 13:42:15.825
  E0603 13:42:16.153544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:17.154490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 06/03/23 13:42:17.849
  STEP: looking for the results for each expected name from probers @ 06/03/23 13:42:17.854
  Jun  3 13:42:17.873: INFO: DNS probes using dns-5256/dns-test-f45d76de-0b8c-47a8-8f61-e410d50c2b7f succeeded

  Jun  3 13:42:17.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 06/03/23 13:42:17.878
  STEP: Destroying namespace "dns-5256" for this suite. @ 06/03/23 13:42:17.89
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 06/03/23 13:42:17.903
  Jun  3 13:42:17.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename secrets @ 06/03/23 13:42:17.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:17.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:17.924
  STEP: Creating secret with name secret-test-6ff32827-0d93-4991-99be-d2b947268b76 @ 06/03/23 13:42:17.929
  STEP: Creating a pod to test consume secrets @ 06/03/23 13:42:17.934
  E0603 13:42:18.154836      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:19.154934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:20.154985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:21.155377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:42:21.962
  Jun  3 13:42:21.966: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-secrets-f22f822a-6eff-4367-acd8-9e88cade8966 container secret-env-test: <nil>
  STEP: delete the pod @ 06/03/23 13:42:21.978
  Jun  3 13:42:21.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2996" for this suite. @ 06/03/23 13:42:21.996
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 06/03/23 13:42:22.004
  Jun  3 13:42:22.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename emptydir @ 06/03/23 13:42:22.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:22.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:22.027
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 06/03/23 13:42:22.031
  E0603 13:42:22.155409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:23.155926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:24.156013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:25.156434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:42:26.057
  Jun  3 13:42:26.062: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-a41afca5-6b29-4461-a872-01cf9ec23323 container test-container: <nil>
  STEP: delete the pod @ 06/03/23 13:42:26.069
  Jun  3 13:42:26.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5157" for this suite. @ 06/03/23 13:42:26.088
• [4.091 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 06/03/23 13:42:26.097
  Jun  3 13:42:26.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 06/03/23 13:42:26.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:26.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:26.124
  STEP: create the container to handle the HTTPGet hook request. @ 06/03/23 13:42:26.133
  E0603 13:42:26.156553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:27.157542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:28.157725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 06/03/23 13:42:28.161
  E0603 13:42:29.157890      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:30.158055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 06/03/23 13:42:30.185
  E0603 13:42:31.158137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:32.158259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 06/03/23 13:42:32.202
  Jun  3 13:42:32.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3803" for this suite. @ 06/03/23 13:42:32.215
• [6.124 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 06/03/23 13:42:32.223
  Jun  3 13:42:32.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:42:32.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:32.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:32.244
  Jun  3 13:42:32.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4615 version'
  Jun  3 13:42:32.326: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jun  3 13:42:32.326: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:20:07Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.2\", GitCommit:\"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647\", GitTreeState:\"clean\", BuildDate:\"2023-05-18T02:06:41Z\", GoVersion:\"go1.20.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jun  3 13:42:32.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4615" for this suite. @ 06/03/23 13:42:32.331
• [0.116 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 06/03/23 13:42:32.34
  Jun  3 13:42:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename deployment @ 06/03/23 13:42:32.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:32.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:32.36
  STEP: creating a Deployment @ 06/03/23 13:42:32.368
  STEP: waiting for Deployment to be created @ 06/03/23 13:42:32.374
  STEP: waiting for all Replicas to be Ready @ 06/03/23 13:42:32.376
  Jun  3 13:42:32.378: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.378: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.389: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.389: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.412: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.412: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.442: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jun  3 13:42:32.442: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0603 13:42:33.158310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:33.350: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun  3 13:42:33.350: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jun  3 13:42:33.436: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 06/03/23 13:42:33.436
  W0603 13:42:33.444971      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jun  3 13:42:33.448: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 06/03/23 13:42:33.448
  Jun  3 13:42:33.450: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.451: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 0
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.452: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.462: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.462: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.481: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.481: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:33.499: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:33.499: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:33.511: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:33.511: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  E0603 13:42:34.158601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:34.372: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:34.372: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:34.405: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  STEP: listing Deployments @ 06/03/23 13:42:34.405
  Jun  3 13:42:34.411: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 06/03/23 13:42:34.411
  Jun  3 13:42:34.428: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 06/03/23 13:42:34.429
  Jun  3 13:42:34.443: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:34.443: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:34.470: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:34.490: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:34.498: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0603 13:42:35.158759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:35.368: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:35.391: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:35.405: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:35.418: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jun  3 13:42:35.433: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0603 13:42:36.159690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:36.498: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 06/03/23 13:42:36.526
  STEP: fetching the DeploymentStatus @ 06/03/23 13:42:36.535
  Jun  3 13:42:36.543: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:36.544: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:36.544: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:36.544: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:36.544: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 1
  Jun  3 13:42:36.544: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:36.545: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:36.545: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:36.546: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:36.546: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 2
  Jun  3 13:42:36.546: INFO: observed Deployment test-deployment in namespace deployment-1996 with ReadyReplicas 3
  STEP: deleting the Deployment @ 06/03/23 13:42:36.546
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.560: INFO: observed event type MODIFIED
  Jun  3 13:42:36.561: INFO: observed event type MODIFIED
  Jun  3 13:42:36.561: INFO: observed event type MODIFIED
  Jun  3 13:42:36.561: INFO: observed event type MODIFIED
  Jun  3 13:42:36.561: INFO: observed event type MODIFIED
  Jun  3 13:42:36.561: INFO: observed event type MODIFIED
  Jun  3 13:42:36.566: INFO: Log out all the ReplicaSets if there is no deployment created
  Jun  3 13:42:36.572: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-1996  ef8a7e82-f120-4e3f-bf6d-d0a2b39e72ab 43346 3 2023-06-03 13:42:32 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment db8b2cda-c5d1-4b92-ad6f-22325b57fc78 0xc00489d4e7 0xc00489d4e8}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db8b2cda-c5d1-4b92-ad6f-22325b57fc78\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:42:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489d570 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun  3 13:42:36.580: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-1996  e25746e0-9211-4c64-bc3c-4f28d0e1b5e1 43448 4 2023-06-03 13:42:33 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment db8b2cda-c5d1-4b92-ad6f-22325b57fc78 0xc00489d5e7 0xc00489d5e8}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:42:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db8b2cda-c5d1-4b92-ad6f-22325b57fc78\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:42:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489d670 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jun  3 13:42:36.588: INFO: pod: "test-deployment-5b5dcbcd95-cpw8z":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-cpw8z test-deployment-5b5dcbcd95- deployment-1996  888f64ab-3691-4fe8-bcd5-32a77fac673c 43443 0 2023-06-03 13:42:33 +0000 UTC 2023-06-03 13:42:37 +0000 UTC 0xc005dca078 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 e25746e0-9211-4c64-bc3c-4f28d0e1b5e1 0xc005dca0a7 0xc005dca0a8}] [] [{kube-controller-manager Update v1 2023-06-03 13:42:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e25746e0-9211-4c64-bc3c-4f28d0e1b5e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:42:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqc77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqc77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.115,StartTime:2023-06-03 13:42:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:42:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://f3d6d4e618d228e139c27b71acb93c2a3ad111c37e245b0b55d38db04d4abb5e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.115,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun  3 13:42:36.588: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-1996  78579043-2ba6-4462-aaa5-7eea6077e9df 43440 2 2023-06-03 13:42:34 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment db8b2cda-c5d1-4b92-ad6f-22325b57fc78 0xc00489d6d7 0xc00489d6d8}] [] [{kube-controller-manager Update apps/v1 2023-06-03 13:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db8b2cda-c5d1-4b92-ad6f-22325b57fc78\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-03 13:42:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489d760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jun  3 13:42:36.597: INFO: pod: "test-deployment-6fc78d85c6-snc97":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-snc97 test-deployment-6fc78d85c6- deployment-1996  7a879582-d4ec-440b-810c-465ef65ab338 43454 0 2023-06-03 13:42:35 +0000 UTC 2023-06-03 13:42:37 +0000 UTC 0xc005dcac88 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 78579043-2ba6-4462-aaa5-7eea6077e9df 0xc005dcacb7 0xc005dcacb8}] [] [{kube-controller-manager Update v1 2023-06-03 13:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78579043-2ba6-4462-aaa5-7eea6077e9df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:42:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.118.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tx4ft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tx4ft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-27-193,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.27.193,PodIP:192.168.118.241,StartTime:2023-06-03 13:42:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:42:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5166f98029159cb8ac2ce4ac307be3a8a987919ab3fbf981008c66e705677039,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.118.241,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun  3 13:42:36.597: INFO: pod: "test-deployment-6fc78d85c6-zd2lk":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-zd2lk test-deployment-6fc78d85c6- deployment-1996  48f5f7bd-791d-4dc2-ba3d-104b3684fa20 43396 0 2023-06-03 13:42:34 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 78579043-2ba6-4462-aaa5-7eea6077e9df 0xc005dcb177 0xc005dcb178}] [] [{kube-controller-manager Update v1 2023-06-03 13:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78579043-2ba6-4462-aaa5-7eea6077e9df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-03 13:42:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.20.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrm6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrm6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-85-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-03 13:42:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.85.85,PodIP:192.168.20.113,StartTime:2023-06-03 13:42:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-03 13:42:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2720e4948445ef9dfac85318b7f8bedc3afe99730e22d586de56c1c5146856b6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.20.113,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jun  3 13:42:36.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1996" for this suite. @ 06/03/23 13:42:36.605
• [4.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 06/03/23 13:42:36.616
  Jun  3 13:42:36.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename job @ 06/03/23 13:42:36.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:36.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:36.64
  STEP: Creating a suspended job @ 06/03/23 13:42:36.652
  STEP: Patching the Job @ 06/03/23 13:42:36.658
  STEP: Watching for Job to be patched @ 06/03/23 13:42:36.669
  Jun  3 13:42:36.672: INFO: Event ADDED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jun  3 13:42:36.673: INFO: Event MODIFIED found for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 06/03/23 13:42:36.673
  STEP: Watching for Job to be updated @ 06/03/23 13:42:36.714
  Jun  3 13:42:36.716: INFO: Event MODIFIED found for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:36.716: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 06/03/23 13:42:36.717
  Jun  3 13:42:36.721: INFO: Job: e2e-g5j2b as labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b]
  STEP: Waiting for job to complete @ 06/03/23 13:42:36.721
  E0603 13:42:37.160511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:38.160606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:39.160733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:40.160883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:41.161567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:42.162073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:43.162306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:44.162509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:45.163255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:46.163370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 06/03/23 13:42:46.725
  STEP: Watching for Job to be deleted @ 06/03/23 13:42:46.734
  Jun  3 13:42:46.737: INFO: Event MODIFIED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:46.737: INFO: Event MODIFIED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:46.737: INFO: Event MODIFIED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:46.737: INFO: Event MODIFIED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:46.737: INFO: Event MODIFIED observed for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jun  3 13:42:46.738: INFO: Event DELETED found for Job e2e-g5j2b in namespace job-6002 with labels: map[e2e-g5j2b:patched e2e-job-label:e2e-g5j2b] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 06/03/23 13:42:46.738
  Jun  3 13:42:46.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6002" for this suite. @ 06/03/23 13:42:46.758
• [10.153 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 06/03/23 13:42:46.771
  Jun  3 13:42:46.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename configmap @ 06/03/23 13:42:46.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:46.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:46.795
  STEP: Creating configMap with name configmap-test-volume-cd18ab75-e248-4070-a7b2-89028f8b2ce2 @ 06/03/23 13:42:46.8
  STEP: Creating a pod to test consume configMaps @ 06/03/23 13:42:46.808
  E0603 13:42:47.163819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:48.163991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:49.164028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:50.164142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 06/03/23 13:42:50.833
  Jun  3 13:42:50.838: INFO: Trying to get logs from node ip-172-31-27-193 pod pod-configmaps-329bad88-7aa6-421f-880a-147398336f71 container configmap-volume-test: <nil>
  STEP: delete the pod @ 06/03/23 13:42:50.845
  Jun  3 13:42:50.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5274" for this suite. @ 06/03/23 13:42:50.866
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 06/03/23 13:42:50.876
  Jun  3 13:42:50.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename webhook @ 06/03/23 13:42:50.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:50.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:50.899
  STEP: Setting up server cert @ 06/03/23 13:42:50.927
  E0603 13:42:51.164881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 06/03/23 13:42:51.445
  STEP: Deploying the webhook pod @ 06/03/23 13:42:51.454
  STEP: Wait for the deployment to be ready @ 06/03/23 13:42:51.469
  Jun  3 13:42:51.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0603 13:42:52.165442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0603 13:42:53.165558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 06/03/23 13:42:53.491
  STEP: Verifying the service has paired with the endpoint @ 06/03/23 13:42:53.51
  E0603 13:42:54.165860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:54.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 06/03/23 13:42:54.514
  Jun  3 13:42:54.531: INFO: Waiting for webhook configuration to be ready...
  STEP: create a namespace for the webhook @ 06/03/23 13:42:54.644
  STEP: create a configmap should be unconditionally rejected by the webhook @ 06/03/23 13:42:54.661
  Jun  3 13:42:54.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7392" for this suite. @ 06/03/23 13:42:54.755
  STEP: Destroying namespace "webhook-markers-1873" for this suite. @ 06/03/23 13:42:54.77
  STEP: Destroying namespace "fail-closed-namespace-4928" for this suite. @ 06/03/23 13:42:54.779
• [3.918 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 06/03/23 13:42:54.796
  Jun  3 13:42:54.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1699954907
  STEP: Building a namespace api object, basename kubectl @ 06/03/23 13:42:54.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 06/03/23 13:42:54.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 06/03/23 13:42:54.818
  Jun  3 13:42:54.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 create -f -'
  Jun  3 13:42:55.154: INFO: stderr: ""
  Jun  3 13:42:55.154: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jun  3 13:42:55.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 create -f -'
  E0603 13:42:55.166902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:55.485: INFO: stderr: ""
  Jun  3 13:42:55.485: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 06/03/23 13:42:55.485
  E0603 13:42:56.166993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:56.490: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:42:56.490: INFO: Found 0 / 1
  E0603 13:42:57.167266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jun  3 13:42:57.491: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:42:57.491: INFO: Found 1 / 1
  Jun  3 13:42:57.491: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jun  3 13:42:57.494: INFO: Selector matched 1 pods for map[app:agnhost]
  Jun  3 13:42:57.494: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jun  3 13:42:57.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 describe pod agnhost-primary-zh4g2'
  Jun  3 13:42:57.588: INFO: stderr: ""
  Jun  3 13:42:57.588: INFO: stdout: "Name:             agnhost-primary-zh4g2\nNamespace:        kubectl-4436\nPriority:         0\nService Account:  default\nNode:             ip-172-31-27-193/172.31.27.193\nStart Time:       Sat, 03 Jun 2023 13:42:55 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.118.239\nIPs:\n  IP:           192.168.118.239\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://97838927c4a5c96c0a1b6ecd424411ddbb506adebbd2574152d907c7b370fea6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 03 Jun 2023 13:42:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txjwd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-txjwd:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4436/agnhost-primary-zh4g2 to ip-172-31-27-193\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jun  3 13:42:57.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 describe rc agnhost-primary'
  Jun  3 13:42:57.686: INFO: stderr: ""
  Jun  3 13:42:57.686: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4436\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zh4g2\n"
  Jun  3 13:42:57.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 describe service agnhost-primary'
  Jun  3 13:42:57.781: INFO: stderr: ""
  Jun  3 13:42:57.781: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4436\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.198\nIPs:               10.152.183.198\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.118.239:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jun  3 13:42:57.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 describe node ip-172-31-14-110'
  Jun  3 13:42:57.896: INFO: stderr: ""
  Jun  3 13:42:57.896: INFO: stdout: "Name:               ip-172-31-14-110\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-control-plane\n                    juju-charm=kubernetes-control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-14-110\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 03 Jun 2023 11:54:42 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-14-110\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 03 Jun 2023 13:42:55 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 03 Jun 2023 13:37:54 +0000   Sat, 03 Jun 2023 11:56:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 03 Jun 2023 13:37:54 +0000   Sat, 03 Jun 2023 11:56:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 03 Jun 2023 13:37:54 +0000   Sat, 03 Jun 2023 11:56:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 03 Jun 2023 13:37:54 +0000   Sat, 03 Jun 2023 11:56:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.14.110\n  Hostname:    ip-172-31-14-110\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16127544Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16025144Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2a4804a94baa6edd3f7214422e3941\n  System UUID:                ec2a4804-a94b-aa6e-dd3f-7214422e3941\n  Boot ID:                    778e216f-f34b-4e0e-bc21-deec5fc6ed37\n  Kernel Version:             5.19.0-1026-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.2\n  Kube-Proxy Version:         v1.27.2\nNon-terminated Pods:          (1 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9086805e944f4091-cdftx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:              <none>\n"
  Jun  3 13:42:57.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1699954907 --namespace=kubectl-4436 describe namespace kubectl-4436'
  Jun  3 13:42:57.996: INFO: stderr: ""
  Jun  3 13:42:57.996: INFO: stdout: "Name:         kubectl-4436\nLabels:       e2e-framework=kubectl\n              e2e-run=890fff61-bdc4-4aa7-8c13-2e2931076cd1\n              kubernetes.io/metadata.name=kubectl-4436\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jun  3 13:42:57.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4436" for this suite. @ 06/03/23 13:42:58.001
• [3.212 seconds]
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jun  3 13:42:58.009: INFO: Running AfterSuite actions on node 1
  Jun  3 13:42:58.009: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.066 seconds]
------------------------------

Ran 378 of 7207 Specs in 6140.791 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h42m21.381172885s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

